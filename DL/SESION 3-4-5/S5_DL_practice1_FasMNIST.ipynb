{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mioti.png\" style=\"height: 100px\">\n",
    "<center style=\"color:#888\">Módulo Data Science in IoT<br/>Asignatura Deep Learning</center>\n",
    "\n",
    "# S5 Practice 1: Fashion MNIST. DNNs en Keras\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "El objetivo de este notebook es optimizar una DNN capaz de distinguir entre imágenes de prendas de ropa de la base de datos Fasion MNIST.\n",
    "\n",
    "## Punto de partida\n",
    "\n",
    "El punto de partida se corresponde con el código que hemos visto en el worksheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:19:00.941427Z",
     "start_time": "2021-10-21T16:19:00.932286Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:19:51.542187Z",
     "start_time": "2021-10-21T16:19:00.948367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 5s 10ms/step - loss: 5.8715 - accuracy: 0.7641 - val_loss: 0.9983 - val_accuracy: 0.7760\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.6885 - accuracy: 0.8131 - val_loss: 0.6570 - val_accuracy: 0.8069\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.5143 - accuracy: 0.8342 - val_loss: 0.5405 - val_accuracy: 0.8342\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4723 - accuracy: 0.8443 - val_loss: 0.5549 - val_accuracy: 0.8403\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4384 - accuracy: 0.8509 - val_loss: 0.5578 - val_accuracy: 0.8266\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4131 - accuracy: 0.8576 - val_loss: 0.5396 - val_accuracy: 0.8393\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4285 - accuracy: 0.8526 - val_loss: 0.5275 - val_accuracy: 0.8382\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.4114 - accuracy: 0.8618 - val_loss: 0.4934 - val_accuracy: 0.8475\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 4s 11ms/step - loss: 0.3966 - accuracy: 0.8610 - val_loss: 0.5062 - val_accuracy: 0.8402\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 4s 10ms/step - loss: 0.3851 - accuracy: 0.8648 - val_loss: 0.4717 - val_accuracy: 0.8483\n",
      "[0.5125571489334106, 0.8346999883651733]\n"
     ]
    }
   ],
   "source": [
    "#%tensorflow_version 2.x  # sólo necesaria si estamos en colab\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Otras librerías\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importamos las capas y modelos que vamos a necesitar para este worksheet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "# Import Fashion MNIST data\n",
    "fashion_mnist = keras.datasets.fashion_mnist.load_data()\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist\n",
    "\n",
    "# Primeras 10000 imágenes, las utilizamos como validación\n",
    "X_valid = train_images[:10000]\n",
    "Y_valid = train_labels[:10000]\n",
    "\n",
    "X_train = train_images[10000:]\n",
    "Y_train = train_labels[10000:]\n",
    "\n",
    "X_test = test_images\n",
    "Y_test = test_labels\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28*28)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], 28*28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28*28)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_valid = X_valid.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = keras.utils.to_categorical(Y_train, 10)\n",
    "Y_valid = keras.utils.to_categorical(Y_valid, 10)\n",
    "Y_test = keras.utils.to_categorical(Y_test, 10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'],)\n",
    "\n",
    "model.fit(X_train, Y_train, \n",
    "          batch_size=128, epochs=10, verbose=1, validation_data=(X_valid, Y_valid))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tareas\n",
    "\n",
    "Vamos a comenzar normalizando los datos de entrada según tres criterios: escalar los valores de entrada al rango 0-1, centrar a una media aproximada de 0 y transformar los datos de entrada aproximadamente a una distribución normal de media 0 y desviación unidad (N(0,1)).\n",
    "\n",
    "A continuación, cambiaremos el criterio de parada del entrenamiento del número máximo de iteraciones (épocas) a terminar el entrenamiento cuando se cumplan unas ciertas condiciones en un subconjunto de los datos u opcionalmente en un conjunto de validación (independiente del entrenamiento).\n",
    "\n",
    "### Normalización 1: escalado de los valores al rango (0, 1) [0.5 pto]\n",
    "\n",
    "\n",
    "A partir del código anterior, realizar las modificaciones necesarias para que los valores de las imágenes estén escalados al rango (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:19:51.573193Z",
     "start_time": "2021-10-21T16:19:51.549186Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Función que nos permitirá hacer:\n",
    "    - El split al dataset de train, valid y test.\n",
    "    - La prepación de las imagenes al tamaño y tipo esperados por la DNN.\n",
    "    - Devuelve el dataset de train, valid y test'''\n",
    "\n",
    "def prepare_dataset (train_dataset,test_dataset):\n",
    "    # Primeras 10000 imágenes, las utilizamos como validación\n",
    "    X_valid = train_dataset[:10000]\n",
    "    X_train = train_dataset[10000:]\n",
    "    \n",
    "    X_test = test_dataset\n",
    "    \n",
    "    X_train = X_train.reshape(X_train.shape[0], 28*28)\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], 28*28)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 28*28)\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_valid = X_valid.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    \n",
    "    return X_train, X_valid, X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:19:52.010329Z",
     "start_time": "2021-10-21T16:19:51.580195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Máximo del dataset de entreno 1.0000\n",
      "Mínimo del dataset de entreno 0.0000\n",
      "Máximo del dataset de test 1.0000\n",
      "Mínimo del dataset de test 0.0000\n"
     ]
    }
   ],
   "source": [
    "# TODO 1\n",
    "\n",
    "'''Partimos de los dataset originales y les aplicamos la división entre 255, \n",
    "ya que los valores de los pixeles contenidos en los dataset se mueven en el rango de 0 a 255\n",
    "por lo que después de noramlilzarlos el rango estará entre (0,1)'''\n",
    "\n",
    "train_scaled = train_images /255.0\n",
    "test_scaled = test_images /255.0\n",
    "print(f'Máximo del dataset de entreno {np.max(train_scaled):0.4f}')\n",
    "print(f'Mínimo del dataset de entreno {np.min(train_scaled):0.4f}')\n",
    "print(f'Máximo del dataset de test {np.max(test_scaled):0.4f}')\n",
    "print(f'Mínimo del dataset de test {np.min(test_scaled):0.4f}')\n",
    "\n",
    "\n",
    "X_train_scaled,X_valid_scaled,X_test_scaled = prepare_dataset(train_scaled, test_scaled) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Normalización 2: centrar a una media aproximada de 0 [0.5 pto]\n",
    "\n",
    "AYUDA: Para centrar los valores a una media aproximada de 0, puedes calcular la media total y restarsela a todos los datos. Recuerda que la información de los datos de evaluación (test) no se puede utilizar, pero deben llevar el mismo procesamiento que los datos con los que se entrena la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:19:52.540308Z",
     "start_time": "2021-10-21T16:19:52.015308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media del dataset de entreno -0.0000\n",
      "Media del dataset de test -0.0000\n"
     ]
    }
   ],
   "source": [
    "# TODO 2\n",
    "'''Partimos de los dataset originales y procedemos a \n",
    "aplicarle la normalización de media aproximada a 0. Otra opción y tal vez más optima en cuanto a rango de valores sería\n",
    "utilizar los dataset con la reducción de escala del apartado anterior y proceder a centrar entorno a la media.'''\n",
    "\n",
    "train_mean_scaled = train_images - np.mean(train_images)\n",
    "test_mean_scaled= test_images - np.mean(test_images)\n",
    "\n",
    "print(f'Media del dataset de entreno {np.mean(train_mean_scaled):0.4f}')\n",
    "print(f'Media del dataset de test {np.mean(test_mean_scaled):0.4f}')\n",
    "\n",
    "X_train_mean_scaled,X_valid_mean_scaled,X_test_mean_scaled = prepare_dataset(train_mean_scaled, test_mean_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalización 3: distribución normal de media 0 y desviación stándard 1 (estandarización N(0,1)) [0.5 pto]\n",
    "\n",
    "AYUDA: Para estandarizar los valores a una distribución aproximadamente normal N(0, 1), puedes calcular la media y la desviación total y aplicar la normalización: x\\_norm = (x - media)/desviacion. \n",
    "\n",
    "Recuerda que la información de los datos de evaluación (test) no se puede utilizar, pero deben llevar el mismo procesamiento que los datos con los que se entrena la red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:19:52.556484Z",
     "start_time": "2021-10-21T16:19:52.544310Z"
    }
   },
   "outputs": [],
   "source": [
    "############## Si al ejecutar el Kernel se bloquea, \n",
    "############## utiliza estas líneas para permitir la \n",
    "############## duplicación de librerías\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "##############\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:19:54.526419Z",
     "start_time": "2021-10-21T16:19:52.561399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media del dataset de entreno -0.0000\n",
      "Desviación típica del dataset de entreno 1.0000\n",
      "Media del dataset de test -0.0000\n",
      "Desviación típica del dataset de test 1.0000\n"
     ]
    }
   ],
   "source": [
    "'''Partimos de los dataset originales. Calculamos su media y su desviación tipica \n",
    "y aplicamos la formula de normalización N(0,1)'''\n",
    "\n",
    "train_standarized = (train_images - np.mean(train_images))/np.std(train_images)\n",
    "test_standarized = (test_images - np.mean(test_images))/np.std(test_images)\n",
    "\n",
    "print(f'Media del dataset de entreno {np.mean(train_standarized):0.4f}')\n",
    "print(f'Desviación típica del dataset de entreno {np.std(train_standarized):0.4f}')\n",
    "print(f'Media del dataset de test {np.mean(test_standarized):0.4f}')\n",
    "print(f'Desviación típica del dataset de test {np.std(test_standarized):0.4f}')\n",
    "\n",
    "X_train_standarized,X_valid_standarized,X_test_standarized = prepare_dataset(train_standarized, test_standarized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué es recomendable hacer estas normalizaciones? ¿Ha mejorado el resultado? ¿Por qué? ¿Con cuál se obtiene el mejor resultado? [1 pto]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:19:54.557997Z",
     "start_time": "2021-10-21T16:19:54.532004Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Función que nos permitirá crear, compilar, entrenar y validar marcadores con el dataset de test. \n",
    "Se le pasa como parámetros los dataset de test, valid, y test así como las respetivas etiquetas.\n",
    "También se le pueden pasar parámetros como el número de epochs, verbose,si queremos logs o no y callbacks.'''\n",
    "\n",
    "def exec_DNN (x_train, y_train, x_valid, y_valid, x_test, y_test,my_epochs=10,my_verbose=0, my_callbacks=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train, \n",
    "              batch_size=128, \n",
    "              epochs=my_epochs, \n",
    "              verbose=my_verbose,\n",
    "              validation_data=(x_valid, y_valid),\n",
    "              callbacks=my_callbacks\n",
    "             )\n",
    "\n",
    "    model_scores = model.evaluate(x_test, y_test, verbose=my_verbose,)\n",
    "\n",
    "    print(f'\\nLa Evaluación del modelo con los datos de test es:')\n",
    "    for indice in range(0,len(model.metrics_names),1):\n",
    "        print(f'El Marcador {model.metrics_names[indice]} del modelo con datos de test es : {model_scores[indice]}')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:20:35.074864Z",
     "start_time": "2021-10-21T16:19:54.567001Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La Evaluación del modelo con los datos de test es:\n",
      "El Marcador loss del modelo con datos de test es : 0.3320726454257965\n",
      "El Marcador accuracy del modelo con datos de test es : 0.8859999775886536\n"
     ]
    }
   ],
   "source": [
    "# Ejecutamos la DNN con el dataset escalado con rango (0,1)\n",
    "exec_DNN(X_train_scaled,Y_train,X_valid_scaled,Y_valid,X_test_scaled,Y_test,my_verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:21:15.915772Z",
     "start_time": "2021-10-21T16:20:35.080864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La Evaluación del modelo con los datos de test es:\n",
      "El Marcador loss del modelo con datos de test es : 0.4595426619052887\n",
      "El Marcador accuracy del modelo con datos de test es : 0.8537999987602234\n"
     ]
    }
   ],
   "source": [
    "# Ejecutamos la DNN con el dataset escalado con valores entorno a una media 0\n",
    "exec_DNN(X_train_mean_scaled,Y_train,X_valid_mean_scaled,Y_valid,X_test_mean_scaled,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:21:55.760160Z",
     "start_time": "2021-10-21T16:21:15.920776Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La Evaluación del modelo con los datos de test es:\n",
      "El Marcador loss del modelo con datos de test es : 0.3723970353603363\n",
      "El Marcador accuracy del modelo con datos de test es : 0.8851000070571899\n"
     ]
    }
   ],
   "source": [
    "# Ejecutamos la DNN con el dataset escalado con valores normalizados entorno a una N(0,1)\n",
    "exec_DNN(X_train_standarized,Y_train,X_valid_standarized,Y_valid,X_test_standarized,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Respuesta: ¿Por qué es recomendable hacer estas normalizaciones?***:\n",
    "    * Al igual que en muchos modelos de machine learning tanto supervisados como no supervisados, las redes neuronales son sensibles al rango de datos con el que trabajan, siendo más fácil para el modelo trabajar con valores pequeños y rangos de valores pequeños y acotados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Respuesta: ¿Ha mejorado el resultado?***:\n",
    "    * Si ha mejorado bastante tanto en la función de coste, entorno a 0,20, como en el acurracy entorno a 0,05. Salvo en la normalización entorno a la media 0, que los ofrece algo más similares a los marcadores cuando no teniamos los datos normalizados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Respuesta: ¿Por qué?***:\n",
    "   *  Por que al estar todos los valores en la misma escala y con valores pequeños y rangos acotados, los calculos se facilitan y el coste de proceso de la red neuronal es mucho menos y más rapido.\n",
    "   * Así también tenemos todos los valores bajo la misma escala y por lo tanto el calculo de optimización de la función con el learning rate se hace con la misma escala. Si no se tuviera así se podrían perder o no calcular correctamente los minimos en alguno de los valores.\n",
    "   * Para el caso con media entorno a 0 más o menos ofrece los mismos valores que si no se hubiera realizado nada. Pese a reducir el rango de valores, sigue siendo un rango muy amplio. A esta normalización entorno a media 0, habría que haber realizado previamente, una reducción del rango como la del apartado 1. Así hubiera sido más efectiva y mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * ***Respuesta: ¿Con cuál se obtiene el mejor resultado?***\n",
    "     * En nuestro caso se obtienen mejores resultados en cuanto al accuracy o precisión con los valores **escalados en un rango (0,1)**, aunque las diferencias son mínimas con respecto a la normalización conforme a la normal N(0,1), por detrás se queda la normalización entorno a la media 0.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de la tasa de aprendizaje para optimizar el rendimiento de la red\n",
    "\n",
    "¿Qué sucede si elegimos una tasa de aprendizaje demasiado alta? ¿Y una demasiado baja? Explica brevemente qué es la tasa de aprendizaje o \"learning rate\" y cómo afecta a nuestro entrenamiento: [1 pto]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Respuesta1**: Si elegimos una tasa de aprendizaje demasiado alta ganaremos rapidez en el módelo, pero por el contrario será más impreciso ya que lo más probable es que no llegue bien a calcular el error mínimo.\n",
    "* **Respuesta2**: Si por el contario elegimos una tasa de aprendizaje demasiado baja, vamos a penalizar el rendimento del modelo, ya que al calcular el valor optimo de error, dando \"pasos\" muy cortos, realizará muchas iteraciones hasta llegar al mínimo, por lo que el modelo requerira de un tiempo muy grande para converger al punto óptimo.\n",
    "* **Respuesta3**:\n",
    "    * A nivel de definición la tasa de aprendizaje o learning rate, dentro del algoritmo de descenso de gradiente, es el tamaño del \"paso\" que damos para volver a calcular el nuevo punto en el que se vuelve a calcular el gradiente. Todo esto con el fin de que en n pasos hayamos encontrado el valor mínimo que optimiza la función de coste. Más formalmente es el número que se le multiplica al gradiente de la recta en ese punto y se le resta al valor de ese punto, la formula se expresaria de esta manera, dado una variable aleatoria w cuyo valor inicial es aleatorio, la formula del descenso de gradiente es Wn+1 = Wn-ta.Gradiente_en_Wn:\n",
    "$$W_{n+1} = W_{n}- ta.\\nabla (W_{n})$$ Siendo ta -> Tasa de Aprendizaje\n",
    "\n",
    "    * La tasa de aprendizaje o learning rate en nuestro entrenamiento afecta desde el punto de vista que debemos encontrar el valor óptimo y normalmente es uno de los hiperparámetros que siempre se debe ajustar, para conseguir un número mínimo de iteracciones con un mayor acercamiento al punto mínimo de error. Valores como el 1 ya harían que el algoritmo no convergiera a ningún valor y estaría rebotando entre los mismos valores. Por experiencia normalmente se obtienen buenos valores y rendimiento con valores entre el 0,01 y el 0,1, pero esto siempre hay que probarlo para cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchas veces, cuando la función de coste llega a una zona cercana al mínimo, la tasa de aprendizaje es muy grande para alcanzar el valor óptimo. Por eso, una de las técnicas utilizadas para evitar este problema consiste en reducir la tasa de aprendizaje cuando llegamos a un punto en que no vemos mejora del rendimiento en nuestro conjunto de validación. \n",
    "\n",
    "Para ello, podemos utilizar uno de los Callbacks de Keras llamado: ReduceLROnPlateau. Puedes encontrar la información sobre él en el siguiente enlace: https://keras.io/callbacks/#reducelronplateau\n",
    "\n",
    "\n",
    "Investiga (la documentación de keras es sencilla y muy muy útil, pero puedes tirar de google) cómo implementar un callback. Después, implementa dicho callback: puedes empezar con el código anterior, con una paciencia de 2 iteraciones y una reducción del 50% del valor de la tasa de aprendizaje.\n",
    "\n",
    "Es posible que tengas que aumentar las iteraciones máximas para ver mejor su funcionamiento. [1 pto]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Respuesta :** Vamos a utilizar dos CallBack:\n",
    "    * Un Callback a medida o Custom, **LossAndAccuracyPrintingCallback**, que hereda de la clase Callback y lo haremos para perosnalizar alguno de los mensajes\n",
    "    * El propuesto, **ReduceLROnPlateau**, para ir bajando el learning rate o tasa de aprendizaje cada vez que la red detecte que ya no hay mejora en los marcadores. En este caso se nos propone un **patiente=2** y una disminución del learning rate del 50%, esto se hace configurando el parámetro **factor=0.5**, también configuraremos el parámetro verbose=1 para que nos vaya informando cuando se realiza algún cambio en el learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:21:55.776145Z",
     "start_time": "2021-10-21T16:21:55.765135Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos un callback personalizado que hereda de la clase Callback, con el objetivo de visualizar\n",
    "# los marcadores al final de cada epoch.\n",
    "class LossAndAccuracyPrintingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "#   def on_train_batch_end(self, batch, logs=None):\n",
    "#      print('Para el batch de entrenamiento {}, la perdida (loss) es {:7.2f}.'.format(batch, logs['loss']))\n",
    "\n",
    "#   def on_test_batch_end(self, batch, logs=None):\n",
    "#      print('Para el  batch de validación {}, la perdida (loss) es {:7.2f}.'.format(batch, logs['loss']))\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "        \n",
    "    print(f\"En la epoch {epoch+1} el marcador val_loss para validación es {logs['val_loss']:7.4f} y el val_accuracy es {logs['val_accuracy']:7.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:21:55.808144Z",
     "start_time": "2021-10-21T16:21:55.782153Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2,verbose=1)\n",
    "my_callbacks=[reduce_lr,LossAndAccuracyPrintingCallback()]\n",
    "# my_callbacks=[reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:23:18.762843Z",
     "start_time": "2021-10-21T16:21:55.813151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En la epoch 1 el marcador val_loss para validación es  0.3601 y el val_accuracy es  0.8685\n",
      "En la epoch 2 el marcador val_loss para validación es  0.3498 y el val_accuracy es  0.8719\n",
      "En la epoch 3 el marcador val_loss para validación es  0.3057 y el val_accuracy es  0.8887\n",
      "En la epoch 4 el marcador val_loss para validación es  0.3217 y el val_accuracy es  0.8860\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "En la epoch 5 el marcador val_loss para validación es  0.3207 y el val_accuracy es  0.8875\n",
      "En la epoch 6 el marcador val_loss para validación es  0.2925 y el val_accuracy es  0.8959\n",
      "En la epoch 7 el marcador val_loss para validación es  0.2911 y el val_accuracy es  0.8967\n",
      "En la epoch 8 el marcador val_loss para validación es  0.2986 y el val_accuracy es  0.8972\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "En la epoch 9 el marcador val_loss para validación es  0.3197 y el val_accuracy es  0.8885\n",
      "En la epoch 10 el marcador val_loss para validación es  0.2958 y el val_accuracy es  0.9036\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "En la epoch 11 el marcador val_loss para validación es  0.3149 y el val_accuracy es  0.8986\n",
      "En la epoch 12 el marcador val_loss para validación es  0.3089 y el val_accuracy es  0.9029\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "En la epoch 13 el marcador val_loss para validación es  0.3175 y el val_accuracy es  0.9021\n",
      "En la epoch 14 el marcador val_loss para validación es  0.3174 y el val_accuracy es  0.9047\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "En la epoch 15 el marcador val_loss para validación es  0.3173 y el val_accuracy es  0.9027\n",
      "En la epoch 16 el marcador val_loss para validación es  0.3213 y el val_accuracy es  0.9037\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "En la epoch 17 el marcador val_loss para validación es  0.3237 y el val_accuracy es  0.9034\n",
      "En la epoch 18 el marcador val_loss para validación es  0.3240 y el val_accuracy es  0.9039\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "En la epoch 19 el marcador val_loss para validación es  0.3253 y el val_accuracy es  0.9037\n",
      "En la epoch 20 el marcador val_loss para validación es  0.3257 y el val_accuracy es  0.9037\n",
      "\n",
      "La Evaluación del modelo con los datos de test es:\n",
      "El Marcador loss del modelo con datos de test es : 0.3541863262653351\n",
      "El Marcador accuracy del modelo con datos de test es : 0.9007999897003174\n"
     ]
    }
   ],
   "source": [
    "my_epoch=20 # Para ver como actua el ReduceLROnPlateau subimos los epochs\n",
    "my_verbose=0 # flag con el que activaremos o desctavaremos los logs.\n",
    "\n",
    "exec_DNN(X_train_standarized,\n",
    "         Y_train,\n",
    "         X_valid_standarized,\n",
    "         Y_valid,\n",
    "         X_test_standarized,\n",
    "         Y_test,my_epoch,\n",
    "         my_verbose,\n",
    "         my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:24:41.030418Z",
     "start_time": "2021-10-21T16:23:18.767827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En la epoch 1 el marcador val_loss para validación es  0.8510 y el val_accuracy es  0.8011\n",
      "En la epoch 2 el marcador val_loss para validación es  0.5744 y el val_accuracy es  0.8371\n",
      "En la epoch 3 el marcador val_loss para validación es  0.5321 y el val_accuracy es  0.8481\n",
      "En la epoch 4 el marcador val_loss para validación es  0.4892 y el val_accuracy es  0.8474\n",
      "En la epoch 5 el marcador val_loss para validación es  0.5001 y el val_accuracy es  0.8488\n",
      "En la epoch 6 el marcador val_loss para validación es  0.4712 y el val_accuracy es  0.8557\n",
      "En la epoch 7 el marcador val_loss para validación es  0.4611 y el val_accuracy es  0.8538\n",
      "En la epoch 8 el marcador val_loss para validación es  0.4639 y el val_accuracy es  0.8670\n",
      "En la epoch 9 el marcador val_loss para validación es  0.4192 y el val_accuracy es  0.8717\n",
      "En la epoch 10 el marcador val_loss para validación es  0.4313 y el val_accuracy es  0.8708\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "En la epoch 11 el marcador val_loss para validación es  0.4792 y el val_accuracy es  0.8509\n",
      "En la epoch 12 el marcador val_loss para validación es  0.3749 y el val_accuracy es  0.8831\n",
      "En la epoch 13 el marcador val_loss para validación es  0.3890 y el val_accuracy es  0.8809\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "En la epoch 14 el marcador val_loss para validación es  0.4108 y el val_accuracy es  0.8780\n",
      "En la epoch 15 el marcador val_loss para validación es  0.3741 y el val_accuracy es  0.8885\n",
      "En la epoch 16 el marcador val_loss para validación es  0.3884 y el val_accuracy es  0.8891\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "En la epoch 17 el marcador val_loss para validación es  0.3987 y el val_accuracy es  0.8856\n",
      "En la epoch 18 el marcador val_loss para validación es  0.4050 y el val_accuracy es  0.8887\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "En la epoch 19 el marcador val_loss para validación es  0.4100 y el val_accuracy es  0.8892\n",
      "En la epoch 20 el marcador val_loss para validación es  0.4081 y el val_accuracy es  0.8926\n",
      "\n",
      "La Evaluación del modelo con los datos de test es:\n",
      "El Marcador loss del modelo con datos de test es : 0.43250221014022827\n",
      "El Marcador accuracy del modelo con datos de test es : 0.8827000260353088\n"
     ]
    }
   ],
   "source": [
    "exec_DNN(X_train_mean_scaled,\n",
    "         Y_train,\n",
    "         X_valid_mean_scaled,\n",
    "         Y_valid,\n",
    "         X_test_mean_scaled,\n",
    "         Y_test,\n",
    "         my_epoch,\n",
    "         my_verbose,\n",
    "         my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-21T16:26:00.369317Z",
     "start_time": "2021-10-21T16:24:41.036429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En la epoch 1 el marcador val_loss para validación es  0.3826 y el val_accuracy es  0.8636\n",
      "En la epoch 2 el marcador val_loss para validación es  0.3379 y el val_accuracy es  0.8721\n",
      "En la epoch 3 el marcador val_loss para validación es  0.3243 y el val_accuracy es  0.8796\n",
      "En la epoch 4 el marcador val_loss para validación es  0.3381 y el val_accuracy es  0.8794\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "En la epoch 5 el marcador val_loss para validación es  0.3328 y el val_accuracy es  0.8777\n",
      "En la epoch 6 el marcador val_loss para validación es  0.2870 y el val_accuracy es  0.8993\n",
      "En la epoch 7 el marcador val_loss para validación es  0.2967 y el val_accuracy es  0.8978\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "En la epoch 8 el marcador val_loss para validación es  0.2963 y el val_accuracy es  0.8972\n",
      "En la epoch 9 el marcador val_loss para validación es  0.2949 y el val_accuracy es  0.9024\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "En la epoch 10 el marcador val_loss para validación es  0.3036 y el val_accuracy es  0.9007\n",
      "En la epoch 11 el marcador val_loss para validación es  0.2966 y el val_accuracy es  0.9017\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "En la epoch 12 el marcador val_loss para validación es  0.3034 y el val_accuracy es  0.9027\n",
      "En la epoch 13 el marcador val_loss para validación es  0.2994 y el val_accuracy es  0.9047\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "En la epoch 14 el marcador val_loss para validación es  0.3054 y el val_accuracy es  0.9021\n",
      "En la epoch 15 el marcador val_loss para validación es  0.3083 y el val_accuracy es  0.9024\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "En la epoch 16 el marcador val_loss para validación es  0.3099 y el val_accuracy es  0.9050\n",
      "En la epoch 17 el marcador val_loss para validación es  0.3110 y el val_accuracy es  0.9034\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "En la epoch 18 el marcador val_loss para validación es  0.3119 y el val_accuracy es  0.9026\n",
      "En la epoch 19 el marcador val_loss para validación es  0.3115 y el val_accuracy es  0.9039\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "En la epoch 20 el marcador val_loss para validación es  0.3124 y el val_accuracy es  0.9035\n",
      "\n",
      "La Evaluación del modelo con los datos de test es:\n",
      "El Marcador loss del modelo con datos de test es : 0.34904801845550537\n",
      "El Marcador accuracy del modelo con datos de test es : 0.8989999890327454\n"
     ]
    }
   ],
   "source": [
    "exec_DNN(X_train_standarized,\n",
    "         Y_train,\n",
    "         X_valid_standarized,\n",
    "         Y_valid,X_test_standarized,\n",
    "         Y_test,\n",
    "         my_epoch,\n",
    "         my_verbose,\n",
    "         my_callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analiza los resultados: ¿Qué hace el callback? ¿Mejora ahora el resultado? ¿Por qué? [0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-17T08:57:13.702876Z",
     "start_time": "2021-10-17T08:57:13.691886Z"
    }
   },
   "source": [
    "* ***Respuesta ¿Qué hace el callback? :***\n",
    "    * Según lo que podemos observar, y se hace más evidente cuando subimos a 20 epochs, en cuanto hay poca mejora en el loss o perdida, aplica una reducción del learnning rate o tasa de aprendizaje de lo paramétrizado en el parámetro factor, que en este caso es **factor=0.5** que equivale a una reducción cada vez que lo reduce del 50%. Empezando con una reducción de **lr** desde **0.0005000000237487257.**, por lo que lr con el que arranco era de 0,001, hasta el **lr** **3.906250185536919e-06** que nos ha aparecido en alguna de las ejecuciones de la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Respuesta ¿Mejora ahora el resultado? :***\n",
    "     * Si los resultados se ven mejorados sobre todo el Accuracy/Precisión que para el test ya llega 90% de acierto en el dataset con normalización (0,1). Sin embargo para la función de coste o loss los valores se mantienen entorno al 0.33-0.34 cuando se validan contra test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Respuesta  ¿Por qué? :***\n",
    "    * Por que al ir reduciendo el **lr**, los pasos que va dando hasta encontrar el valor mínimo u optimo, son cada vez más pequeños y por lo tanto ese valor mínimo es más seguro encontrarlo. Por otro lado la contra está en que contra mayor es el lr, mayor es el coste computacional por que hay que hacer más iteracciones hasta llegar a el."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.837px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
