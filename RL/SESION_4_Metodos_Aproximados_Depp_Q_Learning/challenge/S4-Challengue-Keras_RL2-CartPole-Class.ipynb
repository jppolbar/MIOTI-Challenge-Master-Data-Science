{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lC2WbRzQQubt"
   },
   "source": [
    "### Instalación entorno visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1752,
     "status": "ok",
     "timestamp": 1639331814821,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "KJimHroWRxY5",
    "outputId": "e9cc1810-e435-4d97-f14a-fbdf04cad481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21796,
     "status": "ok",
     "timestamp": 1639331836610,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "YeanGEY0QtF7",
    "outputId": "f6d5bb1d-426e-4b2f-9728-9d0f174a17ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (59.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!apt-get update > /dev/null 2>&1\n",
    "!apt-get install cmake > /dev/null 2>&1\n",
    "!pip install --upgrade setuptools 2>&1\n",
    "!pip install ez_setup > /dev/null 2>&1\n",
    "!pip install gym[atari] > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1639331836612,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "SyEYjKdjRL_W"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "\"\"\"\n",
    "Utility functions to enable video recording of gym environment \n",
    "and displaying it.\n",
    "To enable video, just do \"env = wrap_env(env)\"\"\n",
    "\"\"\"\n",
    "#Path donde guardar los videos\n",
    "video_path = '/content/drive/MyDrive/MIOTI/RL/SESION_4/video/'\n",
    "\n",
    "def show_video(video_path):\n",
    "  mp4list = glob.glob(video_path+'*.mp4')\n",
    "  if len(mp4list) > 0:\n",
    "    mp4 = mp4list[0]\n",
    "    video = io.open(mp4, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "  else: \n",
    "    print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env,modelo):\n",
    "  env = Monitor(env, video_path+modelo, force=True)\n",
    "  return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cea53c80"
   },
   "source": [
    "![image.png](attachment:image.png)\n",
    "<center style=\"color:#888\">Módulo Advanced Data Science<br/>Asignatura Reinforcement Learning</center>\n",
    "\n",
    "\n",
    "# S4. Challenge. Implementación de Deep Q-Network\n",
    "\n",
    "Vamos a implementar DQN, un algoritmo aproximado de Q-learning, con técnicas de *experience replay* y *target networks*. Para ello utilizaremos la librería `keras-rl2` que nos ofrece una abstracción de alto nivel para entrenar modelos de Deep Learning sobre entornos de RL. **Comienza instalando la libreria indicada, cuidado de no instalar keras-rl**, porque es posible que no funcione con TensorFlow 2  \n",
    "\n",
    "Si necesitas consultar la documentación de la librería acude a esta web: *https://keras-rl.readthedocs.io/en/latest/*     \n",
    "\n",
    "**No olvides contestar a las preguntas del final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3595,
     "status": "ok",
     "timestamp": 1639331840197,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "ef63c58d",
    "outputId": "a4553188-3115-44c8-be5d-513d1e0c782d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-rl2 in /usr/local/lib/python3.7/dist-packages (1.0.5)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.19.5)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.0)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.17.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.42.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (2.7.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.10.0.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (1.13.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.37.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (12.0.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.22.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras-rl2) (0.4.0)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras-rl2) (1.5.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (59.5.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (1.8.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (2.23.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (1.35.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow->keras-rl2) (3.3.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->keras-rl2) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow->keras-rl2) (4.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow->keras-rl2) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow->keras-rl2) (0.4.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow->keras-rl2) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->keras-rl2) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-rl2 ## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d27c2e36"
   },
   "source": [
    "Hemos visto que con la red implementada en clase, el agente aprende pero parace que tiene cierto sesgo en irse hacia un lado. Ahora te toca a ti, diseñar una red más eficiente. Es una red muy sencilla, a parte de la capa de la primera capa Flatten, con 4 capas densas más es suficiente. Yo he usado 658 parámetros para que el agente aprenda.    \n",
    "\n",
    "En este ejemplo a la red no le pasamos el estado de observaciones como la imagen que vemos, si no una lista que contiene: *[position of cart, velocity of cart, angle of pole, rotation rate of pole]*. Para más info sobre el juego puedes consultar la descripción aquí: *https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py#L16*     \n",
    "\n",
    "Como veras, tienes que definir la variable *windows_length*, repasa el concepto de **stacking** del Worsheet de la S4. Si a esta red no le pasamos la obervación como un frame (imagen), piensa ¿de cuánto podría ser este *windows_lenght* como mínimo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1639331840198,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "2ab81a5a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26a683d2"
   },
   "source": [
    "Este ejercicio lo haremos sobre el mismo entorno, `CartPole-v0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1639331840199,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "fafb5aaa"
   },
   "outputs": [],
   "source": [
    "#Instanciación de variables globales.\n",
    "ENV_NAME = 'CartPole-v0'\n",
    "env = gym.make(ENV_NAME) # Instancia el entorno\n",
    "env._max_episode_steps = 300 # Busca en internet como aumentar el numero máximo de stepd+s del entorno de 200(por defecto) a 300.\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n # Obtén el número de acciones del entorno\n",
    "#Tamaño de las ventana a entrenar.\n",
    "window_lengths = [1,4]\n",
    "#Politicas con las que entrenar\n",
    "policies=[BoltzmannQPolicy(),EpsGreedyQPolicy()]\n",
    "#Path donde guardar los pesos.\n",
    "weights_path = '/content/drive/MyDrive/MIOTI/RL/SESION_4/weights/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "386e2ce9"
   },
   "source": [
    "Ahora tenemos que declarar la arquitectura de nuestra DQN que utilizaremos para resolver el entorno. Ten en cuenta que el input será la observación del entorno y la salida será el valor de $Q$ para cada una de las acciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 359,
     "status": "ok",
     "timestamp": 1639331840546,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "5dd8f1e5"
   },
   "outputs": [],
   "source": [
    "# Define la red. Recuerda que con 658 parámetro es suficiente\n",
    "# Creamos una función que nos genere el model de DNN.\n",
    "# Como parámetro de entrada tiene la dimensión de la ventana de entrada.\n",
    "def create_model(window_length):\n",
    "  model = Sequential()\n",
    "  model.add(Flatten(input_shape=(window_length,) + env.observation_space.shape))\n",
    "  ## YOUR CODE HERE ## \n",
    "\n",
    "  model.add(Dense(16))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dense(16))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dense(16))\n",
    "  model.add(Activation('relu'))\n",
    "\n",
    "  model.add(Dense(nb_actions)) # Recuerda, el núnero de nueronas de la ultima capa,¿con qué tiene que coincidir?\n",
    "  model.add(Activation('linear'))\n",
    "  # print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2c0b44b"
   },
   "source": [
    "Ahora que tenemos nuestra DQN y entorno preparados, tenemos que hacer que aprende a jugar al `CartPole`. Para ello tendremos que declarar los componentes típicos de un modelo de DQN, la memoria y la política (*¡¡Prueba con las dos!!*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3195961,
     "status": "ok",
     "timestamp": 1639335036498,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "beca90ee",
    "outputId": "564122fa-2ba3-482d-ce2b-15b4d3531baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: BoltzmannQPolicy\n",
      "Window_length: 1\n",
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    19/50000: episode: 1, duration: 2.859s, episode steps:  19, steps per second:   7, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.579 [0.000, 1.000],  loss: 0.553204, mse: 0.555155, mean_q: 0.059912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    55/50000: episode: 2, duration: 0.547s, episode steps:  36, steps per second:  66, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.422739, mse: 0.438737, mean_q: 0.212234\n",
      "    75/50000: episode: 3, duration: 0.326s, episode steps:  20, steps per second:  61, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.212814, mse: 0.366967, mean_q: 0.668391\n",
      "    94/50000: episode: 4, duration: 0.299s, episode steps:  19, steps per second:  64, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.080693, mse: 0.528432, mean_q: 1.104121\n",
      "   105/50000: episode: 5, duration: 0.175s, episode steps:  11, steps per second:  63, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.052493, mse: 0.848875, mean_q: 1.335739\n",
      "   124/50000: episode: 6, duration: 0.324s, episode steps:  19, steps per second:  59, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.737 [0.000, 1.000],  loss: 0.042597, mse: 0.905055, mean_q: 1.303308\n",
      "   146/50000: episode: 7, duration: 0.351s, episode steps:  22, steps per second:  63, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.682 [0.000, 1.000],  loss: 0.040232, mse: 1.113470, mean_q: 1.482102\n",
      "   168/50000: episode: 8, duration: 0.343s, episode steps:  22, steps per second:  64, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.032607, mse: 1.295933, mean_q: 1.642950\n",
      "   186/50000: episode: 9, duration: 0.279s, episode steps:  18, steps per second:  65, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 0.031438, mse: 1.540982, mean_q: 1.795458\n",
      "   202/50000: episode: 10, duration: 0.266s, episode steps:  16, steps per second:  60, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.027209, mse: 1.734534, mean_q: 1.928071\n",
      "   214/50000: episode: 11, duration: 0.183s, episode steps:  12, steps per second:  65, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.051500, mse: 1.827933, mean_q: 1.949074\n",
      "   225/50000: episode: 12, duration: 0.178s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.041316, mse: 2.005158, mean_q: 2.042269\n",
      "   242/50000: episode: 13, duration: 0.257s, episode steps:  17, steps per second:  66, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.039515, mse: 2.206609, mean_q: 2.134909\n",
      "   259/50000: episode: 14, duration: 0.269s, episode steps:  17, steps per second:  63, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.412 [0.000, 1.000],  loss: 0.060475, mse: 2.481239, mean_q: 2.263861\n",
      "   272/50000: episode: 15, duration: 0.218s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.059732, mse: 2.664908, mean_q: 2.348413\n",
      "   284/50000: episode: 16, duration: 0.185s, episode steps:  12, steps per second:  65, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.067409, mse: 2.965226, mean_q: 2.498605\n",
      "   304/50000: episode: 17, duration: 0.302s, episode steps:  20, steps per second:  66, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.065565, mse: 3.203433, mean_q: 2.586997\n",
      "   334/50000: episode: 18, duration: 0.459s, episode steps:  30, steps per second:  65, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.080480, mse: 3.763224, mean_q: 2.785125\n",
      "   357/50000: episode: 19, duration: 0.347s, episode steps:  23, steps per second:  66, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.565 [0.000, 1.000],  loss: 0.080482, mse: 4.280026, mean_q: 2.970937\n",
      "   383/50000: episode: 20, duration: 0.407s, episode steps:  26, steps per second:  64, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.105242, mse: 4.782315, mean_q: 3.127965\n",
      "   415/50000: episode: 21, duration: 0.503s, episode steps:  32, steps per second:  64, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.113676, mse: 5.485770, mean_q: 3.325572\n",
      "   432/50000: episode: 22, duration: 0.265s, episode steps:  17, steps per second:  64, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.353 [0.000, 1.000],  loss: 0.120588, mse: 6.368740, mean_q: 3.565614\n",
      "   440/50000: episode: 23, duration: 0.129s, episode steps:   8, steps per second:  62, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.125 [0.000, 1.000],  loss: 0.083058, mse: 6.788535, mean_q: 3.690778\n",
      "   455/50000: episode: 24, duration: 0.241s, episode steps:  15, steps per second:  62, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.142295, mse: 6.818351, mean_q: 3.725217\n",
      "   492/50000: episode: 25, duration: 0.585s, episode steps:  37, steps per second:  63, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.129443, mse: 7.710094, mean_q: 3.932293\n",
      "   509/50000: episode: 26, duration: 0.274s, episode steps:  17, steps per second:  62, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.183480, mse: 8.171645, mean_q: 4.054133\n",
      "   523/50000: episode: 27, duration: 0.235s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.161232, mse: 8.847475, mean_q: 4.229356\n",
      "   536/50000: episode: 28, duration: 0.212s, episode steps:  13, steps per second:  61, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.136354, mse: 9.156568, mean_q: 4.266524\n",
      "   555/50000: episode: 29, duration: 0.300s, episode steps:  19, steps per second:  63, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.421 [0.000, 1.000],  loss: 0.147385, mse: 9.919858, mean_q: 4.476560\n",
      "   564/50000: episode: 30, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.209491, mse: 10.042690, mean_q: 4.460875\n",
      "   577/50000: episode: 31, duration: 0.213s, episode steps:  13, steps per second:  61, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.177439, mse: 10.842315, mean_q: 4.670959\n",
      "   601/50000: episode: 32, duration: 0.386s, episode steps:  24, steps per second:  62, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.220606, mse: 11.210818, mean_q: 4.739982\n",
      "   628/50000: episode: 33, duration: 0.432s, episode steps:  27, steps per second:  63, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.190773, mse: 12.146660, mean_q: 4.941391\n",
      "   638/50000: episode: 34, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.193345, mse: 12.952873, mean_q: 5.120554\n",
      "   653/50000: episode: 35, duration: 0.250s, episode steps:  15, steps per second:  60, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.207329, mse: 13.835878, mean_q: 5.233470\n",
      "   672/50000: episode: 36, duration: 0.291s, episode steps:  19, steps per second:  65, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.263 [0.000, 1.000],  loss: 0.283982, mse: 13.914699, mean_q: 5.228619\n",
      "   686/50000: episode: 37, duration: 0.237s, episode steps:  14, steps per second:  59, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.357767, mse: 14.781134, mean_q: 5.431262\n",
      "   699/50000: episode: 38, duration: 0.216s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.309246, mse: 15.074478, mean_q: 5.428304\n",
      "   737/50000: episode: 39, duration: 0.575s, episode steps:  38, steps per second:  66, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 0.290121, mse: 16.330452, mean_q: 5.651462\n",
      "   757/50000: episode: 40, duration: 0.328s, episode steps:  20, steps per second:  61, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.325648, mse: 17.648626, mean_q: 5.845319\n",
      "   772/50000: episode: 41, duration: 0.240s, episode steps:  15, steps per second:  63, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.348588, mse: 18.167459, mean_q: 5.995440\n",
      "   804/50000: episode: 42, duration: 0.504s, episode steps:  32, steps per second:  63, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.295157, mse: 19.434177, mean_q: 6.180920\n",
      "   824/50000: episode: 43, duration: 0.315s, episode steps:  20, steps per second:  63, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.650 [0.000, 1.000],  loss: 0.349456, mse: 20.420752, mean_q: 6.323524\n",
      "   842/50000: episode: 44, duration: 0.294s, episode steps:  18, steps per second:  61, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.292889, mse: 21.817516, mean_q: 6.548244\n",
      "   907/50000: episode: 45, duration: 0.971s, episode steps:  65, steps per second:  67, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 0.376116, mse: 23.486641, mean_q: 6.808753\n",
      "   970/50000: episode: 46, duration: 0.954s, episode steps:  63, steps per second:  66, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.413 [0.000, 1.000],  loss: 0.330662, mse: 26.988327, mean_q: 7.356500\n",
      "   982/50000: episode: 47, duration: 0.196s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.378588, mse: 28.381401, mean_q: 7.537710\n",
      "  1000/50000: episode: 48, duration: 0.281s, episode steps:  18, steps per second:  64, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.611 [0.000, 1.000],  loss: 0.278686, mse: 30.462765, mean_q: 7.856654\n",
      "  1032/50000: episode: 49, duration: 0.496s, episode steps:  32, steps per second:  64, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 0.412101, mse: 31.596657, mean_q: 8.014959\n",
      "  1086/50000: episode: 50, duration: 0.825s, episode steps:  54, steps per second:  65, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.414845, mse: 34.800323, mean_q: 8.412679\n",
      "  1158/50000: episode: 51, duration: 1.070s, episode steps:  72, steps per second:  67, episode reward: 72.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.339918, mse: 39.854462, mean_q: 8.996235\n",
      "  1212/50000: episode: 52, duration: 0.847s, episode steps:  54, steps per second:  64, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.370456, mse: 44.896488, mean_q: 9.600142\n",
      "  1291/50000: episode: 53, duration: 1.188s, episode steps:  79, steps per second:  66, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.450476, mse: 51.615273, mean_q: 10.294066\n",
      "  1394/50000: episode: 54, duration: 1.541s, episode steps: 103, steps per second:  67, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 0.476567, mse: 60.583405, mean_q: 11.172803\n",
      "  1497/50000: episode: 55, duration: 1.537s, episode steps: 103, steps per second:  67, episode reward: 103.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.485058, mse: 69.790306, mean_q: 11.943654\n",
      "  1604/50000: episode: 56, duration: 1.600s, episode steps: 107, steps per second:  67, episode reward: 107.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.574589, mse: 82.662766, mean_q: 13.015960\n",
      "  1728/50000: episode: 57, duration: 1.848s, episode steps: 124, steps per second:  67, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.555991, mse: 96.280617, mean_q: 14.090227\n",
      "  1813/50000: episode: 58, duration: 1.282s, episode steps:  85, steps per second:  66, episode reward: 85.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.854436, mse: 110.701065, mean_q: 15.088499\n",
      "  1908/50000: episode: 59, duration: 1.412s, episode steps:  95, steps per second:  67, episode reward: 95.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.727567, mse: 122.500923, mean_q: 15.918466\n",
      "  1986/50000: episode: 60, duration: 1.180s, episode steps:  78, steps per second:  66, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 0.958766, mse: 135.224884, mean_q: 16.636669\n",
      "  2125/50000: episode: 61, duration: 2.118s, episode steps: 139, steps per second:  66, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 0.990457, mse: 151.099197, mean_q: 17.690351\n",
      "  2214/50000: episode: 62, duration: 1.344s, episode steps:  89, steps per second:  66, episode reward: 89.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 1.185267, mse: 169.263092, mean_q: 18.753901\n",
      "  2322/50000: episode: 63, duration: 1.587s, episode steps: 108, steps per second:  68, episode reward: 108.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.931911, mse: 185.128235, mean_q: 19.585350\n",
      "  2427/50000: episode: 64, duration: 1.564s, episode steps: 105, steps per second:  67, episode reward: 105.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.236778, mse: 204.438446, mean_q: 20.570246\n",
      "  2533/50000: episode: 65, duration: 1.574s, episode steps: 106, steps per second:  67, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.913849, mse: 222.062317, mean_q: 21.451262\n",
      "  2693/50000: episode: 66, duration: 2.406s, episode steps: 160, steps per second:  66, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 1.086818, mse: 248.226105, mean_q: 22.726389\n",
      "  2873/50000: episode: 67, duration: 2.666s, episode steps: 180, steps per second:  68, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 1.179294, mse: 283.122437, mean_q: 24.290977\n",
      "  3048/50000: episode: 68, duration: 2.584s, episode steps: 175, steps per second:  68, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 1.365951, mse: 322.661469, mean_q: 25.887520\n",
      "  3192/50000: episode: 69, duration: 2.104s, episode steps: 144, steps per second:  68, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 1.702771, mse: 357.012665, mean_q: 27.208124\n",
      "  3368/50000: episode: 70, duration: 2.649s, episode steps: 176, steps per second:  66, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 1.573678, mse: 394.102448, mean_q: 28.616899\n",
      "  3567/50000: episode: 71, duration: 2.922s, episode steps: 199, steps per second:  68, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.605337, mse: 445.199066, mean_q: 30.479570\n",
      "  3765/50000: episode: 72, duration: 2.921s, episode steps: 198, steps per second:  68, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.207191, mse: 497.828125, mean_q: 32.181423\n",
      "  3950/50000: episode: 73, duration: 2.799s, episode steps: 185, steps per second:  66, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 1.960129, mse: 550.598999, mean_q: 33.906261\n",
      "  4155/50000: episode: 74, duration: 3.108s, episode steps: 205, steps per second:  66, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.133526, mse: 613.124634, mean_q: 35.707462\n",
      "  4328/50000: episode: 75, duration: 2.594s, episode steps: 173, steps per second:  67, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 1.582398, mse: 688.210999, mean_q: 37.972565\n",
      "  4530/50000: episode: 76, duration: 3.027s, episode steps: 202, steps per second:  67, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 2.248195, mse: 749.200500, mean_q: 39.528355\n",
      "  4692/50000: episode: 77, duration: 2.417s, episode steps: 162, steps per second:  67, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.041964, mse: 802.115417, mean_q: 40.772858\n",
      "  4914/50000: episode: 78, duration: 3.333s, episode steps: 222, steps per second:  67, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 2.674457, mse: 868.402283, mean_q: 42.445717\n",
      "  5088/50000: episode: 79, duration: 2.611s, episode steps: 174, steps per second:  67, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.343243, mse: 945.804260, mean_q: 44.207939\n",
      "  5257/50000: episode: 80, duration: 2.539s, episode steps: 169, steps per second:  67, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 3.441414, mse: 1006.760559, mean_q: 45.600258\n",
      "  5421/50000: episode: 81, duration: 2.479s, episode steps: 164, steps per second:  66, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 3.331363, mse: 1054.083862, mean_q: 46.679741\n",
      "  5574/50000: episode: 82, duration: 2.293s, episode steps: 153, steps per second:  67, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 2.632918, mse: 1124.637085, mean_q: 48.072697\n",
      "  5774/50000: episode: 83, duration: 2.987s, episode steps: 200, steps per second:  67, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.995083, mse: 1176.867676, mean_q: 49.167274\n",
      "  5960/50000: episode: 84, duration: 2.740s, episode steps: 186, steps per second:  68, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.023310, mse: 1258.666138, mean_q: 50.883236\n",
      "  6158/50000: episode: 85, duration: 2.971s, episode steps: 198, steps per second:  67, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.133402, mse: 1321.510132, mean_q: 52.119514\n",
      "  6338/50000: episode: 86, duration: 2.734s, episode steps: 180, steps per second:  66, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.016682, mse: 1383.655762, mean_q: 53.149143\n",
      "  6501/50000: episode: 87, duration: 2.450s, episode steps: 163, steps per second:  67, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.002010, mse: 1450.810181, mean_q: 54.361748\n",
      "  6667/50000: episode: 88, duration: 2.571s, episode steps: 166, steps per second:  65, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 3.437743, mse: 1522.044800, mean_q: 55.749023\n",
      "  6852/50000: episode: 89, duration: 2.770s, episode steps: 185, steps per second:  67, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 4.908282, mse: 1576.626709, mean_q: 56.506451\n",
      "  7017/50000: episode: 90, duration: 2.453s, episode steps: 165, steps per second:  67, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 3.161801, mse: 1652.570068, mean_q: 57.912514\n",
      "  7177/50000: episode: 91, duration: 2.369s, episode steps: 160, steps per second:  68, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.984818, mse: 1687.332031, mean_q: 58.374481\n",
      "  7344/50000: episode: 92, duration: 2.520s, episode steps: 167, steps per second:  66, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.629490, mse: 1750.182983, mean_q: 59.429382\n",
      "  7513/50000: episode: 93, duration: 2.500s, episode steps: 169, steps per second:  68, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 3.124139, mse: 1827.532959, mean_q: 60.725903\n",
      "  7677/50000: episode: 94, duration: 2.476s, episode steps: 164, steps per second:  66, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.391627, mse: 1900.903809, mean_q: 61.829632\n",
      "  7887/50000: episode: 95, duration: 3.183s, episode steps: 210, steps per second:  66, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.537294, mse: 1974.614624, mean_q: 62.871418\n",
      "  8054/50000: episode: 96, duration: 2.453s, episode steps: 167, steps per second:  68, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 4.720892, mse: 2037.839600, mean_q: 63.770351\n",
      "  8209/50000: episode: 97, duration: 2.275s, episode steps: 155, steps per second:  68, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 3.906202, mse: 2094.933105, mean_q: 64.442131\n",
      "  8401/50000: episode: 98, duration: 2.914s, episode steps: 192, steps per second:  66, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 4.961055, mse: 2132.197021, mean_q: 65.172279\n",
      "  8575/50000: episode: 99, duration: 2.534s, episode steps: 174, steps per second:  69, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 4.511887, mse: 2198.041748, mean_q: 65.970863\n",
      "  8739/50000: episode: 100, duration: 2.452s, episode steps: 164, steps per second:  67, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 3.766148, mse: 2244.900879, mean_q: 66.519066\n",
      "  8905/50000: episode: 101, duration: 2.458s, episode steps: 166, steps per second:  68, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.158712, mse: 2353.630371, mean_q: 68.265717\n",
      "  9082/50000: episode: 102, duration: 2.623s, episode steps: 177, steps per second:  67, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.196472, mse: 2378.901123, mean_q: 68.509674\n",
      "  9261/50000: episode: 103, duration: 2.630s, episode steps: 179, steps per second:  68, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.885640, mse: 2431.754150, mean_q: 68.848831\n",
      "  9441/50000: episode: 104, duration: 2.684s, episode steps: 180, steps per second:  67, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.899911, mse: 2518.959961, mean_q: 70.216637\n",
      "  9608/50000: episode: 105, duration: 2.508s, episode steps: 167, steps per second:  67, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 2.984418, mse: 2561.563721, mean_q: 70.709671\n",
      "  9781/50000: episode: 106, duration: 2.565s, episode steps: 173, steps per second:  67, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 4.527801, mse: 2616.002441, mean_q: 71.466232\n",
      "  9985/50000: episode: 107, duration: 3.015s, episode steps: 204, steps per second:  68, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 2.411150, mse: 2674.311279, mean_q: 72.152916\n",
      " 10194/50000: episode: 108, duration: 3.136s, episode steps: 209, steps per second:  67, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.461327, mse: 2734.127197, mean_q: 72.740494\n",
      " 10392/50000: episode: 109, duration: 2.945s, episode steps: 198, steps per second:  67, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.231974, mse: 2794.954834, mean_q: 73.382004\n",
      " 10561/50000: episode: 110, duration: 2.508s, episode steps: 169, steps per second:  67, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.389512, mse: 2881.577637, mean_q: 74.464928\n",
      " 10730/50000: episode: 111, duration: 2.539s, episode steps: 169, steps per second:  67, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 4.196246, mse: 2910.832031, mean_q: 74.649513\n",
      " 10911/50000: episode: 112, duration: 2.751s, episode steps: 181, steps per second:  66, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.035516, mse: 3001.266113, mean_q: 75.823051\n",
      " 11156/50000: episode: 113, duration: 3.720s, episode steps: 245, steps per second:  66, episode reward: 245.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 3.593019, mse: 3024.292969, mean_q: 75.902092\n",
      " 11359/50000: episode: 114, duration: 3.126s, episode steps: 203, steps per second:  65, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.345700, mse: 3062.521973, mean_q: 76.209160\n",
      " 11555/50000: episode: 115, duration: 2.963s, episode steps: 196, steps per second:  66, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.718387, mse: 3144.236572, mean_q: 77.133148\n",
      " 11801/50000: episode: 116, duration: 3.760s, episode steps: 246, steps per second:  65, episode reward: 246.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.132081, mse: 3206.489746, mean_q: 77.637001\n",
      " 11985/50000: episode: 117, duration: 2.755s, episode steps: 184, steps per second:  67, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 3.431229, mse: 3249.216064, mean_q: 78.189743\n",
      " 12158/50000: episode: 118, duration: 2.600s, episode steps: 173, steps per second:  67, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.446811, mse: 3286.296875, mean_q: 78.665268\n",
      " 12327/50000: episode: 119, duration: 2.516s, episode steps: 169, steps per second:  67, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 5.613170, mse: 3357.507324, mean_q: 79.360458\n",
      " 12503/50000: episode: 120, duration: 2.636s, episode steps: 176, steps per second:  67, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.881140, mse: 3382.447021, mean_q: 79.564789\n",
      " 12678/50000: episode: 121, duration: 2.624s, episode steps: 175, steps per second:  67, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.133479, mse: 3376.725342, mean_q: 79.116829\n",
      " 12891/50000: episode: 122, duration: 3.143s, episode steps: 213, steps per second:  68, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 2.679843, mse: 3434.084473, mean_q: 79.748993\n",
      " 13049/50000: episode: 123, duration: 2.320s, episode steps: 158, steps per second:  68, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 4.791728, mse: 3429.886963, mean_q: 79.336830\n",
      " 13261/50000: episode: 124, duration: 3.162s, episode steps: 212, steps per second:  67, episode reward: 212.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 3.008235, mse: 3528.508789, mean_q: 80.835487\n",
      " 13460/50000: episode: 125, duration: 2.966s, episode steps: 199, steps per second:  67, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.513763, mse: 3554.638184, mean_q: 81.054474\n",
      " 13630/50000: episode: 126, duration: 2.540s, episode steps: 170, steps per second:  67, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.465 [0.000, 1.000],  loss: 4.222686, mse: 3566.796387, mean_q: 80.928909\n",
      " 13825/50000: episode: 127, duration: 2.885s, episode steps: 195, steps per second:  68, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 1.994958, mse: 3526.579590, mean_q: 80.289116\n",
      " 14026/50000: episode: 128, duration: 2.965s, episode steps: 201, steps per second:  68, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 5.442934, mse: 3576.352539, mean_q: 81.028725\n",
      " 14225/50000: episode: 129, duration: 2.948s, episode steps: 199, steps per second:  67, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 3.669125, mse: 3622.888916, mean_q: 81.461250\n",
      " 14428/50000: episode: 130, duration: 3.021s, episode steps: 203, steps per second:  67, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 3.549390, mse: 3653.634277, mean_q: 81.589226\n",
      " 14590/50000: episode: 131, duration: 2.424s, episode steps: 162, steps per second:  67, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 3.366169, mse: 3665.774414, mean_q: 81.846558\n",
      " 14827/50000: episode: 132, duration: 3.569s, episode steps: 237, steps per second:  66, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 4.605288, mse: 3682.882812, mean_q: 81.762764\n",
      " 15033/50000: episode: 133, duration: 3.069s, episode steps: 206, steps per second:  67, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 5.230999, mse: 3675.951416, mean_q: 81.505066\n",
      " 15206/50000: episode: 134, duration: 2.590s, episode steps: 173, steps per second:  67, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 3.921921, mse: 3726.003662, mean_q: 82.257484\n",
      " 15390/50000: episode: 135, duration: 2.726s, episode steps: 184, steps per second:  68, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.220664, mse: 3746.437500, mean_q: 82.574760\n",
      " 15605/50000: episode: 136, duration: 3.164s, episode steps: 215, steps per second:  68, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.470 [0.000, 1.000],  loss: 3.771481, mse: 3753.148926, mean_q: 82.623749\n",
      " 15780/50000: episode: 137, duration: 2.636s, episode steps: 175, steps per second:  66, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.463 [0.000, 1.000],  loss: 2.576728, mse: 3752.912109, mean_q: 82.543869\n",
      " 15966/50000: episode: 138, duration: 2.836s, episode steps: 186, steps per second:  66, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.585084, mse: 3793.288086, mean_q: 83.042992\n",
      " 16154/50000: episode: 139, duration: 2.762s, episode steps: 188, steps per second:  68, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 2.984963, mse: 3808.945557, mean_q: 83.240013\n",
      " 16333/50000: episode: 140, duration: 2.637s, episode steps: 179, steps per second:  68, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 3.148005, mse: 3787.762451, mean_q: 82.697266\n",
      " 16525/50000: episode: 141, duration: 2.854s, episode steps: 192, steps per second:  67, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.122353, mse: 3804.913330, mean_q: 82.660316\n",
      " 16699/50000: episode: 142, duration: 2.646s, episode steps: 174, steps per second:  66, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.466 [0.000, 1.000],  loss: 3.933597, mse: 3824.870605, mean_q: 83.224617\n",
      " 16909/50000: episode: 143, duration: 3.125s, episode steps: 210, steps per second:  67, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.101857, mse: 3887.108398, mean_q: 83.865067\n",
      " 17085/50000: episode: 144, duration: 2.723s, episode steps: 176, steps per second:  65, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.460 [0.000, 1.000],  loss: 4.465060, mse: 3887.140625, mean_q: 83.721230\n",
      " 17314/50000: episode: 145, duration: 3.460s, episode steps: 229, steps per second:  66, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.472 [0.000, 1.000],  loss: 2.284486, mse: 3859.795898, mean_q: 83.462395\n",
      " 17485/50000: episode: 146, duration: 2.568s, episode steps: 171, steps per second:  67, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.401077, mse: 3866.917480, mean_q: 83.471214\n",
      " 17669/50000: episode: 147, duration: 2.792s, episode steps: 184, steps per second:  66, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 2.931058, mse: 3903.157959, mean_q: 83.915062\n",
      " 17897/50000: episode: 148, duration: 3.459s, episode steps: 228, steps per second:  66, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.776098, mse: 3971.398682, mean_q: 84.540123\n",
      " 18089/50000: episode: 149, duration: 2.865s, episode steps: 192, steps per second:  67, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.469 [0.000, 1.000],  loss: 4.989063, mse: 3887.231689, mean_q: 83.344429\n",
      " 18241/50000: episode: 150, duration: 2.307s, episode steps: 152, steps per second:  66, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.105486, mse: 3856.061768, mean_q: 83.030388\n",
      " 18428/50000: episode: 151, duration: 2.790s, episode steps: 187, steps per second:  67, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 2.260145, mse: 3910.371582, mean_q: 83.348732\n",
      " 18597/50000: episode: 152, duration: 2.532s, episode steps: 169, steps per second:  67, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 6.018410, mse: 3906.590332, mean_q: 83.636185\n",
      " 18781/50000: episode: 153, duration: 2.754s, episode steps: 184, steps per second:  67, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 4.170296, mse: 3945.051514, mean_q: 84.130714\n",
      " 18966/50000: episode: 154, duration: 2.738s, episode steps: 185, steps per second:  68, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 6.951504, mse: 3929.294922, mean_q: 83.797806\n",
      " 19214/50000: episode: 155, duration: 3.669s, episode steps: 248, steps per second:  68, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.804515, mse: 3881.768555, mean_q: 83.256386\n",
      " 19396/50000: episode: 156, duration: 2.773s, episode steps: 182, steps per second:  66, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 3.031764, mse: 3923.716309, mean_q: 83.852036\n",
      " 19624/50000: episode: 157, duration: 3.427s, episode steps: 228, steps per second:  67, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.632015, mse: 3923.793213, mean_q: 83.530273\n",
      " 19829/50000: episode: 158, duration: 3.106s, episode steps: 205, steps per second:  66, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 2.653972, mse: 3861.034668, mean_q: 82.729973\n",
      " 20129/50000: episode: 159, duration: 4.621s, episode steps: 300, steps per second:  65, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 2.447175, mse: 3907.941650, mean_q: 83.270180\n",
      " 20429/50000: episode: 160, duration: 4.481s, episode steps: 300, steps per second:  67, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.703220, mse: 3986.132568, mean_q: 84.171349\n",
      " 20627/50000: episode: 161, duration: 2.947s, episode steps: 198, steps per second:  67, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.708273, mse: 4064.290039, mean_q: 84.610229\n",
      " 20807/50000: episode: 162, duration: 2.699s, episode steps: 180, steps per second:  67, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 4.395093, mse: 4084.468750, mean_q: 84.775513\n",
      " 21046/50000: episode: 163, duration: 3.533s, episode steps: 239, steps per second:  68, episode reward: 239.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.701951, mse: 4163.311035, mean_q: 85.639984\n",
      " 21205/50000: episode: 164, duration: 2.427s, episode steps: 159, steps per second:  66, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 5.694760, mse: 4116.545898, mean_q: 85.080315\n",
      " 21470/50000: episode: 165, duration: 4.021s, episode steps: 265, steps per second:  66, episode reward: 265.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 8.160672, mse: 4151.763184, mean_q: 85.471100\n",
      " 21673/50000: episode: 166, duration: 3.024s, episode steps: 203, steps per second:  67, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 3.402934, mse: 4091.904541, mean_q: 84.569832\n",
      " 21881/50000: episode: 167, duration: 3.113s, episode steps: 208, steps per second:  67, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 4.000397, mse: 4086.233398, mean_q: 84.529228\n",
      " 22042/50000: episode: 168, duration: 2.415s, episode steps: 161, steps per second:  67, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.402898, mse: 4136.906738, mean_q: 85.354500\n",
      " 22251/50000: episode: 169, duration: 3.105s, episode steps: 209, steps per second:  67, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 3.756197, mse: 4139.581055, mean_q: 85.221069\n",
      " 22489/50000: episode: 170, duration: 3.574s, episode steps: 238, steps per second:  67, episode reward: 238.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 4.738928, mse: 4174.355957, mean_q: 85.504707\n",
      " 22763/50000: episode: 171, duration: 4.117s, episode steps: 274, steps per second:  67, episode reward: 274.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 2.714866, mse: 4127.054688, mean_q: 85.037651\n",
      " 22991/50000: episode: 172, duration: 3.381s, episode steps: 228, steps per second:  67, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.292000, mse: 4170.650391, mean_q: 85.419731\n",
      " 23260/50000: episode: 173, duration: 4.036s, episode steps: 269, steps per second:  67, episode reward: 269.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 4.293565, mse: 4182.552734, mean_q: 85.369217\n",
      " 23456/50000: episode: 174, duration: 2.995s, episode steps: 196, steps per second:  65, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 5.784615, mse: 4292.243652, mean_q: 86.813446\n",
      " 23632/50000: episode: 175, duration: 2.632s, episode steps: 176, steps per second:  67, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.861069, mse: 4248.956055, mean_q: 86.068596\n",
      " 23852/50000: episode: 176, duration: 3.309s, episode steps: 220, steps per second:  66, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.437424, mse: 4218.845215, mean_q: 85.944427\n",
      " 24011/50000: episode: 177, duration: 2.359s, episode steps: 159, steps per second:  67, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.380474, mse: 4287.002930, mean_q: 86.527328\n",
      " 24176/50000: episode: 178, duration: 2.517s, episode steps: 165, steps per second:  66, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 3.147332, mse: 4276.237305, mean_q: 86.419167\n",
      " 24363/50000: episode: 179, duration: 2.807s, episode steps: 187, steps per second:  67, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.566454, mse: 4259.628906, mean_q: 86.181358\n",
      " 24634/50000: episode: 180, duration: 4.044s, episode steps: 271, steps per second:  67, episode reward: 271.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 2.706180, mse: 4261.672363, mean_q: 86.160629\n",
      " 24919/50000: episode: 181, duration: 4.259s, episode steps: 285, steps per second:  67, episode reward: 285.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 2.775797, mse: 4303.001953, mean_q: 86.698311\n",
      " 25130/50000: episode: 182, duration: 3.131s, episode steps: 211, steps per second:  67, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.107419, mse: 4260.754883, mean_q: 86.128120\n",
      " 25352/50000: episode: 183, duration: 3.334s, episode steps: 222, steps per second:  67, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 2.311499, mse: 4316.196289, mean_q: 87.116158\n",
      " 25605/50000: episode: 184, duration: 3.792s, episode steps: 253, steps per second:  67, episode reward: 253.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.047631, mse: 4311.439453, mean_q: 86.733421\n",
      " 25762/50000: episode: 185, duration: 2.399s, episode steps: 157, steps per second:  65, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.056345, mse: 4296.445801, mean_q: 86.507889\n",
      " 25923/50000: episode: 186, duration: 2.445s, episode steps: 161, steps per second:  66, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.220952, mse: 4274.097656, mean_q: 86.056679\n",
      " 26159/50000: episode: 187, duration: 3.573s, episode steps: 236, steps per second:  66, episode reward: 236.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.762672, mse: 4368.917969, mean_q: 87.244370\n",
      " 26337/50000: episode: 188, duration: 2.676s, episode steps: 178, steps per second:  67, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.979198, mse: 4373.243164, mean_q: 86.995613\n",
      " 26490/50000: episode: 189, duration: 2.306s, episode steps: 153, steps per second:  66, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 4.050848, mse: 4345.058594, mean_q: 86.934944\n",
      " 26700/50000: episode: 190, duration: 3.167s, episode steps: 210, steps per second:  66, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.781533, mse: 4379.574219, mean_q: 87.020874\n",
      " 26858/50000: episode: 191, duration: 2.427s, episode steps: 158, steps per second:  65, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.038024, mse: 4406.229980, mean_q: 87.568314\n",
      " 27027/50000: episode: 192, duration: 2.578s, episode steps: 169, steps per second:  66, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.903077, mse: 4419.008789, mean_q: 87.387993\n",
      " 27208/50000: episode: 193, duration: 2.738s, episode steps: 181, steps per second:  66, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.923792, mse: 4369.991699, mean_q: 86.690575\n",
      " 27380/50000: episode: 194, duration: 2.847s, episode steps: 172, steps per second:  60, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.452710, mse: 4420.960938, mean_q: 87.467537\n",
      " 27549/50000: episode: 195, duration: 2.903s, episode steps: 169, steps per second:  58, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 3.981033, mse: 4307.113770, mean_q: 86.291664\n",
      " 27707/50000: episode: 196, duration: 2.386s, episode steps: 158, steps per second:  66, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.497518, mse: 4331.333984, mean_q: 86.160820\n",
      " 27868/50000: episode: 197, duration: 2.472s, episode steps: 161, steps per second:  65, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.757982, mse: 4374.633301, mean_q: 86.564255\n",
      " 28041/50000: episode: 198, duration: 2.664s, episode steps: 173, steps per second:  65, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.868821, mse: 4305.370605, mean_q: 85.490326\n",
      " 28197/50000: episode: 199, duration: 2.422s, episode steps: 156, steps per second:  64, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.550005, mse: 4365.360352, mean_q: 86.189041\n",
      " 28365/50000: episode: 200, duration: 2.577s, episode steps: 168, steps per second:  65, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.919850, mse: 4311.534180, mean_q: 85.614487\n",
      " 28519/50000: episode: 201, duration: 2.358s, episode steps: 154, steps per second:  65, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.772740, mse: 4329.986328, mean_q: 85.721680\n",
      " 28685/50000: episode: 202, duration: 2.587s, episode steps: 166, steps per second:  64, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.350763, mse: 4311.703613, mean_q: 85.342644\n",
      " 28859/50000: episode: 203, duration: 2.706s, episode steps: 174, steps per second:  64, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.613374, mse: 4342.379883, mean_q: 85.919266\n",
      " 29016/50000: episode: 204, duration: 2.410s, episode steps: 157, steps per second:  65, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.711226, mse: 4368.219238, mean_q: 86.030136\n",
      " 29190/50000: episode: 205, duration: 2.583s, episode steps: 174, steps per second:  67, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 4.134315, mse: 4262.522949, mean_q: 85.064163\n",
      " 29355/50000: episode: 206, duration: 2.480s, episode steps: 165, steps per second:  67, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.930663, mse: 4286.774902, mean_q: 85.112846\n",
      " 29518/50000: episode: 207, duration: 2.484s, episode steps: 163, steps per second:  66, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.571353, mse: 4315.911621, mean_q: 85.205879\n",
      " 29684/50000: episode: 208, duration: 2.528s, episode steps: 166, steps per second:  66, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.967751, mse: 4276.637207, mean_q: 84.822250\n",
      " 29846/50000: episode: 209, duration: 2.438s, episode steps: 162, steps per second:  66, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.680403, mse: 4271.790527, mean_q: 84.627693\n",
      " 30015/50000: episode: 210, duration: 2.574s, episode steps: 169, steps per second:  66, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.591784, mse: 4240.249023, mean_q: 83.921944\n",
      " 30186/50000: episode: 211, duration: 2.632s, episode steps: 171, steps per second:  65, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 5.454263, mse: 4259.880859, mean_q: 84.306023\n",
      " 30387/50000: episode: 212, duration: 3.081s, episode steps: 201, steps per second:  65, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.610871, mse: 4282.564453, mean_q: 84.742966\n",
      " 30548/50000: episode: 213, duration: 2.509s, episode steps: 161, steps per second:  64, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.156237, mse: 4288.545898, mean_q: 84.800774\n",
      " 30703/50000: episode: 214, duration: 2.435s, episode steps: 155, steps per second:  64, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.739570, mse: 4221.015137, mean_q: 83.984299\n",
      " 30867/50000: episode: 215, duration: 2.527s, episode steps: 164, steps per second:  65, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.754169, mse: 4167.434082, mean_q: 82.943550\n",
      " 31026/50000: episode: 216, duration: 2.424s, episode steps: 159, steps per second:  66, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.789079, mse: 4227.402832, mean_q: 83.777214\n",
      " 31203/50000: episode: 217, duration: 2.706s, episode steps: 177, steps per second:  65, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.959822, mse: 4178.732422, mean_q: 83.285683\n",
      " 31452/50000: episode: 218, duration: 3.840s, episode steps: 249, steps per second:  65, episode reward: 249.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.861701, mse: 4159.514648, mean_q: 83.219971\n",
      " 31675/50000: episode: 219, duration: 3.369s, episode steps: 223, steps per second:  66, episode reward: 223.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.545065, mse: 4211.670410, mean_q: 83.820023\n",
      " 31869/50000: episode: 220, duration: 2.953s, episode steps: 194, steps per second:  66, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.828135, mse: 4177.518066, mean_q: 83.467567\n",
      " 32128/50000: episode: 221, duration: 3.973s, episode steps: 259, steps per second:  65, episode reward: 259.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.609229, mse: 4194.600586, mean_q: 83.774826\n",
      " 32295/50000: episode: 222, duration: 2.537s, episode steps: 167, steps per second:  66, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.987965, mse: 4229.031250, mean_q: 83.925430\n",
      " 32468/50000: episode: 223, duration: 2.643s, episode steps: 173, steps per second:  65, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.698799, mse: 4103.356445, mean_q: 82.412018\n",
      " 32657/50000: episode: 224, duration: 2.914s, episode steps: 189, steps per second:  65, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 3.697984, mse: 4079.480469, mean_q: 82.489960\n",
      " 32841/50000: episode: 225, duration: 2.830s, episode steps: 184, steps per second:  65, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.744447, mse: 4098.889160, mean_q: 82.508064\n",
      " 33019/50000: episode: 226, duration: 2.734s, episode steps: 178, steps per second:  65, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.799599, mse: 4126.902344, mean_q: 82.999870\n",
      " 33214/50000: episode: 227, duration: 2.995s, episode steps: 195, steps per second:  65, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 1.439094, mse: 4106.777344, mean_q: 82.685066\n",
      " 33406/50000: episode: 228, duration: 2.878s, episode steps: 192, steps per second:  67, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.174470, mse: 4113.726562, mean_q: 83.160599\n",
      " 33571/50000: episode: 229, duration: 2.506s, episode steps: 165, steps per second:  66, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 4.025266, mse: 4070.101562, mean_q: 82.567543\n",
      " 33779/50000: episode: 230, duration: 3.139s, episode steps: 208, steps per second:  66, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.660020, mse: 4049.423096, mean_q: 82.262329\n",
      " 34004/50000: episode: 231, duration: 3.407s, episode steps: 225, steps per second:  66, episode reward: 225.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 2.061657, mse: 4007.943115, mean_q: 81.492950\n",
      " 34148/50000: episode: 232, duration: 2.186s, episode steps: 144, steps per second:  66, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.866077, mse: 4128.578125, mean_q: 82.904999\n",
      " 34448/50000: episode: 233, duration: 4.526s, episode steps: 300, steps per second:  66, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.661617, mse: 4046.390381, mean_q: 82.117256\n",
      " 34618/50000: episode: 234, duration: 2.599s, episode steps: 170, steps per second:  65, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.786926, mse: 4000.722168, mean_q: 81.336021\n",
      " 34907/50000: episode: 235, duration: 4.359s, episode steps: 289, steps per second:  66, episode reward: 289.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 1.513074, mse: 4025.193848, mean_q: 81.810516\n",
      " 35080/50000: episode: 236, duration: 2.622s, episode steps: 173, steps per second:  66, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.839536, mse: 4056.174072, mean_q: 82.252609\n",
      " 35243/50000: episode: 237, duration: 2.465s, episode steps: 163, steps per second:  66, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.794081, mse: 4069.325195, mean_q: 82.457893\n",
      " 35448/50000: episode: 238, duration: 3.172s, episode steps: 205, steps per second:  65, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.883889, mse: 3958.572510, mean_q: 81.241104\n",
      " 35712/50000: episode: 239, duration: 4.038s, episode steps: 264, steps per second:  65, episode reward: 264.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 2.178139, mse: 4010.792725, mean_q: 81.911331\n",
      " 35917/50000: episode: 240, duration: 3.118s, episode steps: 205, steps per second:  66, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.969204, mse: 3976.278076, mean_q: 81.593292\n",
      " 36113/50000: episode: 241, duration: 2.962s, episode steps: 196, steps per second:  66, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.100042, mse: 3966.809326, mean_q: 81.525833\n",
      " 36320/50000: episode: 242, duration: 3.175s, episode steps: 207, steps per second:  65, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.807320, mse: 3948.616943, mean_q: 81.337875\n",
      " 36496/50000: episode: 243, duration: 2.696s, episode steps: 176, steps per second:  65, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 3.694067, mse: 4057.729004, mean_q: 82.485664\n",
      " 36796/50000: episode: 244, duration: 4.573s, episode steps: 300, steps per second:  66, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 1.938109, mse: 3932.612549, mean_q: 81.272530\n",
      " 36989/50000: episode: 245, duration: 2.949s, episode steps: 193, steps per second:  65, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 3.352547, mse: 3887.552490, mean_q: 80.861351\n",
      " 37185/50000: episode: 246, duration: 3.036s, episode steps: 196, steps per second:  65, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 3.036331, mse: 3927.523682, mean_q: 81.146950\n",
      " 37445/50000: episode: 247, duration: 3.983s, episode steps: 260, steps per second:  65, episode reward: 260.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.623500, mse: 3946.690186, mean_q: 81.362183\n",
      " 37613/50000: episode: 248, duration: 2.527s, episode steps: 168, steps per second:  66, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.629704, mse: 3887.650391, mean_q: 80.807442\n",
      " 37868/50000: episode: 249, duration: 3.788s, episode steps: 255, steps per second:  67, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 2.109494, mse: 3864.500488, mean_q: 80.745430\n",
      " 38058/50000: episode: 250, duration: 2.841s, episode steps: 190, steps per second:  67, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.842052, mse: 3901.618408, mean_q: 80.980438\n",
      " 38223/50000: episode: 251, duration: 2.485s, episode steps: 165, steps per second:  66, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.080866, mse: 3810.653809, mean_q: 79.799751\n",
      " 38427/50000: episode: 252, duration: 3.063s, episode steps: 204, steps per second:  67, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.594373, mse: 3844.281250, mean_q: 80.178299\n",
      " 38584/50000: episode: 253, duration: 2.396s, episode steps: 157, steps per second:  66, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 4.772069, mse: 3782.111572, mean_q: 79.357697\n",
      " 38749/50000: episode: 254, duration: 2.504s, episode steps: 165, steps per second:  66, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.928443, mse: 3762.473389, mean_q: 79.312607\n",
      " 38943/50000: episode: 255, duration: 2.925s, episode steps: 194, steps per second:  66, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.500730, mse: 3786.038574, mean_q: 79.728226\n",
      " 39126/50000: episode: 256, duration: 2.751s, episode steps: 183, steps per second:  67, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.960833, mse: 3803.475342, mean_q: 79.888321\n",
      " 39292/50000: episode: 257, duration: 2.501s, episode steps: 166, steps per second:  66, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.875446, mse: 3822.793701, mean_q: 80.191292\n",
      " 39477/50000: episode: 258, duration: 2.801s, episode steps: 185, steps per second:  66, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.856322, mse: 3710.704834, mean_q: 78.900932\n",
      " 39662/50000: episode: 259, duration: 2.865s, episode steps: 185, steps per second:  65, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.972229, mse: 3702.126953, mean_q: 79.019623\n",
      " 39853/50000: episode: 260, duration: 2.921s, episode steps: 191, steps per second:  65, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.218744, mse: 3830.263428, mean_q: 80.386200\n",
      " 40028/50000: episode: 261, duration: 2.651s, episode steps: 175, steps per second:  66, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 3.584809, mse: 3752.054932, mean_q: 79.755852\n",
      " 40243/50000: episode: 262, duration: 3.262s, episode steps: 215, steps per second:  66, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.101569, mse: 3759.514648, mean_q: 79.718277\n",
      " 40422/50000: episode: 263, duration: 2.683s, episode steps: 179, steps per second:  67, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.747050, mse: 3731.275146, mean_q: 79.552284\n",
      " 40632/50000: episode: 264, duration: 3.245s, episode steps: 210, steps per second:  65, episode reward: 210.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.544850, mse: 3764.409180, mean_q: 80.014565\n",
      " 40817/50000: episode: 265, duration: 2.822s, episode steps: 185, steps per second:  66, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.622426, mse: 3701.721924, mean_q: 79.154854\n",
      " 41011/50000: episode: 266, duration: 2.965s, episode steps: 194, steps per second:  65, episode reward: 194.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.070751, mse: 3655.363281, mean_q: 78.605415\n",
      " 41311/50000: episode: 267, duration: 4.607s, episode steps: 300, steps per second:  65, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.550305, mse: 3722.949951, mean_q: 79.538765\n",
      " 41519/50000: episode: 268, duration: 3.167s, episode steps: 208, steps per second:  66, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.467592, mse: 3742.161621, mean_q: 79.714348\n",
      " 41726/50000: episode: 269, duration: 3.148s, episode steps: 207, steps per second:  66, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.188961, mse: 3707.908203, mean_q: 79.260757\n",
      " 41905/50000: episode: 270, duration: 2.693s, episode steps: 179, steps per second:  66, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.069863, mse: 3739.361816, mean_q: 79.617981\n",
      " 42109/50000: episode: 271, duration: 3.103s, episode steps: 204, steps per second:  66, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.563890, mse: 3709.470215, mean_q: 79.160988\n",
      " 42291/50000: episode: 272, duration: 2.775s, episode steps: 182, steps per second:  66, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.748599, mse: 3712.667969, mean_q: 79.410004\n",
      " 42476/50000: episode: 273, duration: 2.834s, episode steps: 185, steps per second:  65, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.846696, mse: 3704.181641, mean_q: 79.182800\n",
      " 42694/50000: episode: 274, duration: 3.282s, episode steps: 218, steps per second:  66, episode reward: 218.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.747280, mse: 3684.793213, mean_q: 79.092010\n",
      " 42866/50000: episode: 275, duration: 2.643s, episode steps: 172, steps per second:  65, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.828398, mse: 3689.515381, mean_q: 79.074051\n",
      " 43050/50000: episode: 276, duration: 2.775s, episode steps: 184, steps per second:  66, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.656828, mse: 3617.187500, mean_q: 78.526688\n",
      " 43240/50000: episode: 277, duration: 2.854s, episode steps: 190, steps per second:  67, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.522033, mse: 3757.626221, mean_q: 80.028976\n",
      " 43427/50000: episode: 278, duration: 2.884s, episode steps: 187, steps per second:  65, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.209728, mse: 3730.869629, mean_q: 79.434669\n",
      " 43612/50000: episode: 279, duration: 2.856s, episode steps: 185, steps per second:  65, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.410182, mse: 3707.039795, mean_q: 79.221779\n",
      " 43809/50000: episode: 280, duration: 2.974s, episode steps: 197, steps per second:  66, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.825572, mse: 3702.802734, mean_q: 79.148430\n",
      " 44053/50000: episode: 281, duration: 3.710s, episode steps: 244, steps per second:  66, episode reward: 244.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.571155, mse: 3745.398926, mean_q: 79.996834\n",
      " 44280/50000: episode: 282, duration: 3.501s, episode steps: 227, steps per second:  65, episode reward: 227.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.434211, mse: 3700.137939, mean_q: 79.376587\n",
      " 44505/50000: episode: 283, duration: 3.427s, episode steps: 225, steps per second:  66, episode reward: 225.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.286765, mse: 3701.571289, mean_q: 79.329628\n",
      " 44742/50000: episode: 284, duration: 3.666s, episode steps: 237, steps per second:  65, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.551524, mse: 3723.387451, mean_q: 79.820808\n",
      " 44976/50000: episode: 285, duration: 3.598s, episode steps: 234, steps per second:  65, episode reward: 234.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.206243, mse: 3794.150146, mean_q: 80.518898\n",
      " 45189/50000: episode: 286, duration: 3.276s, episode steps: 213, steps per second:  65, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.222871, mse: 3809.725830, mean_q: 80.561012\n",
      " 45415/50000: episode: 287, duration: 3.469s, episode steps: 226, steps per second:  65, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 1.240329, mse: 3814.865234, mean_q: 80.653557\n",
      " 45641/50000: episode: 288, duration: 3.467s, episode steps: 226, steps per second:  65, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.473450, mse: 3829.990479, mean_q: 80.489006\n",
      " 45863/50000: episode: 289, duration: 3.392s, episode steps: 222, steps per second:  65, episode reward: 222.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.530542, mse: 3790.731934, mean_q: 80.464615\n",
      " 46140/50000: episode: 290, duration: 4.205s, episode steps: 277, steps per second:  66, episode reward: 277.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.044868, mse: 3897.963867, mean_q: 81.731155\n",
      " 46361/50000: episode: 291, duration: 3.435s, episode steps: 221, steps per second:  64, episode reward: 221.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.485667, mse: 3824.717285, mean_q: 80.987183\n",
      " 46635/50000: episode: 292, duration: 4.171s, episode steps: 274, steps per second:  66, episode reward: 274.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.902971, mse: 3948.070312, mean_q: 82.475601\n",
      " 46851/50000: episode: 293, duration: 3.349s, episode steps: 216, steps per second:  64, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.336478, mse: 3904.681396, mean_q: 82.128021\n",
      " 47057/50000: episode: 294, duration: 3.157s, episode steps: 206, steps per second:  65, episode reward: 206.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.339441, mse: 4004.794189, mean_q: 83.333832\n",
      " 47298/50000: episode: 295, duration: 3.700s, episode steps: 241, steps per second:  65, episode reward: 241.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.560478, mse: 3897.674316, mean_q: 82.208115\n",
      " 47545/50000: episode: 296, duration: 3.780s, episode steps: 247, steps per second:  65, episode reward: 247.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 2.378596, mse: 3971.220459, mean_q: 82.804153\n",
      " 47768/50000: episode: 297, duration: 3.410s, episode steps: 223, steps per second:  65, episode reward: 223.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.743044, mse: 4017.727051, mean_q: 83.167091\n",
      " 47996/50000: episode: 298, duration: 3.472s, episode steps: 228, steps per second:  66, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.205798, mse: 3955.584473, mean_q: 82.476891\n",
      " 48296/50000: episode: 299, duration: 4.593s, episode steps: 300, steps per second:  65, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 1.349618, mse: 4037.035889, mean_q: 83.405502\n",
      " 48596/50000: episode: 300, duration: 4.513s, episode steps: 300, steps per second:  66, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.698479, mse: 4179.654297, mean_q: 85.178024\n",
      " 48896/50000: episode: 301, duration: 4.518s, episode steps: 300, steps per second:  66, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.777578, mse: 4119.037598, mean_q: 84.410583\n",
      " 49194/50000: episode: 302, duration: 4.568s, episode steps: 298, steps per second:  65, episode reward: 298.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 2.004662, mse: 4222.269531, mean_q: 85.449463\n",
      " 49494/50000: episode: 303, duration: 4.531s, episode steps: 300, steps per second:  66, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.568840, mse: 4300.920410, mean_q: 86.428535\n",
      " 49794/50000: episode: 304, duration: 4.559s, episode steps: 300, steps per second:  66, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.440463, mse: 4379.718262, mean_q: 87.499260\n",
      "done, took 759.746 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 300.000, steps: 300\n",
      "Episode 2: reward: 300.000, steps: 300\n",
      "Episode 3: reward: 300.000, steps: 300\n",
      "Episode 4: reward: 300.000, steps: 300\n",
      "Episode 5: reward: 300.000, steps: 300\n",
      "Window_length: 4\n",
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    31/50000: episode: 1, duration: 3.257s, episode steps:  31, steps per second:  10, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.645 [0.000, 1.000],  loss: 0.441886, mse: 0.450916, mean_q: -0.001558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    51/50000: episode: 2, duration: 0.343s, episode steps:  20, steps per second:  58, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.322765, mse: 0.335538, mean_q: 0.110414\n",
      "    67/50000: episode: 3, duration: 0.270s, episode steps:  16, steps per second:  59, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.375 [0.000, 1.000],  loss: 0.257872, mse: 0.317981, mean_q: 0.338624\n",
      "    84/50000: episode: 4, duration: 0.284s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.198336, mse: 0.376641, mean_q: 0.567846\n",
      "    95/50000: episode: 5, duration: 0.196s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.148077, mse: 0.510622, mean_q: 0.763263\n",
      "   106/50000: episode: 6, duration: 0.190s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.111021, mse: 0.590241, mean_q: 0.893900\n",
      "   128/50000: episode: 7, duration: 0.359s, episode steps:  22, steps per second:  61, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.080761, mse: 0.760229, mean_q: 1.145380\n",
      "   141/50000: episode: 8, duration: 0.224s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.061402, mse: 0.881544, mean_q: 1.318986\n",
      "   171/50000: episode: 9, duration: 0.489s, episode steps:  30, steps per second:  61, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.052887, mse: 1.018966, mean_q: 1.437580\n",
      "   186/50000: episode: 10, duration: 0.247s, episode steps:  15, steps per second:  61, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.028372, mse: 1.205836, mean_q: 1.596634\n",
      "   218/50000: episode: 11, duration: 0.518s, episode steps:  32, steps per second:  62, episode reward: 32.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.052257, mse: 1.418872, mean_q: 1.727022\n",
      "   230/50000: episode: 12, duration: 0.202s, episode steps:  12, steps per second:  59, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.049882, mse: 1.654867, mean_q: 1.868587\n",
      "   251/50000: episode: 13, duration: 0.348s, episode steps:  21, steps per second:  60, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.571 [0.000, 1.000],  loss: 0.049772, mse: 1.897350, mean_q: 2.003336\n",
      "   267/50000: episode: 14, duration: 0.270s, episode steps:  16, steps per second:  59, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.050467, mse: 2.180318, mean_q: 2.123276\n",
      "   280/50000: episode: 15, duration: 0.230s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.058848, mse: 2.261678, mean_q: 2.151779\n",
      "   314/50000: episode: 16, duration: 0.535s, episode steps:  34, steps per second:  64, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.067650, mse: 2.754819, mean_q: 2.387896\n",
      "   331/50000: episode: 17, duration: 0.281s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.085047, mse: 3.143595, mean_q: 2.547953\n",
      "   341/50000: episode: 18, duration: 0.188s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.078618, mse: 3.437809, mean_q: 2.666882\n",
      "   363/50000: episode: 19, duration: 0.363s, episode steps:  22, steps per second:  61, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.318 [0.000, 1.000],  loss: 0.074069, mse: 3.833381, mean_q: 2.823356\n",
      "   377/50000: episode: 20, duration: 0.236s, episode steps:  14, steps per second:  59, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.080649, mse: 4.127483, mean_q: 2.928471\n",
      "   396/50000: episode: 21, duration: 0.315s, episode steps:  19, steps per second:  60, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.632 [0.000, 1.000],  loss: 0.108241, mse: 4.453279, mean_q: 3.056001\n",
      "   419/50000: episode: 22, duration: 0.384s, episode steps:  23, steps per second:  60, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.435 [0.000, 1.000],  loss: 0.100068, mse: 5.144160, mean_q: 3.237942\n",
      "   434/50000: episode: 23, duration: 0.254s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.117872, mse: 5.394445, mean_q: 3.328419\n",
      "   459/50000: episode: 24, duration: 0.408s, episode steps:  25, steps per second:  61, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.082636, mse: 6.178171, mean_q: 3.570748\n",
      "   487/50000: episode: 25, duration: 0.485s, episode steps:  28, steps per second:  58, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.393 [0.000, 1.000],  loss: 0.134403, mse: 6.737501, mean_q: 3.716827\n",
      "   498/50000: episode: 26, duration: 0.188s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.124026, mse: 7.328159, mean_q: 3.870749\n",
      "   509/50000: episode: 27, duration: 0.190s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.178524, mse: 7.768918, mean_q: 3.976317\n",
      "   531/50000: episode: 28, duration: 0.384s, episode steps:  22, steps per second:  57, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.152369, mse: 8.148818, mean_q: 4.088752\n",
      "   555/50000: episode: 29, duration: 0.392s, episode steps:  24, steps per second:  61, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.176202, mse: 8.906951, mean_q: 4.275841\n",
      "   585/50000: episode: 30, duration: 0.500s, episode steps:  30, steps per second:  60, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.433 [0.000, 1.000],  loss: 0.214623, mse: 9.736907, mean_q: 4.432394\n",
      "   602/50000: episode: 31, duration: 0.281s, episode steps:  17, steps per second:  61, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.167508, mse: 10.681973, mean_q: 4.628704\n",
      "   616/50000: episode: 32, duration: 0.234s, episode steps:  14, steps per second:  60, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.202357, mse: 11.029229, mean_q: 4.742278\n",
      "   653/50000: episode: 33, duration: 0.593s, episode steps:  37, steps per second:  62, episode reward: 37.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.177277, mse: 12.153810, mean_q: 4.942005\n",
      "   687/50000: episode: 34, duration: 0.550s, episode steps:  34, steps per second:  62, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.382 [0.000, 1.000],  loss: 0.237456, mse: 13.289282, mean_q: 5.151789\n",
      "   699/50000: episode: 35, duration: 0.214s, episode steps:  12, steps per second:  56, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.177238, mse: 14.498271, mean_q: 5.375505\n",
      "   712/50000: episode: 36, duration: 0.241s, episode steps:  13, steps per second:  54, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.231 [0.000, 1.000],  loss: 0.135404, mse: 15.604499, mean_q: 5.642694\n",
      "   773/50000: episode: 37, duration: 1.001s, episode steps:  61, steps per second:  61, episode reward: 61.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.168923, mse: 16.905800, mean_q: 5.838807\n",
      "   791/50000: episode: 38, duration: 0.300s, episode steps:  18, steps per second:  60, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.314911, mse: 18.113873, mean_q: 5.972952\n",
      "   824/50000: episode: 39, duration: 0.531s, episode steps:  33, steps per second:  62, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.197436, mse: 19.994015, mean_q: 6.333928\n",
      "   836/50000: episode: 40, duration: 0.228s, episode steps:  12, steps per second:  53, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.205773, mse: 20.821674, mean_q: 6.528337\n",
      "   857/50000: episode: 41, duration: 0.351s, episode steps:  21, steps per second:  60, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.188287, mse: 22.106005, mean_q: 6.646603\n",
      "   879/50000: episode: 42, duration: 0.357s, episode steps:  22, steps per second:  62, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.455 [0.000, 1.000],  loss: 0.166174, mse: 22.868940, mean_q: 6.725825\n",
      "   906/50000: episode: 43, duration: 0.451s, episode steps:  27, steps per second:  60, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 0.261948, mse: 23.729591, mean_q: 6.771261\n",
      "   916/50000: episode: 44, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.241105, mse: 25.494379, mean_q: 7.030213\n",
      "   945/50000: episode: 45, duration: 0.470s, episode steps:  29, steps per second:  62, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.241623, mse: 25.900328, mean_q: 7.069387\n",
      "   981/50000: episode: 46, duration: 0.628s, episode steps:  36, steps per second:  57, episode reward: 36.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.199258, mse: 27.521727, mean_q: 7.297205\n",
      "  1001/50000: episode: 47, duration: 0.330s, episode steps:  20, steps per second:  61, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.150372, mse: 29.074905, mean_q: 7.482680\n",
      "  1028/50000: episode: 48, duration: 0.448s, episode steps:  27, steps per second:  60, episode reward: 27.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.408828, mse: 29.495653, mean_q: 7.458790\n",
      "  1057/50000: episode: 49, duration: 0.467s, episode steps:  29, steps per second:  62, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.448 [0.000, 1.000],  loss: 0.183381, mse: 33.452141, mean_q: 8.079281\n",
      "  1079/50000: episode: 50, duration: 0.370s, episode steps:  22, steps per second:  59, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.305450, mse: 32.937885, mean_q: 7.913574\n",
      "  1114/50000: episode: 51, duration: 0.585s, episode steps:  35, steps per second:  60, episode reward: 35.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.457 [0.000, 1.000],  loss: 0.162894, mse: 34.539558, mean_q: 8.164865\n",
      "  1136/50000: episode: 52, duration: 0.381s, episode steps:  22, steps per second:  58, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.252283, mse: 36.662819, mean_q: 8.392359\n",
      "  1167/50000: episode: 53, duration: 0.513s, episode steps:  31, steps per second:  60, episode reward: 31.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.293088, mse: 38.991535, mean_q: 8.670532\n",
      "  1205/50000: episode: 54, duration: 0.639s, episode steps:  38, steps per second:  59, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.447 [0.000, 1.000],  loss: 0.384752, mse: 40.862476, mean_q: 8.859230\n",
      "  1220/50000: episode: 55, duration: 0.252s, episode steps:  15, steps per second:  60, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.201655, mse: 42.359360, mean_q: 9.083027\n",
      "  1288/50000: episode: 56, duration: 1.089s, episode steps:  68, steps per second:  62, episode reward: 68.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.221995, mse: 44.905567, mean_q: 9.337995\n",
      "  1350/50000: episode: 57, duration: 0.973s, episode steps:  62, steps per second:  64, episode reward: 62.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 0.312116, mse: 50.913136, mean_q: 9.994641\n",
      "  1380/50000: episode: 58, duration: 0.485s, episode steps:  30, steps per second:  62, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.284725, mse: 55.314648, mean_q: 10.478667\n",
      "  1421/50000: episode: 59, duration: 0.663s, episode steps:  41, steps per second:  62, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.281704, mse: 58.853428, mean_q: 10.857180\n",
      "  1450/50000: episode: 60, duration: 0.499s, episode steps:  29, steps per second:  58, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.586 [0.000, 1.000],  loss: 0.348521, mse: 60.247360, mean_q: 10.896946\n",
      "  1505/50000: episode: 61, duration: 0.882s, episode steps:  55, steps per second:  62, episode reward: 55.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.291584, mse: 65.707397, mean_q: 11.439671\n",
      "  1528/50000: episode: 62, duration: 0.392s, episode steps:  23, steps per second:  59, episode reward: 23.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.303136, mse: 68.518684, mean_q: 11.693013\n",
      "  1652/50000: episode: 63, duration: 2.026s, episode steps: 124, steps per second:  61, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.356726, mse: 76.418983, mean_q: 12.372679\n",
      "  1790/50000: episode: 64, duration: 2.223s, episode steps: 138, steps per second:  62, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.366888, mse: 90.942810, mean_q: 13.526017\n",
      "  1896/50000: episode: 65, duration: 1.711s, episode steps: 106, steps per second:  62, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 0.414819, mse: 109.186119, mean_q: 14.854914\n",
      "  2033/50000: episode: 66, duration: 2.208s, episode steps: 137, steps per second:  62, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.450307, mse: 125.044762, mean_q: 15.986264\n",
      "  2164/50000: episode: 67, duration: 2.091s, episode steps: 131, steps per second:  63, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 0.453324, mse: 145.655624, mean_q: 17.262074\n",
      "  2322/50000: episode: 68, duration: 2.503s, episode steps: 158, steps per second:  63, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.553952, mse: 171.713379, mean_q: 18.830164\n",
      "  2514/50000: episode: 69, duration: 3.052s, episode steps: 192, steps per second:  63, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.687045, mse: 205.050369, mean_q: 20.524382\n",
      "  2740/50000: episode: 70, duration: 3.534s, episode steps: 226, steps per second:  64, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.688196, mse: 245.547531, mean_q: 22.530529\n",
      "  2980/50000: episode: 71, duration: 3.698s, episode steps: 240, steps per second:  65, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 0.887243, mse: 298.016998, mean_q: 24.752180\n",
      "  3244/50000: episode: 72, duration: 4.167s, episode steps: 264, steps per second:  63, episode reward: 264.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 0.780504, mse: 358.941742, mean_q: 27.285334\n",
      "  3502/50000: episode: 73, duration: 3.984s, episode steps: 258, steps per second:  65, episode reward: 258.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 1.104366, mse: 433.505035, mean_q: 29.932705\n",
      "  3802/50000: episode: 74, duration: 4.728s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.345590, mse: 517.354980, mean_q: 32.637772\n",
      "  4083/50000: episode: 75, duration: 4.398s, episode steps: 281, steps per second:  64, episode reward: 281.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 1.503464, mse: 611.225403, mean_q: 35.488026\n",
      "  4383/50000: episode: 76, duration: 4.713s, episode steps: 300, steps per second:  64, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 1.727080, mse: 720.156677, mean_q: 38.510780\n",
      "  4683/50000: episode: 77, duration: 4.683s, episode steps: 300, steps per second:  64, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 1.923244, mse: 824.625916, mean_q: 41.083996\n",
      "  4983/50000: episode: 78, duration: 4.763s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 1.998885, mse: 940.032593, mean_q: 43.895458\n",
      "  5242/50000: episode: 79, duration: 4.137s, episode steps: 259, steps per second:  63, episode reward: 259.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 2.310390, mse: 1060.714966, mean_q: 46.708828\n",
      "  5494/50000: episode: 80, duration: 3.935s, episode steps: 252, steps per second:  64, episode reward: 252.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 2.826092, mse: 1157.494263, mean_q: 48.735138\n",
      "  5794/50000: episode: 81, duration: 4.633s, episode steps: 300, steps per second:  65, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.666103, mse: 1267.037476, mean_q: 50.960918\n",
      "  6082/50000: episode: 82, duration: 4.462s, episode steps: 288, steps per second:  65, episode reward: 288.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.331779, mse: 1397.596191, mean_q: 53.407955\n",
      "  6382/50000: episode: 83, duration: 4.678s, episode steps: 300, steps per second:  64, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.180389, mse: 1512.910645, mean_q: 55.544037\n",
      "  6682/50000: episode: 84, duration: 4.690s, episode steps: 300, steps per second:  64, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 4.000231, mse: 1624.573120, mean_q: 57.535240\n",
      "  6941/50000: episode: 85, duration: 4.000s, episode steps: 259, steps per second:  65, episode reward: 259.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 3.126076, mse: 1738.905884, mean_q: 59.507423\n",
      "  7241/50000: episode: 86, duration: 4.758s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 4.393014, mse: 1852.551270, mean_q: 61.343029\n",
      "  7541/50000: episode: 87, duration: 4.780s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 4.549079, mse: 1961.816040, mean_q: 63.113880\n",
      "  7841/50000: episode: 88, duration: 4.775s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 3.922883, mse: 2094.871338, mean_q: 65.225868\n",
      "  8126/50000: episode: 89, duration: 4.456s, episode steps: 285, steps per second:  64, episode reward: 285.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.438772, mse: 2208.583252, mean_q: 66.949738\n",
      "  8426/50000: episode: 90, duration: 4.757s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.025580, mse: 2315.443848, mean_q: 68.411293\n",
      "  8726/50000: episode: 91, duration: 4.786s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 5.998591, mse: 2422.637451, mean_q: 69.972015\n",
      "  8989/50000: episode: 92, duration: 4.246s, episode steps: 263, steps per second:  62, episode reward: 263.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.142610, mse: 2535.367920, mean_q: 71.553398\n",
      "  9289/50000: episode: 93, duration: 4.785s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 4.955740, mse: 2625.387939, mean_q: 72.765831\n",
      "  9584/50000: episode: 94, duration: 4.727s, episode steps: 295, steps per second:  62, episode reward: 295.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 5.189819, mse: 2737.970459, mean_q: 74.253502\n",
      "  9884/50000: episode: 95, duration: 4.815s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.515460, mse: 2832.696045, mean_q: 75.466331\n",
      " 10184/50000: episode: 96, duration: 4.778s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 5.946826, mse: 2918.715820, mean_q: 76.604843\n",
      " 10484/50000: episode: 97, duration: 4.731s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 4.880328, mse: 3036.577881, mean_q: 78.102516\n",
      " 10784/50000: episode: 98, duration: 4.827s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 5.379469, mse: 3136.981201, mean_q: 79.296158\n",
      " 11084/50000: episode: 99, duration: 4.828s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 6.411944, mse: 3207.475342, mean_q: 80.140533\n",
      " 11384/50000: episode: 100, duration: 4.845s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.856352, mse: 3279.342041, mean_q: 81.023720\n",
      " 11684/50000: episode: 101, duration: 4.862s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.930338, mse: 3369.691162, mean_q: 82.066383\n",
      " 11984/50000: episode: 102, duration: 4.814s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.226771, mse: 3442.874268, mean_q: 82.929535\n",
      " 12284/50000: episode: 103, duration: 4.850s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.642691, mse: 3516.172119, mean_q: 83.763916\n",
      " 12521/50000: episode: 104, duration: 3.848s, episode steps: 237, steps per second:  62, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 4.755654, mse: 3593.379639, mean_q: 84.683762\n",
      " 12821/50000: episode: 105, duration: 4.847s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.415861, mse: 3690.306152, mean_q: 85.724228\n",
      " 13121/50000: episode: 106, duration: 4.880s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.764265, mse: 3740.510010, mean_q: 86.073059\n",
      " 13338/50000: episode: 107, duration: 3.522s, episode steps: 217, steps per second:  62, episode reward: 217.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 6.306275, mse: 3786.883057, mean_q: 86.762489\n",
      " 13638/50000: episode: 108, duration: 4.851s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.705186, mse: 3852.863037, mean_q: 87.373764\n",
      " 13938/50000: episode: 109, duration: 4.810s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 6.583354, mse: 3903.745117, mean_q: 87.803253\n",
      " 14238/50000: episode: 110, duration: 4.804s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 7.318358, mse: 3970.175049, mean_q: 88.725166\n",
      " 14538/50000: episode: 111, duration: 4.864s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.154848, mse: 4000.877441, mean_q: 89.026237\n",
      " 14838/50000: episode: 112, duration: 4.877s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 7.070444, mse: 4071.379150, mean_q: 89.823349\n",
      " 15138/50000: episode: 113, duration: 4.795s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.990410, mse: 4152.643555, mean_q: 90.770409\n",
      " 15438/50000: episode: 114, duration: 4.880s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 7.785861, mse: 4190.752441, mean_q: 90.995964\n",
      " 15710/50000: episode: 115, duration: 4.328s, episode steps: 272, steps per second:  63, episode reward: 272.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 4.887047, mse: 4212.590820, mean_q: 91.170837\n",
      " 16010/50000: episode: 116, duration: 4.880s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.069086, mse: 4237.609375, mean_q: 91.245049\n",
      " 16310/50000: episode: 117, duration: 4.845s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 4.863901, mse: 4286.862793, mean_q: 91.879364\n",
      " 16569/50000: episode: 118, duration: 4.208s, episode steps: 259, steps per second:  62, episode reward: 259.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 6.995085, mse: 4305.430664, mean_q: 92.004601\n",
      " 16740/50000: episode: 119, duration: 2.788s, episode steps: 171, steps per second:  61, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 5.892707, mse: 4282.039551, mean_q: 91.512589\n",
      " 17040/50000: episode: 120, duration: 4.819s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 7.188680, mse: 4359.702148, mean_q: 92.268112\n",
      " 17340/50000: episode: 121, duration: 4.827s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.460302, mse: 4376.441895, mean_q: 92.404762\n",
      " 17513/50000: episode: 122, duration: 2.810s, episode steps: 173, steps per second:  62, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 9.654955, mse: 4403.307129, mean_q: 92.744629\n",
      " 17813/50000: episode: 123, duration: 4.900s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.001076, mse: 4392.819824, mean_q: 92.539841\n",
      " 18113/50000: episode: 124, duration: 4.790s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.423275, mse: 4385.492676, mean_q: 92.539665\n",
      " 18303/50000: episode: 125, duration: 3.053s, episode steps: 190, steps per second:  62, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 6.802241, mse: 4401.594727, mean_q: 92.634491\n",
      " 18603/50000: episode: 126, duration: 4.761s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 6.968623, mse: 4415.523926, mean_q: 92.679115\n",
      " 18868/50000: episode: 127, duration: 4.237s, episode steps: 265, steps per second:  63, episode reward: 265.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 7.021019, mse: 4426.987305, mean_q: 92.638573\n",
      " 19168/50000: episode: 128, duration: 4.787s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.590359, mse: 4433.609863, mean_q: 92.514671\n",
      " 19344/50000: episode: 129, duration: 2.799s, episode steps: 176, steps per second:  63, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 4.399740, mse: 4458.732910, mean_q: 92.705925\n",
      " 19644/50000: episode: 130, duration: 4.854s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 6.016500, mse: 4502.413574, mean_q: 93.219612\n",
      " 19944/50000: episode: 131, duration: 4.835s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.052450, mse: 4522.511230, mean_q: 93.482780\n",
      " 20188/50000: episode: 132, duration: 3.897s, episode steps: 244, steps per second:  63, episode reward: 244.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 10.671575, mse: 4528.901855, mean_q: 93.355743\n",
      " 20488/50000: episode: 133, duration: 4.859s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 7.869263, mse: 4500.747559, mean_q: 92.807358\n",
      " 20776/50000: episode: 134, duration: 4.547s, episode steps: 288, steps per second:  63, episode reward: 288.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 4.376770, mse: 4481.213379, mean_q: 92.509491\n",
      " 21076/50000: episode: 135, duration: 4.777s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 4.268794, mse: 4552.078125, mean_q: 93.150322\n",
      " 21376/50000: episode: 136, duration: 4.775s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.209024, mse: 4581.246582, mean_q: 93.604347\n",
      " 21676/50000: episode: 137, duration: 4.745s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 4.030322, mse: 4585.947754, mean_q: 93.636223\n",
      " 21921/50000: episode: 138, duration: 3.900s, episode steps: 245, steps per second:  63, episode reward: 245.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.610369, mse: 4580.652344, mean_q: 93.555870\n",
      " 22068/50000: episode: 139, duration: 2.365s, episode steps: 147, steps per second:  62, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 5.560278, mse: 4585.016113, mean_q: 93.597321\n",
      " 22368/50000: episode: 140, duration: 4.795s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.241452, mse: 4578.337402, mean_q: 93.201698\n",
      " 22569/50000: episode: 141, duration: 3.218s, episode steps: 201, steps per second:  62, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 3.693378, mse: 4600.645508, mean_q: 93.600647\n",
      " 22796/50000: episode: 142, duration: 3.602s, episode steps: 227, steps per second:  63, episode reward: 227.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 6.283451, mse: 4534.386230, mean_q: 92.640518\n",
      " 22995/50000: episode: 143, duration: 3.154s, episode steps: 199, steps per second:  63, episode reward: 199.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 5.580342, mse: 4613.235352, mean_q: 93.418419\n",
      " 23135/50000: episode: 144, duration: 2.234s, episode steps: 140, steps per second:  63, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.757313, mse: 4646.260742, mean_q: 93.702957\n",
      " 23272/50000: episode: 145, duration: 2.161s, episode steps: 137, steps per second:  63, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 7.253600, mse: 4603.627441, mean_q: 93.185600\n",
      " 23572/50000: episode: 146, duration: 4.727s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.606198, mse: 4614.207520, mean_q: 93.121544\n",
      " 23743/50000: episode: 147, duration: 2.780s, episode steps: 171, steps per second:  62, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 2.459255, mse: 4579.666992, mean_q: 92.894852\n",
      " 23882/50000: episode: 148, duration: 2.282s, episode steps: 139, steps per second:  61, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 5.127502, mse: 4602.066406, mean_q: 92.991081\n",
      " 24069/50000: episode: 149, duration: 2.995s, episode steps: 187, steps per second:  62, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.064966, mse: 4577.288086, mean_q: 92.467438\n",
      " 24203/50000: episode: 150, duration: 2.129s, episode steps: 134, steps per second:  63, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 3.558078, mse: 4607.615723, mean_q: 92.852318\n",
      " 24316/50000: episode: 151, duration: 1.858s, episode steps: 113, steps per second:  61, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 5.773819, mse: 4630.444336, mean_q: 92.909180\n",
      " 24459/50000: episode: 152, duration: 2.296s, episode steps: 143, steps per second:  62, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 4.265079, mse: 4626.628906, mean_q: 92.956200\n",
      " 24580/50000: episode: 153, duration: 1.930s, episode steps: 121, steps per second:  63, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 7.928297, mse: 4632.759766, mean_q: 93.188896\n",
      " 24768/50000: episode: 154, duration: 2.975s, episode steps: 188, steps per second:  63, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 4.516479, mse: 4604.228027, mean_q: 92.613159\n",
      " 24991/50000: episode: 155, duration: 3.604s, episode steps: 223, steps per second:  62, episode reward: 223.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 5.708470, mse: 4621.622070, mean_q: 92.688919\n",
      " 25164/50000: episode: 156, duration: 2.799s, episode steps: 173, steps per second:  62, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 5.156651, mse: 4571.960449, mean_q: 92.193413\n",
      " 25322/50000: episode: 157, duration: 2.469s, episode steps: 158, steps per second:  64, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 5.443779, mse: 4574.724121, mean_q: 92.297768\n",
      " 25499/50000: episode: 158, duration: 2.768s, episode steps: 177, steps per second:  64, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 2.262280, mse: 4527.291016, mean_q: 91.727676\n",
      " 25667/50000: episode: 159, duration: 2.709s, episode steps: 168, steps per second:  62, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 4.901138, mse: 4533.511230, mean_q: 91.751595\n",
      " 25871/50000: episode: 160, duration: 3.241s, episode steps: 204, steps per second:  63, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.200830, mse: 4481.646973, mean_q: 90.805061\n",
      " 26100/50000: episode: 161, duration: 3.611s, episode steps: 229, steps per second:  63, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.008146, mse: 4530.995117, mean_q: 91.588242\n",
      " 26258/50000: episode: 162, duration: 2.558s, episode steps: 158, steps per second:  62, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 2.239249, mse: 4554.333008, mean_q: 91.658020\n",
      " 26402/50000: episode: 163, duration: 2.280s, episode steps: 144, steps per second:  63, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 4.169095, mse: 4500.636230, mean_q: 91.095963\n",
      " 26592/50000: episode: 164, duration: 3.013s, episode steps: 190, steps per second:  63, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.850890, mse: 4517.727539, mean_q: 91.210587\n",
      " 26753/50000: episode: 165, duration: 2.581s, episode steps: 161, steps per second:  62, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 3.565506, mse: 4507.320801, mean_q: 91.017105\n",
      " 26902/50000: episode: 166, duration: 2.384s, episode steps: 149, steps per second:  62, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 3.132964, mse: 4513.657715, mean_q: 90.968773\n",
      " 27118/50000: episode: 167, duration: 3.445s, episode steps: 216, steps per second:  63, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.957166, mse: 4476.916016, mean_q: 90.669991\n",
      " 27302/50000: episode: 168, duration: 2.995s, episode steps: 184, steps per second:  61, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 2.083201, mse: 4505.200195, mean_q: 90.810143\n",
      " 27602/50000: episode: 169, duration: 4.782s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.687316, mse: 4466.311523, mean_q: 90.413017\n",
      " 27787/50000: episode: 170, duration: 2.998s, episode steps: 185, steps per second:  62, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 4.648089, mse: 4443.548828, mean_q: 89.967461\n",
      " 27995/50000: episode: 171, duration: 3.327s, episode steps: 208, steps per second:  63, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 6.017167, mse: 4428.417480, mean_q: 89.759727\n",
      " 28162/50000: episode: 172, duration: 2.669s, episode steps: 167, steps per second:  63, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.485 [0.000, 1.000],  loss: 4.078288, mse: 4346.900391, mean_q: 88.831139\n",
      " 28349/50000: episode: 173, duration: 3.056s, episode steps: 187, steps per second:  61, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.310370, mse: 4402.027832, mean_q: 89.732231\n",
      " 28644/50000: episode: 174, duration: 4.780s, episode steps: 295, steps per second:  62, episode reward: 295.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 4.047013, mse: 4325.597168, mean_q: 88.716309\n",
      " 28858/50000: episode: 175, duration: 3.480s, episode steps: 214, steps per second:  61, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.694553, mse: 4329.805664, mean_q: 89.038353\n",
      " 29120/50000: episode: 176, duration: 4.233s, episode steps: 262, steps per second:  62, episode reward: 262.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 3.790215, mse: 4341.697754, mean_q: 89.136864\n",
      " 29293/50000: episode: 177, duration: 2.781s, episode steps: 173, steps per second:  62, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 3.362539, mse: 4279.838379, mean_q: 88.319092\n",
      " 29463/50000: episode: 178, duration: 2.716s, episode steps: 170, steps per second:  63, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 3.611528, mse: 4242.290527, mean_q: 87.565063\n",
      " 29763/50000: episode: 179, duration: 4.828s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 2.725225, mse: 4265.067871, mean_q: 87.890556\n",
      " 29967/50000: episode: 180, duration: 3.297s, episode steps: 204, steps per second:  62, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.565246, mse: 4269.684570, mean_q: 88.271233\n",
      " 30267/50000: episode: 181, duration: 4.890s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 4.492000, mse: 4227.403320, mean_q: 87.552406\n",
      " 30451/50000: episode: 182, duration: 2.988s, episode steps: 184, steps per second:  62, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 4.015543, mse: 4197.072754, mean_q: 87.298279\n",
      " 30629/50000: episode: 183, duration: 2.908s, episode steps: 178, steps per second:  61, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.738482, mse: 4230.950684, mean_q: 87.680092\n",
      " 30793/50000: episode: 184, duration: 2.699s, episode steps: 164, steps per second:  61, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 4.898164, mse: 4154.165039, mean_q: 86.964569\n",
      " 30945/50000: episode: 185, duration: 2.451s, episode steps: 152, steps per second:  62, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 2.561887, mse: 4108.661621, mean_q: 86.435211\n",
      " 31109/50000: episode: 186, duration: 2.658s, episode steps: 164, steps per second:  62, episode reward: 164.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 4.173992, mse: 4108.989258, mean_q: 86.069458\n",
      " 31409/50000: episode: 187, duration: 4.915s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 2.213546, mse: 4124.678711, mean_q: 86.570145\n",
      " 31617/50000: episode: 188, duration: 3.370s, episode steps: 208, steps per second:  62, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.414283, mse: 4144.901367, mean_q: 86.920982\n",
      " 31817/50000: episode: 189, duration: 3.248s, episode steps: 200, steps per second:  62, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.171682, mse: 4127.537109, mean_q: 86.783478\n",
      " 32043/50000: episode: 190, duration: 3.689s, episode steps: 226, steps per second:  61, episode reward: 226.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.122693, mse: 4046.654297, mean_q: 85.684837\n",
      " 32309/50000: episode: 191, duration: 4.268s, episode steps: 266, steps per second:  62, episode reward: 266.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 1.779747, mse: 4063.183350, mean_q: 85.774895\n",
      " 32497/50000: episode: 192, duration: 3.041s, episode steps: 188, steps per second:  62, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.385601, mse: 4065.515381, mean_q: 85.847191\n",
      " 32664/50000: episode: 193, duration: 2.703s, episode steps: 167, steps per second:  62, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 4.159180, mse: 4042.248047, mean_q: 85.626312\n",
      " 32840/50000: episode: 194, duration: 2.919s, episode steps: 176, steps per second:  60, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 3.131495, mse: 4040.713135, mean_q: 85.698380\n",
      " 32997/50000: episode: 195, duration: 2.576s, episode steps: 157, steps per second:  61, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.862003, mse: 3978.029541, mean_q: 84.735542\n",
      " 33176/50000: episode: 196, duration: 2.974s, episode steps: 179, steps per second:  60, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 1.754812, mse: 4012.282715, mean_q: 85.196381\n",
      " 33344/50000: episode: 197, duration: 2.791s, episode steps: 168, steps per second:  60, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 3.250223, mse: 3950.240967, mean_q: 84.426933\n",
      " 33590/50000: episode: 198, duration: 4.088s, episode steps: 246, steps per second:  60, episode reward: 246.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 3.084581, mse: 3948.250488, mean_q: 84.597748\n",
      " 33763/50000: episode: 199, duration: 2.895s, episode steps: 173, steps per second:  60, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 4.753287, mse: 3856.774658, mean_q: 83.553490\n",
      " 33965/50000: episode: 200, duration: 3.350s, episode steps: 202, steps per second:  60, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 3.170419, mse: 3889.119385, mean_q: 83.865410\n",
      " 34176/50000: episode: 201, duration: 3.496s, episode steps: 211, steps per second:  60, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.662331, mse: 3903.736328, mean_q: 84.050743\n",
      " 34337/50000: episode: 202, duration: 2.679s, episode steps: 161, steps per second:  60, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 1.714782, mse: 3826.461914, mean_q: 83.028496\n",
      " 34548/50000: episode: 203, duration: 3.507s, episode steps: 211, steps per second:  60, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 4.442832, mse: 3791.613037, mean_q: 82.522736\n",
      " 34753/50000: episode: 204, duration: 3.416s, episode steps: 205, steps per second:  60, episode reward: 205.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.775711, mse: 3774.991455, mean_q: 82.572723\n",
      " 35003/50000: episode: 205, duration: 4.120s, episode steps: 250, steps per second:  61, episode reward: 250.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 4.779541, mse: 3771.747070, mean_q: 82.607300\n",
      " 35284/50000: episode: 206, duration: 4.694s, episode steps: 281, steps per second:  60, episode reward: 281.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.495 [0.000, 1.000],  loss: 2.752149, mse: 3762.620117, mean_q: 82.551163\n",
      " 35564/50000: episode: 207, duration: 4.612s, episode steps: 280, steps per second:  61, episode reward: 280.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 1.754909, mse: 3717.181641, mean_q: 81.886238\n",
      " 35741/50000: episode: 208, duration: 2.921s, episode steps: 177, steps per second:  61, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 2.821542, mse: 3715.200928, mean_q: 81.925972\n",
      " 35973/50000: episode: 209, duration: 3.781s, episode steps: 232, steps per second:  61, episode reward: 232.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 2.862577, mse: 3732.114502, mean_q: 82.015892\n",
      " 36132/50000: episode: 210, duration: 2.590s, episode steps: 159, steps per second:  61, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.247659, mse: 3714.852295, mean_q: 81.818573\n",
      " 36279/50000: episode: 211, duration: 2.390s, episode steps: 147, steps per second:  62, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.317063, mse: 3731.319824, mean_q: 81.967758\n",
      " 36503/50000: episode: 212, duration: 3.673s, episode steps: 224, steps per second:  61, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 2.679766, mse: 3694.902100, mean_q: 81.595909\n",
      " 36673/50000: episode: 213, duration: 2.780s, episode steps: 170, steps per second:  61, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 3.094676, mse: 3671.351807, mean_q: 81.277481\n",
      " 36852/50000: episode: 214, duration: 2.975s, episode steps: 179, steps per second:  60, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.744560, mse: 3640.847656, mean_q: 80.944725\n",
      " 37152/50000: episode: 215, duration: 4.967s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 4.229451, mse: 3675.760498, mean_q: 81.342682\n",
      " 37406/50000: episode: 216, duration: 4.225s, episode steps: 254, steps per second:  60, episode reward: 254.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.333620, mse: 3678.899658, mean_q: 81.409462\n",
      " 37654/50000: episode: 217, duration: 4.003s, episode steps: 248, steps per second:  62, episode reward: 248.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 2.421834, mse: 3617.475342, mean_q: 80.580566\n",
      " 37869/50000: episode: 218, duration: 3.537s, episode steps: 215, steps per second:  61, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.668148, mse: 3584.297607, mean_q: 80.092606\n",
      " 38077/50000: episode: 219, duration: 3.379s, episode steps: 208, steps per second:  62, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.341450, mse: 3607.096680, mean_q: 80.471664\n",
      " 38306/50000: episode: 220, duration: 3.738s, episode steps: 229, steps per second:  61, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.921964, mse: 3615.424072, mean_q: 80.518005\n",
      " 38504/50000: episode: 221, duration: 3.229s, episode steps: 198, steps per second:  61, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.536664, mse: 3664.058105, mean_q: 81.220215\n",
      " 38754/50000: episode: 222, duration: 4.131s, episode steps: 250, steps per second:  61, episode reward: 250.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.419121, mse: 3593.523926, mean_q: 80.099129\n",
      " 39018/50000: episode: 223, duration: 4.328s, episode steps: 264, steps per second:  61, episode reward: 264.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.581586, mse: 3580.109375, mean_q: 80.107452\n",
      " 39318/50000: episode: 224, duration: 4.914s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.317558, mse: 3582.070068, mean_q: 80.070610\n",
      " 39511/50000: episode: 225, duration: 3.209s, episode steps: 193, steps per second:  60, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.654471, mse: 3555.550537, mean_q: 79.648148\n",
      " 39679/50000: episode: 226, duration: 2.876s, episode steps: 168, steps per second:  58, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.040471, mse: 3594.920654, mean_q: 80.105202\n",
      " 39866/50000: episode: 227, duration: 3.139s, episode steps: 187, steps per second:  60, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.669863, mse: 3551.081543, mean_q: 79.722229\n",
      " 40024/50000: episode: 228, duration: 2.612s, episode steps: 158, steps per second:  60, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.235190, mse: 3629.401123, mean_q: 80.658005\n",
      " 40253/50000: episode: 229, duration: 3.769s, episode steps: 229, steps per second:  61, episode reward: 229.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 3.577320, mse: 3593.295898, mean_q: 80.251038\n",
      " 40553/50000: episode: 230, duration: 4.878s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.842432, mse: 3627.596191, mean_q: 80.786667\n",
      " 40796/50000: episode: 231, duration: 3.953s, episode steps: 243, steps per second:  61, episode reward: 243.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.327066, mse: 3612.224365, mean_q: 80.622986\n",
      " 40956/50000: episode: 232, duration: 2.644s, episode steps: 160, steps per second:  61, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 4.758433, mse: 3628.354248, mean_q: 80.827866\n",
      " 41125/50000: episode: 233, duration: 2.775s, episode steps: 169, steps per second:  61, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.432776, mse: 3584.095703, mean_q: 80.116371\n",
      " 41303/50000: episode: 234, duration: 2.930s, episode steps: 178, steps per second:  61, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 3.763895, mse: 3571.837158, mean_q: 80.135178\n",
      " 41495/50000: episode: 235, duration: 3.145s, episode steps: 192, steps per second:  61, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.883991, mse: 3596.670166, mean_q: 80.420761\n",
      " 41795/50000: episode: 236, duration: 4.954s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 4.327744, mse: 3588.625000, mean_q: 80.282059\n",
      " 42095/50000: episode: 237, duration: 5.027s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 1.535877, mse: 3552.357422, mean_q: 79.779190\n",
      " 42395/50000: episode: 238, duration: 4.919s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.218215, mse: 3549.772461, mean_q: 79.757660\n",
      " 42695/50000: episode: 239, duration: 4.881s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.851842, mse: 3543.344238, mean_q: 79.573349\n",
      " 42878/50000: episode: 240, duration: 2.998s, episode steps: 183, steps per second:  61, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 3.670917, mse: 3569.311523, mean_q: 80.155388\n",
      " 43178/50000: episode: 241, duration: 4.943s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.805390, mse: 3614.232422, mean_q: 80.763840\n",
      " 43478/50000: episode: 242, duration: 4.981s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.113221, mse: 3570.161621, mean_q: 80.151741\n",
      " 43778/50000: episode: 243, duration: 4.945s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.305303, mse: 3593.127197, mean_q: 80.481812\n",
      " 43978/50000: episode: 244, duration: 3.261s, episode steps: 200, steps per second:  61, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 2.517145, mse: 3534.908691, mean_q: 79.678001\n",
      " 44212/50000: episode: 245, duration: 3.845s, episode steps: 234, steps per second:  61, episode reward: 234.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.739735, mse: 3574.092529, mean_q: 80.180008\n",
      " 44512/50000: episode: 246, duration: 4.945s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 3.572613, mse: 3589.986572, mean_q: 80.405090\n",
      " 44812/50000: episode: 247, duration: 4.904s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 2.909264, mse: 3571.360840, mean_q: 80.093330\n",
      " 45028/50000: episode: 248, duration: 3.543s, episode steps: 216, steps per second:  61, episode reward: 216.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 3.223567, mse: 3572.373535, mean_q: 80.114861\n",
      " 45302/50000: episode: 249, duration: 4.495s, episode steps: 274, steps per second:  61, episode reward: 274.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.030707, mse: 3598.340820, mean_q: 80.616814\n",
      " 45602/50000: episode: 250, duration: 4.917s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.555829, mse: 3592.662109, mean_q: 80.628357\n",
      " 45902/50000: episode: 251, duration: 4.937s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 4.141717, mse: 3583.097412, mean_q: 80.622620\n",
      " 46202/50000: episode: 252, duration: 4.908s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 3.701475, mse: 3580.519287, mean_q: 80.701851\n",
      " 46502/50000: episode: 253, duration: 4.936s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 2.043432, mse: 3615.270020, mean_q: 81.218170\n",
      " 46685/50000: episode: 254, duration: 3.061s, episode steps: 183, steps per second:  60, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 4.917139, mse: 3585.805664, mean_q: 81.042725\n",
      " 46866/50000: episode: 255, duration: 2.982s, episode steps: 181, steps per second:  61, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 3.360223, mse: 3650.834229, mean_q: 81.726173\n",
      " 47166/50000: episode: 256, duration: 4.974s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 2.415863, mse: 3627.673828, mean_q: 81.446495\n",
      " 47466/50000: episode: 257, duration: 4.993s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 1.731686, mse: 3693.495117, mean_q: 82.039528\n",
      " 47669/50000: episode: 258, duration: 3.385s, episode steps: 203, steps per second:  60, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.063799, mse: 3778.065186, mean_q: 83.265854\n",
      " 47969/50000: episode: 259, duration: 4.997s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 3.060432, mse: 3765.594971, mean_q: 82.941109\n",
      " 48269/50000: episode: 260, duration: 5.015s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 2.887019, mse: 3796.260742, mean_q: 83.394821\n",
      " 48569/50000: episode: 261, duration: 5.038s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.409422, mse: 3778.429199, mean_q: 82.943527\n",
      " 48869/50000: episode: 262, duration: 5.113s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 3.807148, mse: 3854.949463, mean_q: 83.972794\n",
      " 48963/50000: episode: 263, duration: 1.610s, episode steps:  94, steps per second:  58, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 3.890440, mse: 3803.513672, mean_q: 83.061653\n",
      " 49235/50000: episode: 264, duration: 4.654s, episode steps: 272, steps per second:  58, episode reward: 272.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.655677, mse: 3878.228760, mean_q: 84.152115\n",
      " 49515/50000: episode: 265, duration: 4.771s, episode steps: 280, steps per second:  59, episode reward: 280.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 5.753050, mse: 3844.994141, mean_q: 83.613243\n",
      " 49815/50000: episode: 266, duration: 5.047s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 2.571130, mse: 3892.250732, mean_q: 84.290634\n",
      "done, took 813.352 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 300.000, steps: 300\n",
      "Episode 2: reward: 300.000, steps: 300\n",
      "Episode 3: reward: 300.000, steps: 300\n",
      "Episode 4: reward: 74.000, steps: 74\n",
      "Episode 5: reward: 92.000, steps: 92\n",
      "Policy: EpsGreedyQPolicy\n",
      "Window_length: 1\n",
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    40/50000: episode: 1, duration: 3.807s, episode steps:  40, steps per second:  11, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.450 [0.000, 1.000],  loss: 0.432797, mse: 0.436813, mean_q: 0.092385\n",
      "    49/50000: episode: 2, duration: 0.163s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.306216, mse: 0.340292, mean_q: 0.310529\n",
      "    61/50000: episode: 3, duration: 0.219s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.214071, mse: 0.320088, mean_q: 0.557900\n",
      "    71/50000: episode: 4, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.151401, mse: 0.361324, mean_q: 0.847342\n",
      "    81/50000: episode: 5, duration: 0.170s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.131824, mse: 0.425959, mean_q: 0.951190\n",
      "    92/50000: episode: 6, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.097379, mse: 0.449446, mean_q: 0.954055\n",
      "   101/50000: episode: 7, duration: 0.149s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.080363, mse: 0.613233, mean_q: 1.128509\n",
      "   111/50000: episode: 8, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.065170, mse: 0.670355, mean_q: 1.184767\n",
      "   120/50000: episode: 9, duration: 0.171s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.046034, mse: 0.726419, mean_q: 1.288644\n",
      "   133/50000: episode: 10, duration: 0.244s, episode steps:  13, steps per second:  53, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.044639, mse: 0.893399, mean_q: 1.432281\n",
      "   143/50000: episode: 11, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.048802, mse: 1.001831, mean_q: 1.528076\n",
      "   152/50000: episode: 12, duration: 0.159s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.050923, mse: 1.032205, mean_q: 1.578473\n",
      "   165/50000: episode: 13, duration: 0.221s, episode steps:  13, steps per second:  59, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.051951, mse: 1.173224, mean_q: 1.668249\n",
      "   177/50000: episode: 14, duration: 0.216s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.047645, mse: 1.270883, mean_q: 1.780004\n",
      "   189/50000: episode: 15, duration: 0.205s, episode steps:  12, steps per second:  59, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.056067, mse: 1.320576, mean_q: 1.833701\n",
      "   199/50000: episode: 16, duration: 0.168s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.052593, mse: 1.295545, mean_q: 1.829597\n",
      "   210/50000: episode: 17, duration: 0.198s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.069473, mse: 1.554224, mean_q: 1.949977\n",
      "   219/50000: episode: 18, duration: 0.173s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.047173, mse: 1.662296, mean_q: 2.031538\n",
      "   230/50000: episode: 19, duration: 0.191s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.058040, mse: 1.865772, mean_q: 2.126549\n",
      "   241/50000: episode: 20, duration: 0.195s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.070052, mse: 2.010697, mean_q: 2.189703\n",
      "   251/50000: episode: 21, duration: 0.168s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.081946, mse: 2.166673, mean_q: 2.270572\n",
      "   262/50000: episode: 22, duration: 0.180s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.086525, mse: 2.254366, mean_q: 2.316530\n",
      "   274/50000: episode: 23, duration: 0.197s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.075033, mse: 2.519137, mean_q: 2.462585\n",
      "   285/50000: episode: 24, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.096475, mse: 2.717251, mean_q: 2.514761\n",
      "   295/50000: episode: 25, duration: 0.174s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.092894, mse: 2.708756, mean_q: 2.519148\n",
      "   308/50000: episode: 26, duration: 0.234s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.124157, mse: 2.971730, mean_q: 2.649623\n",
      "   317/50000: episode: 27, duration: 0.145s, episode steps:   9, steps per second:  62, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.099425, mse: 3.283921, mean_q: 2.807062\n",
      "   326/50000: episode: 28, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.102295, mse: 3.366205, mean_q: 2.808014\n",
      "   336/50000: episode: 29, duration: 0.171s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.152695, mse: 3.591260, mean_q: 2.851959\n",
      "   350/50000: episode: 30, duration: 0.225s, episode steps:  14, steps per second:  62, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.286 [0.000, 1.000],  loss: 0.142205, mse: 3.762077, mean_q: 2.953865\n",
      "   360/50000: episode: 31, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.152772, mse: 3.689537, mean_q: 2.896061\n",
      "   369/50000: episode: 32, duration: 0.152s, episode steps:   9, steps per second:  59, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.180019, mse: 4.349474, mean_q: 3.145797\n",
      "   379/50000: episode: 33, duration: 0.162s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.172178, mse: 4.064598, mean_q: 3.003845\n",
      "   388/50000: episode: 34, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.079155, mse: 4.841486, mean_q: 3.340671\n",
      "   399/50000: episode: 35, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.097634, mse: 5.035435, mean_q: 3.359829\n",
      "   410/50000: episode: 36, duration: 0.185s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.154529, mse: 5.168093, mean_q: 3.359649\n",
      "   423/50000: episode: 37, duration: 0.224s, episode steps:  13, steps per second:  58, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.308 [0.000, 1.000],  loss: 0.149137, mse: 5.320460, mean_q: 3.428313\n",
      "   433/50000: episode: 38, duration: 0.161s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.117623, mse: 5.957736, mean_q: 3.609027\n",
      "   445/50000: episode: 39, duration: 0.200s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.135019, mse: 6.027090, mean_q: 3.574882\n",
      "   455/50000: episode: 40, duration: 0.168s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.155237, mse: 6.620531, mean_q: 3.728736\n",
      "   468/50000: episode: 41, duration: 0.216s, episode steps:  13, steps per second:  60, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.187495, mse: 6.255790, mean_q: 3.613858\n",
      "   479/50000: episode: 42, duration: 0.186s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.144589, mse: 7.024888, mean_q: 3.830527\n",
      "   491/50000: episode: 43, duration: 0.197s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.169246, mse: 7.138064, mean_q: 3.834937\n",
      "   504/50000: episode: 44, duration: 0.209s, episode steps:  13, steps per second:  62, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.149433, mse: 7.686172, mean_q: 3.971570\n",
      "   515/50000: episode: 45, duration: 0.185s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.141070, mse: 7.871139, mean_q: 4.026144\n",
      "   530/50000: episode: 46, duration: 0.256s, episode steps:  15, steps per second:  59, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.400 [0.000, 1.000],  loss: 0.153153, mse: 8.290559, mean_q: 4.092350\n",
      "   546/50000: episode: 47, duration: 0.271s, episode steps:  16, steps per second:  59, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.145384, mse: 8.599981, mean_q: 4.150799\n",
      "   561/50000: episode: 48, duration: 0.270s, episode steps:  15, steps per second:  56, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.130415, mse: 9.551835, mean_q: 4.368378\n",
      "   573/50000: episode: 49, duration: 0.197s, episode steps:  12, steps per second:  61, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.131222, mse: 9.425076, mean_q: 4.329826\n",
      "   591/50000: episode: 50, duration: 0.287s, episode steps:  18, steps per second:  63, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.130480, mse: 10.363943, mean_q: 4.526408\n",
      "   609/50000: episode: 51, duration: 0.320s, episode steps:  18, steps per second:  56, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.444 [0.000, 1.000],  loss: 0.150272, mse: 10.095804, mean_q: 4.448699\n",
      "   626/50000: episode: 52, duration: 0.284s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.122364, mse: 11.422359, mean_q: 4.738756\n",
      "   643/50000: episode: 53, duration: 0.298s, episode steps:  17, steps per second:  57, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.129712, mse: 11.301806, mean_q: 4.692714\n",
      "   660/50000: episode: 54, duration: 0.292s, episode steps:  17, steps per second:  58, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 0.118911, mse: 12.163331, mean_q: 4.862998\n",
      "   677/50000: episode: 55, duration: 0.301s, episode steps:  17, steps per second:  56, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.107575, mse: 12.738644, mean_q: 4.969051\n",
      "   710/50000: episode: 56, duration: 0.536s, episode steps:  33, steps per second:  62, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.123465, mse: 13.517275, mean_q: 5.077572\n",
      "   736/50000: episode: 57, duration: 0.432s, episode steps:  26, steps per second:  60, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.112608, mse: 14.698315, mean_q: 5.278021\n",
      "   788/50000: episode: 58, duration: 0.837s, episode steps:  52, steps per second:  62, episode reward: 52.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.673 [0.000, 1.000],  loss: 0.119228, mse: 15.708464, mean_q: 5.418271\n",
      "   800/50000: episode: 59, duration: 0.186s, episode steps:  12, steps per second:  64, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.100517, mse: 17.502474, mean_q: 5.773685\n",
      "   809/50000: episode: 60, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.135572, mse: 17.868355, mean_q: 5.756353\n",
      "   818/50000: episode: 61, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.244958, mse: 18.605696, mean_q: 5.844641\n",
      "   836/50000: episode: 62, duration: 0.292s, episode steps:  18, steps per second:  62, episode reward: 18.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.722 [0.000, 1.000],  loss: 0.313318, mse: 20.030861, mean_q: 6.064837\n",
      "   857/50000: episode: 63, duration: 0.358s, episode steps:  21, steps per second:  59, episode reward: 21.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.383193, mse: 18.848299, mean_q: 5.894893\n",
      "   877/50000: episode: 64, duration: 0.331s, episode steps:  20, steps per second:  60, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.457149, mse: 20.163101, mean_q: 6.082499\n",
      "   903/50000: episode: 65, duration: 0.435s, episode steps:  26, steps per second:  60, episode reward: 26.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.769 [0.000, 1.000],  loss: 0.376218, mse: 21.380032, mean_q: 6.294044\n",
      "   911/50000: episode: 66, duration: 0.133s, episode steps:   8, steps per second:  60, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.680729, mse: 21.932398, mean_q: 6.308221\n",
      "   922/50000: episode: 67, duration: 0.187s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.509677, mse: 21.616816, mean_q: 6.303119\n",
      "   963/50000: episode: 68, duration: 0.667s, episode steps:  41, steps per second:  61, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.659 [0.000, 1.000],  loss: 0.423946, mse: 23.659779, mean_q: 6.634387\n",
      "   973/50000: episode: 69, duration: 0.169s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.460954, mse: 24.797899, mean_q: 6.806407\n",
      "   982/50000: episode: 70, duration: 0.152s, episode steps:   9, steps per second:  59, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.355016, mse: 25.620844, mean_q: 6.933573\n",
      "   991/50000: episode: 71, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.523506, mse: 26.139011, mean_q: 6.959101\n",
      "  1001/50000: episode: 72, duration: 0.184s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.226192, mse: 25.333508, mean_q: 6.934684\n",
      "  1011/50000: episode: 73, duration: 0.171s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.508062, mse: 26.127254, mean_q: 6.980187\n",
      "  1019/50000: episode: 74, duration: 0.147s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.421247, mse: 26.828718, mean_q: 7.108561\n",
      "  1029/50000: episode: 75, duration: 0.186s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.800794, mse: 25.402309, mean_q: 6.829423\n",
      "  1048/50000: episode: 76, duration: 0.309s, episode steps:  19, steps per second:  61, episode reward: 19.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.789 [0.000, 1.000],  loss: 0.463506, mse: 27.789110, mean_q: 7.188241\n",
      "  1058/50000: episode: 77, duration: 0.174s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.511795, mse: 28.453430, mean_q: 7.351626\n",
      "  1067/50000: episode: 78, duration: 0.157s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.572842, mse: 28.933506, mean_q: 7.420179\n",
      "  1076/50000: episode: 79, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.443146, mse: 28.801561, mean_q: 7.310664\n",
      "  1085/50000: episode: 80, duration: 0.169s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.527406, mse: 29.914131, mean_q: 7.507005\n",
      "  1096/50000: episode: 81, duration: 0.178s, episode steps:  11, steps per second:  62, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.549346, mse: 27.925646, mean_q: 7.165308\n",
      "  1106/50000: episode: 82, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.471414, mse: 28.915731, mean_q: 7.355479\n",
      "  1116/50000: episode: 83, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.371348, mse: 31.931997, mean_q: 7.779760\n",
      "  1128/50000: episode: 84, duration: 0.201s, episode steps:  12, steps per second:  60, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.381194, mse: 29.665480, mean_q: 7.490664\n",
      "  1137/50000: episode: 85, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.354508, mse: 31.625170, mean_q: 7.742862\n",
      "  1145/50000: episode: 86, duration: 0.157s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.335177, mse: 32.651714, mean_q: 7.795631\n",
      "  1154/50000: episode: 87, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.275776, mse: 33.445988, mean_q: 7.912498\n",
      "  1195/50000: episode: 88, duration: 0.679s, episode steps:  41, steps per second:  60, episode reward: 41.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.610 [0.000, 1.000],  loss: 0.452430, mse: 33.311615, mean_q: 7.839138\n",
      "  1204/50000: episode: 89, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.256564, mse: 34.486347, mean_q: 8.045942\n",
      "  1213/50000: episode: 90, duration: 0.165s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.368454, mse: 36.225384, mean_q: 8.255527\n",
      "  1223/50000: episode: 91, duration: 0.180s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.539376, mse: 34.755337, mean_q: 8.023232\n",
      "  1252/50000: episode: 92, duration: 0.475s, episode steps:  29, steps per second:  61, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.724 [0.000, 1.000],  loss: 0.435297, mse: 36.764122, mean_q: 8.264543\n",
      "  1299/50000: episode: 93, duration: 0.772s, episode steps:  47, steps per second:  61, episode reward: 47.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.617 [0.000, 1.000],  loss: 0.320647, mse: 38.641548, mean_q: 8.490880\n",
      "  1353/50000: episode: 94, duration: 0.839s, episode steps:  54, steps per second:  64, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.648 [0.000, 1.000],  loss: 0.319647, mse: 39.156174, mean_q: 8.476378\n",
      "  1404/50000: episode: 95, duration: 0.817s, episode steps:  51, steps per second:  62, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.647 [0.000, 1.000],  loss: 0.283010, mse: 42.980019, mean_q: 8.913024\n",
      "  1438/50000: episode: 96, duration: 0.543s, episode steps:  34, steps per second:  63, episode reward: 34.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.706 [0.000, 1.000],  loss: 0.258247, mse: 43.915405, mean_q: 9.015385\n",
      "  1516/50000: episode: 97, duration: 1.208s, episode steps:  78, steps per second:  65, episode reward: 78.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.235527, mse: 45.522449, mean_q: 9.108534\n",
      "  1562/50000: episode: 98, duration: 0.740s, episode steps:  46, steps per second:  62, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 0.429036, mse: 49.380913, mean_q: 9.501074\n",
      "  1602/50000: episode: 99, duration: 0.633s, episode steps:  40, steps per second:  63, episode reward: 40.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.402121, mse: 52.577171, mean_q: 9.853836\n",
      "  1644/50000: episode: 100, duration: 0.657s, episode steps:  42, steps per second:  64, episode reward: 42.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 0.326731, mse: 53.829067, mean_q: 9.963689\n",
      "  1717/50000: episode: 101, duration: 1.145s, episode steps:  73, steps per second:  64, episode reward: 73.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 0.308239, mse: 58.571007, mean_q: 10.440523\n",
      "  1781/50000: episode: 102, duration: 1.024s, episode steps:  64, steps per second:  62, episode reward: 64.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.541024, mse: 61.408279, mean_q: 10.630131\n",
      "  1858/50000: episode: 103, duration: 1.226s, episode steps:  77, steps per second:  63, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.347825, mse: 66.146736, mean_q: 11.097545\n",
      "  1950/50000: episode: 104, duration: 1.431s, episode steps:  92, steps per second:  64, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 0.546758, mse: 73.805756, mean_q: 11.829539\n",
      "  2029/50000: episode: 105, duration: 1.238s, episode steps:  79, steps per second:  64, episode reward: 79.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 0.548641, mse: 79.452621, mean_q: 12.324932\n",
      "  2133/50000: episode: 106, duration: 1.644s, episode steps: 104, steps per second:  63, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.770986, mse: 87.438309, mean_q: 12.982024\n",
      "  2256/50000: episode: 107, duration: 1.926s, episode steps: 123, steps per second:  64, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 0.576969, mse: 97.047737, mean_q: 13.745483\n",
      "  2350/50000: episode: 108, duration: 1.429s, episode steps:  94, steps per second:  66, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 0.707833, mse: 107.146889, mean_q: 14.468434\n",
      "  2456/50000: episode: 109, duration: 1.674s, episode steps: 106, steps per second:  63, episode reward: 106.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 0.638765, mse: 119.511177, mean_q: 15.372235\n",
      "  2574/50000: episode: 110, duration: 1.830s, episode steps: 118, steps per second:  64, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.017361, mse: 131.713684, mean_q: 16.154984\n",
      "  2690/50000: episode: 111, duration: 1.803s, episode steps: 116, steps per second:  64, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 0.969861, mse: 144.190811, mean_q: 17.026657\n",
      "  2822/50000: episode: 112, duration: 2.017s, episode steps: 132, steps per second:  65, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.030913, mse: 160.139267, mean_q: 17.877878\n",
      "  2931/50000: episode: 113, duration: 1.669s, episode steps: 109, steps per second:  65, episode reward: 109.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 1.074905, mse: 174.288773, mean_q: 18.689714\n",
      "  3088/50000: episode: 114, duration: 2.432s, episode steps: 157, steps per second:  65, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.937156, mse: 197.209274, mean_q: 19.986761\n",
      "  3226/50000: episode: 115, duration: 2.211s, episode steps: 138, steps per second:  62, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.982264, mse: 216.101089, mean_q: 20.929914\n",
      "  3373/50000: episode: 116, duration: 2.310s, episode steps: 147, steps per second:  64, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.175678, mse: 237.586334, mean_q: 21.904007\n",
      "  3527/50000: episode: 117, duration: 2.388s, episode steps: 154, steps per second:  64, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.247002, mse: 260.243958, mean_q: 22.906265\n",
      "  3662/50000: episode: 118, duration: 2.136s, episode steps: 135, steps per second:  63, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 1.356852, mse: 291.279755, mean_q: 24.329630\n",
      "  3807/50000: episode: 119, duration: 2.281s, episode steps: 145, steps per second:  64, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.078738, mse: 311.101349, mean_q: 25.111858\n",
      "  3959/50000: episode: 120, duration: 2.327s, episode steps: 152, steps per second:  65, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.989986, mse: 341.214355, mean_q: 26.292641\n",
      "  4125/50000: episode: 121, duration: 2.556s, episode steps: 166, steps per second:  65, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.084499, mse: 373.149902, mean_q: 27.515142\n",
      "  4270/50000: episode: 122, duration: 2.190s, episode steps: 145, steps per second:  66, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.234176, mse: 403.612488, mean_q: 28.514727\n",
      "  4415/50000: episode: 123, duration: 2.232s, episode steps: 145, steps per second:  65, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.397746, mse: 428.480804, mean_q: 29.327271\n",
      "  4567/50000: episode: 124, duration: 2.356s, episode steps: 152, steps per second:  65, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.790112, mse: 454.757568, mean_q: 30.114897\n",
      "  4714/50000: episode: 125, duration: 2.232s, episode steps: 147, steps per second:  66, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.501687, mse: 488.958862, mean_q: 31.324358\n",
      "  4856/50000: episode: 126, duration: 2.168s, episode steps: 142, steps per second:  66, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.344381, mse: 522.892151, mean_q: 32.468895\n",
      "  4997/50000: episode: 127, duration: 2.160s, episode steps: 141, steps per second:  65, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.761522, mse: 550.332825, mean_q: 33.187569\n",
      "  5141/50000: episode: 128, duration: 2.223s, episode steps: 144, steps per second:  65, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.594182, mse: 581.690186, mean_q: 34.096176\n",
      "  5299/50000: episode: 129, duration: 2.391s, episode steps: 158, steps per second:  66, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.402234, mse: 618.970154, mean_q: 35.173706\n",
      "  5436/50000: episode: 130, duration: 2.088s, episode steps: 137, steps per second:  66, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 2.198554, mse: 651.624451, mean_q: 36.091980\n",
      "  5583/50000: episode: 131, duration: 2.225s, episode steps: 147, steps per second:  66, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.634888, mse: 694.279358, mean_q: 37.345200\n",
      "  5731/50000: episode: 132, duration: 2.240s, episode steps: 148, steps per second:  66, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 1.979710, mse: 731.148987, mean_q: 38.231564\n",
      "  5873/50000: episode: 133, duration: 2.146s, episode steps: 142, steps per second:  66, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 2.712219, mse: 769.403137, mean_q: 39.236248\n",
      "  6010/50000: episode: 134, duration: 2.084s, episode steps: 137, steps per second:  66, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 2.051768, mse: 800.337708, mean_q: 39.989643\n",
      "  6145/50000: episode: 135, duration: 2.063s, episode steps: 135, steps per second:  65, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.237947, mse: 834.986328, mean_q: 40.800926\n",
      "  6290/50000: episode: 136, duration: 2.222s, episode steps: 145, steps per second:  65, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.272509, mse: 877.182739, mean_q: 41.728142\n",
      "  6436/50000: episode: 137, duration: 2.251s, episode steps: 146, steps per second:  65, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 1.702481, mse: 905.085205, mean_q: 42.256134\n",
      "  6572/50000: episode: 138, duration: 2.110s, episode steps: 136, steps per second:  64, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.633476, mse: 928.799805, mean_q: 42.763119\n",
      "  6708/50000: episode: 139, duration: 2.081s, episode steps: 136, steps per second:  65, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.558528, mse: 967.066956, mean_q: 43.531456\n",
      "  6854/50000: episode: 140, duration: 2.201s, episode steps: 146, steps per second:  66, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.217478, mse: 1008.149109, mean_q: 44.618999\n",
      "  7006/50000: episode: 141, duration: 2.281s, episode steps: 152, steps per second:  67, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.544691, mse: 1040.713989, mean_q: 45.190647\n",
      "  7154/50000: episode: 142, duration: 2.288s, episode steps: 148, steps per second:  65, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 2.575378, mse: 1076.418335, mean_q: 45.950047\n",
      "  7289/50000: episode: 143, duration: 2.047s, episode steps: 135, steps per second:  66, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.200408, mse: 1119.900879, mean_q: 46.808399\n",
      "  7431/50000: episode: 144, duration: 2.132s, episode steps: 142, steps per second:  67, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 3.287734, mse: 1148.164185, mean_q: 47.366138\n",
      "  7582/50000: episode: 145, duration: 2.288s, episode steps: 151, steps per second:  66, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.041562, mse: 1190.045288, mean_q: 48.096539\n",
      "  7714/50000: episode: 146, duration: 1.989s, episode steps: 132, steps per second:  66, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.419009, mse: 1239.977539, mean_q: 49.105186\n",
      "  7851/50000: episode: 147, duration: 2.136s, episode steps: 137, steps per second:  64, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.971650, mse: 1245.990479, mean_q: 49.015495\n",
      "  7992/50000: episode: 148, duration: 2.137s, episode steps: 141, steps per second:  66, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 2.420880, mse: 1283.102783, mean_q: 49.582001\n",
      "  8134/50000: episode: 149, duration: 2.166s, episode steps: 142, steps per second:  66, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.245777, mse: 1315.294067, mean_q: 50.338364\n",
      "  8267/50000: episode: 150, duration: 2.034s, episode steps: 133, steps per second:  65, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 2.604006, mse: 1357.990356, mean_q: 51.081993\n",
      "  8407/50000: episode: 151, duration: 2.110s, episode steps: 140, steps per second:  66, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.540270, mse: 1410.363770, mean_q: 52.104656\n",
      "  8548/50000: episode: 152, duration: 2.187s, episode steps: 141, steps per second:  64, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 1.741691, mse: 1435.143799, mean_q: 52.308178\n",
      "  8684/50000: episode: 153, duration: 2.071s, episode steps: 136, steps per second:  66, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.682230, mse: 1468.243774, mean_q: 52.952103\n",
      "  8818/50000: episode: 154, duration: 2.033s, episode steps: 134, steps per second:  66, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.551074, mse: 1498.859131, mean_q: 53.387375\n",
      "  8956/50000: episode: 155, duration: 2.106s, episode steps: 138, steps per second:  66, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.208562, mse: 1497.363403, mean_q: 53.259243\n",
      "  9104/50000: episode: 156, duration: 2.252s, episode steps: 148, steps per second:  66, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 1.744395, mse: 1547.432251, mean_q: 54.226273\n",
      "  9239/50000: episode: 157, duration: 2.121s, episode steps: 135, steps per second:  64, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.108642, mse: 1575.237915, mean_q: 54.467396\n",
      "  9391/50000: episode: 158, duration: 2.304s, episode steps: 152, steps per second:  66, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 3.025763, mse: 1603.618164, mean_q: 55.075928\n",
      "  9535/50000: episode: 159, duration: 2.193s, episode steps: 144, steps per second:  66, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 2.868153, mse: 1637.514893, mean_q: 55.294678\n",
      "  9673/50000: episode: 160, duration: 2.112s, episode steps: 138, steps per second:  65, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.709396, mse: 1661.647705, mean_q: 55.683353\n",
      "  9803/50000: episode: 161, duration: 1.996s, episode steps: 130, steps per second:  65, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.553104, mse: 1663.147339, mean_q: 55.689903\n",
      "  9938/50000: episode: 162, duration: 2.089s, episode steps: 135, steps per second:  65, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.342322, mse: 1679.419556, mean_q: 55.971939\n",
      " 10076/50000: episode: 163, duration: 2.100s, episode steps: 138, steps per second:  66, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.822539, mse: 1737.498047, mean_q: 57.001446\n",
      " 10201/50000: episode: 164, duration: 1.899s, episode steps: 125, steps per second:  66, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.581298, mse: 1756.208618, mean_q: 56.985630\n",
      " 10329/50000: episode: 165, duration: 1.948s, episode steps: 128, steps per second:  66, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 2.414785, mse: 1776.363037, mean_q: 57.301590\n",
      " 10455/50000: episode: 166, duration: 1.921s, episode steps: 126, steps per second:  66, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 2.452788, mse: 1815.576416, mean_q: 57.818142\n",
      " 10593/50000: episode: 167, duration: 2.123s, episode steps: 138, steps per second:  65, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.220066, mse: 1794.353882, mean_q: 57.255299\n",
      " 10729/50000: episode: 168, duration: 2.110s, episode steps: 136, steps per second:  64, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 1.945544, mse: 1848.368774, mean_q: 58.311874\n",
      " 10870/50000: episode: 169, duration: 2.140s, episode steps: 141, steps per second:  66, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 2.187298, mse: 1892.378052, mean_q: 58.782738\n",
      " 11015/50000: episode: 170, duration: 2.204s, episode steps: 145, steps per second:  66, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.943595, mse: 1871.090576, mean_q: 58.150795\n",
      " 11152/50000: episode: 171, duration: 2.085s, episode steps: 137, steps per second:  66, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 2.414117, mse: 1911.215576, mean_q: 58.772282\n",
      " 11283/50000: episode: 172, duration: 1.992s, episode steps: 131, steps per second:  66, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.325226, mse: 1930.568604, mean_q: 59.244640\n",
      " 11414/50000: episode: 173, duration: 1.983s, episode steps: 131, steps per second:  66, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.597042, mse: 1956.264038, mean_q: 59.195320\n",
      " 11548/50000: episode: 174, duration: 2.022s, episode steps: 134, steps per second:  66, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.613056, mse: 1935.394409, mean_q: 58.813110\n",
      " 11681/50000: episode: 175, duration: 2.010s, episode steps: 133, steps per second:  66, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 2.554225, mse: 1998.044189, mean_q: 59.882179\n",
      " 11820/50000: episode: 176, duration: 2.129s, episode steps: 139, steps per second:  65, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.742126, mse: 2005.183472, mean_q: 59.944710\n",
      " 11954/50000: episode: 177, duration: 2.050s, episode steps: 134, steps per second:  65, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.029946, mse: 2020.176270, mean_q: 60.049667\n",
      " 12098/50000: episode: 178, duration: 2.219s, episode steps: 144, steps per second:  65, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.777578, mse: 2019.103027, mean_q: 59.811943\n",
      " 12232/50000: episode: 179, duration: 2.037s, episode steps: 134, steps per second:  66, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.590433, mse: 2036.653931, mean_q: 60.093639\n",
      " 12367/50000: episode: 180, duration: 2.028s, episode steps: 135, steps per second:  67, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 1.505174, mse: 2054.516846, mean_q: 60.360474\n",
      " 12501/50000: episode: 181, duration: 2.016s, episode steps: 134, steps per second:  66, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.930938, mse: 2071.675293, mean_q: 60.502056\n",
      " 12625/50000: episode: 182, duration: 1.883s, episode steps: 124, steps per second:  66, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 1.843737, mse: 2104.112305, mean_q: 61.142838\n",
      " 12757/50000: episode: 183, duration: 1.995s, episode steps: 132, steps per second:  66, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.915895, mse: 2083.349365, mean_q: 60.405354\n",
      " 12887/50000: episode: 184, duration: 1.927s, episode steps: 130, steps per second:  67, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.747089, mse: 2050.862061, mean_q: 59.852005\n",
      " 13026/50000: episode: 185, duration: 2.115s, episode steps: 139, steps per second:  66, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.467173, mse: 2109.671875, mean_q: 60.508095\n",
      " 13161/50000: episode: 186, duration: 2.043s, episode steps: 135, steps per second:  66, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.209184, mse: 2134.265381, mean_q: 61.175587\n",
      " 13288/50000: episode: 187, duration: 1.922s, episode steps: 127, steps per second:  66, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 2.204546, mse: 2116.906250, mean_q: 60.865746\n",
      " 13419/50000: episode: 188, duration: 1.966s, episode steps: 131, steps per second:  67, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 1.384011, mse: 2096.680664, mean_q: 60.110863\n",
      " 13558/50000: episode: 189, duration: 2.082s, episode steps: 139, steps per second:  67, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.635003, mse: 2133.231201, mean_q: 60.701817\n",
      " 13687/50000: episode: 190, duration: 1.924s, episode steps: 129, steps per second:  67, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 1.552841, mse: 2200.116455, mean_q: 61.816307\n",
      " 13818/50000: episode: 191, duration: 1.959s, episode steps: 131, steps per second:  67, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.673095, mse: 2184.397217, mean_q: 61.515015\n",
      " 13943/50000: episode: 192, duration: 1.891s, episode steps: 125, steps per second:  66, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 1.702020, mse: 2209.906006, mean_q: 62.250664\n",
      " 14077/50000: episode: 193, duration: 2.007s, episode steps: 134, steps per second:  67, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 2.301517, mse: 2191.081055, mean_q: 61.518921\n",
      " 14217/50000: episode: 194, duration: 2.115s, episode steps: 140, steps per second:  66, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.692334, mse: 2180.004395, mean_q: 61.452469\n",
      " 14357/50000: episode: 195, duration: 2.111s, episode steps: 140, steps per second:  66, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.328194, mse: 2216.423584, mean_q: 62.314125\n",
      " 14497/50000: episode: 196, duration: 2.123s, episode steps: 140, steps per second:  66, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.387371, mse: 2193.234863, mean_q: 61.784290\n",
      " 14621/50000: episode: 197, duration: 1.891s, episode steps: 124, steps per second:  66, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.787036, mse: 2278.204590, mean_q: 63.497967\n",
      " 14757/50000: episode: 198, duration: 2.077s, episode steps: 136, steps per second:  65, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.449532, mse: 2283.883545, mean_q: 63.547695\n",
      " 14881/50000: episode: 199, duration: 1.888s, episode steps: 124, steps per second:  66, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 2.129992, mse: 2238.141846, mean_q: 62.594711\n",
      " 15002/50000: episode: 200, duration: 1.843s, episode steps: 121, steps per second:  66, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 1.677981, mse: 2296.008545, mean_q: 63.780048\n",
      " 15127/50000: episode: 201, duration: 1.921s, episode steps: 125, steps per second:  65, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.544 [0.000, 1.000],  loss: 1.564774, mse: 2238.111816, mean_q: 62.617908\n",
      " 15253/50000: episode: 202, duration: 1.907s, episode steps: 126, steps per second:  66, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 1.951556, mse: 2265.898193, mean_q: 63.330662\n",
      " 15387/50000: episode: 203, duration: 2.022s, episode steps: 134, steps per second:  66, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.806813, mse: 2335.122314, mean_q: 64.795753\n",
      " 15521/50000: episode: 204, duration: 2.002s, episode steps: 134, steps per second:  67, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.792985, mse: 2327.752686, mean_q: 64.517593\n",
      " 15659/50000: episode: 205, duration: 2.117s, episode steps: 138, steps per second:  65, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.809371, mse: 2328.507324, mean_q: 64.427467\n",
      " 15789/50000: episode: 206, duration: 1.977s, episode steps: 130, steps per second:  66, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.749508, mse: 2307.288574, mean_q: 64.113785\n",
      " 15914/50000: episode: 207, duration: 1.938s, episode steps: 125, steps per second:  64, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.595933, mse: 2301.480225, mean_q: 64.304207\n",
      " 16053/50000: episode: 208, duration: 2.129s, episode steps: 139, steps per second:  65, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.950632, mse: 2320.407227, mean_q: 64.572380\n",
      " 16174/50000: episode: 209, duration: 1.831s, episode steps: 121, steps per second:  66, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 2.034558, mse: 2344.256836, mean_q: 65.126289\n",
      " 16290/50000: episode: 210, duration: 1.799s, episode steps: 116, steps per second:  64, episode reward: 116.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.543 [0.000, 1.000],  loss: 2.013149, mse: 2361.082275, mean_q: 65.287682\n",
      " 16408/50000: episode: 211, duration: 1.826s, episode steps: 118, steps per second:  65, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.613779, mse: 2383.642822, mean_q: 65.644005\n",
      " 16466/50000: episode: 212, duration: 0.887s, episode steps:  58, steps per second:  65, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 2.645889, mse: 2330.355957, mean_q: 64.690819\n",
      " 16586/50000: episode: 213, duration: 1.845s, episode steps: 120, steps per second:  65, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.761818, mse: 2329.831787, mean_q: 64.838654\n",
      " 16709/50000: episode: 214, duration: 1.860s, episode steps: 123, steps per second:  66, episode reward: 123.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 2.466490, mse: 2408.089111, mean_q: 66.106941\n",
      " 16840/50000: episode: 215, duration: 1.989s, episode steps: 131, steps per second:  66, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.878288, mse: 2343.835449, mean_q: 65.109375\n",
      " 16976/50000: episode: 216, duration: 2.079s, episode steps: 136, steps per second:  65, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.047283, mse: 2380.215332, mean_q: 65.493179\n",
      " 17096/50000: episode: 217, duration: 1.825s, episode steps: 120, steps per second:  66, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 1.581518, mse: 2392.101318, mean_q: 65.789894\n",
      " 17220/50000: episode: 218, duration: 1.895s, episode steps: 124, steps per second:  65, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 1.511553, mse: 2354.781006, mean_q: 65.296463\n",
      " 17345/50000: episode: 219, duration: 1.893s, episode steps: 125, steps per second:  66, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 2.234742, mse: 2336.795654, mean_q: 64.682762\n",
      " 17489/50000: episode: 220, duration: 2.210s, episode steps: 144, steps per second:  65, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.579141, mse: 2337.517334, mean_q: 64.644043\n",
      " 17604/50000: episode: 221, duration: 1.771s, episode steps: 115, steps per second:  65, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 1.598346, mse: 2361.450195, mean_q: 65.290451\n",
      " 17730/50000: episode: 222, duration: 1.936s, episode steps: 126, steps per second:  65, episode reward: 126.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 1.421615, mse: 2364.251953, mean_q: 65.092186\n",
      " 17857/50000: episode: 223, duration: 1.934s, episode steps: 127, steps per second:  66, episode reward: 127.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.190049, mse: 2341.940186, mean_q: 64.593910\n",
      " 17976/50000: episode: 224, duration: 1.853s, episode steps: 119, steps per second:  64, episode reward: 119.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.829360, mse: 2396.472168, mean_q: 65.424767\n",
      " 18107/50000: episode: 225, duration: 1.972s, episode steps: 131, steps per second:  66, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.534 [0.000, 1.000],  loss: 2.052936, mse: 2340.388428, mean_q: 64.612793\n",
      " 18236/50000: episode: 226, duration: 1.951s, episode steps: 129, steps per second:  66, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 1.754624, mse: 2380.396240, mean_q: 65.319885\n",
      " 18357/50000: episode: 227, duration: 1.877s, episode steps: 121, steps per second:  64, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.367739, mse: 2309.084229, mean_q: 63.953712\n",
      " 18485/50000: episode: 228, duration: 1.936s, episode steps: 128, steps per second:  66, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.531 [0.000, 1.000],  loss: 1.322778, mse: 2297.333008, mean_q: 63.843784\n",
      " 18623/50000: episode: 229, duration: 2.109s, episode steps: 138, steps per second:  65, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.258702, mse: 2299.772949, mean_q: 63.874825\n",
      " 18748/50000: episode: 230, duration: 1.908s, episode steps: 125, steps per second:  66, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.242689, mse: 2302.891846, mean_q: 63.850105\n",
      " 18865/50000: episode: 231, duration: 1.777s, episode steps: 117, steps per second:  66, episode reward: 117.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 2.054439, mse: 2296.882324, mean_q: 63.706200\n",
      " 18985/50000: episode: 232, duration: 1.841s, episode steps: 120, steps per second:  65, episode reward: 120.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.863598, mse: 2311.996094, mean_q: 63.899487\n",
      " 19123/50000: episode: 233, duration: 2.076s, episode steps: 138, steps per second:  66, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.196050, mse: 2297.918457, mean_q: 63.583961\n",
      " 19262/50000: episode: 234, duration: 2.091s, episode steps: 139, steps per second:  66, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.271423, mse: 2305.639404, mean_q: 63.708927\n",
      " 19392/50000: episode: 235, duration: 1.997s, episode steps: 130, steps per second:  65, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 1.512469, mse: 2273.522461, mean_q: 63.006596\n",
      " 19524/50000: episode: 236, duration: 1.987s, episode steps: 132, steps per second:  66, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.379872, mse: 2271.480957, mean_q: 62.894894\n",
      " 19618/50000: episode: 237, duration: 1.430s, episode steps:  94, steps per second:  66, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.184023, mse: 2234.414551, mean_q: 62.384315\n",
      " 19757/50000: episode: 238, duration: 2.136s, episode steps: 139, steps per second:  65, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.715604, mse: 2283.168457, mean_q: 63.214821\n",
      " 19881/50000: episode: 239, duration: 1.867s, episode steps: 124, steps per second:  66, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.540 [0.000, 1.000],  loss: 1.143938, mse: 2246.889404, mean_q: 62.314442\n",
      " 20017/50000: episode: 240, duration: 2.080s, episode steps: 136, steps per second:  65, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.444444, mse: 2240.835693, mean_q: 62.239502\n",
      " 20150/50000: episode: 241, duration: 1.995s, episode steps: 133, steps per second:  67, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 1.732344, mse: 2255.125488, mean_q: 62.754185\n",
      " 20284/50000: episode: 242, duration: 2.025s, episode steps: 134, steps per second:  66, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.356179, mse: 2235.547119, mean_q: 62.238808\n",
      " 20425/50000: episode: 243, duration: 2.170s, episode steps: 141, steps per second:  65, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.414936, mse: 2219.236816, mean_q: 61.937660\n",
      " 20549/50000: episode: 244, duration: 1.865s, episode steps: 124, steps per second:  66, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1.250613, mse: 2233.068115, mean_q: 62.454357\n",
      " 20680/50000: episode: 245, duration: 1.992s, episode steps: 131, steps per second:  66, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 1.322048, mse: 2184.525879, mean_q: 61.617817\n",
      " 20821/50000: episode: 246, duration: 2.143s, episode steps: 141, steps per second:  66, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.558089, mse: 2181.175537, mean_q: 61.459656\n",
      " 20963/50000: episode: 247, duration: 2.165s, episode steps: 142, steps per second:  66, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 0.949348, mse: 2152.812256, mean_q: 61.037487\n",
      " 21113/50000: episode: 248, duration: 2.251s, episode steps: 150, steps per second:  67, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.464010, mse: 2170.365234, mean_q: 61.206242\n",
      " 21262/50000: episode: 249, duration: 2.260s, episode steps: 149, steps per second:  66, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 0.526554, mse: 2152.695068, mean_q: 61.063477\n",
      " 21279/50000: episode: 250, duration: 0.285s, episode steps:  17, steps per second:  60, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.364508, mse: 2172.134766, mean_q: 61.025478\n",
      " 21412/50000: episode: 251, duration: 2.046s, episode steps: 133, steps per second:  65, episode reward: 133.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.973177, mse: 2132.598633, mean_q: 60.620476\n",
      " 21575/50000: episode: 252, duration: 2.460s, episode steps: 163, steps per second:  66, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.589175, mse: 2097.783447, mean_q: 60.032135\n",
      " 21731/50000: episode: 253, duration: 2.338s, episode steps: 156, steps per second:  67, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 0.864785, mse: 2122.981934, mean_q: 60.390999\n",
      " 21891/50000: episode: 254, duration: 2.446s, episode steps: 160, steps per second:  65, episode reward: 160.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 0.871176, mse: 2128.809082, mean_q: 60.424946\n",
      " 22037/50000: episode: 255, duration: 2.210s, episode steps: 146, steps per second:  66, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 1.510295, mse: 2104.512207, mean_q: 60.298153\n",
      " 22182/50000: episode: 256, duration: 2.171s, episode steps: 145, steps per second:  67, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.810708, mse: 2057.342773, mean_q: 59.300472\n",
      " 22310/50000: episode: 257, duration: 1.947s, episode steps: 128, steps per second:  66, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 0.777187, mse: 2111.471191, mean_q: 60.439945\n",
      " 22438/50000: episode: 258, duration: 1.946s, episode steps: 128, steps per second:  66, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 0.923514, mse: 2087.712402, mean_q: 60.068810\n",
      " 22590/50000: episode: 259, duration: 2.285s, episode steps: 152, steps per second:  67, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 0.825059, mse: 2035.540283, mean_q: 59.102997\n",
      " 22727/50000: episode: 260, duration: 2.070s, episode steps: 137, steps per second:  66, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.547 [0.000, 1.000],  loss: 2.223296, mse: 2032.326172, mean_q: 59.117023\n",
      " 22868/50000: episode: 261, duration: 2.150s, episode steps: 141, steps per second:  66, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.316648, mse: 2011.023315, mean_q: 58.808136\n",
      " 23009/50000: episode: 262, duration: 2.173s, episode steps: 141, steps per second:  65, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.132471, mse: 1995.928589, mean_q: 58.653625\n",
      " 23146/50000: episode: 263, duration: 2.102s, episode steps: 137, steps per second:  65, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.555 [0.000, 1.000],  loss: 0.616280, mse: 2000.608521, mean_q: 58.645061\n",
      " 23278/50000: episode: 264, duration: 1.989s, episode steps: 132, steps per second:  66, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.553 [0.000, 1.000],  loss: 1.085366, mse: 2039.456909, mean_q: 59.461121\n",
      " 23420/50000: episode: 265, duration: 2.179s, episode steps: 142, steps per second:  65, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.549 [0.000, 1.000],  loss: 0.890791, mse: 2034.943481, mean_q: 59.462601\n",
      " 23575/50000: episode: 266, duration: 2.344s, episode steps: 155, steps per second:  66, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.585232, mse: 2022.679810, mean_q: 59.233292\n",
      " 23722/50000: episode: 267, duration: 2.278s, episode steps: 147, steps per second:  65, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.475975, mse: 2014.355713, mean_q: 59.087692\n",
      " 23871/50000: episode: 268, duration: 2.269s, episode steps: 149, steps per second:  66, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.523 [0.000, 1.000],  loss: 1.308925, mse: 1989.253784, mean_q: 58.564262\n",
      " 24039/50000: episode: 269, duration: 2.555s, episode steps: 168, steps per second:  66, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 1.472758, mse: 2013.839111, mean_q: 59.057198\n",
      " 24208/50000: episode: 270, duration: 2.578s, episode steps: 169, steps per second:  66, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.963696, mse: 1980.828003, mean_q: 58.617359\n",
      " 24411/50000: episode: 271, duration: 3.051s, episode steps: 203, steps per second:  67, episode reward: 203.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.079427, mse: 2017.176880, mean_q: 59.156128\n",
      " 24642/50000: episode: 272, duration: 3.458s, episode steps: 231, steps per second:  67, episode reward: 231.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.614047, mse: 2065.788330, mean_q: 60.078724\n",
      " 24849/50000: episode: 273, duration: 3.271s, episode steps: 207, steps per second:  63, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.544087, mse: 2074.276855, mean_q: 60.115398\n",
      " 25034/50000: episode: 274, duration: 2.848s, episode steps: 185, steps per second:  65, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 1.560581, mse: 2127.580322, mean_q: 60.994350\n",
      " 25334/50000: episode: 275, duration: 4.492s, episode steps: 300, steps per second:  67, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 0.822272, mse: 2151.723633, mean_q: 61.028919\n",
      " 25542/50000: episode: 276, duration: 3.130s, episode steps: 208, steps per second:  66, episode reward: 208.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.011651, mse: 2204.045410, mean_q: 62.008034\n",
      " 25681/50000: episode: 277, duration: 2.087s, episode steps: 139, steps per second:  67, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.468 [0.000, 1.000],  loss: 1.877579, mse: 2258.277832, mean_q: 62.830063\n",
      " 25879/50000: episode: 278, duration: 3.104s, episode steps: 198, steps per second:  64, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.475 [0.000, 1.000],  loss: 3.765601, mse: 2330.211670, mean_q: 63.670101\n",
      " 26155/50000: episode: 279, duration: 4.306s, episode steps: 276, steps per second:  64, episode reward: 276.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 4.553800, mse: 2361.876221, mean_q: 64.046577\n",
      " 26342/50000: episode: 280, duration: 2.906s, episode steps: 187, steps per second:  64, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 6.469999, mse: 2333.273438, mean_q: 63.365089\n",
      " 26534/50000: episode: 281, duration: 2.990s, episode steps: 192, steps per second:  64, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.390008, mse: 2342.491943, mean_q: 63.637131\n",
      " 26770/50000: episode: 282, duration: 3.639s, episode steps: 236, steps per second:  65, episode reward: 236.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 3.212249, mse: 2311.413086, mean_q: 63.118568\n",
      " 26960/50000: episode: 283, duration: 2.989s, episode steps: 190, steps per second:  64, episode reward: 190.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.537 [0.000, 1.000],  loss: 5.429397, mse: 2405.848389, mean_q: 64.567047\n",
      " 27090/50000: episode: 284, duration: 2.026s, episode steps: 130, steps per second:  64, episode reward: 130.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.546 [0.000, 1.000],  loss: 3.495853, mse: 2327.408691, mean_q: 63.424442\n",
      " 27230/50000: episode: 285, duration: 2.184s, episode steps: 140, steps per second:  64, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 1.735459, mse: 2399.060791, mean_q: 64.704926\n",
      " 27388/50000: episode: 286, duration: 2.447s, episode steps: 158, steps per second:  65, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.316683, mse: 2343.948486, mean_q: 63.493263\n",
      " 27541/50000: episode: 287, duration: 2.442s, episode steps: 153, steps per second:  63, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 3.926463, mse: 2383.233398, mean_q: 64.221062\n",
      " 27697/50000: episode: 288, duration: 2.429s, episode steps: 156, steps per second:  64, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.538 [0.000, 1.000],  loss: 0.975548, mse: 2372.965576, mean_q: 64.179855\n",
      " 27838/50000: episode: 289, duration: 2.150s, episode steps: 141, steps per second:  66, episode reward: 141.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.539 [0.000, 1.000],  loss: 0.642218, mse: 2367.344482, mean_q: 64.250336\n",
      " 27975/50000: episode: 290, duration: 2.070s, episode steps: 137, steps per second:  66, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.555 [0.000, 1.000],  loss: 1.893578, mse: 2383.919678, mean_q: 64.295731\n",
      " 28122/50000: episode: 291, duration: 2.229s, episode steps: 147, steps per second:  66, episode reward: 147.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.198703, mse: 2382.375000, mean_q: 64.251648\n",
      " 28247/50000: episode: 292, duration: 1.915s, episode steps: 125, steps per second:  65, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.464 [0.000, 1.000],  loss: 7.613154, mse: 2338.894531, mean_q: 63.926075\n",
      " 28401/50000: episode: 293, duration: 2.354s, episode steps: 154, steps per second:  65, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.552 [0.000, 1.000],  loss: 1.547298, mse: 2372.890137, mean_q: 64.404396\n",
      " 28535/50000: episode: 294, duration: 2.064s, episode steps: 134, steps per second:  65, episode reward: 134.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.560 [0.000, 1.000],  loss: 6.649171, mse: 2399.560547, mean_q: 64.919380\n",
      " 28681/50000: episode: 295, duration: 2.212s, episode steps: 146, steps per second:  66, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.548 [0.000, 1.000],  loss: 5.332756, mse: 2353.102783, mean_q: 64.123131\n",
      " 28831/50000: episode: 296, duration: 2.327s, episode steps: 150, steps per second:  64, episode reward: 150.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.963008, mse: 2375.339600, mean_q: 64.673897\n",
      " 28998/50000: episode: 297, duration: 2.518s, episode steps: 167, steps per second:  66, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.312087, mse: 2367.950195, mean_q: 64.355354\n",
      " 29137/50000: episode: 298, duration: 2.110s, episode steps: 139, steps per second:  66, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 5.755460, mse: 2380.960693, mean_q: 64.577507\n",
      " 29258/50000: episode: 299, duration: 1.849s, episode steps: 121, steps per second:  65, episode reward: 121.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 2.901329, mse: 2344.550293, mean_q: 63.947536\n",
      " 29401/50000: episode: 300, duration: 2.182s, episode steps: 143, steps per second:  66, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 1.002561, mse: 2361.189941, mean_q: 64.269798\n",
      " 29612/50000: episode: 301, duration: 3.216s, episode steps: 211, steps per second:  66, episode reward: 211.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.479 [0.000, 1.000],  loss: 4.460791, mse: 2401.895264, mean_q: 65.151833\n",
      " 29744/50000: episode: 302, duration: 2.038s, episode steps: 132, steps per second:  65, episode reward: 132.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.297348, mse: 2374.547852, mean_q: 64.177010\n",
      " 29899/50000: episode: 303, duration: 2.388s, episode steps: 155, steps per second:  65, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 2.757980, mse: 2383.907715, mean_q: 64.675301\n",
      " 30043/50000: episode: 304, duration: 2.197s, episode steps: 144, steps per second:  66, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.877030, mse: 2431.999512, mean_q: 65.340782\n",
      " 30194/50000: episode: 305, duration: 2.337s, episode steps: 151, steps per second:  65, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 3.118425, mse: 2408.811768, mean_q: 64.746651\n",
      " 30383/50000: episode: 306, duration: 2.968s, episode steps: 189, steps per second:  64, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 2.496127, mse: 2460.803223, mean_q: 65.747574\n",
      " 30570/50000: episode: 307, duration: 2.960s, episode steps: 187, steps per second:  63, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 2.263205, mse: 2426.334473, mean_q: 65.185326\n",
      " 30729/50000: episode: 308, duration: 2.459s, episode steps: 159, steps per second:  65, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.253708, mse: 2438.254395, mean_q: 65.413803\n",
      " 30908/50000: episode: 309, duration: 2.768s, episode steps: 179, steps per second:  65, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.564606, mse: 2471.480469, mean_q: 65.679771\n",
      " 31073/50000: episode: 310, duration: 2.513s, episode steps: 165, steps per second:  66, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.028793, mse: 2500.012207, mean_q: 65.981331\n",
      " 31238/50000: episode: 311, duration: 2.558s, episode steps: 165, steps per second:  64, episode reward: 165.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.749390, mse: 2429.558350, mean_q: 65.259781\n",
      " 31418/50000: episode: 312, duration: 2.768s, episode steps: 180, steps per second:  65, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.002275, mse: 2466.573242, mean_q: 65.779968\n",
      " 31613/50000: episode: 313, duration: 3.034s, episode steps: 195, steps per second:  64, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 3.923115, mse: 2510.073486, mean_q: 66.403610\n",
      " 31753/50000: episode: 314, duration: 2.124s, episode steps: 140, steps per second:  66, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.902030, mse: 2464.277100, mean_q: 65.810150\n",
      " 31924/50000: episode: 315, duration: 2.606s, episode steps: 171, steps per second:  66, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 2.693647, mse: 2434.490479, mean_q: 65.197815\n",
      " 32120/50000: episode: 316, duration: 3.007s, episode steps: 196, steps per second:  65, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.362200, mse: 2468.173584, mean_q: 65.873573\n",
      " 32317/50000: episode: 317, duration: 3.109s, episode steps: 197, steps per second:  63, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.605815, mse: 2485.046387, mean_q: 66.312378\n",
      " 32499/50000: episode: 318, duration: 2.858s, episode steps: 182, steps per second:  64, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.417042, mse: 2501.017822, mean_q: 66.438194\n",
      " 32672/50000: episode: 319, duration: 2.760s, episode steps: 173, steps per second:  63, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.337123, mse: 2528.210938, mean_q: 66.895325\n",
      " 32838/50000: episode: 320, duration: 2.627s, episode steps: 166, steps per second:  63, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 3.055633, mse: 2561.057861, mean_q: 67.665047\n",
      " 33018/50000: episode: 321, duration: 2.854s, episode steps: 180, steps per second:  63, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.565795, mse: 2555.497314, mean_q: 67.759521\n",
      " 33193/50000: episode: 322, duration: 2.728s, episode steps: 175, steps per second:  64, episode reward: 175.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.385853, mse: 2527.503662, mean_q: 67.164413\n",
      " 33355/50000: episode: 323, duration: 2.574s, episode steps: 162, steps per second:  63, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 2.207595, mse: 2525.928955, mean_q: 67.038177\n",
      " 33539/50000: episode: 324, duration: 2.880s, episode steps: 184, steps per second:  64, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 4.241110, mse: 2554.134521, mean_q: 67.275101\n",
      " 33692/50000: episode: 325, duration: 2.410s, episode steps: 153, steps per second:  63, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.959709, mse: 2621.419434, mean_q: 68.425270\n",
      " 33836/50000: episode: 326, duration: 2.281s, episode steps: 144, steps per second:  63, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 6.742507, mse: 2557.367188, mean_q: 67.394951\n",
      " 33967/50000: episode: 327, duration: 2.059s, episode steps: 131, steps per second:  64, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 2.211738, mse: 2572.604492, mean_q: 67.642029\n",
      " 34107/50000: episode: 328, duration: 2.210s, episode steps: 140, steps per second:  63, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 2.251968, mse: 2520.822266, mean_q: 66.685898\n",
      " 34270/50000: episode: 329, duration: 2.543s, episode steps: 163, steps per second:  64, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 2.180413, mse: 2581.592041, mean_q: 67.595329\n",
      " 34418/50000: episode: 330, duration: 2.327s, episode steps: 148, steps per second:  64, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.089358, mse: 2544.608643, mean_q: 66.896179\n",
      " 34573/50000: episode: 331, duration: 2.467s, episode steps: 155, steps per second:  63, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.471 [0.000, 1.000],  loss: 3.127836, mse: 2599.762451, mean_q: 67.818008\n",
      " 34727/50000: episode: 332, duration: 2.423s, episode steps: 154, steps per second:  64, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.490322, mse: 2576.084717, mean_q: 67.307861\n",
      " 34895/50000: episode: 333, duration: 2.641s, episode steps: 168, steps per second:  64, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 2.976196, mse: 2564.291016, mean_q: 67.157890\n",
      " 35017/50000: episode: 334, duration: 1.928s, episode steps: 122, steps per second:  63, episode reward: 122.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 1.838442, mse: 2516.109619, mean_q: 66.411636\n",
      " 35209/50000: episode: 335, duration: 3.042s, episode steps: 192, steps per second:  63, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 2.297086, mse: 2567.314941, mean_q: 67.166237\n",
      " 35353/50000: episode: 336, duration: 2.274s, episode steps: 144, steps per second:  63, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.542 [0.000, 1.000],  loss: 0.737387, mse: 2605.783447, mean_q: 67.749977\n",
      " 35499/50000: episode: 337, duration: 2.245s, episode steps: 146, steps per second:  65, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.647192, mse: 2546.113281, mean_q: 66.921967\n",
      " 35637/50000: episode: 338, duration: 2.107s, episode steps: 138, steps per second:  66, episode reward: 138.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 2.519262, mse: 2570.334229, mean_q: 67.265869\n",
      " 35790/50000: episode: 339, duration: 2.351s, episode steps: 153, steps per second:  65, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.719373, mse: 2535.880615, mean_q: 66.713936\n",
      " 35952/50000: episode: 340, duration: 2.466s, episode steps: 162, steps per second:  66, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.593817, mse: 2517.508789, mean_q: 66.247467\n",
      " 36113/50000: episode: 341, duration: 2.536s, episode steps: 161, steps per second:  63, episode reward: 161.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.814533, mse: 2570.485596, mean_q: 67.132072\n",
      " 36259/50000: episode: 342, duration: 2.236s, episode steps: 146, steps per second:  65, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.424566, mse: 2529.505127, mean_q: 66.471146\n",
      " 36416/50000: episode: 343, duration: 2.425s, episode steps: 157, steps per second:  65, episode reward: 157.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 1.794242, mse: 2559.611816, mean_q: 66.941391\n",
      " 36579/50000: episode: 344, duration: 2.568s, episode steps: 163, steps per second:  63, episode reward: 163.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1.839801, mse: 2552.273438, mean_q: 66.904137\n",
      " 36719/50000: episode: 345, duration: 2.138s, episode steps: 140, steps per second:  65, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.692995, mse: 2570.243408, mean_q: 67.029625\n",
      " 36878/50000: episode: 346, duration: 2.461s, episode steps: 159, steps per second:  65, episode reward: 159.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.651046, mse: 2513.456787, mean_q: 66.400124\n",
      " 37054/50000: episode: 347, duration: 2.702s, episode steps: 176, steps per second:  65, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.862405, mse: 2522.018799, mean_q: 66.393059\n",
      " 37209/50000: episode: 348, duration: 2.400s, episode steps: 155, steps per second:  65, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.228409, mse: 2558.070068, mean_q: 67.233490\n",
      " 37364/50000: episode: 349, duration: 2.380s, episode steps: 155, steps per second:  65, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 1.483485, mse: 2521.042480, mean_q: 66.230919\n",
      " 37522/50000: episode: 350, duration: 2.458s, episode steps: 158, steps per second:  64, episode reward: 158.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.927670, mse: 2496.306885, mean_q: 66.151482\n",
      " 37675/50000: episode: 351, duration: 2.363s, episode steps: 153, steps per second:  65, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.721418, mse: 2495.112305, mean_q: 65.998825\n",
      " 37821/50000: episode: 352, duration: 2.268s, episode steps: 146, steps per second:  64, episode reward: 146.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.569735, mse: 2529.602783, mean_q: 66.717339\n",
      " 37976/50000: episode: 353, duration: 2.414s, episode steps: 155, steps per second:  64, episode reward: 155.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.750142, mse: 2514.191162, mean_q: 66.640953\n",
      " 38138/50000: episode: 354, duration: 2.511s, episode steps: 162, steps per second:  65, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.111579, mse: 2502.083008, mean_q: 66.325005\n",
      " 38282/50000: episode: 355, duration: 2.226s, episode steps: 144, steps per second:  65, episode reward: 144.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.915346, mse: 2546.526367, mean_q: 67.094467\n",
      " 38434/50000: episode: 356, duration: 2.391s, episode steps: 152, steps per second:  64, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 1.182240, mse: 2506.942871, mean_q: 66.298775\n",
      " 38605/50000: episode: 357, duration: 2.648s, episode steps: 171, steps per second:  65, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 3.077351, mse: 2520.587402, mean_q: 66.803658\n",
      " 38745/50000: episode: 358, duration: 2.121s, episode steps: 140, steps per second:  66, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 1.324152, mse: 2510.560303, mean_q: 66.415359\n",
      " 38888/50000: episode: 359, duration: 2.237s, episode steps: 143, steps per second:  64, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.734020, mse: 2509.920898, mean_q: 66.452049\n",
      " 39039/50000: episode: 360, duration: 2.344s, episode steps: 151, steps per second:  64, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.349350, mse: 2455.854736, mean_q: 65.653214\n",
      " 39225/50000: episode: 361, duration: 2.906s, episode steps: 186, steps per second:  64, episode reward: 186.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.084638, mse: 2488.031250, mean_q: 66.124802\n",
      " 39393/50000: episode: 362, duration: 2.572s, episode steps: 168, steps per second:  65, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 1.794444, mse: 2491.612793, mean_q: 66.252884\n",
      " 39566/50000: episode: 363, duration: 2.693s, episode steps: 173, steps per second:  64, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.432636, mse: 2489.090332, mean_q: 66.053680\n",
      " 39746/50000: episode: 364, duration: 2.788s, episode steps: 180, steps per second:  65, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.601242, mse: 2502.953857, mean_q: 66.415634\n",
      " 39912/50000: episode: 365, duration: 2.566s, episode steps: 166, steps per second:  65, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.299331, mse: 2489.393066, mean_q: 66.101265\n",
      " 40088/50000: episode: 366, duration: 2.709s, episode steps: 176, steps per second:  65, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.818525, mse: 2513.997559, mean_q: 66.494263\n",
      " 40269/50000: episode: 367, duration: 2.807s, episode steps: 181, steps per second:  64, episode reward: 181.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 1.218685, mse: 2439.860596, mean_q: 65.048889\n",
      " 40448/50000: episode: 368, duration: 2.764s, episode steps: 179, steps per second:  65, episode reward: 179.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.002055, mse: 2479.617920, mean_q: 65.926193\n",
      " 40631/50000: episode: 369, duration: 2.806s, episode steps: 183, steps per second:  65, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.734083, mse: 2480.025146, mean_q: 65.761368\n",
      " 40814/50000: episode: 370, duration: 2.808s, episode steps: 183, steps per second:  65, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.668861, mse: 2471.366455, mean_q: 65.532379\n",
      " 40954/50000: episode: 371, duration: 2.225s, episode steps: 140, steps per second:  63, episode reward: 140.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 0.820491, mse: 2455.237305, mean_q: 65.196320\n",
      " 41158/50000: episode: 372, duration: 3.214s, episode steps: 204, steps per second:  63, episode reward: 204.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.321227, mse: 2469.024902, mean_q: 65.370789\n",
      " 41327/50000: episode: 373, duration: 2.680s, episode steps: 169, steps per second:  63, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.478586, mse: 2487.088379, mean_q: 65.704201\n",
      " 41516/50000: episode: 374, duration: 2.944s, episode steps: 189, steps per second:  64, episode reward: 189.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 1.033245, mse: 2474.162109, mean_q: 65.478081\n",
      " 41689/50000: episode: 375, duration: 2.649s, episode steps: 173, steps per second:  65, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 1.897351, mse: 2440.898926, mean_q: 65.073051\n",
      " 41861/50000: episode: 376, duration: 2.668s, episode steps: 172, steps per second:  64, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.506 [0.000, 1.000],  loss: 0.933932, mse: 2478.691650, mean_q: 65.604279\n",
      " 42070/50000: episode: 377, duration: 3.229s, episode steps: 209, steps per second:  65, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 1.115936, mse: 2458.396484, mean_q: 65.177078\n",
      " 42257/50000: episode: 378, duration: 2.945s, episode steps: 187, steps per second:  64, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 0.904272, mse: 2515.579834, mean_q: 66.054825\n",
      " 42424/50000: episode: 379, duration: 2.639s, episode steps: 167, steps per second:  63, episode reward: 167.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.875995, mse: 2464.988281, mean_q: 65.291389\n",
      " 42622/50000: episode: 380, duration: 3.142s, episode steps: 198, steps per second:  63, episode reward: 198.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.805558, mse: 2500.720703, mean_q: 65.823013\n",
      " 42771/50000: episode: 381, duration: 2.297s, episode steps: 149, steps per second:  65, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 1.296133, mse: 2486.735840, mean_q: 65.573730\n",
      " 42949/50000: episode: 382, duration: 2.774s, episode steps: 178, steps per second:  64, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.144188, mse: 2468.256104, mean_q: 65.239761\n",
      " 43145/50000: episode: 383, duration: 3.060s, episode steps: 196, steps per second:  64, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.535949, mse: 2498.313721, mean_q: 65.615044\n",
      " 43380/50000: episode: 384, duration: 3.617s, episode steps: 235, steps per second:  65, episode reward: 235.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 0.791708, mse: 2470.793701, mean_q: 65.107567\n",
      " 43531/50000: episode: 385, duration: 2.359s, episode steps: 151, steps per second:  64, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.056096, mse: 2465.797363, mean_q: 64.976097\n",
      " 43728/50000: episode: 386, duration: 3.103s, episode steps: 197, steps per second:  63, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.875518, mse: 2500.031494, mean_q: 65.567291\n",
      " 43900/50000: episode: 387, duration: 2.680s, episode steps: 172, steps per second:  64, episode reward: 172.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.512 [0.000, 1.000],  loss: 0.748609, mse: 2514.268066, mean_q: 66.045067\n",
      " 44080/50000: episode: 388, duration: 2.833s, episode steps: 180, steps per second:  64, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.121573, mse: 2509.667480, mean_q: 65.832535\n",
      " 44229/50000: episode: 389, duration: 2.363s, episode steps: 149, steps per second:  63, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 0.925546, mse: 2515.145020, mean_q: 65.983574\n",
      " 44414/50000: episode: 390, duration: 2.881s, episode steps: 185, steps per second:  64, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.508 [0.000, 1.000],  loss: 0.669381, mse: 2467.498291, mean_q: 64.904190\n",
      " 44583/50000: episode: 391, duration: 2.653s, episode steps: 169, steps per second:  64, episode reward: 169.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.966554, mse: 2500.466064, mean_q: 65.470406\n",
      " 44765/50000: episode: 392, duration: 2.820s, episode steps: 182, steps per second:  65, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.505 [0.000, 1.000],  loss: 1.030388, mse: 2480.843506, mean_q: 65.091972\n",
      " 44967/50000: episode: 393, duration: 3.134s, episode steps: 202, steps per second:  64, episode reward: 202.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.053411, mse: 2525.199219, mean_q: 65.857918\n",
      " 45115/50000: episode: 394, duration: 2.321s, episode steps: 148, steps per second:  64, episode reward: 148.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.835862, mse: 2466.562256, mean_q: 64.915550\n",
      " 45306/50000: episode: 395, duration: 2.913s, episode steps: 191, steps per second:  66, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 0.761394, mse: 2517.633301, mean_q: 65.660973\n",
      " 45458/50000: episode: 396, duration: 2.612s, episode steps: 152, steps per second:  58, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.971833, mse: 2513.879639, mean_q: 65.475479\n",
      " 45610/50000: episode: 397, duration: 2.361s, episode steps: 152, steps per second:  64, episode reward: 152.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.747707, mse: 2547.583496, mean_q: 66.084564\n",
      " 45739/50000: episode: 398, duration: 2.052s, episode steps: 129, steps per second:  63, episode reward: 129.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 0.636777, mse: 2593.563965, mean_q: 66.768303\n",
      " 45905/50000: episode: 399, duration: 2.609s, episode steps: 166, steps per second:  64, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.921102, mse: 2562.925781, mean_q: 66.219574\n",
      " 46059/50000: episode: 400, duration: 2.405s, episode steps: 154, steps per second:  64, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.091742, mse: 2546.900635, mean_q: 66.048790\n",
      " 46221/50000: episode: 401, duration: 2.573s, episode steps: 162, steps per second:  63, episode reward: 162.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 0.991174, mse: 2537.884766, mean_q: 66.014778\n",
      " 46389/50000: episode: 402, duration: 2.681s, episode steps: 168, steps per second:  63, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 1.071126, mse: 2566.613525, mean_q: 66.403343\n",
      " 46559/50000: episode: 403, duration: 2.681s, episode steps: 170, steps per second:  63, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 0.661386, mse: 2574.499023, mean_q: 66.415024\n",
      " 46737/50000: episode: 404, duration: 2.789s, episode steps: 178, steps per second:  64, episode reward: 178.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.596882, mse: 2608.744629, mean_q: 67.031906\n",
      " 46905/50000: episode: 405, duration: 2.697s, episode steps: 168, steps per second:  62, episode reward: 168.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 1.194020, mse: 2550.367920, mean_q: 66.006622\n",
      " 47056/50000: episode: 406, duration: 2.447s, episode steps: 151, steps per second:  62, episode reward: 151.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.477 [0.000, 1.000],  loss: 0.666813, mse: 2575.417969, mean_q: 66.463745\n",
      " 47274/50000: episode: 407, duration: 3.448s, episode steps: 218, steps per second:  63, episode reward: 218.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 0.857400, mse: 2578.062012, mean_q: 66.569992\n",
      " 47475/50000: episode: 408, duration: 3.236s, episode steps: 201, steps per second:  62, episode reward: 201.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 1.259443, mse: 2609.891113, mean_q: 66.994545\n",
      " 47670/50000: episode: 409, duration: 3.119s, episode steps: 195, steps per second:  63, episode reward: 195.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.702632, mse: 2584.523193, mean_q: 66.635498\n",
      " 47877/50000: episode: 410, duration: 3.243s, episode steps: 207, steps per second:  64, episode reward: 207.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.912480, mse: 2558.698730, mean_q: 66.400528\n",
      " 48050/50000: episode: 411, duration: 2.798s, episode steps: 173, steps per second:  62, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.584423, mse: 2575.903076, mean_q: 66.607697\n",
      " 48243/50000: episode: 412, duration: 3.099s, episode steps: 193, steps per second:  62, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 1.180929, mse: 2565.624023, mean_q: 66.589043\n",
      " 48436/50000: episode: 413, duration: 3.074s, episode steps: 193, steps per second:  63, episode reward: 193.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 0.979559, mse: 2531.011963, mean_q: 65.979111\n",
      " 48609/50000: episode: 414, duration: 2.824s, episode steps: 173, steps per second:  61, episode reward: 173.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 0.895215, mse: 2592.899902, mean_q: 67.050949\n",
      " 48780/50000: episode: 415, duration: 2.790s, episode steps: 171, steps per second:  61, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 0.693917, mse: 2500.664795, mean_q: 65.843338\n",
      " 48933/50000: episode: 416, duration: 2.440s, episode steps: 153, steps per second:  63, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 0.521216, mse: 2515.409180, mean_q: 65.683388\n",
      " 49113/50000: episode: 417, duration: 2.899s, episode steps: 180, steps per second:  62, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.632742, mse: 2522.622803, mean_q: 65.893463\n",
      " 49283/50000: episode: 418, duration: 2.712s, episode steps: 170, steps per second:  63, episode reward: 170.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 0.511615, mse: 2515.304688, mean_q: 65.967308\n",
      " 49460/50000: episode: 419, duration: 2.808s, episode steps: 177, steps per second:  63, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 1.070198, mse: 2550.829346, mean_q: 66.681068\n",
      " 49651/50000: episode: 420, duration: 3.008s, episode steps: 191, steps per second:  64, episode reward: 191.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 0.757604, mse: 2521.366699, mean_q: 66.073654\n",
      " 49827/50000: episode: 421, duration: 2.797s, episode steps: 176, steps per second:  63, episode reward: 176.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.158442, mse: 2492.289795, mean_q: 65.410721\n",
      "done, took 777.755 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 283.000, steps: 283\n",
      "Episode 2: reward: 214.000, steps: 214\n",
      "Episode 3: reward: 300.000, steps: 300\n",
      "Episode 4: reward: 259.000, steps: 259\n",
      "Episode 5: reward: 262.000, steps: 262\n",
      "Window_length: 4\n",
      "Training for 50000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    10/50000: episode: 1, duration: 0.733s, episode steps:  10, steps per second:  14, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: --, mse: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    19/50000: episode: 2, duration: 2.996s, episode steps:   9, steps per second:   3, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.524034, mse: 0.594394, mean_q: 0.651873\n",
      "    28/50000: episode: 3, duration: 0.169s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.363264, mse: 0.496213, mean_q: 0.983008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    37/50000: episode: 4, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.286177, mse: 0.564956, mean_q: 1.242197\n",
      "    47/50000: episode: 5, duration: 0.190s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.264803, mse: 0.556139, mean_q: 1.477296\n",
      "    58/50000: episode: 6, duration: 0.196s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.232300, mse: 0.471558, mean_q: 1.515074\n",
      "    72/50000: episode: 7, duration: 0.255s, episode steps:  14, steps per second:  55, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.143 [0.000, 1.000],  loss: 0.243182, mse: 0.614428, mean_q: 1.644238\n",
      "    82/50000: episode: 8, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.209777, mse: 0.638462, mean_q: 1.732659\n",
      "    91/50000: episode: 9, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.143618, mse: 0.736569, mean_q: 1.943895\n",
      "   101/50000: episode: 10, duration: 0.176s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.107627, mse: 0.797254, mean_q: 2.026465\n",
      "   110/50000: episode: 11, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.136568, mse: 0.963749, mean_q: 2.153431\n",
      "   118/50000: episode: 12, duration: 0.138s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.122747, mse: 1.029437, mean_q: 2.222236\n",
      "   128/50000: episode: 13, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.120154, mse: 1.102525, mean_q: 2.297836\n",
      "   141/50000: episode: 14, duration: 0.231s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.131971, mse: 1.201502, mean_q: 2.303730\n",
      "   151/50000: episode: 15, duration: 0.172s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.136081, mse: 1.262347, mean_q: 2.362722\n",
      "   162/50000: episode: 16, duration: 0.232s, episode steps:  11, steps per second:  47, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.132872, mse: 1.393185, mean_q: 2.454300\n",
      "   173/50000: episode: 17, duration: 0.224s, episode steps:  11, steps per second:  49, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.114749, mse: 1.580662, mean_q: 2.584841\n",
      "   183/50000: episode: 18, duration: 0.168s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.114220, mse: 1.646197, mean_q: 2.549684\n",
      "   193/50000: episode: 19, duration: 0.185s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.100358, mse: 2.046919, mean_q: 2.840487\n",
      "   202/50000: episode: 20, duration: 0.169s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.092725, mse: 1.934872, mean_q: 2.682009\n",
      "   211/50000: episode: 21, duration: 0.162s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.083628, mse: 2.142634, mean_q: 2.858306\n",
      "   222/50000: episode: 22, duration: 0.201s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.093271, mse: 2.198061, mean_q: 2.873349\n",
      "   235/50000: episode: 23, duration: 0.235s, episode steps:  13, steps per second:  55, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.154 [0.000, 1.000],  loss: 0.082492, mse: 2.437316, mean_q: 2.979495\n",
      "   245/50000: episode: 24, duration: 0.187s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.093382, mse: 2.574708, mean_q: 3.007974\n",
      "   253/50000: episode: 25, duration: 0.139s, episode steps:   8, steps per second:  58, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.084863, mse: 2.885495, mean_q: 3.136128\n",
      "   264/50000: episode: 26, duration: 0.189s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.089056, mse: 2.982733, mean_q: 3.101296\n",
      "   275/50000: episode: 27, duration: 0.206s, episode steps:  11, steps per second:  53, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.071699, mse: 3.205079, mean_q: 3.177115\n",
      "   286/50000: episode: 28, duration: 0.191s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.059112, mse: 3.577232, mean_q: 3.305145\n",
      "   295/50000: episode: 29, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.067340, mse: 3.798794, mean_q: 3.375515\n",
      "   305/50000: episode: 30, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.062636, mse: 3.984724, mean_q: 3.398130\n",
      "   314/50000: episode: 31, duration: 0.166s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.047130, mse: 3.879197, mean_q: 3.445410\n",
      "   323/50000: episode: 32, duration: 0.162s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.052539, mse: 3.980291, mean_q: 3.437632\n",
      "   333/50000: episode: 33, duration: 0.193s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.041892, mse: 4.203619, mean_q: 3.584466\n",
      "   344/50000: episode: 34, duration: 0.188s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.035861, mse: 4.552042, mean_q: 3.713628\n",
      "   356/50000: episode: 35, duration: 0.208s, episode steps:  12, steps per second:  58, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 0.036686, mse: 4.607893, mean_q: 3.647696\n",
      "   366/50000: episode: 36, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.034109, mse: 4.866107, mean_q: 3.745317\n",
      "   376/50000: episode: 37, duration: 0.191s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.031577, mse: 4.891508, mean_q: 3.659762\n",
      "   385/50000: episode: 38, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.027827, mse: 4.998535, mean_q: 3.696914\n",
      "   396/50000: episode: 39, duration: 0.197s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.022809, mse: 5.732647, mean_q: 3.950671\n",
      "   405/50000: episode: 40, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.023113, mse: 5.499324, mean_q: 3.803062\n",
      "   416/50000: episode: 41, duration: 0.194s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.026845, mse: 5.523593, mean_q: 3.791653\n",
      "   427/50000: episode: 42, duration: 0.188s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.020358, mse: 6.020817, mean_q: 4.021315\n",
      "   437/50000: episode: 43, duration: 0.184s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.030130, mse: 6.445643, mean_q: 4.183787\n",
      "   447/50000: episode: 44, duration: 0.182s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.024917, mse: 6.033501, mean_q: 3.882878\n",
      "   457/50000: episode: 45, duration: 0.171s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.023799, mse: 7.274426, mean_q: 4.301551\n",
      "   467/50000: episode: 46, duration: 0.174s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.022693, mse: 7.269276, mean_q: 4.215830\n",
      "   476/50000: episode: 47, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.020383, mse: 7.304617, mean_q: 4.176370\n",
      "   486/50000: episode: 48, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.021346, mse: 7.293238, mean_q: 4.141904\n",
      "   496/50000: episode: 49, duration: 0.174s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.019764, mse: 8.423910, mean_q: 4.540000\n",
      "   505/50000: episode: 50, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.024948, mse: 7.993362, mean_q: 4.258654\n",
      "   517/50000: episode: 51, duration: 0.206s, episode steps:  12, steps per second:  58, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.167 [0.000, 1.000],  loss: 0.020827, mse: 8.426311, mean_q: 4.370611\n",
      "   526/50000: episode: 52, duration: 0.158s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.021730, mse: 9.257838, mean_q: 4.537137\n",
      "   537/50000: episode: 53, duration: 0.195s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.027094, mse: 9.373424, mean_q: 4.500211\n",
      "   545/50000: episode: 54, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.024976, mse: 10.053629, mean_q: 4.619476\n",
      "   554/50000: episode: 55, duration: 0.179s, episode steps:   9, steps per second:  50, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.035622, mse: 9.672474, mean_q: 4.495369\n",
      "   563/50000: episode: 56, duration: 0.150s, episode steps:   9, steps per second:  60, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.019198, mse: 10.583078, mean_q: 4.708823\n",
      "   575/50000: episode: 57, duration: 0.210s, episode steps:  12, steps per second:  57, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.250 [0.000, 1.000],  loss: 0.023058, mse: 10.513985, mean_q: 4.612594\n",
      "   585/50000: episode: 58, duration: 0.175s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.024700, mse: 10.874993, mean_q: 4.729746\n",
      "   594/50000: episode: 59, duration: 0.169s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.024473, mse: 10.854910, mean_q: 4.610368\n",
      "   604/50000: episode: 60, duration: 0.171s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.028465, mse: 11.959112, mean_q: 4.790820\n",
      "   619/50000: episode: 61, duration: 0.267s, episode steps:  15, steps per second:  56, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.267 [0.000, 1.000],  loss: 0.021466, mse: 12.512348, mean_q: 4.839357\n",
      "   628/50000: episode: 62, duration: 0.155s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.222 [0.000, 1.000],  loss: 0.018072, mse: 12.911419, mean_q: 4.820459\n",
      "   637/50000: episode: 63, duration: 0.164s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.020855, mse: 13.244040, mean_q: 4.906252\n",
      "   647/50000: episode: 64, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.019845, mse: 14.337458, mean_q: 5.047651\n",
      "   657/50000: episode: 65, duration: 0.179s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.367643, mse: 15.194044, mean_q: 5.192056\n",
      "   667/50000: episode: 66, duration: 0.165s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.259515, mse: 13.122665, mean_q: 4.897362\n",
      "   677/50000: episode: 67, duration: 0.184s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.195856, mse: 12.010035, mean_q: 4.739891\n",
      "   691/50000: episode: 68, duration: 0.253s, episode steps:  14, steps per second:  55, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.057599, mse: 12.693605, mean_q: 5.052171\n",
      "   701/50000: episode: 69, duration: 0.167s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.035871, mse: 12.794134, mean_q: 4.963589\n",
      "   709/50000: episode: 70, duration: 0.157s, episode steps:   8, steps per second:  51, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.041797, mse: 14.547838, mean_q: 5.119312\n",
      "   720/50000: episode: 71, duration: 0.184s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.030311, mse: 16.224224, mean_q: 5.375993\n",
      "   729/50000: episode: 72, duration: 0.174s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.025039, mse: 15.942198, mean_q: 5.154219\n",
      "   740/50000: episode: 73, duration: 0.185s, episode steps:  11, steps per second:  59, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.029611, mse: 15.751069, mean_q: 5.047897\n",
      "   748/50000: episode: 74, duration: 0.148s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.673536, mse: 19.571400, mean_q: 5.646522\n",
      "   756/50000: episode: 75, duration: 0.164s, episode steps:   8, steps per second:  49, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.364141, mse: 16.589243, mean_q: 5.189871\n",
      "   766/50000: episode: 76, duration: 0.191s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.073594, mse: 17.583241, mean_q: 5.341372\n",
      "   774/50000: episode: 77, duration: 0.150s, episode steps:   8, steps per second:  53, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.322771, mse: 17.529995, mean_q: 5.324503\n",
      "   783/50000: episode: 78, duration: 0.175s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.492869, mse: 17.811081, mean_q: 5.358764\n",
      "   793/50000: episode: 79, duration: 0.165s, episode steps:  10, steps per second:  61, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.286288, mse: 19.938379, mean_q: 5.704407\n",
      "   802/50000: episode: 80, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.655667, mse: 20.153503, mean_q: 5.615388\n",
      "   816/50000: episode: 81, duration: 0.240s, episode steps:  14, steps per second:  58, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.786 [0.000, 1.000],  loss: 0.882130, mse: 18.108707, mean_q: 5.405715\n",
      "   831/50000: episode: 82, duration: 0.260s, episode steps:  15, steps per second:  58, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.376867, mse: 21.833403, mean_q: 5.894161\n",
      "   840/50000: episode: 83, duration: 0.172s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.363977, mse: 20.737085, mean_q: 5.947515\n",
      "   850/50000: episode: 84, duration: 0.188s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.837850, mse: 23.685734, mean_q: 6.381795\n",
      "   858/50000: episode: 85, duration: 0.146s, episode steps:   8, steps per second:  55, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.704768, mse: 22.718342, mean_q: 6.280607\n",
      "   869/50000: episode: 86, duration: 0.204s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.296575, mse: 24.342798, mean_q: 6.504413\n",
      "   880/50000: episode: 87, duration: 0.191s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.802448, mse: 25.352190, mean_q: 6.608194\n",
      "   889/50000: episode: 88, duration: 0.170s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.982836, mse: 22.383781, mean_q: 6.400535\n",
      "   898/50000: episode: 89, duration: 0.168s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.868923, mse: 25.272934, mean_q: 6.616626\n",
      "   908/50000: episode: 90, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.598925, mse: 26.953344, mean_q: 6.930707\n",
      "   916/50000: episode: 91, duration: 0.147s, episode steps:   8, steps per second:  54, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.692898, mse: 26.734232, mean_q: 7.013884\n",
      "   926/50000: episode: 92, duration: 0.183s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.718463, mse: 23.763996, mean_q: 6.465250\n",
      "   936/50000: episode: 93, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.733159, mse: 27.237015, mean_q: 6.902616\n",
      "   945/50000: episode: 94, duration: 0.157s, episode steps:   9, steps per second:  57, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.644405, mse: 29.893555, mean_q: 7.294378\n",
      "   955/50000: episode: 95, duration: 0.190s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.433531, mse: 27.036875, mean_q: 6.969107\n",
      "   965/50000: episode: 96, duration: 0.195s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.344233, mse: 31.097055, mean_q: 7.608592\n",
      "   975/50000: episode: 97, duration: 0.177s, episode steps:  10, steps per second:  57, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.365416, mse: 30.693151, mean_q: 7.434702\n",
      "   986/50000: episode: 98, duration: 0.204s, episode steps:  11, steps per second:  54, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.548447, mse: 27.650444, mean_q: 6.954094\n",
      "   995/50000: episode: 99, duration: 0.162s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.411415, mse: 29.048794, mean_q: 7.011259\n",
      "  1006/50000: episode: 100, duration: 0.195s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.233142, mse: 32.274418, mean_q: 7.573866\n",
      "  1015/50000: episode: 101, duration: 0.178s, episode steps:   9, steps per second:  51, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.386967, mse: 31.841238, mean_q: 7.788517\n",
      "  1025/50000: episode: 102, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.337632, mse: 31.997976, mean_q: 7.619534\n",
      "  1036/50000: episode: 103, duration: 0.200s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.469558, mse: 30.742189, mean_q: 7.363307\n",
      "  1047/50000: episode: 104, duration: 0.201s, episode steps:  11, steps per second:  55, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.281294, mse: 32.917473, mean_q: 7.639505\n",
      "  1057/50000: episode: 105, duration: 0.172s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.345865, mse: 33.606892, mean_q: 7.830940\n",
      "  1067/50000: episode: 106, duration: 0.188s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.303880, mse: 34.066235, mean_q: 7.756949\n",
      "  1076/50000: episode: 107, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.230189, mse: 33.162838, mean_q: 7.624212\n",
      "  1085/50000: episode: 108, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.274219, mse: 34.892708, mean_q: 7.958700\n",
      "  1095/50000: episode: 109, duration: 0.181s, episode steps:  10, steps per second:  55, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.199214, mse: 34.908432, mean_q: 7.917136\n",
      "  1105/50000: episode: 110, duration: 0.186s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.312007, mse: 33.684071, mean_q: 7.579755\n",
      "  1114/50000: episode: 111, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.278283, mse: 36.227604, mean_q: 7.945741\n",
      "  1124/50000: episode: 112, duration: 0.188s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.151189, mse: 35.324760, mean_q: 7.902632\n",
      "  1134/50000: episode: 113, duration: 0.177s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.158087, mse: 37.182682, mean_q: 8.146434\n",
      "  1144/50000: episode: 114, duration: 0.197s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.133823, mse: 38.236000, mean_q: 8.312426\n",
      "  1153/50000: episode: 115, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.133543, mse: 36.974068, mean_q: 8.151197\n",
      "  1163/50000: episode: 116, duration: 0.180s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.099306, mse: 38.633789, mean_q: 8.283034\n",
      "  1173/50000: episode: 117, duration: 0.172s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.104016, mse: 35.947514, mean_q: 7.897117\n",
      "  1182/50000: episode: 118, duration: 0.174s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.129414, mse: 39.399639, mean_q: 8.479305\n",
      "  1192/50000: episode: 119, duration: 0.173s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.087718, mse: 35.055454, mean_q: 7.617182\n",
      "  1204/50000: episode: 120, duration: 0.218s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.100885, mse: 37.047276, mean_q: 8.040668\n",
      "  1215/50000: episode: 121, duration: 0.188s, episode steps:  11, steps per second:  58, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.086593, mse: 36.884640, mean_q: 7.942368\n",
      "  1226/50000: episode: 122, duration: 0.193s, episode steps:  11, steps per second:  57, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.084530, mse: 36.834610, mean_q: 7.934741\n",
      "  1236/50000: episode: 123, duration: 0.185s, episode steps:  10, steps per second:  54, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.105623, mse: 37.611633, mean_q: 8.053889\n",
      "  1247/50000: episode: 124, duration: 0.196s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.099300, mse: 38.388012, mean_q: 8.142689\n",
      "  1258/50000: episode: 125, duration: 0.196s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.091628, mse: 39.595928, mean_q: 8.148515\n",
      "  1267/50000: episode: 126, duration: 0.167s, episode steps:   9, steps per second:  54, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.078582, mse: 39.409500, mean_q: 8.160293\n",
      "  1276/50000: episode: 127, duration: 0.156s, episode steps:   9, steps per second:  58, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.094803, mse: 39.408588, mean_q: 8.099987\n",
      "  1286/50000: episode: 128, duration: 0.178s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.081906, mse: 37.345623, mean_q: 7.894486\n",
      "  1296/50000: episode: 129, duration: 0.189s, episode steps:  10, steps per second:  53, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.054890, mse: 38.829750, mean_q: 7.963867\n",
      "  1308/50000: episode: 130, duration: 0.213s, episode steps:  12, steps per second:  56, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.083829, mse: 40.224213, mean_q: 8.222690\n",
      "  1323/50000: episode: 131, duration: 0.281s, episode steps:  15, steps per second:  53, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.086673, mse: 41.391365, mean_q: 8.360025\n",
      "  1333/50000: episode: 132, duration: 0.171s, episode steps:  10, steps per second:  58, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.161568, mse: 43.871830, mean_q: 8.615123\n",
      "  1343/50000: episode: 133, duration: 0.198s, episode steps:  10, steps per second:  50, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.090833, mse: 40.802338, mean_q: 8.210169\n",
      "  1351/50000: episode: 134, duration: 0.154s, episode steps:   8, steps per second:  52, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.142951, mse: 37.407776, mean_q: 7.918277\n",
      "  1360/50000: episode: 135, duration: 0.173s, episode steps:   9, steps per second:  52, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.121802, mse: 40.008224, mean_q: 8.253038\n",
      "  1371/50000: episode: 136, duration: 0.196s, episode steps:  11, steps per second:  56, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.157329, mse: 38.630619, mean_q: 7.762161\n",
      "  1384/50000: episode: 137, duration: 0.231s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.100978, mse: 39.832336, mean_q: 8.029136\n",
      "  1395/50000: episode: 138, duration: 0.180s, episode steps:  11, steps per second:  61, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.146932, mse: 41.637669, mean_q: 8.165880\n",
      "  1405/50000: episode: 139, duration: 0.195s, episode steps:  10, steps per second:  51, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.184850, mse: 38.739017, mean_q: 7.788393\n",
      "  1418/50000: episode: 140, duration: 0.233s, episode steps:  13, steps per second:  56, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.615 [0.000, 1.000],  loss: 0.138297, mse: 41.370667, mean_q: 8.206841\n",
      "  1434/50000: episode: 141, duration: 0.284s, episode steps:  16, steps per second:  56, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.562 [0.000, 1.000],  loss: 0.119127, mse: 39.614155, mean_q: 7.891151\n",
      "  1443/50000: episode: 142, duration: 0.160s, episode steps:   9, steps per second:  56, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.282935, mse: 37.660305, mean_q: 7.658246\n",
      "  1459/50000: episode: 143, duration: 0.295s, episode steps:  16, steps per second:  54, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.182448, mse: 38.265907, mean_q: 7.754721\n",
      "  1475/50000: episode: 144, duration: 0.288s, episode steps:  16, steps per second:  56, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.249660, mse: 41.395966, mean_q: 8.139090\n",
      "  1520/50000: episode: 145, duration: 0.746s, episode steps:  45, steps per second:  60, episode reward: 45.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.238826, mse: 38.274502, mean_q: 7.769520\n",
      "  1585/50000: episode: 146, duration: 1.066s, episode steps:  65, steps per second:  61, episode reward: 65.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.277794, mse: 41.064022, mean_q: 8.075976\n",
      "  1631/50000: episode: 147, duration: 0.739s, episode steps:  46, steps per second:  62, episode reward: 46.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.622231, mse: 44.998619, mean_q: 8.530968\n",
      "  1784/50000: episode: 148, duration: 2.466s, episode steps: 153, steps per second:  62, episode reward: 153.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 0.407995, mse: 49.866505, mean_q: 9.114105\n",
      "  1837/50000: episode: 149, duration: 0.864s, episode steps:  53, steps per second:  61, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 0.792387, mse: 55.846985, mean_q: 9.687504\n",
      "  1900/50000: episode: 150, duration: 1.027s, episode steps:  63, steps per second:  61, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 0.877590, mse: 60.483051, mean_q: 10.148321\n",
      "  1977/50000: episode: 151, duration: 1.270s, episode steps:  77, steps per second:  61, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.549566, mse: 65.562950, mean_q: 10.675298\n",
      "  2048/50000: episode: 152, duration: 1.149s, episode steps:  71, steps per second:  62, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 0.510821, mse: 66.716469, mean_q: 10.714133\n",
      "  2098/50000: episode: 153, duration: 0.820s, episode steps:  50, steps per second:  61, episode reward: 50.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 0.412980, mse: 74.090965, mean_q: 11.356663\n",
      "  2192/50000: episode: 154, duration: 1.510s, episode steps:  94, steps per second:  62, episode reward: 94.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 0.571243, mse: 79.576309, mean_q: 11.767630\n",
      "  2269/50000: episode: 155, duration: 1.273s, episode steps:  77, steps per second:  60, episode reward: 77.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.532 [0.000, 1.000],  loss: 0.634833, mse: 87.041634, mean_q: 12.253470\n",
      "  2327/50000: episode: 156, duration: 0.957s, episode steps:  58, steps per second:  61, episode reward: 58.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.872471, mse: 93.932945, mean_q: 12.835525\n",
      "  2387/50000: episode: 157, duration: 0.962s, episode steps:  60, steps per second:  62, episode reward: 60.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.758674, mse: 98.463676, mean_q: 13.204845\n",
      "  2454/50000: episode: 158, duration: 1.090s, episode steps:  67, steps per second:  61, episode reward: 67.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 0.670037, mse: 104.843674, mean_q: 13.665619\n",
      "  2578/50000: episode: 159, duration: 1.971s, episode steps: 124, steps per second:  63, episode reward: 124.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 0.535164, mse: 110.965141, mean_q: 14.050799\n",
      "  2641/50000: episode: 160, duration: 1.010s, episode steps:  63, steps per second:  62, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.437659, mse: 122.865959, mean_q: 14.897213\n",
      "  2707/50000: episode: 161, duration: 1.060s, episode steps:  66, steps per second:  62, episode reward: 66.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 0.485767, mse: 130.276398, mean_q: 15.346426\n",
      "  2770/50000: episode: 162, duration: 1.017s, episode steps:  63, steps per second:  62, episode reward: 63.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.476 [0.000, 1.000],  loss: 0.449026, mse: 135.028046, mean_q: 15.612038\n",
      "  2841/50000: episode: 163, duration: 1.136s, episode steps:  71, steps per second:  62, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.535 [0.000, 1.000],  loss: 0.504527, mse: 143.097183, mean_q: 16.057520\n",
      "  2933/50000: episode: 164, duration: 1.469s, episode steps:  92, steps per second:  63, episode reward: 92.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.599589, mse: 152.607193, mean_q: 16.705481\n",
      "  3113/50000: episode: 165, duration: 2.859s, episode steps: 180, steps per second:  63, episode reward: 180.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.483 [0.000, 1.000],  loss: 0.584522, mse: 170.115143, mean_q: 17.555664\n",
      "  3187/50000: episode: 166, duration: 1.175s, episode steps:  74, steps per second:  63, episode reward: 74.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.527 [0.000, 1.000],  loss: 0.472272, mse: 188.890579, mean_q: 18.548498\n",
      "  3434/50000: episode: 167, duration: 3.917s, episode steps: 247, steps per second:  63, episode reward: 247.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.478 [0.000, 1.000],  loss: 0.532697, mse: 212.886169, mean_q: 19.772377\n",
      "  3649/50000: episode: 168, duration: 3.406s, episode steps: 215, steps per second:  63, episode reward: 215.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 0.785359, mse: 254.670914, mean_q: 21.716610\n",
      "  3907/50000: episode: 169, duration: 4.136s, episode steps: 258, steps per second:  62, episode reward: 258.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.488 [0.000, 1.000],  loss: 1.219111, mse: 309.066437, mean_q: 23.968416\n",
      "  4207/50000: episode: 170, duration: 4.778s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 1.465452, mse: 391.643463, mean_q: 26.873785\n",
      "  4507/50000: episode: 171, duration: 4.793s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.851517, mse: 482.001770, mean_q: 29.900215\n",
      "  4807/50000: episode: 172, duration: 4.733s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 2.642909, mse: 572.493835, mean_q: 32.717957\n",
      "  5107/50000: episode: 173, duration: 4.768s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 2.352626, mse: 663.610962, mean_q: 35.385098\n",
      "  5407/50000: episode: 174, duration: 4.757s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 3.220084, mse: 741.880981, mean_q: 37.587410\n",
      "  5705/50000: episode: 175, duration: 4.718s, episode steps: 298, steps per second:  63, episode reward: 298.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 3.219273, mse: 830.708252, mean_q: 40.087708\n",
      "  5949/50000: episode: 176, duration: 3.910s, episode steps: 244, steps per second:  62, episode reward: 244.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 4.185197, mse: 914.506470, mean_q: 42.175697\n",
      "  6249/50000: episode: 177, duration: 4.789s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 4.349222, mse: 987.522888, mean_q: 43.774734\n",
      "  6549/50000: episode: 178, duration: 4.742s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 3.751099, mse: 1073.103516, mean_q: 45.804096\n",
      "  6849/50000: episode: 179, duration: 4.788s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 3.867455, mse: 1164.932617, mean_q: 47.755543\n",
      "  7149/50000: episode: 180, duration: 4.751s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 3.705489, mse: 1244.300781, mean_q: 49.403336\n",
      "  7449/50000: episode: 181, duration: 4.758s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 3.666099, mse: 1341.040405, mean_q: 51.517273\n",
      "  7749/50000: episode: 182, duration: 4.867s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.336068, mse: 1441.569946, mean_q: 53.626011\n",
      "  8049/50000: episode: 183, duration: 4.780s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.396091, mse: 1536.735962, mean_q: 55.223530\n",
      "  8349/50000: episode: 184, duration: 4.786s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.220338, mse: 1617.156616, mean_q: 56.768925\n",
      "  8649/50000: episode: 185, duration: 4.854s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.711458, mse: 1699.845947, mean_q: 58.098732\n",
      "  8949/50000: episode: 186, duration: 4.853s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.022465, mse: 1763.634399, mean_q: 59.188000\n",
      "  9249/50000: episode: 187, duration: 4.801s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 5.211308, mse: 1836.938354, mean_q: 60.537090\n",
      "  9533/50000: episode: 188, duration: 4.551s, episode steps: 284, steps per second:  62, episode reward: 284.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 5.528872, mse: 1930.403198, mean_q: 62.176117\n",
      "  9833/50000: episode: 189, duration: 4.790s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 4.848035, mse: 2020.916870, mean_q: 63.682747\n",
      " 10133/50000: episode: 190, duration: 4.883s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.939348, mse: 2127.597900, mean_q: 65.452286\n",
      " 10433/50000: episode: 191, duration: 4.907s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 5.393448, mse: 2229.341553, mean_q: 66.759010\n",
      " 10733/50000: episode: 192, duration: 4.918s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 3.637805, mse: 2328.799072, mean_q: 68.294884\n",
      " 11033/50000: episode: 193, duration: 4.930s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.227102, mse: 2410.564697, mean_q: 69.297302\n",
      " 11333/50000: episode: 194, duration: 4.805s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.289662, mse: 2503.992676, mean_q: 70.481285\n",
      " 11633/50000: episode: 195, duration: 4.886s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 6.415805, mse: 2562.692383, mean_q: 71.227600\n",
      " 11933/50000: episode: 196, duration: 4.888s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 5.822184, mse: 2663.804688, mean_q: 72.624931\n",
      " 12233/50000: episode: 197, duration: 4.836s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 6.603102, mse: 2754.930420, mean_q: 73.982567\n",
      " 12520/50000: episode: 198, duration: 4.581s, episode steps: 287, steps per second:  63, episode reward: 287.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.516 [0.000, 1.000],  loss: 5.913585, mse: 2821.938965, mean_q: 74.694473\n",
      " 12707/50000: episode: 199, duration: 2.987s, episode steps: 187, steps per second:  63, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 7.320673, mse: 2872.314941, mean_q: 75.489052\n",
      " 13007/50000: episode: 200, duration: 4.812s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.680840, mse: 2894.180420, mean_q: 75.640549\n",
      " 13307/50000: episode: 201, duration: 4.827s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 5.877132, mse: 3017.636963, mean_q: 77.088005\n",
      " 13607/50000: episode: 202, duration: 4.798s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.771107, mse: 3114.726562, mean_q: 78.372978\n",
      " 13839/50000: episode: 203, duration: 3.692s, episode steps: 232, steps per second:  63, episode reward: 232.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 8.026787, mse: 3184.447754, mean_q: 79.115608\n",
      " 14139/50000: episode: 204, duration: 4.806s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 5.837965, mse: 3226.861328, mean_q: 79.584503\n",
      " 14439/50000: episode: 205, duration: 4.786s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 7.908336, mse: 3296.654053, mean_q: 80.391441\n",
      " 14739/50000: episode: 206, duration: 4.883s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.150755, mse: 3369.468018, mean_q: 81.379005\n",
      " 15039/50000: episode: 207, duration: 4.832s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.113791, mse: 3455.304199, mean_q: 82.388634\n",
      " 15339/50000: episode: 208, duration: 4.780s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.309321, mse: 3504.149170, mean_q: 82.725029\n",
      " 15548/50000: episode: 209, duration: 3.423s, episode steps: 209, steps per second:  61, episode reward: 209.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.536 [0.000, 1.000],  loss: 7.978708, mse: 3561.876465, mean_q: 83.580948\n",
      " 15848/50000: episode: 210, duration: 4.771s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.344629, mse: 3622.872070, mean_q: 84.303375\n",
      " 16148/50000: episode: 211, duration: 4.850s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.871818, mse: 3672.276367, mean_q: 84.812477\n",
      " 16448/50000: episode: 212, duration: 4.801s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 7.970913, mse: 3731.702148, mean_q: 85.554543\n",
      " 16748/50000: episode: 213, duration: 4.852s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 7.600915, mse: 3766.751221, mean_q: 86.014534\n",
      " 17048/50000: episode: 214, duration: 4.844s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 8.795855, mse: 3809.227051, mean_q: 86.393570\n",
      " 17348/50000: episode: 215, duration: 4.748s, episode steps: 300, steps per second:  63, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 9.899716, mse: 3857.807617, mean_q: 86.979195\n",
      " 17648/50000: episode: 216, duration: 4.814s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 7.895298, mse: 3951.254150, mean_q: 88.096870\n",
      " 17899/50000: episode: 217, duration: 4.091s, episode steps: 251, steps per second:  61, episode reward: 251.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 8.234191, mse: 4006.926270, mean_q: 88.430840\n",
      " 18173/50000: episode: 218, duration: 4.543s, episode steps: 274, steps per second:  60, episode reward: 274.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 8.293220, mse: 4039.541504, mean_q: 89.008255\n",
      " 18473/50000: episode: 219, duration: 4.943s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.630218, mse: 4116.068359, mean_q: 89.573097\n",
      " 18735/50000: episode: 220, duration: 4.286s, episode steps: 262, steps per second:  61, episode reward: 262.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 7.782827, mse: 4090.550537, mean_q: 89.299934\n",
      " 18931/50000: episode: 221, duration: 3.154s, episode steps: 196, steps per second:  62, episode reward: 196.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.520 [0.000, 1.000],  loss: 8.070006, mse: 4132.081055, mean_q: 89.815590\n",
      " 19203/50000: episode: 222, duration: 4.428s, episode steps: 272, steps per second:  61, episode reward: 272.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 9.190957, mse: 4158.209473, mean_q: 89.949287\n",
      " 19503/50000: episode: 223, duration: 4.971s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.492065, mse: 4221.117188, mean_q: 90.654366\n",
      " 19769/50000: episode: 224, duration: 4.404s, episode steps: 266, steps per second:  60, episode reward: 266.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 8.991366, mse: 4218.984863, mean_q: 90.442520\n",
      " 20069/50000: episode: 225, duration: 4.906s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 8.978053, mse: 4265.453125, mean_q: 90.922829\n",
      " 20369/50000: episode: 226, duration: 4.955s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 9.371471, mse: 4313.076660, mean_q: 91.570625\n",
      " 20669/50000: episode: 227, duration: 5.025s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 8.290030, mse: 4331.421875, mean_q: 91.773109\n",
      " 20883/50000: episode: 228, duration: 3.575s, episode steps: 214, steps per second:  60, episode reward: 214.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.491 [0.000, 1.000],  loss: 7.349932, mse: 4332.191406, mean_q: 91.734543\n",
      " 21123/50000: episode: 229, duration: 3.890s, episode steps: 240, steps per second:  62, episode reward: 240.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.487 [0.000, 1.000],  loss: 8.895310, mse: 4431.407227, mean_q: 92.817596\n",
      " 21423/50000: episode: 230, duration: 4.865s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 7.834394, mse: 4356.991211, mean_q: 91.609009\n",
      " 21678/50000: episode: 231, duration: 4.137s, episode steps: 255, steps per second:  62, episode reward: 255.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 9.909636, mse: 4359.278809, mean_q: 91.617554\n",
      " 21978/50000: episode: 232, duration: 4.850s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 7.093628, mse: 4401.386230, mean_q: 92.042915\n",
      " 22278/50000: episode: 233, duration: 4.860s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 8.125443, mse: 4414.366699, mean_q: 92.117683\n",
      " 22578/50000: episode: 234, duration: 4.977s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.353992, mse: 4431.493652, mean_q: 92.140114\n",
      " 22878/50000: episode: 235, duration: 4.916s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 8.458878, mse: 4418.792969, mean_q: 92.073174\n",
      " 23178/50000: episode: 236, duration: 4.874s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 10.554515, mse: 4419.430664, mean_q: 91.925003\n",
      " 23398/50000: episode: 237, duration: 3.596s, episode steps: 220, steps per second:  61, episode reward: 220.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.518 [0.000, 1.000],  loss: 7.917300, mse: 4391.304199, mean_q: 91.406364\n",
      " 23698/50000: episode: 238, duration: 4.832s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.384589, mse: 4420.088867, mean_q: 91.879509\n",
      " 23998/50000: episode: 239, duration: 4.892s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.765990, mse: 4465.168945, mean_q: 92.310074\n",
      " 24298/50000: episode: 240, duration: 4.946s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.121444, mse: 4415.572266, mean_q: 91.486382\n",
      " 24598/50000: episode: 241, duration: 4.967s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 10.930286, mse: 4369.056641, mean_q: 91.021759\n",
      " 24898/50000: episode: 242, duration: 4.902s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 9.030322, mse: 4378.122559, mean_q: 91.212158\n",
      " 25198/50000: episode: 243, duration: 4.880s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.923807, mse: 4341.642578, mean_q: 90.701485\n",
      " 25479/50000: episode: 244, duration: 4.599s, episode steps: 281, steps per second:  61, episode reward: 281.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 8.057296, mse: 4369.950195, mean_q: 90.950928\n",
      " 25779/50000: episode: 245, duration: 4.897s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 6.923952, mse: 4376.319336, mean_q: 90.921837\n",
      " 26079/50000: episode: 246, duration: 4.949s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 4.594460, mse: 4350.035645, mean_q: 90.627151\n",
      " 26379/50000: episode: 247, duration: 5.099s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 5.138374, mse: 4389.305176, mean_q: 91.037277\n",
      " 26679/50000: episode: 248, duration: 5.000s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.906177, mse: 4369.618164, mean_q: 90.598282\n",
      " 26979/50000: episode: 249, duration: 5.037s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 8.612879, mse: 4439.449707, mean_q: 91.543213\n",
      " 27150/50000: episode: 250, duration: 2.889s, episode steps: 171, steps per second:  59, episode reward: 171.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 9.124179, mse: 4354.151367, mean_q: 90.436310\n",
      " 27338/50000: episode: 251, duration: 3.158s, episode steps: 188, steps per second:  60, episode reward: 188.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 8.640378, mse: 4452.474609, mean_q: 91.547585\n",
      " 27522/50000: episode: 252, duration: 3.108s, episode steps: 184, steps per second:  59, episode reward: 184.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 11.454241, mse: 4474.005371, mean_q: 91.931198\n",
      " 27822/50000: episode: 253, duration: 5.024s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 7.578634, mse: 4428.450684, mean_q: 91.326561\n",
      " 28009/50000: episode: 254, duration: 3.125s, episode steps: 187, steps per second:  60, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 8.208348, mse: 4364.666016, mean_q: 90.308632\n",
      " 28196/50000: episode: 255, duration: 3.141s, episode steps: 187, steps per second:  60, episode reward: 187.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 9.775801, mse: 4370.433105, mean_q: 90.522057\n",
      " 28433/50000: episode: 256, duration: 3.983s, episode steps: 237, steps per second:  60, episode reward: 237.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 6.956147, mse: 4353.988281, mean_q: 90.354477\n",
      " 28733/50000: episode: 257, duration: 5.041s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 9.172863, mse: 4333.841797, mean_q: 90.159782\n",
      " 29033/50000: episode: 258, duration: 5.012s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 5.438342, mse: 4337.274902, mean_q: 90.234421\n",
      " 29308/50000: episode: 259, duration: 4.539s, episode steps: 275, steps per second:  61, episode reward: 275.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 10.503095, mse: 4301.384766, mean_q: 89.874687\n",
      " 29576/50000: episode: 260, duration: 4.506s, episode steps: 268, steps per second:  59, episode reward: 268.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.522 [0.000, 1.000],  loss: 6.175437, mse: 4330.099609, mean_q: 90.186180\n",
      " 29789/50000: episode: 261, duration: 3.562s, episode steps: 213, steps per second:  60, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.502 [0.000, 1.000],  loss: 5.905616, mse: 4337.438477, mean_q: 90.265862\n",
      " 30002/50000: episode: 262, duration: 3.596s, episode steps: 213, steps per second:  59, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.484 [0.000, 1.000],  loss: 6.364575, mse: 4291.375488, mean_q: 89.605019\n",
      " 30158/50000: episode: 263, duration: 2.629s, episode steps: 156, steps per second:  59, episode reward: 156.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.481 [0.000, 1.000],  loss: 8.579195, mse: 4413.878418, mean_q: 91.075699\n",
      " 30390/50000: episode: 264, duration: 3.839s, episode steps: 232, steps per second:  60, episode reward: 232.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 5.260440, mse: 4340.980957, mean_q: 90.074837\n",
      " 30690/50000: episode: 265, duration: 4.941s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 5.757316, mse: 4283.089844, mean_q: 89.686615\n",
      " 30918/50000: episode: 266, duration: 3.710s, episode steps: 228, steps per second:  61, episode reward: 228.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.474 [0.000, 1.000],  loss: 9.875164, mse: 4311.641113, mean_q: 89.969078\n",
      " 31131/50000: episode: 267, duration: 3.489s, episode steps: 213, steps per second:  61, episode reward: 213.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 5.741401, mse: 4356.895996, mean_q: 90.380829\n",
      " 31308/50000: episode: 268, duration: 2.890s, episode steps: 177, steps per second:  61, episode reward: 177.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 4.665292, mse: 4311.185547, mean_q: 89.873314\n",
      " 31457/50000: episode: 269, duration: 2.386s, episode steps: 149, steps per second:  62, episode reward: 149.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 11.845153, mse: 4361.765137, mean_q: 90.470718\n",
      " 31623/50000: episode: 270, duration: 2.704s, episode steps: 166, steps per second:  61, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.482 [0.000, 1.000],  loss: 4.540316, mse: 4286.447266, mean_q: 89.650246\n",
      " 31759/50000: episode: 271, duration: 2.250s, episode steps: 136, steps per second:  60, episode reward: 136.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.456 [0.000, 1.000],  loss: 11.020734, mse: 4322.741699, mean_q: 90.230507\n",
      " 31904/50000: episode: 272, duration: 2.390s, episode steps: 145, steps per second:  61, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 7.302120, mse: 4286.142090, mean_q: 89.828125\n",
      " 32070/50000: episode: 273, duration: 2.721s, episode steps: 166, steps per second:  61, episode reward: 166.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.494 [0.000, 1.000],  loss: 10.352704, mse: 4294.841309, mean_q: 89.706688\n",
      " 32370/50000: episode: 274, duration: 4.926s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.813823, mse: 4233.976074, mean_q: 88.919586\n",
      " 32615/50000: episode: 275, duration: 4.051s, episode steps: 245, steps per second:  60, episode reward: 245.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 7.767587, mse: 4271.763184, mean_q: 89.382774\n",
      " 32915/50000: episode: 276, duration: 4.923s, episode steps: 300, steps per second:  61, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 7.822373, mse: 4280.270020, mean_q: 89.679619\n",
      " 33139/50000: episode: 277, duration: 3.726s, episode steps: 224, steps per second:  60, episode reward: 224.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 7.258972, mse: 4250.918457, mean_q: 89.477425\n",
      " 33439/50000: episode: 278, duration: 5.043s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 4.390250, mse: 4279.387207, mean_q: 89.765549\n",
      " 33739/50000: episode: 279, duration: 5.033s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 8.876156, mse: 4214.286621, mean_q: 89.109665\n",
      " 33922/50000: episode: 280, duration: 3.032s, episode steps: 183, steps per second:  60, episode reward: 183.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.541 [0.000, 1.000],  loss: 6.559386, mse: 4269.121094, mean_q: 89.547043\n",
      " 34222/50000: episode: 281, duration: 5.059s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 10.476151, mse: 4234.398926, mean_q: 89.442917\n",
      " 34407/50000: episode: 282, duration: 3.177s, episode steps: 185, steps per second:  58, episode reward: 185.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 8.282618, mse: 4230.678711, mean_q: 89.444183\n",
      " 34707/50000: episode: 283, duration: 5.039s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 7.222250, mse: 4255.603516, mean_q: 89.718018\n",
      " 35007/50000: episode: 284, duration: 5.101s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 7.332763, mse: 4281.443359, mean_q: 90.009872\n",
      " 35199/50000: episode: 285, duration: 3.215s, episode steps: 192, steps per second:  60, episode reward: 192.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.521 [0.000, 1.000],  loss: 11.279405, mse: 4316.818848, mean_q: 90.390541\n",
      " 35499/50000: episode: 286, duration: 5.002s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.464570, mse: 4271.390625, mean_q: 90.034294\n",
      " 35799/50000: episode: 287, duration: 5.086s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.842801, mse: 4311.342773, mean_q: 90.640709\n",
      " 36099/50000: episode: 288, duration: 5.063s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 11.697492, mse: 4279.733398, mean_q: 90.259392\n",
      " 36399/50000: episode: 289, duration: 5.162s, episode steps: 300, steps per second:  58, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 9.428538, mse: 4284.741699, mean_q: 90.570496\n",
      " 36699/50000: episode: 290, duration: 5.099s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 6.676657, mse: 4345.958496, mean_q: 91.288696\n",
      " 36834/50000: episode: 291, duration: 2.300s, episode steps: 135, steps per second:  59, episode reward: 135.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.496 [0.000, 1.000],  loss: 7.142757, mse: 4353.509277, mean_q: 91.481415\n",
      " 36949/50000: episode: 292, duration: 1.955s, episode steps: 115, steps per second:  59, episode reward: 115.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 14.805905, mse: 4333.827637, mean_q: 91.119583\n",
      " 37249/50000: episode: 293, duration: 5.055s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 9.143629, mse: 4328.833984, mean_q: 91.103882\n",
      " 37423/50000: episode: 294, duration: 2.994s, episode steps: 174, steps per second:  58, episode reward: 174.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 10.171923, mse: 4340.224121, mean_q: 91.382866\n",
      " 37723/50000: episode: 295, duration: 5.027s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 12.566065, mse: 4360.901367, mean_q: 91.645042\n",
      " 38023/50000: episode: 296, duration: 5.127s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 7.743128, mse: 4384.525879, mean_q: 92.094177\n",
      " 38323/50000: episode: 297, duration: 5.047s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 11.173047, mse: 4427.725098, mean_q: 92.615181\n",
      " 38623/50000: episode: 298, duration: 5.084s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 7.828999, mse: 4409.250000, mean_q: 92.492706\n",
      " 38923/50000: episode: 299, duration: 5.052s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 11.941797, mse: 4484.766602, mean_q: 93.355400\n",
      " 39120/50000: episode: 300, duration: 3.346s, episode steps: 197, steps per second:  59, episode reward: 197.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 7.096241, mse: 4494.592773, mean_q: 93.531105\n",
      " 39420/50000: episode: 301, duration: 5.063s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.027238, mse: 4550.878906, mean_q: 94.127167\n",
      " 39720/50000: episode: 302, duration: 5.081s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 9.221100, mse: 4568.240723, mean_q: 94.364861\n",
      " 40020/50000: episode: 303, duration: 5.053s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 10.746740, mse: 4609.476074, mean_q: 94.796509\n",
      " 40320/50000: episode: 304, duration: 5.080s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 10.654201, mse: 4659.359863, mean_q: 95.715172\n",
      " 40620/50000: episode: 305, duration: 5.138s, episode steps: 300, steps per second:  58, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 9.412406, mse: 4775.992188, mean_q: 97.070702\n",
      " 40920/50000: episode: 306, duration: 5.122s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 10.074484, mse: 4887.757324, mean_q: 98.305222\n",
      " 41220/50000: episode: 307, duration: 5.040s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 12.660137, mse: 4977.284668, mean_q: 99.257538\n",
      " 41520/50000: episode: 308, duration: 5.039s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 8.737806, mse: 5042.913574, mean_q: 99.747253\n",
      " 41820/50000: episode: 309, duration: 5.088s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 16.766823, mse: 5092.430176, mean_q: 100.321968\n",
      " 42120/50000: episode: 310, duration: 5.058s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 12.143815, mse: 5130.949219, mean_q: 100.723465\n",
      " 42420/50000: episode: 311, duration: 5.067s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 12.974421, mse: 5209.884277, mean_q: 101.613564\n",
      " 42720/50000: episode: 312, duration: 5.034s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.493 [0.000, 1.000],  loss: 15.295983, mse: 5265.189453, mean_q: 102.091347\n",
      " 43020/50000: episode: 313, duration: 5.102s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 16.099497, mse: 5347.365723, mean_q: 103.006462\n",
      " 43320/50000: episode: 314, duration: 5.088s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 12.470473, mse: 5465.813477, mean_q: 104.258850\n",
      " 43620/50000: episode: 315, duration: 5.075s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 15.729108, mse: 5504.511719, mean_q: 104.552536\n",
      " 43920/50000: episode: 316, duration: 5.152s, episode steps: 300, steps per second:  58, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.497 [0.000, 1.000],  loss: 20.006872, mse: 5557.345215, mean_q: 104.961922\n",
      " 44220/50000: episode: 317, duration: 5.080s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 13.689548, mse: 5595.323242, mean_q: 105.382957\n",
      " 44504/50000: episode: 318, duration: 4.803s, episode steps: 284, steps per second:  59, episode reward: 284.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 16.029661, mse: 5621.256348, mean_q: 105.520561\n",
      " 44798/50000: episode: 319, duration: 5.037s, episode steps: 294, steps per second:  58, episode reward: 294.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.514 [0.000, 1.000],  loss: 13.742333, mse: 5657.667480, mean_q: 105.926514\n",
      " 45098/50000: episode: 320, duration: 5.141s, episode steps: 300, steps per second:  58, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 12.912784, mse: 5680.961426, mean_q: 106.004036\n",
      " 45398/50000: episode: 321, duration: 5.081s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 8.889503, mse: 5768.124512, mean_q: 106.847572\n",
      " 45698/50000: episode: 322, duration: 5.123s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.513 [0.000, 1.000],  loss: 12.052458, mse: 5786.413574, mean_q: 106.881020\n",
      " 45998/50000: episode: 323, duration: 5.204s, episode steps: 300, steps per second:  58, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 13.273604, mse: 5792.317871, mean_q: 106.838348\n",
      " 46298/50000: episode: 324, duration: 5.157s, episode steps: 300, steps per second:  58, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 13.682159, mse: 5836.931152, mean_q: 107.483322\n",
      " 46598/50000: episode: 325, duration: 5.083s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 8.266687, mse: 5878.048340, mean_q: 107.695198\n",
      " 46843/50000: episode: 326, duration: 4.143s, episode steps: 245, steps per second:  59, episode reward: 245.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 17.236805, mse: 5944.063477, mean_q: 108.202705\n",
      " 47143/50000: episode: 327, duration: 5.112s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 10.900339, mse: 5955.015625, mean_q: 108.313042\n",
      " 47443/50000: episode: 328, duration: 5.174s, episode steps: 300, steps per second:  58, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 11.343778, mse: 5915.226074, mean_q: 107.701286\n",
      " 47743/50000: episode: 329, duration: 5.120s, episode steps: 300, steps per second:  59, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 12.302723, mse: 6008.778809, mean_q: 108.826973\n",
      " 48043/50000: episode: 330, duration: 5.018s, episode steps: 300, steps per second:  60, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 16.395672, mse: 5931.615723, mean_q: 107.942368\n",
      " 48343/50000: episode: 331, duration: 5.160s, episode steps: 300, steps per second:  58, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 15.025243, mse: 5922.637695, mean_q: 107.825874\n",
      " 48643/50000: episode: 332, duration: 5.140s, episode steps: 300, steps per second:  58, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 12.060802, mse: 5966.412598, mean_q: 108.313805\n",
      " 48780/50000: episode: 333, duration: 2.296s, episode steps: 137, steps per second:  60, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 10.331943, mse: 5925.095215, mean_q: 107.906219\n",
      " 49080/50000: episode: 334, duration: 4.858s, episode steps: 300, steps per second:  62, episode reward: 300.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.503 [0.000, 1.000],  loss: 15.426094, mse: 5999.728516, mean_q: 108.550240\n",
      " 49205/50000: episode: 335, duration: 2.065s, episode steps: 125, steps per second:  61, episode reward: 125.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.480 [0.000, 1.000],  loss: 13.611739, mse: 5970.400879, mean_q: 108.328926\n",
      " 49333/50000: episode: 336, duration: 2.133s, episode steps: 128, steps per second:  60, episode reward: 128.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.492 [0.000, 1.000],  loss: 18.091518, mse: 5956.271484, mean_q: 107.973114\n",
      " 49472/50000: episode: 337, duration: 2.366s, episode steps: 139, steps per second:  59, episode reward: 139.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.489 [0.000, 1.000],  loss: 7.827984, mse: 5981.401855, mean_q: 108.371490\n",
      " 49617/50000: episode: 338, duration: 2.471s, episode steps: 145, steps per second:  59, episode reward: 145.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.490 [0.000, 1.000],  loss: 16.027658, mse: 6036.659668, mean_q: 108.852844\n",
      " 49748/50000: episode: 339, duration: 2.133s, episode steps: 131, steps per second:  61, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.473 [0.000, 1.000],  loss: 14.147353, mse: 5993.900879, mean_q: 108.251518\n",
      " 49890/50000: episode: 340, duration: 2.343s, episode steps: 142, steps per second:  61, episode reward: 142.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.486 [0.000, 1.000],  loss: 11.259271, mse: 6007.707031, mean_q: 108.426155\n",
      "done, took 831.055 seconds\n",
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 211.000, steps: 211\n",
      "Episode 2: reward: 138.000, steps: 138\n",
      "Episode 3: reward: 119.000, steps: 119\n",
      "Episode 4: reward: 122.000, steps: 122\n",
      "Episode 5: reward: 125.000, steps: 125\n"
     ]
    }
   ],
   "source": [
    "# Parte del entrenamiento consistente en bucle anidado en el que primero de ejecutan 2 entrenmientos para la política\n",
    "# BoltzmannQPolicy para ventana 1 y 4 y lo mismo para la política EpsGreedyQPolicy ventana 1 y 4.\n",
    "model=None\n",
    "dqn = None\n",
    "for policy in policies:\n",
    "  print (f'Policy: {type(policy).__name__}')\n",
    "  for window_length in window_lengths:\n",
    "    \n",
    "    print(f'Window_length: {window_length}')\n",
    "    model=create_model(window_length)\n",
    "    memory = SequentialMemory(limit=50000, window_length=window_length) # Llama al método SequentialMemory que has importado y establece un límite de 50000 steps y una longitud de ventana de 1\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "                  target_model_update=1e-2, policy=policy)\n",
    "    \n",
    "    optimizer =Adam(learning_rate=0.001)\n",
    "    dqn.compile(optimizer, metrics=['mse']) # Compila la red llamando al optimizador Adam con un learning_rate=1e-3 y usa la métrica 'mse'\n",
    "\n",
    "    dqn.fit(env, nb_steps=50000, visualize=False, verbose=2)\n",
    "\n",
    "    # After training is done, we save the final weights.\n",
    "    weights_file=f'dqn_{ENV_NAME}_{type(policy).__name__}_{window_length}_weights.h5f'\n",
    "\n",
    "    dqn.save_weights(weights_path+weights_file, overwrite=True)\n",
    "    # dqn.save_weights(f'dqn_{ENV_NAME}_weights.h5f', overwrite=True)\n",
    "\n",
    "    # Finally, evaluate our algorithm for 5 episodes.\n",
    "    dqn.test(env, nb_episodes=5, visualize=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dba1589e"
   },
   "source": [
    "### Probamos el modelo\n",
    "\n",
    "Si al cargar el modelo, te da un error referente a un string, ejecuta la siguiente celda.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1639335036500,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "75df9ed0"
   },
   "outputs": [],
   "source": [
    "# pip install 'h5py==2.10.0' --force-reinstall "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52eb898e"
   },
   "source": [
    "\n",
    "Si estás en otra sesión ejecuta las celdas anteriores y no olvides comentar la línea de código donde se llama al método dqn.fit() para que no vuelva a entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1639335036501,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "NR9feI9DQmIg"
   },
   "outputs": [],
   "source": [
    "# weight_file=f'./dqn_CartPole-v0_{type(policy).__name__}_{window_length}_weights.h5f'\n",
    "# print(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 54810,
     "status": "ok",
     "timestamp": 1639335208253,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "OuHCHtF3z8jC",
    "outputId": "876ae74f-c75d-4ade-847d-5d5862dfea20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: BoltzmannQPolicy\n",
      "Policy: BoltzmannQPolicy Window_length: 1\n",
      "/content/drive/MyDrive/MIOTI/RL/SESION_4/weights/dqn_CartPole-v0_BoltzmannQPolicy_1_weights.h5f\n",
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 300.000, steps: 300\n",
      "Episode 2: reward: 300.000, steps: 300\n",
      "Episode 3: reward: 300.000, steps: 300\n",
      "Episode 4: reward: 300.000, steps: 300\n",
      "Episode 5: reward: 300.000, steps: 300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" autoplay \n",
       "                loop controls style=\"height: 400px;\">\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: BoltzmannQPolicy Window_length: 4\n",
      "/content/drive/MyDrive/MIOTI/RL/SESION_4/weights/dqn_CartPole-v0_BoltzmannQPolicy_4_weights.h5f\n",
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 300.000, steps: 300\n",
      "Episode 2: reward: 300.000, steps: 300\n",
      "Episode 3: reward: 300.000, steps: 300\n",
      "Episode 4: reward: 266.000, steps: 266\n",
      "Episode 5: reward: 300.000, steps: 300\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" autoplay \n",
       "                loop controls style=\"height: 400px;\">\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: EpsGreedyQPolicy\n",
      "Policy: EpsGreedyQPolicy Window_length: 1\n",
      "/content/drive/MyDrive/MIOTI/RL/SESION_4/weights/dqn_CartPole-v0_EpsGreedyQPolicy_1_weights.h5f\n",
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 289.000, steps: 289\n",
      "Episode 2: reward: 247.000, steps: 247\n",
      "Episode 3: reward: 248.000, steps: 248\n",
      "Episode 4: reward: 229.000, steps: 229\n",
      "Episode 5: reward: 207.000, steps: 207\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" autoplay \n",
       "                loop controls style=\"height: 400px;\">\n",
       "                <source src=\"data:video/mp4;base64,AAAAIGZ0eXBpc29tAAACAGlzb21pc28yYXZjMW1wNDEAAAAIZnJlZQAAVyNtZGF0AAACrgYF//+q3EXpvebZSLeWLNgg2SPu73gyNjQgLSBjb3JlIDE1MiByMjg1NCBlOWE1OTAzIC0gSC4yNjQvTVBFRy00IEFWQyBjb2RlYyAtIENvcHlsZWZ0IDIwMDMtMjAxNyAtIGh0dHA6Ly93d3cudmlkZW9sYW4ub3JnL3gyNjQuaHRtbCAtIG9wdGlvbnM6IGNhYmFjPTEgcmVmPTMgZGVibG9jaz0xOjA6MCBhbmFseXNlPTB4MzoweDExMyBtZT1oZXggc3VibWU9NyBwc3k9MSBwc3lfcmQ9MS4wMDowLjAwIG1peGVkX3JlZj0xIG1lX3JhbmdlPTE2IGNocm9tYV9tZT0xIHRyZWxsaXM9MSA4eDhkY3Q9MSBjcW09MCBkZWFkem9uZT0yMSwxMSBmYXN0X3Bza2lwPTEgY2hyb21hX3FwX29mZnNldD0tMiB0aHJlYWRzPTMgbG9va2FoZWFkX3RocmVhZHM9MSBzbGljZWRfdGhyZWFkcz0wIG5yPTAgZGVjaW1hdGU9MSBpbnRlcmxhY2VkPTAgYmx1cmF5X2NvbXBhdD0wIGNvbnN0cmFpbmVkX2ludHJhPTAgYmZyYW1lcz0zIGJfcHlyYW1pZD0yIGJfYWRhcHQ9MSBiX2JpYXM9MCBkaXJlY3Q9MSB3ZWlnaHRiPTEgb3Blbl9nb3A9MCB3ZWlnaHRwPTIga2V5aW50PTI1MCBrZXlpbnRfbWluPTI1IHNjZW5lY3V0PTQwIGludHJhX3JlZnJlc2g9MCByY19sb29rYWhlYWQ9NDAgcmM9Y3JmIG1idHJlZT0xIGNyZj0yMy4wIHFjb21wPTAuNjAgcXBtaW49MCBxcG1heD02OSBxcHN0ZXA9NCBpcF9yYXRpbz0xLjQwIGFxPTE6MS4wMACAAAABkmWIhAAz//727L4FNf2f0JcRLMXaSnA+KqSAgHc0wAAAAwAAAwAAFgn0I7DkqgN3QAAAHGAFBCwCPCVC2EhH2OkN/wYuFAAV8CVxMpfCj66ivCHKzKS6x12qJE3IJQs05DBjAJTekQ7gsICneShjAw+VyfY908M0+qR7gpJ+y+HdmFL7zCHzaGJaUb0ONMFMzEWNcAWNgrfG9OneWW0sD5yXH0zt7xMlTLZB/c4xC18JDQnl+oMo7+NePfJlsq2d7KpVf+AULuyj9dWKQUgsGOKnCEOZUQ+D4ISt1RcgEdre4XmJRTEtPPB+1qrFbDWPTRqmb6MBcw02LZo+wssEOJFR1wUmWEGNclG4FhZqRP47fqAy9vg3VMslUDUsm1JZhhEYaN9faD3k8zDgCsLs/z4iNOJw8Vd+DGihFePbvnJ3i5j9QCsCbhjGaozLkpA/W3ucdI8o2pcokMu1n41lidmCkdj5wc3rKf+Ca2mXCeYg0FQO/8TkqRNf1MNKUAAJmAAEGqWQAAADAAADAAADAAALmQAAAN5BmiRsQz/+nhAAAEWNOnlUgCaUqe89ekFS4aIf+LSQ/ANrfHu9CkBsb58Pv/n/d01a1ACfcKn2SHWo24Ul0peWScGUJNAt2JE784IkIWOOh0FfU8uUkBgW7636HWA5cMfveRo3yF6wzv2PZcI5OdyhKZH/6+y5r4upT3xoTmpAUyxqRhQ2sLbK0fq+pvMD8okLnXDcbi2/KK4astDlU9AXBoztcJxn78n8aR1txbUO0V1eQ7HfN4uM6NY4y6dGoP+nnhk/7Hq9FVZiN0HzFqEt8k9tqVvvBcHYz6RoNPwAAAA+QZ5CeIR/AAAWtVlFhBX0YF//AcgAfh5IPga/c+CJJYa1COCkAAADAABPERNZesLAH8uArwhb8hYzUqgAJGEAAAA/AZ5hdEf/AAAjwJqSF6bgAuola57XfM6oqNfMbLuqxVUGyw0jM0wAAAMAAAMBut8n1AAAAwBEBwDra1KoADZgAAAAOAGeY2pH/wAAI5kuiSa3gugw1Z0PJ3G1QtYzco2ABMoZyfSRCM6CpLWUw5ifewAAAwA2pvGCqA/xAAAAZEGaaEmoQWiZTAhn//6eEAAAGl5fwjjsShaV/hlkAF8zAkh7PuNvlSoVAFHusPIlOemt2/ef1hHk351F2WdGgFkUqOsLxZLCVQ/kaWNlbVkYuS3uNomaWmGeU81OYz47EwI7w10AAAA9QZ6GRREsI/8AAAhtnF/9JHrwdltkw6ZWscgAfuYnsfcQOWSJUH6/qn64VnDDaX/dptewuAA55VZ7BYCygQAAACwBnqV0R/8AACPAmo2G1JK1WHW6gElHihELKobioB2At1wgAAWfAo130qoB6QAAACsBnqdqR/8AACOuMwH9UO5otjDTpibjkxw+Zz9QY/O43zvgAAAHytV7LA/IAAAAhUGarEmoQWyZTAhn//6eEAAARUiXIA5RvBZyoqK3FFFOgEmuKIv6Uw/9o2ukk12eVZFFYKO+Xalhboihr275JpmFOEisTLB1O/WYLUP64q9SUpMk4vO8/Kh0DIld/T+/8gWAs3+8BhUOMPSDC4GgVH8H3wuyPvcE1Bn+yIoqlmpJ5ZZvlRMAAAA9QZ7KRRUsI/8AABa1WLDOgCG1xSrDUyry53kvLyzHwx8QVY8S0O3UoQbAY/3MK8qYO6AAAAMATMUeg1AMCQAAADcBnul0R/8AACO7WTAAthVipNCF/FpQU+nuTzJwBHPEp+qzLg4T/WEeAAADAAADADPif4RBYDpgAAAAKwGe62pH/wAADYI99Wa5AJh1RLtMZBWTHo6JE1RwW6G1IAAAAwAA4VNhBUwAAABhQZrwSahBbJlMCGf//p4QAABFVL9QcAOgbh+to1cf+mF2o3igVZTkZNm0T4qDlyXU+EftlIrHmcozClhSMH2y3i8DtLjb4UKjAeHGiwPR+ESxKS5DQSp/JaXVo8XHV7mAwQAAADtBnw5FFSwj/wAAFsrQgAXR8R0gwintKTVV8HB3XdybdSHusOTV3o7rVjF13uC8Me2qHDSEgc9CjI82KwAAACkBny10R/8AACPDPqB7//YZ7bozJgzxHbECiqikPkRwG0ACT9rrBDl/gQAAACwBny9qR/8AACND96dIxIkJEmAQASx+aI8cbWEnJ2Po/uHiCTHRJ8AtEEyiEwAAAFtBmzRJqEFsmUwIZ//+nhAAAEVIlyAK1Sp+iAXsFN6Bquj9PsEgjRjAlkkxQGaAL/0dhnfWET0na5/f9T5694hkVNRZ6NHgiQzWw0ztbt5Fg4o5KV2SD5iOHvFgAAAAMEGfUkUVLCP/AAAWtVqP4zqcAG3JocFaj8Fla9N/IshJz2d0x+l89Ecvl7iyVhMXAwAAACIBn3F0R/8AACOsPJ+76P5W8KJIALoANEeONrCS/T8c6I4eAAAAHgGfc2pH/wAAI8fHJ/KYRB6HnsJckCpob5IxbVvhKgAAAG9Bm3hJqEFsmUwIZ//+nhAAAEVtZ3vhwByjeCzlHFFZL1+CMoyr59eksXqO4ccnaP5DPK6d0YaCXNx2zrAbwIOpnaulurREQB7z01uvLewEPzZl8BOtbllXpZz5pBJs/76E20F3TuPhXovMSkkIRo8AAAAjQZ+WRRUsI/8AABa+pr4LpvqPzw5W/PFGCUnkIJj17EWC2LAAAAAlAZ+1dEf/AAAjvYkwALZzny950U/99cuGTFWmEPes3m8L8GgM+QAAAB4Bn7dqR/8AACO/HAg5cwY3kJmqZwze63M7QAmeRKkAAABEQZu8SahBbJlMCGf//p4QAABFVMM0AAmlQPEy12J1a1IYuThcEfCgpvshXq2k9cOCBX0faZEwR9/Co7JI4+UyhtOWhbgAAAAvQZ/aRRUsI/8AABbAn3HVbjf6u43iURFBRUlsADVIllgwSrTAubGLRuNJRpJSq2cAAAAoAZ/5dEf/AAAjlG3Ae1eBbFsGvMX2AOJJpEQAfzEPboaIOmAY2OwNmAAAAC0Bn/tqR/8AACO/Gs6tm9Li/2Do1+pMoAJX02YzUrDQ9jv0gasHZ61GWd7itWEAAABUQZvgSahBbJlMCGf//p4QAAAaWg8PRRIrIAF2eRfQAvIZeUhR0pIQmO5c+ACMtDisIL23Ijtmcy/0zM+bv9HO463nzYT0AIl+E9PgErck2Z97CNuNAAAAMUGeHkUVLCP/AAAIZClSAIb5fnGBG8hpCtdVxXACWqTKPxqEnz4WH0eiCxX13yH3oSoAAAAeAZ49dEf/AAANWAuExJ5x5BQdaJ0k0DQWIB5t9itWAAAAHgGeP2pH/wAADX/Wj0pxMBm5zbRk384B21XlRUlEqQAAAFVBmiRJqEFsmUwIZ//+nhAAAAnvh4ePtqa5I4AGU5ZHDa0G8KPa/z209Ii4ModIvzfPsrjeJ2gmv94RY9/I2wTmpZjVpQ/118cTmIWhvTJXtRs4iMBgAAAALEGeQkUVLCP/AAADAzhTDhBACauopRD1c+cUY668jP8uqGuOVQs17QXcZE+zAAAAEwGeYXRH/wAABRxbxSgU20ltj7oAAAAdAZ5jakf/AAADAeXNUZ2ThNqxEJiDnvPGB2moIhMAAABJQZpoSahBbJlMCGf//p4QAABFRSi0ALydqOi1pbf19vIwsv/9iAEjpcDwOac0rxnwf36gXiX3cwyfxLgrG/aPL1k2i2dn+5XFgQAAADFBnoZFFSwj/wAAFrxObN7vsN5A6c8ReAFvFyI/G4biANY9/S51o1T200G0fiTFD/bBAAAAKgGepXRH/wAAAwC6kVqOP/IG18C3m1DrZaAB77iDVZzaZUR87fqQDqgtwQAAABUBnqdqR/8AACOyJ1sAd/bQiWg0gQMAAABRQZqsSahBbJlMCGf//p4QAABHum8+MHD605HK4PCXkAKlPhq4DFhJ7HqxorpyHhGPkTY7QWyzDev5IgHh6zTaaC4xMQg/8IVlfvQ+idvA0z6AAAAANUGeykUVLCP/AAAWvE4aW/eABNt1U0Fq5QGfvl7+HP/BD6GYPmEawLseU5zvG//pN61M+tNxAAAALwGe6XRH/wAADX4TmsHABcMIe/A5dbUNielnTo74VWe82ggb5uDvadtuFKXnneyTAAAAMAGe62pH/wAAI78dLB0CnH710x7zaB0zDXwAXE+m5ZtF+pzJdKuz3eyfQbutKaswYAAAAElBmvBJqEFsmUwIZ//+nhAAAEVRSF4KbIxLYAA7I3Gvca8aN7PFlCZgcOcfTXcRtcdsou8ELD0zz6IrDdDA6H2ZbvjE/VfOGS6zAAAAMEGfDkUVLCP/AAAWpPsiXO4SIPgWmyCyXzZheCnpFcXAX6RRjtRMycI5KteVbnQqjQAAACABny10R/8AAAMAugsfNphG+kaXyygDvoU19LqJcPKZ8QAAADUBny9qR/8AACOxy/yTQ0AFvEFvvJGgXKGrgtFE24n1fdNdMxbBK3pi+VngtY8Bjp7NN9/yTAAAAERBmzRJqEFsmUwIZ//+nhAAAEVRSLWREXHGm8SKGrFnm2DmKptVd0L9feIKAHF+jb00uTt4Ebm4w1bVDx74lo+H0bQu1gAAADNBn1JFFSwj/wAAFrVe8bPe3N21+q6J2K4gAfcjtEDr2ROD1z9Fj//t8Z+tr8aktPiIVRkAAAAoAZ9xdEf/AAAjw0Z+Hbcl1lqBgAF1EFvvJGNytniv5ql4A3aKI8BDwAAAACcBn3NqR/8AAAMARYQbCOYAWr7GjhQXrHwYQASjL0op9IObkOeh24AAAAA/QZt4SahBbJlMCGf//p4QAABFRSmgAFArmhs560iU1/gdUvZvtGQi8rojeXRqACG4C2N5uECKXs/2HmIxdHjBAAAAKUGflkUVLCP/AAAWb5cqGuUARICf6jbpDyVu8sGZSNOl6M25etf1zkFtAAAANwGftXRH/wAAI6v4V+ASXgvDP2g/5L/olUOzYAE40QBt9pMrEgaKDsT91QqVF7QPXMOkJ221sgMAAAAXAZ+3akf/AAAjsjTZ2CgL7477Y750g2sAAABRQZu8SahBbJlMCGf//p4QAABFRSWfI09btERNd+FwhxMlAAAXXRWiAEeIcHWXGtPX+z0E6HX6wdCV3NwKlwfwOUn6Afcdhop21cQMGOLewIiwAAAAKEGf2kUVLCP/AAAWvEPNtxXMrJlJResCcbzu1QAloHfDgc/gcHrlnJEAAAAaAZ/5dEf/AAADABpvbrd1n8EG1PfjmlWztswAAAAjAZ/7akf/AAAjscwJb7IQABLFPwJ2ru/zXacg9RvdOshzBZUAAABPQZvgSahBbJlMCF///oywAABGEMIkYAoNtSCjcqiThQWBZPnmoMKbnDbaSYU3n0Anl5iVFtGoblNMLhglszn9+Ulv6Ws/YhIGgVtt3A7PwQAAADFBnh5FFSwj/wAAFrxDzbcV1eAIkLB84HOv1/FiqoT/gFcTtmnMItM4W55DD8Q2WsGzAAAAJAGePXRH/wAAAwC6Ck1OEDABaM7bzvlyxzbLXna2a6JSiY5lbAAAACEBnj9qR/8AACOxzAlv20xw1KSgXoCN2AwmQzwf4AWt5IEAAABwQZokSahBbJlMCF///oywAABGAvbyeg8mJv3M7Z2inK1U3DIR7D23aJRQdkmBHdB/EbDY1nbsAR2tRj1Mkf/OoEiDDJ5vmfpLs+ola4ETQPQ6Pf5pPb1XQPkd3EWyDkNbThJJrvua5zU0hvFT/gRkYAAAAEZBnkJFFSwj/wAAFsBaBYEkhoA5UhBrTR3ZfQOXsjhX3E0lrcT+RqHhoUTyrb9k48nsPBSHK+r6Jb+ZzApbxTHJNFp9krZhAAAAGAGeYXRH/wAAI8NAqpfhIRtV2sRsbFD4/wAAACIBnmNqR/8AACOxzAlv20xw1GJn5dkUD0SBHRGbenru+qyZAAAAT0GaaEmoQWyZTAhf//6MsAAARhRdRScEA0QpDvJPigji+HVbZTkTNXXVu9FJpFBzuKQ3o0EFv6WHsb0AN2jWqAoxrEJkOjurrrpAzy/apQ0AAAAhQZ6GRRUsI/8AABa1Kzm0qCTALWUreN4aeS9/WFhwwBy5AAAAOAGepXRH/wAAI8MXdfxaqKmgAAWETfRvow8dEMl2Haw/Dcmbe1RaZ1KI7FNP150vvVLU1v+pzFUZAAAAJQGep2pH/wAADX+2h+AMgAXQAaI2hPWQOYqybjx0fsHAJHshEjAAAABXQZqqSahBbJlMFEwv//6MsAAARgL8MQDMjb4gzqBfUIHGj7TDBGfCENPf7DY8smmN9F/E4flPlo5LY+JDDF4v/g/iqafSeKX4SfuXiGuEap0HbIwebBhIAAAAMAGeyWpH/wAAI7IsbP1zfnpyQd9vEAFqJiRASI13ctiNX8/Gm+Iln3iKnqGMMnHJMQAAAE9Bms5J4QpSZTAhf/6MsAAARhDBXL/PxCpO+FIAOVqMQcXdSlx5kD8XJLWdcEI0lsJAg3CdKRrC4Tf3dzqMtk+i+Z9rg2oNBMMyeuPopETAAAAAQkGe7EU0TCP/AAAWtS3NUARCpsYQ/zYPqk3gZCp4SupcJSPXV721gTpv4dLybSscRjDavDG1vkbUs5LF7l3qQCTK2gAAADYBnwt0R/8AACPDF18ue8EdyFkQPFETVqp7ktfRcAD9x3jutO/u7Jyma4QULkadzCnSGCcU1IEAAAAgAZ8Nakf/AAANf7aNh+2tBpJghkc2LvethWckCQ2g7oEAAAA/QZsQSahBaJlMFPC//oywAABGAvZY49kuHQNpxBl1uum0mb5w8wZSXsZRA5JG6e6wbHKkqQAGxItEtCadtNOBAAAALwGfL2pH/wAAI8e/i/dFtD9TePAkBX66NlOftYsL4AG1BB+bmtnheAc8AiBobqEBAAAAS0GbNEnhClJlMCF//oywAABGem2f2q7hKTEY9bMfypzFsK2BpYcprT0IohPk7Cmhgh9w/QAc4bSLJ84TfDFtP360qcwcFbxsfWUHNAAAAC1Bn1JFNEwj/wAAFrUO8m7sckr0GmMvKiysugNLYEuFqOZPBEammLT7krQ8raEAAAA8AZ9xdEf/AAAjwxdfLnvBHckyqAC0K0/SwJbamIo9f98LcB8glcTmmF9JemXBUcRDkrQnytNJpDeCKUnAAAAALwGfc2pH/wAAIscyxwznNsit3ACrPU/vS1dVF/flzqG1cDSsufjXbZl9QcRJSQd0AAAAaEGbdUmoQWiZTAhn//6eEAAAQ1SrTpKbsEDk+K1AYgvZd+BuPgAuo+4V9Tcec/DQIZKkx9KDpuetp/YlyB8B78mczBr3leRvQUeogdZ2xMWPAaWFuxPxqUMhInNQsoW0mlLPnFw/Q5pBAAAAUEGbmUnhClJlMCGf/p4QAABDVJDm/EWobKZtRQ26o+72QrH6+CktKxujmCADPh5dtoEAxNFeAIWhLsi6Kty5eUF+khBnGypivP6Wr1MRd+XwAAAAT0Gft0U0TCP/AAAWIyyPfy+soYWGeZPutwa/NaX1hMmsZ2crJN8le+WTlkrwXS/PTdShbAW/HE9O5R/8gXebrKDmu9rLdt7dute0cj0oSoEAAAAuAZ/WdEf/AAAiwz4N6ZNpUAFqJfSkgglbO3rA+grekX4jsPlAAE1HZD2MGVPwYQAAACsBn9hqR/8AAA00nVpIRzkB8PihRv+sZFAAQ17EtW7lrc/Fm7fIiLwpb+DAAAAAbkGb3UmoQWiZTAhn//6eEAAAQ0RqdJUXhpLre2RmJiU8y4YPhwiW9bOLCNJQYcq94VBho+yXs04zXzKXomg48AJpoQ7HJC99Noo15RfquXoyDNYLBmbwUndS7P9insqpgZ46X/9wcsE+LYWE4CUFAAAAQUGf+0URLCP/AAAWIyySUlGbsRa9/Bb0G09uWuyAAue41v7Wq5OpV9fJhCmoSzZv1fFKXT55rD+9SdTvQwDOipyQAAAALAGeGnRH/wAAIsM5p4pElIvyw7cvgAW3e6nYeo1o5h0vmmlbjYEmzMvddqZhAAAAOQGeHGpH/wAAIrIoKY1wHyAAugA0R442sJMVG8coYfHcAxXtgD2/nNFbp8Rn0eUsq7erjiW4ZQcgIQAAAFlBmgFJqEFsmUwIZ//+nhAAAENkp8lWEMV4/7o3s9S2gOuLdr+eb/GPUeIMdB2HoXcAi5YgOOf4Aa45Knxe9igeDIOpHV1L+BXHTnL5e6DNR4So2ipEfSmXkAAAAFBBnj9FFSwj/wAAFiuUWDqSO4R44ChcYn2fuVjhaNvcVVYDR/dI+NxksoHVotZ9BreGiDvRw7+IiRxFaxZNg3+zTc9/Yw02l8AIZ3N6VysRvQAAABwBnl50R/8AACKlPOLcEmLY7/m8as00U7Q2Z+hdAAAAUAGeQGpH/wAAIr2VwmRIDzzkdp5tZmahSCcAFwwDA7wHQ9xlXSIqEcKfOhuOGAZboyG8gEzFADHZAjwQlz9EI6oNE6y9hVb6QoKLvNzTKjBYAAAAikGaRUmoQWyZTAhn//6eEAAAQ0VDfkABFk4iQBqfpTtsUzLiNl+3T7EdFDPFjn8YIjln7WinBdofC/WJ3BOIRLMn8TGWruFGsVr/ZI7GxHWh1jaRpvWJQZmWVXG/AhpcW2kn8BBnOwWE5/GPNikqEZqmOQow+f4rDq5t7alJ1Kf4jSN5ygKadogHawAAAD9BnmNFFSwj/wAAFeOCuYoAV/qXcr4W8hZdQ2lTBkNsMRvW20FpsiXM/7BZQbXM9OBBxTC8H2OO6n31pMw7DygAAABaAZ6CdEf/AAAimLSPpiYy0gAXUStEO66zkX3mY+eCZRNo831Ecz3LE78owwK4RJ47qbt+aJsa9zbJL3ZYPx2JEP4kJKVAr4W86MkX540NFtFVyWiqDDgSmbxZAAAAMwGehGpH/wAAIr8ELFrOyiyiFg6udGysmjJveDEaW6fEINwovNKlj25O0F6sfXIh4TD3oQAAAHBBmolJqEFsmUwIZ//+nhAAAENDlyZeayx9/cTqZM8fI6AAXAtD5TRpcvrnZBa+NnDzFiKdTNix4lJUdY0eOTlJibeSdK49RLC0odIcvmhrfyaSlwN3rDfmrG/x8DdpnRIwOQeJIK1alODywbPFI2LhAAAAQ0Gep0UVLCP/AAAWLmsIvAFakRcGNEYmzC4tx7imsxM5uzztaZ/7wUmtIH5FlWKwBN0QYTiUT/7Q9kusEoAzcTaA3oEAAABAAZ7GdEf/AAAiwLgyfOIN3rxr4KleAFj/b31NUFd5R84xTbEcJi9K8h637W3NmN3aoCdUcwx6OBeI30g+uduE+AAAADQBnshqR/8AACK9la264lphEuABc9XzbvJ7SWP8rI9gZX9WA93UhzeeFg4DTIhlN9/LAiVgAAAAdUGazUmoQWyZTAhn//6eEAAAQ0OdqS3PtaEsAg8reWIg6MQAog7r+z0kC6G5aelWBXLv6pPKXkHMdYY8/bb/s6uAefNkDYbkN2L9ikVPaiH0bDCr4XR8dUWNCi+5ek8D4i3o68TpTYCWui6tzbqdZF4uGiBJwQAAAERBnutFFSwj/wAAFiMrtQW5/FwHarqAEyxVMeW9UyfzRTYi+hQBa1aVMajFDOBjd40AhIFDS5iwWXN0+Hm2WnpWhpdswAAAABsBnwp0R/8AACKpzsWThXRoLd8NWESWlOSNfoQAAAAnAZ8Makf/AAAiphEwBWzO31Zi6kwDz8ZyNvORByBV05jLRb1UozbVAAAAcUGbEUmoQWyZTAhn//6eEAAAQ7pvOsiunAAjI+p+iAXtu8on4yrJsZicpLzabZdkNyXZk68f1rekHwc0sfUORW0qsjDeRPNhPOHZaUbPR4LfD36j/AT5A9iU7P6pF9mYsI/vikO2bQKrNacLDjACn0lBAAAAS0GfL0UVLCP/AAAWJQ6geiaWQAJ29q35+78a3uKUS3ODYS1qY1iEtXF/waw2uGI1HN2mRSRlQOxAfKoRKCeI/1N+jpN5WJ2Y87SELQAAACMBn050R/8AACKpz1TPXkNsTOzNVTxp0LyHNwnBaLIE8n9ZQAAAAFwBn1BqR/8AACG9lp3aEu+kAC6ADPg+7RuiSIKT3Tf+YCQcyKbMCuDkr784o6r08bkH/OZshOfgsYYgLX4ehLQu15jcGdCmSvkxQXxvSMv46iJg4GC2c86I7RR4IAAAAFBBm1VJqEFsmUwIZ//+nhAAAEFQzaSUAJath1MLfK4PdhVIfs2KvbY5ujzLNff/9OZF4zZ9prMZMNCiv/B0c+pZceX1odid0dFxxzTM1VK2IQAAADpBn3NFFSwj/wAAFZ5gqGGmS+V7rN2EalDi/qECyQAlOU+GENF73EcwaUtDamXCQHqH4xiY5nA71mOOAAAAJgGfknRH/wAAIcC5kg3ScRAisJac3NE/nCeX+2WCGs/+stk8gZPBAAAAIgGflGpH/wAAIa4zBic3CXW4RomUTJQu8R/P4KXGqQR51IEAAAByQZuZSahBbJlMCGf//p4QAABBZKfJTs0QANdFPeVbsdI6yymQvFABS6vaja1C9tzGI4MleoGIQJfsiXXDC7Qs1yiPalWCgHKoieJU56ps9aFTIkSS/+kjB53Rn6vX9xUP//P9mc7VDsvv/YjhwOefBxweAAAAOEGft0UVLCP/AAAVkyyPfzPt5xYMlChzAATVJgD9Y4ioDzcZpxY0tTqOfX0m9oWuuuKjRs7wwB7RAAAAJQGf1nRH/wAAIanP525gMNG8nL+WZjey2lmsTVUC3n5Uqu9K44EAAAA5AZ/Yakf/AAAM39bPk2WJYm/JNxVoeHTT7/ChcII3uwAJloea4ccACsPEpaV6T7m3hgyLO8KGTG+OAAAAaUGb3UmoQWyZTAhn//6eEAAAQVScxzwAWBYsXOvpCca4Gl/o1ySnOzG91S46rt/+tHQL1AH5stE41W70f/eMbdylMDrqDePa6hXqjU3XqrPgTEJi9elmh08Kbksruk8eIRnHxJwUqKTiwQAAADxBn/tFFSwj/wAAFZtfbrRgwO4hSP1rzK3fibCCFz78XrHACZOlaN0OLd17vN//2vSpmUTyxlstwwEO5UkAAAA0AZ4adEf/AAAhwLj6iP62bhdCgMAAJUE/FhHr8rCdiDwtnggpZi+Jikks1uZoe8UE598CkwAAADEBnhxqR/8AACGuMS/ReUQJ0AJYWrL3VvTzS6pXVHo7DJsYL+LDijIrzX3sc39KI/xxAAAAkEGaAUmoQWyZTAhn//6eEAAAQUPa4yACtv+UBMcQNrMndyGQCIfskSkR9Na9BmWLAB4sK7NddbYiadUHaVjm+xu+5NIQJygdG/gyNK3b3eGvzJxDNsCaghCMyXmDU+pKdrWlbakv7i4YDaQFhirwKe03Xtgk5K8v6mtcOGC2vdcRHJvJzE2X9fNkSFUU+khacAAAAGZBnj9FFSwj/wAAFY0z/K4gAK/h7I+tOEpcSMv1XZeDwZTr03Bh9nAUR4HLGrk0c2pEWegbIrW6vU2g0l3+zGxVY2FgiDSXZbTvYYfLZZgAiThdBNvBRPu7BNkuKgN+BnZqL4DbnbAAAAAvAZ5edEf/AAAhqc9Uz15DbEzsxLH4opvqRhulxRAJjZsCgsoIQD36lbxXk7cwfKMAAAA+AZ5Aakf/AAAhrjEv0jOh4AJY/NEdGNqxVBTEVsMc4EfZorxkQs6FPZuLL5eUKVKR8XSNKOYqXf7TFvR1/jgAAABTQZpFSahBbJlMCGf//p4QAABBVJ05zH+QLfTnRVyA5TALwW1LkE/iivvMYyQ+ooANAybTKGtz3sAB7tB2AAHq5Dr6fp4WmpjDHsqRu3VP9kySSMEAAAA1QZ5jRRUsI/8AABWTK1gtetZZ7zMhxTMTUDbfaZtC7R7OQQ1Ps783W6jSZ2wd1HYSOCdi44AAAABBAZ6CdEf/AAAhqc9ndH6Bk2P/BIAF1EuLnProVXHpVZ46IW7S67HSRts7VSU85HrcUObW4T6yzjtLKA+wEjwEPJEAAAA3AZ6Eakf/AAAhvvUZC4wMdO+HDQQ9g82aFMGn7cWhT4VlRK/sAAbDBzpP6IX7MygTnS4Etap5IQAAAFxBmolJqEFsmUwIZ//+nhAAAEG6bzrIrpwAHG8Zohw7aorq8DdbrlvqDVxwkQFsZGEkeAVpzqSoWy1+jmg55ahheo984fJF/0cxaCO2dl1VTaBjpT3AKMn8x7QPIQAAACpBnqdFFSwj/wAAFZUl/UyrDkFY9V7ejf9mVh5SrdvEqZpxkiGPpJCMm0cAAABAAZ7GdEf/AAAhrBUN+e2VACt9qAWUzdvXEWPGYzG3Vij9BToMiA8swgo4VsqgKjdOmw1EMaruVG0tglRGvh6yYAAAACMBnshqR/8AACCuMvr9EIzY7AiD4tdbTS01PnR7hBgr5/7/HAAAAFVBms1JqEFsmUwIZ//+nhAAAD+IttGhsryAIR43EhA11XPgUNyGv4xfVYPI2/pIu7kIvp6/1Q+S/ntqpiBJdNpAlfj5Bno9rS0MsocIE/10KdqNvemBAAAAN0Ge60UVLCP/AAAVAqFrwBESTzIzlsZqVQI9J/2qSta+e4sdsI3xn2Zc3tna1o4jPFQte51Ko7gAAAA5AZ8KdEf/AAAgt42TsunPIASguhhSYg8zne9YU6hAPo2eyW6IvNgvTf3O09tNuSNpQvRCAcTv746YAAAAMAGfDGpH/wAAIEPb5lPVmT86QA45N0i3k5bGfwHhoSSGJGFgATJ2tP7hp3yuZpjpgQAAAFVBmxFJqEFsmUwIZ//+nhAAAD9eREPAE7mZN+RSdh2qDjGV+0w8c2li0LQ0Wo0K0x/u4q6hD05GPhdIseet9jZqO1fF7X5avYAUVGW0ecUgrFLSg+nBAAAAQ0GfL0UVLCP/AAAVDmIVNoeokwtME+rCHmLKO/e1wAmTns03PbmvUxPMNJBv9ElMfrbOY50LcMMFrEHzsyZxmFWrb0EAAAAvAZ9OdEf/AAAgqc9ndH6x0709gKwAFZYj6MWScAA/lub+9njhY6FqWadrU814CzAAAAAwAZ9Qakf/AAAgrjEv1tZedQaAC1n9cv0qXLOWr/l/oHU3aPDhSRUHGkBdblZsk+bAAAAAV0GbVUmoQWyZTAhn//6eEAAAP151h88+gH+kA1K53nKin0FRJPMs0MUBMEfUT7CWMSZ+YZmDt8fceF8kO2qd9OSBv+AgPXVJUvMXKZUE+AJJkRFCQNo4OQAAADVBn3NFFSwj/wAAFRAxfeItVwEq5BwcU64vd2BdYqAXyL5zez1KIn1aGeaygnAyQKrBQGMV2AAAACMBn5J0R/8AACCSorKYqLFfEPdwsZZ7vIWkjA60YCarv5q7pgAAADABn5RqR/8AACC9la264lpZBa4ybinMojpTwAXP7t9YlVujNTFBVEQw6kTlE5vjlWEAAABFQZuZSahBbJlMCGf//p4QAAA/XkPExewLQBcqf7u6xIURPVbF3p/K4Pvb1HOaNvgUwViimjF6JsCBPAj4hSDx132OFxisAAAAOEGft0UVLCP/AAAVAyv5QG+vXLMfl2872i31hO150Cnbq5dzYAQd0q1A2JdirAbIEKckjGceFSPhAAAAOgGf1nRH/wAAIMC438FekIeu7syDKJ9MaAmuFgROwnl8GomfYANoFH7nWMkgPYROiy68ppNScYVbzYEAAAApAZ/Yakf/AAAgvvUZC4wrgAC6P2b9JhX7nGfLRDFgLpLsBbjO3D4g8oAAAABSQZvdSahBbJlMCGf//p4QAAA/vCc8ThrFoAiujRXPQrjHLU1mfl73VnVhL7XwkqT4JLR2q9OFMkbCezO/Tb15USf30SPNSbm8s8idna8QY9qlgQAAACpBn/tFFSwj/wAAFQU+ufurs8SbCaaSazI8qi6RGIUBTVp56SKJi3xHtWAAAAAzAZ4adEf/AAAgrBUOC7IgfbT06Cc85kAHKtLWcQRODoNTFi5Hyz8eM64H2Y4BeBYmiLphAAAALAGeHGpH/wAAH8bAz/BuEAKgBbfu3951TQI1B+EmjGhm6nZJX9ZTsLZugVdhAAAAb0GaAUmoQWyZTAhn//6eEAAAPkJu2MWIjAADpNfqAvHi9NfoNzIO9O/rYSd7shoCOSwMTvHkP6MY97YwoYRiRzAGHlG5pmx02IOvSfxM0Q0/BlLm7QAoKYb35EVJrdjAysePdKTIrYy6fKLtW5ScgAAAADRBnj9FFSwj/wAAFH5iFh2rCUaH5+0Y7MO9fp6EZw9NeI8HO/F0UDmWlfwADhxlTWOqB8RgAAAALwGeXnRH/wAAH7bpe6+mygnzcuMT0AOFNuW26sAEs51BApPVwXqREiGX/Pc249IZAAAALgGeQGpH/wAAH8bAz/B4LFpIFGekC9M+AFvA9vBRHk3KM8SiXEikm8zLUqRoSwwAAABnQZpFSahBbJlMCGf//p4QAAA9/nDXRs4Q0AN+wy/QYPrUIk6hLPd2Gecpn3JqWKuLz5ZyB/RaMJbhiDSaBiq8S026kJkl4fze23W3YdX5obEvzMMjwWjcdqMd7JTHojvb1fOgcNgSJQAAAENBnmNFFSwj/wAAFHKeqLQ80zPZ8AORiHrXgfpOlS+pWN0klYIERaJZ55iC4Pzj3dlSGZ/JoKcBYID1OvUcmFen33lmAAAAOQGegnRH/wAAH8jKWRQrZNXbSbUfG2HP+zR3wqs95tA65oPwa9YmMpqxDzQDZpnUvdvBtpgW5Ztx3QAAADIBnoRqR/8AAB+5pn9ReUUgpOhTIzG+IdKAEsKxIWcsY0GZiRHa2h1UnrPBhO0C9vrtgQAAAFdBmolJqEFsmUwIZ//+nhAAAD3+cNdGzgKZs101+RDq2UyN+AC9eY0QenDtrbdIxx+0bZJ82Pvzidr8ZHORu++VpYj24gAKBIODXT+gdlWiCu8X9qmlBYEAAAAwQZ6nRRUsI/8AABRynqi0PNPSyiXP4+ILKefFiJ5lYAW/KQr6Sula5OfbcL8yYz2hAAAAMwGexnRH/wAAH7boevryQnAYD5oFvCyZMq5PD5O07+HNABsix6MsM/e0wbV0WRVqoA+3dAAAAC8BnshqR/8AAB+5pn9ReUS36xXkW9tkQzhWCvHdeMXm6xYqL4OdABLxmYHmC33/RgAAAGhBms1JqEFsmUwIZ//+nhAAAD5cJzrOTpwAIyPuFq+Lyo8hRXV3zACsDdjRBR9aYb0/jK2j9L8h7AB7gCFyVPe/19o2mi5R/4eimdNTpHJS4GOxmkkXP4p9Ukm9Q3N+RceJWIl1vpQh0QAAADlBnutFFSwj/wAAFHU3P1DIflwtwT4V8grUXHazazBBN49usS0awLC4n65njw4wAC68EE/bgQmlIlIAAAAmAZ8KdEf/AAAfyMpZE1lC4B3O5mHq29fnbGHCaggQsb6WjTLNXdAAAAApAZ8Makf/AAAfFsDmEmdz+HCrvrYnDhQGSnIN4SGO2c+1ic5DbJ+IJZkAAABVQZsRSahBbJlMCF///oywAAA9B9EAgACrAZlQ13RM4mPX2OaypvN+QBSubMr8dTspDRnMpi0gthj8YW5q14sv+84sgXVC5EbzIncQRJ6f2Buyyp4EZQAAAEpBny9FFSwj/wAAE+KfCc7v5rxGCT/d7Wt3IruJ2ryEKk5OUvOmgF5lQUG4ZD+96BjvLaPUwJXKlstdHgVNaTCH0dg+MSaVJyCpNQAAACgBn050R/8AAB8G6Xu7ycp0Mk6rJY3hNlHzVoZAvAEp56LNbcZPSh7YAAAAOAGfUGpH/wAAHwmnFZh1CzIJm0cmbuDXlx+8HRA+2uI5KfZmgAHvXTCOMblb+wCLZ00XSXIDtpNAAAAAYUGbVUmoQWyZTAhf//6MsAAAPPxs73HhJytsNcd+ADHSiv6S/5s7Jmm4gYmX3G/dMlrdbon36c2YLXY9JV2NnAVW2uGmLZGRvdOTKDeBjyepR1sHFMc3RRGZ8mRB12JSUtUAAABKQZ9zRRUsI/8AABPinqi0PNPSyiKImVxXAI0Zzf+zkjbao9h75qe6RCPpvvmPWRgAIg7aUSVLTvU1nfegBHY32Lvv3asq4fiWAsAAAABEAZ+SdEf/AAAfGMpZFCtjbVLmWfCQd6PcUT//e6XZs7x5vYQYn4m/YuoR1kKQquUKMdss8PZY90W6dCR7XjZU8nQVszAAAAArAZ+Uakf/AAAfCaZ/UXlFIyYAKew164/w4i9i+oCLHmc59MbijZ9iKbFY4QAAAHBBm5lJqEFsmUwIX//+jLAAADz8ezW4ArZnZDKaivk4YcAJvLSx5NS++rRBYnhj79kksWUZlpBJ2ybEHF4hIm1umWGN693TMJWvWOoK4bcrnnnPgRczcHSU9oCfCCcArPHyT8BgvNbuUOSFxBGreXqoAAAAOkGft0UVLCP/AAAT619utGYO8TKPG0T2Gz3QZBIljT/TmaAxx1M4zFaDzgjGwIcsqfyU3UyP3zdec+EAAAApAZ/WdEf/AAAfGMpZFCtlKSzl2FZ+6PaK+OOvgSzdHgDzGQkI2i0eb9MAAAAyAZ/Yakf/AAAfFsBhB6ubJwT97Ag0wlFQk9fq3nYFb0ozDgA2gA0R3hKytn2GxZYRD5QAAABUQZvcSahBbJlMCF///oywAAA9XCZL/fV4AlCxZeTlnI3+UWjgpBUjqDhTsJP7ioXGBLM7TymJ7Anw1LNvWPxUAx5ZIin/x2o7M/PD8iH85z3OpBuXAAAAQEGf+kUVLCP/AAAT5SSh/3cxEBe87njsV8UThp/eMIb1YZqlUEVV5iW/kVALWACZH91UpqGtnZg6ejcTZVdAZ9wAAAA5AZ4bakf/AAAeWag7wYJ5hXS0ToHBnKKYtllpfOkXBjxyCgAbAOPo3u3ZWqIZUHprWMN8RCqpC3lBAAAATEGaHkmoQWyZTBRML//+jLAAADudCsOniBI6GBONAVzEIioCrjK066xpEC1YU0Y8A0y9JxoU4v7j4XU2X1ES50STIlMymfHTtQ4zstUAAAA2AZ49akf/AAAeWaf/tOz6oJM//8N5H3ukYUF93TfE9mp5L6I05pL8yoASugNqP98I8E7u+mfcAAAAY0GaIEnhClJlMFLDP/6eEAAAO1YNGzVLm40Qv2AC9d30PsvfeLDLbrlcNSeOvhInPW7QzlaJMkRbgJIHGaOetA9LP3YtI+Sh7PHBuJ6SFqBQE6af4ZbxqM077fdP50ULjSVogAAAACcBnl9qR/8AAB5mo85SC6AbMjFEb9ANh40j3Aqz+kjVUGiprFbZIYEAAAB6QZpESeEOiZTAhf/+jLAAADucgycXv8MdInwtxorLGp/ENPZAAh8rR3X6wfjPca/H8UquMNVVKLdPuW9SaXbQvpY9yErzScKYXpc9o01rsAmekN1NSUiqBdtl6IIhpRx5LnU+nGSyehhR4JTQhnElAA/PYaXpBasXuMAAAABIQZ5iRRU8I/8AABNiObd4j6W9uLD3poATTCAedAEJpXPnFbN29ZITeQ2sNbItTbBCoxdIbR+tMZtdSEMgTjWM82cZTbpt/DTBAAAAMAGegXRH/wAAHmjJsvSSslHwYwYmnyiBFhVY7M0SvDpE0vnp5LF6xAOBeY/w/On62gAAAEcBnoNqR/8AAB5mo5JnA7EC3T0gAXGhNNDH+c1yX6/vI7TGLbqwSPm6WVQkRNtUId/dSRpAL14guWIBENYSjWzPP+Mv6k5N8wAAAE1BmohJqEFomUwIX//+jLAAADv8Jz3OaVg9ecGdlgBNKrzXLa2xT47SUGqrGn/U0WB/Qk/i/hwE7modCxnfE2MMtk+0tDUNCOthxSFvzwAAAE1BnqZFESwj/wAAE11fTfUWO8E/AABO6v8tP1w1mdB7mLbPPbGdHOmPI6lBq7iVWv+iIXDbjAL7OqjZaRLIDbfq1+ZN2dH26OEch7HfMQAAAEQBnsV0R/8AAB5oylkRP2CAFovqn2DfnNU+u0XBhs8KB2VNTa0Fs2+0z8GcFqhAzw6qMUzyqt4h+s50xYtQD0KP6bXfZwAAADwBnsdqR/8AAB5cp7eqwPZ0lO5JeWEwRKB/mrpDXJphnqaNmW8DNFzmoyWAAmaz+8+1SEsgS3v9MLaL/2YAAABoQZrMSahBbJlMCF///oywAAA6VoIXnnq/vPmpcrXTDfuGah8AEqUbojt1ESpfcc0T9o7ZAnXDymK1EFDqwTRVEc8BgWrZag6g7UtZN6gmN2reIFxXKdLsP16R4n/5xg3W4MpSai/0ysAAAABkQZ7qRRUsI/8AABLVvlrtRvb5p/CDhkA224blVsLkN8VigDJdhgUN6BoHfAZGGHOtYJhafoTDdyCGZ1dn1cbHu6E7XG6sVosaEOgXLI7SJNpsVDMVN6KRYl2sSnqZKd8MBAnswQAAAEcBnwl0R/8AAB2mxZ0GNByG0EB0PNFD/RgIrKdspL7T4nePvlPAc3xxXmo8Ee6yMUAEtVaeslnMllr0ODaFqrOt3tqlp7XfMAAAADQBnwtqR/8AAB2+PeFZ9Ee8PfJd6UQ2h+HqPTSoy5+zRUYc40fs8PKr0Rh6R+BqnQwe2vHAAAAAeUGbDUmoQWyZTAhf//6MsAAAOj3GDwDxQA49xAhzbC7WI5SjCDY0mKH1buXoJB0lkyjLbk13NpU4C+49L06XPrwd+DcKprjaZ4q8/Za2/ZMV+UAoGiwiy8u+AlmjaffDxDJ39ipfPIDV0Bcc+ha4/9u4RONqzR3p95EAAACMQZsxSeEKUmUwIX/+jLAAADo8mUzOAK3obYVKxLdUNcd8rRxvcKwd2AVELZ9pkzo7NLajn8CITBtadHx7YVzyLahrfcLVBpGw3TUjSi7gwgQTXJdE6ofi1OQi6Ytr1hJx8/lxoz1SI4+U2x9YdXGRcyYQ1J6weus6L4s7fgayOwWwjjAf/Tqk1sghpAUAAABVQZ9PRTRMI/8AABLVvxigBF+8sgAKAzBKHTE9aJCD26KDf1dQLtVqNwe9dMG478ktvt4ATPEUV8dw2VB1yXMZdr5HPYLJMNkzcuO1wFATh5Lj4dxXpQAAADsBn250R/8AAB24LKKkrcVCw82ps+RdDeCZO9pT5GUhT7VXgwdSzrPxOvkyEqEE91PE2cZl8weetoxhugAAAFQBn3BqR/8AAB2+QYNi9A/kGXkg0S3pZCacR+g/fqhZ0Nbrcnv1zQx7CUkb5Yj9njXHaXAA/ZG+OppbeTkmRFCKmX7gICbYoxbHiBjqSb03QyxBP8AAAABqQZtzSahBaJlMFPDP/p4QAAA6PCc75c9dRXTD9rTTwGNbJi6PHPvlovYv7QTrVMfBKFlNwcOehBW8oDRetLBmuSsdjwsUbZB1EYGp6BGtOzzVwO2J0eIbYd/fagniqt4+lWdzQWv/PzdkkQAAAFABn5JqR/8AAB2si7Aup+H7q60rQzH5xW7g5gL4pFEbqFJAaQFNVx8OWQ5mQANoAU/JnEQ9lrrnAgP6RSYIYsfB251P3ky9cqVdiRSuEbK7gAAAAHxBm5dJ4QpSZTAhn/6eEAAAOJZp+vOcQtAFmXhi/w6e+jOqpv9f2j5P0veF6MPvdwEowEmub+4Ij8M2hsazXv23Z+iDXusIxDaU/7bEvqQ4tGyu2hkCj+YBflGzq1nPxTPHTdsXwf5sKQSwSRNsTunwpDNbC4C+ep4i00wsAAAAUEGftUU0TCP/AAASVb5a7jXt8tFqA0Dy0BkWbnNBn4mGkp7jrwnYHURZbxeOHvugEh6eACdnbm5JtIgkR8Iry60YruHiaNqF9Wqa5Aa1NpfdAAAARwGf1HRH/wAAHPZziE/3NVo0DjYy7KnzUh7zUv05bjcT9SYy1nCbap6tjNbjgAO0MXwASvoUiwp91U7N4GqyGMmS5AOl8XuwAAAASQGf1mpH/wAAHPmmf1Lo/1qTx55ZvCa+vhgH3Yh8xLfRhnT9azFojSleKGg+ufacDzO+I3BXgQA3RUOJzNZTXk1E8tJf9NiMpmkAAABhQZvbSahBaJlMCF///oywAAA43KRHGO7wWZo68cWX2GC2/xWzpgFOS46xl0y6uKOM+E6FqLKv6sUztICk242ogCHW7G1LIOqqO0wgy0di2R5PHJt5tgLJ6Ft6eMdpb06T9QAAAEpBn/lFESwj/wAAEl02jIgLI3WAMtSEVXcN3JKfFP6BCWgALC4cK/7nqzo3r50bHNIKdiL2i66OJSHMVpZs7nuB6IbylwhBlPLaQAAAAEkBnhh0R/8AAB0ILP8vE76geA3bZRASrbEcoca+j7g6SixZP9zHS7jK6pCndERFAB+5pGqjzMCptk/oD4lXKTEHSFned54joGqBAAAAUAGeGmpH/wAAHPlwMQAsN1JssyjCpXNlmusIqIjTVrq7JojbHUUPnejN8c54O4OXxTmPyykbJC9nxeqT3/ch0X3wDczI2+yzBLJD+YA0PLUwAAAAUUGaH0moQWyZTAhf//6MsAAAOTwnPdSc1jd2HL4d5qzUaO/+CJ7TBntIiyeXUcCH5c1/+6JFlWoyeIAhFJd7ZfGHVrgcgi02I3iznIYkeSbAQQAAADRBnj1FFSwj/wAAElfxEhBIXpaCvb5M0s0yjbvtdKRBhHXpnHOmtRABHS096cWF9j7LzzVBAAAAVAGeXHRH/wAAHPiNjcYgcs2Rs8m5VmVfKkkMYnIHDbIDpPQfid0QuR99EwZJ03z0AEQa+Mnesz5s5dyfXfzto+6AO6DmBglwGbhctRlVmfpuropmqAAAAEQBnl5qR/8AABxI7YrHZYrR0AW8HGBp55ofwXq1vrm8pNbacs8V9nmpzmqQ0VUY8AJTXV9GmNHMyuAzAcHYGNZaFeeucAAAAFJBmkFJqEFsmUwUTDP//p4QAAA3K7CNU2+xUWGrXPBJToOvHXe0ADM6egz7b1/zGqGJO+5Lxkye461CU3H3kuRkh8mKEb5XnchVz+xmFmkIAEvtAAAATQGeYGpH/wAAHEjs2m+FBx9Gl3Bpq1LtsEdfOaqBPCaOp+2HcR2cQBN9quFHFEzgHCsXjkQAkPhh6tRQPEuVSDp2VyVLwk+zzeqzhrzAAAAATUGaZUnhClJlMCF//oywAAA33Cc91J9Xpu0UCi4sx++H75ui/m8m27AjRQAC5IW8e+PRNloMgOPKLcuo4qXZ5tYMuhBiH7yKArQ+W0E9AAAAMEGeg0U0TCP/AAAR1eDL/xyYq86kSfqWHODPxwL6pwoUFRyt/EmW4wIJNhC6N3cycwAAAEMBnqJ0R/8AABxYLKKkcgEOMCMB23II9zjriFU+DyljARi61fjNIpF51zj0GwQU/KbOu3JQwAeZwQlWRFhm+KPBZh5hAAAATAGepGpH/wAAHEyLsCQIemygPDn/DoQ3xz95D5Ak3ZSoe8/NlxhCEeksAVSIAP3LS6saWmsyEzRRllv/IZOwdNJo5MUvBEoxAgXNeYEAAABWQZqpSahBaJlMCF///oywAAA2SzaonIlxWifFO3qEs29R+JcjrAADjc1oT1f4eGRjsOogBAtaOpTOWdizIRlEfLDt0eVaVN8o7u7UFMDx6aP1zmx4MUEAAAA8QZ7HRREsI/8AABFV4Lvs495OaRM7fk4D1RZI2K7FrVEe8hPEEXX+Bi08Aey5bCiIT70xzpApr6Sk6vK/AAAASwGe5nRH/wAAG6luGKInxzdsjpaFZhzsscm+EKhZ9ry3T0h60ggeUFGZWABCE09JHJhzCLJ7enR0429GjqY+Zgj2hVV7pFeDJRheIAAAAE8BnuhqR/8AABucX2P0lM/pA5j4TEn9bayV7FuMACSFozmiTpKAcMzEFdDjxYqwM2QASg7V7Crptpy6oVrhU32OkjrsscWDLxCjzWCKgaeQAAAAeEGa7UmoQWyZTAhf//6MsAAANpwnP91MQ2MAHHiboDL3RP2uSMIKdhyt4FFRNbXh7EUSY1qLuQMvI6Wb6gLNRv17VGQJYDyvkovyf1W4dEjQ1bLP2eG+hO8WSN7iX6cAxiEXm+2rAQdi36yyYcParW9D10DOhlV+MQAAAERBnwtFFSwj/wAAEVfxL0QRNGVPVE/PFfcABETEAPHSu/8BN4yzmFUMSX5cPB6XF0m2x0bNMAZlVSjxqRBeyfYbj7FWLgAAAEABnyp0R/8AABuaNcNOkwpy8jtay06bmKlThRfbBNUS5WI3MTvfHgATzZXsOpStr38CACatL59ZCm6ce91Iuo/gAAAAUQGfLGpH/wAAGw6YFBBE170l5+w27L1Omri9KF3ETQLSJpE/0cBI6t6QS83BdGFKTmmOxdABI6tEcqZ5f1dKMY510pahQY64JeJkFQ1cPcDNfwAAAGpBmy9JqEFsmUwUTC///oywAAA1FH+jPoSAIuJXGGgnTOwef29i/NJcR8YOIsoOAMnRF1nh/4Vbxoqzowqu6s+zOSkIlX8KUGxvLJ/DRNt4+WTPwKN7Mn3OylgwR8gp5zjjSJiB599Tq/PBAAAAWwGfTmpH/wAAGvMUZOhSgA4xVs4jmViYPRO/0bzm1Mm7KVViSgWcg7NpvVmI+n+Mi8QZu20zYSPPSS/N21fj/qO0PJBNgqmJKQ3baHf1kfHEp9tmonq91pQ+CoMAAAB6QZtTSeEKUmUwIX/+jLAAADVcJz3VUkWAAtnqUJjzV8XyYj7KUGmXaOIYBl0Pz1Kv6wTQSFTNqAm9V1vDc9l5W7cR8ltCFVOmWdVVdWOIBXIeYPsYibwwPOVoqa9KKBEuHHwOplYKMHNMQsygcb6HcZB+Qm51cYLulvsAAAB3QZ9xRTRMI/8AABDdNobumXvK+gUAp5NvH7EBGY/CyYc5BZa1AeKablx0HelxCF3WuLal6uYD/zMtGwGvxqT0ngx1dcpQ5aSzEeKbXWyO5R7LWSREhxxhiDThCHzE5XA1vQ0Bp/KDd8MOytc2GWyfyt+ST1Q1EfAAAABDAZ+QdEf/AAAbCW28pHIBmgEBlVEFadZxHS0Hzkjpm7s4QtDqQnQT4GfPmJhDoa10SATyCwgAJq0vnsHvWICWmMBJDwAAAFQBn5JqR/8AABr/Y7AjuqhNypJAALY2IRf9qBZ4kpFJVRcFds5mAjAEUDjt/cuvY8+uT8lgvIy3yDphxhQt2ik+GIy9QRQKgEb0+uvMuxTfK2o/D4AAAABjQZuVSahBaJlMFPC//oywAAAz1H+jPihh2Qh9nxQAp+evK4ppWgBa9ffF0WzZEO1sD3u2gU/PyoUNIMmdPp1Ct9fGl4beEPpw5/+FqcLTqP7sgyy064JU9S5n3g9PsS+zzsWcAAAARAGftGpH/wAAGluDsafHhPxyLBACVPfB7B7+rEU6W5YpaY3eXK/J5S8bCpWMSDIZQIXCVe6HTSp4RjevxbUfI8NlzmfhAAAAT0GbuEnhClJlMCF//oywAAAzyyah1Zfa1AEVzyYsOpyM17B6S7vzOymvPztc67+i+TNEWfwmBPFlRJPjhxz9Z0+jJC2kHMrq8O/u4/ErxXgAAAAzQZ/WRTRMI/8AABBV75Y7tCD3N3sITIcdwLWwKgzvDtVurPWeLkWKXDZdLPvmR905WWThAAAANgGf92pH/wAAGlxfoanj7GusZuwEUdHT6UCHfpa3yAEsU9w9zoylm3cDq8JPAPf0yh4KgL/k4QAAAFVBm/lJqEFomUwIX//+jLAAADQcJz9sbsTJHkP8DvkZNuAM6mNDgKoeNZpzRth2n/KZKh97YNxw0qI98QhqVLOp55uZZZNz5XwsX3X1cSmu32i2RzdAAAABamWIggAO//73Tr8Cm0WXagOSVwr2yqQmWblSawHypgAAAwAAAwAAAwLa9HiTO+TU8TYAAAMBBwAoYVMRISAUsbwqBIxZjvx/hKHs4aFEVKH96+ds06Trva8GEMY0hTPAyEOQXM9TQ5KbZr5gDwioJtJ/U+pu6PKd3MIpFXJYuMJ8e7eV32La5CfWJPk9YjBr1VVI8oR4QjRGL82tEC3+21eHUU9wISwGYddbLS/fnuPrvEfovs42mR8r5r64d9qfjpg6tFxuS6Va86oHeGzLHtr2M5GuHnEMz3FqwZgcRXAcoFjaCH/6rz4kh1Gyvv4dKJlZha8Feyt5vgADGGfFPwpv7Ftx3Z+UKyu8Rpmo6fWzNZWtngUD1OGKCi88/eHw8H2XVq23DNMU5lDUswWsvI1fLs5ABDW9QpGCubSJKTm1zXqvvVf+Pw8QLgBMtKaysgTTkSvw/LBeZ83HXiHWYAIeAAADAAADAIWBAAAAZEGaI2xC//6MsAAAMo73t4mJ4OMAUzHTgrRJskNPCTGzHa64ztnA/aVVPG879thcdXHq+Hb//qAF4j6Dkzu91XXe6fjfUfCpSdqpC5cq4vcMdxo99rdZaJEPR7YWNczy0lWPNXgAAABAQZ5BeIR/AAAP5ClaVeAFu8oGnVIf72oMgpER8M7T2bbYkKLNSQfznpC3pc5ujRakj4r2wi8qtqiJbh6LUYRRlwAAAC8BnmJqR/8AABm7gthEJ0+SacQkSKeQ0nuH02GsPxwwivJeIRU+FN7XPVpF3NBqhQAAAGFBmmVJqEFomUwU8L/+jLAAADLcJz3OaVg9e/3+iuaDbUP+aVv8n+ZLFUgANEnPbVZwnVXYUMo+ElX4DPhgIZAeUiV12jUEkyxq6Um+rnaxJ0qHAqjOpv1JkwxhtYc7YNIvAAAAQgGehGpH/wAAGcfaUIajNDXJGLfh2zogpgBD1GTm9haMput9rxhahmOATC4ZyORQXO8ZTajvkxlYhCnKd6UyKx4MXAAAAFxBmolJ4QpSZTAhf/6MsAAAMVdiza1QPC0AF+B7voj2qzlVNh2Ixbsa/P1HHQCb7m8ZPo2+alATrk6qlepdKe28Eylh1BX3YggZ9QG7/5mS39wKGvmpkz9yNhZ9NQAAAEZBnqdFNEwj/wAAD4ZOfjwA3MsgENw7bems8TNGcTzDT9B6vI5Gxpb+ArNEe+G1rKQj6EgX/ESrFF3SqiwVXiMbmpQU1gJ6AAAASAGexnRH/wAAGSNoveO+p0xt7sC77LemxE78cALeJWuex4mdogH7u7hCTpUyKXy04Ivb1hXXDYF3WsXtpDoXACd68kna/QWM0wAAAD8BnshqR/8AABkbg4vw6JHABLH5rnk47HmKIwkOBI/x5sOVh0XP9ZiDoQR6dV4CH4Tz2exOkxkbe5bOv6Fum4AAAACRQZrLSahBaJlMFPC//oywAAAxVIBvm4TjpIeXjHgwhmy99WmukJJbk5SMgm8e8B+YnjHziEsM6rOHljhkSr3IuqlJfWPSlLxBuCg80/zJTPFZsVGflgkmarMo0eU+yjOCBj3BahFWvnYwtiFKr6Ts6H5Y/0CH7XiB2icTQOsrsNDpwCirV2xUcLhteH4kmAmqzQAAAEgBnupqR/8AABkn2okwADw4xpm84hmpjiN9Sk8AIqoyc3sLRrkSctT/dQKS5S4zs48v3mNGhe80r/yVg1KlSO2oXwrqmEomVsEAAABuQZrvSeEKUmUwIX/+jLAAADGcJz/dUvx2AKBfArk5RK/d0qMVr9hPrx2/wFx16ovusY88B71hiUNiLDHWt2pia+/dl648+KzybwN2RRdomZeh6y6EH08haz9BmvG54Oa5Ae+9eLJ8yyg+0MByesEAAABIQZ8NRTRMI/8AAA+H8a6LghKnUADQuDLI2LQAX7c9rEEThkzTdWsrawBDoB1kXPApu12rvaIrCGFyoblmjvSZckRzI9TDp1x9AAAARAGfLHRH/wAAGRv+sUYXrTcyIxSq/ktJGtOAFu/iUXjUt83z0uX+GJ0p7+r3UE2uRJolkdAmUc22jOzj3XsXb/RKPbQQAAAAOgGfLmpH/wAAGHxfY/SqMDsCNqtg8dcXmaeM4FmgGbraRWSTATaoFSO91YA+uDgyKwPZeqDB+uaSIIAAAAB+QZsxSahBaJlMFPC//oywAAAwCw6iTVBYauSkkyzjhQMPhwHIVflASNhr8d6Ye3Bdh/KGwmKizd8FENMEwG8jnKD0dEVA5Fgwjq671Fha5y6r3YfQ7Av+doBNZrUXjwpN94g3lxc+whSmLbabOOfGyYQfNGnOr6BLNO3DqDIHAAAAQgGfUGpH/wAAGHxfoapEy8PmGfot2CoDcAiho8qDsFubAnK6lou8gBCmvmaiV8nw+k4l1ehF9srGd2OXU3A7bRagXgAAAJlBm1VJ4QpSZTAhf/6MsAAAMFwnQY1RMW6hMrxjBovnOExvR5x+iexXNG+cexUrDX38ADiyo0gs5v8Qd3PINmh33H9pVrjNM8OytCm5hi9A4JDNRhi+AKTDyhXpdddMJr6lT3LA3bhmVel3sIygLfh4Ka2M30uIaXvfUK00KFBO2Zxts0JKu/Mz/sFt55Nra+YZC7c6uukMUogAAABMQZ9zRTRMI/8AAA8v8ZxOCDmCp73DjHwpnlPE/9iewZuIoSUYXk5VoLyPzIY6iGh5rrYw4cAAuracPqm8fzjDV7aRSsAAyJ4m70xioQAAAEIBn5J0R/8AABh7/fffC/fSuMmAO8T5beRL/81Uu9qwI1VHSjV0ccIRK9w0AB+/9wN8iNPYdb8InSQmt3Z2yvuKz8EAAABEAZ+Uakf/AAAX3F/aIbp1tNyBMZkGVEphiHg3HfXfQhpSANxqEutTRIk25cADZvXyLzFDukg/f7VGWcpCFQUnlm2NAV8AAABnQZuYSahBaJlMCF///oywAAAu4WJ22DniH+NyshiyRRP0+10NZvxckG6fTBDRVgajtWPgAG66on8CFyG/uYqBZQxArWVvp5/oR/VkJNYcnuR0c9v5t6/c54fexSYbto6NZTkYmc6vWgAAAFhBn7ZFESwj/wAADtY7SQW2LkDm8ZGlE3sBt4Zp4XtBc8cFm+xPZIUo51pjsMXvX+mZWCnJ1N93jahiS/T+MLJWcNQK+lnFPjeTQ3tLj1ypLONTqbkbNvFYAAAAOwGf12pH/wAAF9xfY/SU0AjajRQaO2CawnOs+Ye2EZKb8v6ZIN4qydvmS4qnKHosQzLCUd4uV+V7ZazNAAAAY0Gb3EmoQWyZTAhf//6MsAAALxzLjzpHD+WMYLK2QfZYABWnJZSgYoXdbW0VbdtKJa/WOh0hRfb8KmKRwu6KjpzGcXDTUs4+iEGuK7uq8weeFCX40pNC9B3fjtkhzvjDUzZeoAAAAEVBn/pFFSwj/wAADtf6eR8cUyBLRxlADH0ZgAasVNEGuL+n5hOxG3VUXjZddrKnsrfISkv52pTMhuTdTbxaxO55MbyW54AAAABZAZ4ZdEf/AAAX2/ch0j2Xu3Y7kBhBZ1ayCb7CdfLNvBQVsW2aj/nd9sAAffEK/Bo+T8JelTz4Ki/2fVor1Qm4d3KBPtv8Dxgq/FanSavmT8afP/N1aoFf2DMAAABOAZ4bakf/AAAXQgB8i037nZcl0JdCPmzRU3GiwaBBwToJRLhFClDCMU2RYMM9JEMnaOMvEmQ8AH7jvVbEc6Kq/IUiixmg2aTvXrrWQB6AAAAAgkGaH0moQWyZTAhf//6MsAAALbUaoU7iIA5hzKEQEtQwtFlpcyjft8hyfVsKFZqiK4lf6Jq9aDaZUN643sE6xz+/HHXVZH31Y3JD0mVD317W3h6C0PbC7Rz9Jq8OW1r50lLNEIYNW/pAtk6pVadbvseSXL5Q07t9YinTmRIxAHNzSEkAAAA5QZ49RRUsI/8AAA5+O0kFslFa1l6Ee+PfI3qdD0pbflfh69JJTfxpPt6T/YkxiM80+ucXF5RBtB4XAAAAOAGeXmpH/wAAF0IAS36NS6bkkC8FWqQr8vHYQtE94LgBCdjuEzL+Emj0/ArWYBPy092EeDgr0gfgAAAAQUGaQ0moQWyZTAhX//44QAAAsfJ9Vsy+pmepD6EI06xE+imr2hxNXWZ4ZgbpgBdjsbVnRXFiGYT+czo9ZXsb/QEzAAAALUGeYUUVLCP/AAAOf/Gcdsv3xdov53zC7HmWT9gjeJmfirks6WqDErV6+2GwQAAAAE0BnoB0R/8AABc/x63N27Qyie0wDqr55cOm4xZGF0+Tvxx6VCfX7vWcMiqR6gJRLi9xQATXR+Om53ueuecynK6+MFWa4jSv40glFw2RxwAAAD4BnoJqR/8AABayALQVRf3tncmYfxDuQV+zxf1CK2InvViMZa9+/fNV5ACR9LjpyK9zzcWC+aqcLZC2dpC2cQAAAIRBmodJqEFsmUwI//yEAAAKMl8RBdgAIj35fSOEXKxIp9Y//crP+Tl+ubjYaLky7/+0JteqLvxWd4bpY3KGUuTa0mQ1sROIl9t1xQ7U2uDvhI4T3+F/uoLlbRBo6EtRWHoJPajEkL+VFenonBvjM8hsKWGL0QMC6HKuvJrFU+356ok01+4AAABnQZ6lRRUsI/8AAA4mO18IRC5ya6/i7k4C/g359n1fxGD7RCa4gBXvtgTilt3RX/GSNKEsWAfkyIB2XQ4779a08pjS5kvQgzvPyOSDa2OgPQhBEWeKWn/MmbjHCprFunKTAkqfuKLckgAAAEwBnsR0R/8AABa9V7aSK7jOlmJW1M5o1Ujd53Oa11MQMoASjH1YLKFAGFP0D+Pje+vCkkulCz+SuFFN1VrmjHbzLWP79Mi8FrDdbwDxAAAAUwGexmpH/wAAFrIAdyVNfiqtJJIQ77QAlIp3/dImYcfYWqXRf5lys0JiriOX9+68Hu4G7K2i5/1JsorpwxAfxvc3E90kkU/KriW7qAgyW6SquL7zAAAQf21vb3YAAABsbXZoZAAAAAAAAAAAAAAAAAAAA+gAABaoAAEAAAEAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIAAA+pdHJhawAAAFx0a2hkAAAAAwAAAAAAAAAAAAAAAQAAAAAAABaoAAAAAAAAAAAAAAAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAQAAAAAAAAAAAAAAAAAAQAAAAAJYAAABkAAAAAAAJGVkdHMAAAAcZWxzdAAAAAAAAAABAAAWqAAAAgAAAQAAAAAPIW1kaWEAAAAgbWRoZAAAAAAAAAAAAAAAAAAAMgAAASIAVcQAAAAAAC1oZGxyAAAAAAAAAAB2aWRlAAAAAAAAAAAAAAAAVmlkZW9IYW5kbGVyAAAADsxtaW5mAAAAFHZtaGQAAAABAAAAAAAAAAAAAAAkZGluZgAAABxkcmVmAAAAAAAAAAEAAAAMdXJsIAAAAAEAAA6Mc3RibAAAAJhzdHNkAAAAAAAAAAEAAACIYXZjMQAAAAAAAAABAAAAAAAAAAAAAAAAAAAAAAJYAZAASAAAAEgAAAAAAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABj//wAAADJhdmNDAWQAH//hABlnZAAfrNlAmDPl4QAAAwABAAADAGQPGDGWAQAGaOvjyyLAAAAAGHN0dHMAAAAAAAAAAQAAASIAAAEAAAAAGHN0c3MAAAAAAAAAAgAAAAEAAAD7AAAI8GN0dHMAAAAAAAABHAAAAAEAAAIAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAAAgAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAQAAAAAAgAAAQAAAAABAAADAAAAAAEAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAIAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAADAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAMAAAAAAQAAAQAAAAABAAAFAAAAAAEAAAIAAAAAAQAAAAAAAAABAAABAAAAAAEAAAMAAAAAAQAAAQAAAAABAAAEAAAAAAIAAAEAAAAAAgAAAgAAAAABAAAEAAAAAAIAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAAAwAAAAABAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABAAAAAACAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABAAAAAACAAABAAAAAAEAAAUAAAAAAQAAAgAAAAABAAAAAAAAAAEAAAEAAAAAAQAABQAAAAABAAACAAAAAAEAAAAAAAAAAQAAAQAAAAAcc3RzYwAAAAAAAAABAAAAAQAAASIAAAABAAAEnHN0c3oAAAAAAAAAAAAAASIAAARIAAAA4gAAAEIAAABDAAAAPAAAAGgAAABBAAAAMAAAAC8AAACJAAAAQQAAADsAAAAvAAAAZQAAAD8AAAAtAAAAMAAAAF8AAAA0AAAAJgAAACIAAABzAAAAJwAAACkAAAAiAAAASAAAADMAAAAsAAAAMQAAAFgAAAA1AAAAIgAAACIAAABZAAAAMAAAABcAAAAhAAAATQAAADUAAAAuAAAAGQAAAFUAAAA5AAAAMwAAADQAAABNAAAANAAAACQAAAA5AAAASAAAADcAAAAsAAAAKwAAAEMAAAAtAAAAOwAAABsAAABVAAAALAAAAB4AAAAnAAAAUwAAADUAAAAoAAAAJQAAAHQAAABKAAAAHAAAACYAAABTAAAAJQAAADwAAAApAAAAWwAAADQAAABTAAAARgAAADoAAAAkAAAAQwAAADMAAABPAAAAMQAAAEAAAAAzAAAAbAAAAFQAAABTAAAAMgAAAC8AAAByAAAARQAAADAAAAA9AAAAXQAAAFQAAAAgAAAAVAAAAI4AAABDAAAAXgAAADcAAAB0AAAARwAAAEQAAAA4AAAAeQAAAEgAAAAfAAAAKwAAAHUAAABPAAAAJwAAAGAAAABUAAAAPgAAACoAAAAmAAAAdgAAADwAAAApAAAAPQAAAG0AAABAAAAAOAAAADUAAACUAAAAagAAADMAAABCAAAAVwAAADkAAABFAAAAOwAAAGAAAAAuAAAARAAAACcAAABZAAAAOwAAAD0AAAA0AAAAWQAAAEcAAAAzAAAANAAAAFsAAAA5AAAAJwAAADQAAABJAAAAPAAAAD4AAAAtAAAAVgAAAC4AAAA3AAAAMAAAAHMAAAA4AAAAMwAAADIAAABrAAAARwAAAD0AAAA2AAAAWwAAADQAAAA3AAAAMwAAAGwAAAA9AAAAKgAAAC0AAABZAAAATgAAACwAAAA8AAAAZQAAAE4AAABIAAAALwAAAHQAAAA+AAAALQAAADYAAABYAAAARAAAAD0AAABQAAAAOgAAAGcAAAArAAAAfgAAAEwAAAA0AAAASwAAAFEAAABRAAAASAAAAEAAAABsAAAAaAAAAEsAAAA4AAAAfQAAAJAAAABZAAAAPwAAAFgAAABuAAAAVAAAAIAAAABUAAAASwAAAE0AAABlAAAATgAAAE0AAABUAAAAVQAAADgAAABYAAAASAAAAFYAAABRAAAAUQAAADQAAABHAAAAUAAAAFoAAABAAAAATwAAAFMAAAB8AAAASAAAAEQAAABVAAAAbgAAAF8AAAB+AAAAewAAAEcAAABYAAAAZwAAAEgAAABTAAAANwAAADoAAABZAAABbgAAAGgAAABEAAAAMwAAAGUAAABGAAAAYAAAAEoAAABMAAAAQwAAAJUAAABMAAAAcgAAAEwAAABIAAAAPgAAAIIAAABGAAAAnQAAAFAAAABGAAAASAAAAGsAAABcAAAAPwAAAGcAAABJAAAAXQAAAFIAAACGAAAAPQAAADwAAABFAAAAMQAAAFEAAABCAAAAiAAAAGsAAABQAAAAVwAAABRzdGNvAAAAAAAAAAEAAAAwAAAAYnVkdGEAAABabWV0YQAAAAAAAAAhaGRscgAAAAAAAAAAbWRpcmFwcGwAAAAAAAAAAAAAAAAtaWxzdAAAACWpdG9vAAAAHWRhdGEAAAABAAAAAExhdmY1Ny44My4xMDA=\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: EpsGreedyQPolicy Window_length: 4\n",
      "/content/drive/MyDrive/MIOTI/RL/SESION_4/weights/dqn_CartPole-v0_EpsGreedyQPolicy_4_weights.h5f\n",
      "Testing for 5 episodes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: 124.000, steps: 124\n",
      "Episode 2: reward: 122.000, steps: 122\n",
      "Episode 3: reward: 116.000, steps: 116\n",
      "Episode 4: reward: 115.000, steps: 115\n",
      "Episode 5: reward: 120.000, steps: 120\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video alt=\"test\" autoplay \n",
       "                loop controls style=\"height: 400px;\">\n",
       "                <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\" />\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for policy in policies:\n",
    "  print (f'Policy: {type(policy).__name__}')\n",
    "  for window_length in window_lengths:\n",
    "    \n",
    "    print(f'Policy: {type(policy).__name__} Window_length: {window_length}')\n",
    "    model=create_model(window_length)\n",
    "    memory = SequentialMemory(limit=50000, window_length=window_length) # Llama al método SequentialMemory que has importado y establece un límite de 50000 steps y una longitud de ventana de 1\n",
    "    dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "                  target_model_update=1e-2, policy=policy)\n",
    "    optimizer =Adam(learning_rate=0.001)\n",
    "    dqn.compile(optimizer, metrics=['mse']) # Compila la red llamando al optimizador Adam con un learning_rate=1e-3 y usa la métrica 'mse'\n",
    "\n",
    "    # Se sonstruye el nombre de fichero de pesos a cargar\n",
    "    weights_file=f'dqn_{ENV_NAME}_{type(policy).__name__}_{window_length}_weights.h5f'\n",
    "    print(weights_path+weights_file)\n",
    "    dqn.load_weights(weights_path+weights_file) # Especifica la ruta donde hayas guardado los pesos en tu local\n",
    "    #Se crea el subdirectorio de videos para cada politica y ventana\n",
    "    sub_path_video=f'{type(policy).__name__}_{window_length}'\n",
    "    dqn.test(wrap_env(env,sub_path_video), nb_episodes=5, visualize=True)\n",
    "    show_video(video_path+sub_path_video+'/')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2cf541f9"
   },
   "source": [
    "**¿Has observado alguna diferencia aplicando una u otra política?**      \n",
    "**¿Hay alguna diferencia si el valor del window_length es 1 o 4?¿Por què?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGLsHjYWBUM0"
   },
   "source": [
    "* **Respuesta 1:** Si se observan diferencias. Con la política **BoltzmannQPolicy** se observa que se obtienen las máximas recompensas y el juego en la mayoría de los episodios completan los pasos sin que se incline el palo en más de 15º o el carro se mueve más de 2.4 unidades, tal como indican las reglas del juego CartPole [https://gym.openai.com/envs/CartPole-v0/](https://gym.openai.com/envs/CartPole-v0/).\n",
    "Con la política **EpsGreedyQPolicy** se obtiene resultados algo peores tal vez por que le penalice la fase de exploración. Las recompensas son menores en ambas ventanas comparada con la otra política."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hptZtCVnD1eH"
   },
   "source": [
    "* **Respuesta 2:** Dentro de cada política se obtienen mejores resultados cuando la longitud de la ventana **stacking** es 1 (una imagen o frame que aporta información adicional) que 4 (4 imagenes o frames que aportan información adicional al contexto). Por alguna razón en el juego de CartPole, la aportación de 4 frames penaliza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1639335089076,
     "user": {
      "displayName": "Juan Pedro Polvorinos Barrio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwKQaaN4KECTwkByySCIwnxJrULE8wNvdusboahqmVQFNw54KwDYMsXqMnvQlgJ3AUx8zv9n8i4luTFOE1VXtugWqRTjIC5klhT3ym6ohoFJFZQ8zBf3MpM-KrazXqM266jjwPJfOYrndprLel_Q9PfaGoQ5GPgdK0EnOPDaikECImYud-njTsdFuHWa37ul7J9HZ2TQHnuCLJ4IRO2xNi7f3Wk3gYQICMrh1x7_BvAoz3A-5cRGRsyt9zVTijQmO4A1TRoE93KrhWW-jw2JlWaXC-CSzhnl_NEhW_noMULm7SEJCjxSEizbHLR4Yu2-MZBmDRYOPGjB47BKA0XxdCtwD0Lp_TaLzbyDVrszdMe7fgnWseaTxHqRqieMj2e4CQaNhVceCXnuyOh2L72SkG80rMeDQSJP178sFdYkyII1wnNqOZUdI2pl7ovhmKcU96DhaoQg5VXXDLXEmGN4MptrfpKSHJGS0OePfMJVOctQtnGFZ6zcmXGqc_vmYAlxP7DseTjEEHffrZwRYBKVBSxnjmbGMSSJzM0KJZE5n5WM6H3jJhCGPUM1XHc_mzbIeZGb9kSgByKV6zNYKWyuwBi9-1ugBV1GEERfX_smRD7wHBou2Dm4fnaf0YKeab0ox-H8NkJf6vlci64dd5UpeP4JqrJt3gBlogsCj-lo_Uezvn72Q7BcgLhX_UJ-jx8pZgIsyBBDVbxkL9c6lOQeSUASIWjj7YxkbHj-tL808BzP8jzduzm7zyy-C5xVcjzgYcJQ=s64",
      "userId": "13012321559226237493"
     },
     "user_tz": -60
    },
    "id": "8ebe76ce"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "S4-Challengue-Keras_RL2-CartPole-Class.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.85px",
    "left": "1127px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
