learn classify complex pattern use vlsi network spike neuron center theoretical neuroscience abstract propose compact low power vlsi network spike neuron learn classify complex pattern mean firing rate online network integrateandfire neuron connect bistable synapsis change weight use local spikebase plasticity mechanism learn supervise teacher provide extra input output neuron train synaptic weight update current generate plastic synapsis match output desire teacher perceptron learn rule present experimental result demonstrate vlsi network able robustly classify uncorrelated linearly separable spatial pattern mean firing rate drive synaptic plasticity mechanism investigate recent year solve important problem learn modify synapsis order generate new memory old memory time new memory ongoing activity temporal pattern spike encode dependent plasticity mechanism simple form suitable learn pattern mean firing rate propose solve problem memory encoding memory preservation relatively simple pattern mean firing rate recently new model stochastic spikedriven synaptic plasticity propose effective old learn memory capture rich observe experiment synaptic plasticity include protocol network spike neuron use synaptic plasticity model learn classify complex pattern spike train range stimulus generate sensor image handwritten digit mnist database describe neuromorphic vlsi implementation spikedriven synaptic plasticity model present classification experiment use vlsi device validate model implementation neuron synapsis chip implement use hybrid circuit network spike receive input transmit output use asynchronous digital circuit spike represent address encode source neuron synapse device increase collection spikebase computing chip recently develop framework representation multiple implementation spikedriven plasticity model investigate parallel focus paper vlsi device propose successfully classify complex pattern spike train produce result accordance theoretical prediction figure layout test chip comprise network neuron plastic synapsis placement single neuron synapsis highlight figure circuit describe test section describe main feature spikebased plasticity model suited future scale vlsi technology section characterize functionality spikebased learning circuit section control experiment learn property vlsi network section present experimental result complex pattern mean firing rate section present remark point future potential application system implementation spikebase plasticity mechanism physical implementation long memory biological electronic hard limit synaptic weight bound grow negative resolution synapse limit ie synaptic weight infinite number state constraint usually ignore vast majority software model strong impact classification performance network memory storage capacity demonstrate number random uncorrelated pattern p classify store network neuron connect bounded synapsis grow number synapsis addition synapse n stable state ie weight traverse state lower bind bind number pattern p grow quadratically happen scenario fine tuning network parameter allow realistic scenario inhomogeneity variability case biology largely independent efficient strategy implement long memory vlsi network spike neuron use large number synapsis stable state modify weight stochastic manner small probability slow learning process positive effect previously store memory use strategy build large network spike neuron compact learning circuit require local gate cell store weight value construction type device operate massively parallel fashion considerable fraction synaptic circuit problem overall functionality chip compromise favorable property view potential problem future scale vlsi process vlsi test chip use carry classification experiment implement strategy fig chip comprise integrateandfire neuron dynamic synapsis fabricate use standard occupy area use communication allow chip receive transmit asynchronous event spike datum log prototype andor neuromorphic device use neuron internal dendritic tree connectivity single neuron connect synapsis depend state number active neuron decrease work chip use neuron synapsis synapsis divide different functional block excitatory fix weight inhibitory excitatory local learning circuit neuron chip use classifier separate input pattern category train pattern classify present presynaptic synapsis synapse v iup s vmem vmem ik bistable pre input dpi ik vdn dn s dpi stop learn block b figure plastic synapse circuit belong neuron dendritic tree synaptic modify presynaptic input ie s depend value vdn parallel bistable circuit slowly drive node stable state depend amplitude integrator circuit produce excitatory postsynaptic current amplitude depend synaptic weight neuron block diagram module comprise block dpi integrator voltage current winnertakeall circuit use current set output bias current ib voltage enable iup block depend value vmem respect voltage vdn use value iup neuron dendritic tree parallel teacher signal represent desire response respond activity proportional net input current generate input pattern weight learn synaptic efficacy teacher signal neuron mean activity accordance teacher signal typically high low output neuron produce correct response case synapsis update synapsis update time arrival poisson distribute input spike eventually transition stable state stochasticity addition mechanism prevent synapsis modify output correct allow neuron classify wide class highly correlated linearly separable pattern furthermore use neuron class possible classify complex separable pattern vlsi learning circuit learn circuit responsible locally update synaptic weight spikebased learn rule propose arrival presynaptic spike plastic synapse circuit update weight accord spikedriven learning rule synapse produce excitatory postsynaptic amplitude proportional weight exponential time course set millisecond current synapsis afferent target neuron sum neuron eventually neuron membrane potential exceed threshold generate output spike prescribe model postsynaptic neuron potential mean firing rate use determine weight change value weight change value express chip current specifically signal trigger positive weight update represent iup current signal trigger weight decrease represent current weight update perform locally synapse presynaptic weight update module value compute globally neuron postsynaptic weight module time vdn vup time s b figure postsynaptic circuit datum state vup vdn voltage function calcium concentration voltage state vup vdn voltage function potential vmem data correspond datum presynaptic module module fig comprise main block input interface bistable weight weight update circuit integrator integrator dpi circuit fully characterize arrival input event presynaptic asynchronous interface circuit produce output activate switch s depend value iup mirror postsynaptic weight control module ground update input event activate circuit produce current amplitude depend synaptic weight value w parallel bistable weight slowly drive stable state depend high low set threshold value stable state global analog parameter set external voltage postsynaptic weight control module module responsible generate global signal vup vdn mirror synapsis belong dendritic tree postsynaptic spike generate integrate instance circuit produce current proportional neuron average spiking activity current compare threshold value ik fig b use winnertakeall circuit parallel instantaneous value neuron membrane potential vmem compare threshold fig b value iup depend state neuron membrane potential average frequency specifically ik vmem iup ib vmem ib iup null characterize circuit inject step current neuron produce regular output mean firing rate measure voltage vdn gate voltage transistor produce vdn gate voltage p transistor mirror iup respectively fig neuron spike integrate output current increase exponential profile time accordingly time fig asymptotic value depend average input frequency circuit bias parameter large threshold decrease corresponding threshold voltage vdn activate large second threshold vdn signal finally large threshold signal switch small change vdn produce current iup mirror synapsis fig fig vdn vup signal membrane potential postsynaptic value depend state vmem pre time time figure stochastic synaptic ltp transition synapse stimulate poisson distribute spike rate postsynaptic neuron fire approximately plastic synapse stimulate poisson distribute spike train update synaptic weight produce ltp transition stimulus presentation update synaptic weight produce ltp transition remain vmem signal vup vdn activate null current iup complementary nature equal characterize stochastic nature weight update process stimulate neuron synapsis poisson distribute spike train irregular spike train use presynaptic input synaptic weight voltage cross synapse bistability threshold stochastic manner probability cross threshold depend input mean frequency long term ltp long occur stochastically mean firing rate input output fig instance learning experiment mean input firing rate row mean output fire rate row frequency experiment occur case compare synaptic weight change middle row panel experiment set efficacy high state plastic synapsis relatively low value way neuron mean output firing rate depend primarily teacher signal irrespective state plastic synapsis essential feature learning rule probability function postsynaptic firing frequency post essential slow eventually post high low indicate learn synaptic weight correctly classify input pattern fig experimental result measure transition synapsis train session case row initialize synapsis high state white pixel plot black pixel final state low end training session transition white black random nature occur probability increase decrease post analogous experiment ltp transition row complementary setting initial state set low value fig b plot row ltp row probability measure single synapse shape curve modify act postsynaptic weight control module bias parameter synapse number synapse number post b figure ltp transition synapsis measure trial different value postsynaptic frequency post label panel black pixel represent low synaptic state pixel high panel plot trial number yaxis state synapsis end trial row transition occur initialize synapsis high state row transition occur initialize synapsis low state transition stochastic probability peak different frequency fall high post validate algorithm data gray panel b transition probability measure single synapse function post transition probability reduce decrease value ib probability peak modify change bias set fig b t t inhibitory synapse excitatory high input state low input state integrate c figure typical training scenario random binary spatial pattern high low input encode poisson train mean frequency hz respectively binary pattern assign class arbitrarily training pattern belong class combine t teacher input spike train mean firing rate similarly pattern belong class combine t spike train mean fire rate new poisson distribute spike train generate training iteration classification random spatial pattern order evaluate chip classification ability use spatial binary pattern activity randomly generate fig neuron plastic synapsis stimulate train high hz low mean fire rate binary state input choose randomly number synapsis use input binary pattern randomly assign c c class train spatial pattern belong class present neuron conjunction t teacher signal ie train conversely pattern belong class combine t teacher signal t t spike train present neuron synapsis train session c c pattern interleave random order iteration stimulus presentation new distribution generate training session train neuron test correctly distinguish pattern belong class c c binary pattern use training present teacher signal neuron mean firing rate measure fig plot response neuron label train produce high output firing rate response pattern belong class c train respond pattern belong class c single threshold hz classify output high frequency low frequency class b figure classification result train pattern mean output frequency neuron train recognize class c pattern class c pattern pattern belong class c pattern belong class c b output frequency probability distribution c pattern pattern compute independent experiment fig b probability distribution postsynaptic frequency different classification experiment new set random spatial pattern quantify chip classification behavior statistically employ receiver operate characteristic analysis figure area curve auc plot yaxis increase number pattern auc magnitude represent correct classification represent chance level fig b storage capacity p express number pattern auc large plot number synapsis trace theoretical prediction p p stop learn condition respectively performance vlsi system synapsis condition lie theoretical curve conclusion implement neuromorphic vlsi device recently propose spikedriven synaptic plasticity model classify complex pattern spike train present result vlsi chip demonstrate correct functionality spikebased learning circuit perform classification experiment random uncorrelated binary pattern confirm theoretical prediction additional experiment demonstrate chip apply classification correlated spatial pattern mean firing rate knowledge classification performance achieve chip report system result device test perform realtime classification sequence spike ideal computational block adaptive neuromorphic system brainmachine interface work support national science foundation grant grant th storage capacity pattern input synapsis b figure area curve auc measure perform classification experiment b storage capacity number pattern auc value function number plastic synapsis use solid line represent datum obtain chip trace represent theoretical prediction stop learn condition reference r sompolinsky neuron learn spike decision nature neuroscience neuron learn plasticity neural computation w stochastic selection synaptic update chaos nonlinear science learn real world stimuli neural network spikedriven synaptic dynamic neural computation press e r vlsi array spike neuron bistable synapsis dependent plasticity ieee transaction neural network learn timing weiss b scholkopf editor advance neural information processing system mit m p avlsi recurrent network spike neuron plastic synapsis proceeding ieee international symposium circuit system page ieee ieee spikebase learn vlsi network integrateandfire neuron international symposium circuit system page l limit memory storage capacity bounded synapsis nature neuroscience e p model orientation selectivity use neuromorphic system proceeding ieee international symposium circuit system page ieee c dynamic analog vlsi neural connectivity neuromorphic chip use ieee transaction circuit system mead winnertakeall network complexity advance neural information processing system volume page introduction analysis pattern recognition letter robust classification correlated pattern neuromorphic vlsi network spike neuron ieee proceeding biomedical circuit system press
calibrate structured prediction computer computer abstract application display calibrate confidence measure probability correspond true important obtain high accuracy interested calibration structured prediction problem speech recognition optical character recognition medical diagnosis structure prediction present new challenge calibration output space large user issue type probability query eg marginal structured output extend notion calibration handle structured setting provide simple recalibration method train binary classifier predict probability interest explore range feature appropriate structured recalibration demonstrate efficacy realworld dataset introduction application speech recognition medical diagnosis optical character recognition machine translation scene label property instance structured prediction predict output complex structured object application important provide accurate estimate confidence paper explore confidence estimation structured prediction central paper idea probability calibration prominent econometric literature calibration require probability system output event reflect true frequency event time system rain probability time rain context structured prediction single event fix set event event depend input correspond different conditional marginal probability ask structured prediction model extend definition calibration way deal complexity arise structured set consider practical question build system output calibrate probability introduce new framework calibration structured prediction involve define probability interest train binary classifier predict probability base set feature framework generalize current method binary multiclass classification predict class probability base single feature prediction score structured prediction space interesting probability useful feature considerably rich motivate introduce new concept event range vary computational demand perform thorough study feature yield good calibration find feature good calibrate map marginal estimate recognition optical character recognition scene understand interestingly feature base map inference achieve good calibration marginal probability difficult compute figure context ocr system framework augment structured predictor calibrate confidence measure set event eg letter l l d event probability land l structured prediction model b forecaster output structured prediction structured prediction want assign structured label input example optical character recognition ocr sequence image sequence associated character figure note number possible output exponentially large common approach structured prediction conditional random field crf probabilistic model p train p optimize maximumlikelihood objective training set assume draw unknown distribution px promise probabilistic model addition compute likely output arg probability p marginal probability p probabilistic forecasting probability crf p number sum order probability useful confidence measure ideally like calibrate calibration intuitively mean forecaster assign probability event case event actually hold time case binary classification forecaster perfectly calibrate possible probability p p p calibration guarantee useful confidence measure forecaster output marginal class probability calibrate accurate prediction good forecast sharp ie probability close calibration sharpness forecaster define t true probability receive forecast f use t decompose prediction loss follow t et vary et z z z uncertainty sharpness calibration error equality follow expectation condition second equality follow variance decomposition term formalize intuition calibration sharpness calibration term measure close predict probability true probability region natural generalization perfect calibration correspond calibration error sharpness term measure variation true probability forecast depend numerical value forecaster induce grouping point maximize x close uncertainty depend ignore note great sharpness ensure loss stay positive input sharp example illustrate true difference calibrate p calibration error low sharp p sharpness balance p high consider follow binary classification example uniform distribution px input probability probability set p achieve perfect calibration sharpness excellent sharpness suffer calibration predict probability trade sharpness perfect calibration predict discretized probability assume far forecaster return arbitrary probability case need infinite datum estimate x accurately value order estimate calibration sharpness datum use discretized version calibration sharpness let partitioning interval example let b map probability p interval contain p case simply t true probability x lie bucket t bf hard discretized calibration estimate form upper bind calibration error calibration context structured prediction far present calibration context binary classification section extend definition structured prediction motivation construct forecaster pretraine structured model p yx confidence estimate set learn forecaster target cardinality large fact user probably interested event interest instead assume associate prediction user interested set ix event concern event e ix subset e like determine probability e e ix useful type event serve run example mapx encode mapx arg correct encode label position mapx correct ocr example figure suppose predict mapx land define event interest map marginal mapx mapx ix land l d note event interest depend mapx event pool define calibration analogy construct forecaster e try predict e remark early statement hold uniformly event guarantee expectation let e draw uniformly p extend joint distribution e forecaster perfectly calibrate p e e p p word average y event interest e ix output probability p event e actually hold probability p note definition correspond perfect binary calibration pair variable x e example mapx map prediction confidence p p fraction correct state marginal pool sample position confidence p p fraction correct recalibration procedure calibrate structured prediction input feature e train model p event set ix recalibration set xi output forecaster construct event dataset e e e train forecaster decision tree second example hint important inherent multiple event structured prediction confidence score marginal calibrate average position user look marginal position extreme example suppose probability forecaster output confidence event perfectly calibrate event calibrate isolation finally perfect calibration relaxed follow define calibration error et x e e t e e e construct calibrate forecaster discuss aspect calibration specific structured prediction let turn problem construct calibrate sharp forecaster recalibration framework propose framework generalize exist recalibration strategy structured prediction model p user specify set event interest feature e general depend train model p train forecaster predict event e hold e feature e train minimize loss recalibration set disjoint training example e e outline procedure example consider ocr setting figure margin feature e log p map log p map map x map second high scoring label accord p respectively typically correlate event map prediction correct perform regression use feature recalibration set produce probability limit infinite datum minimize expect loss ef e e expectation e calibration error et x e e small feature drive loss close nonparametric method knn regression sensible binary recalibration project datum highly informative onedimensional feature space predict label space obtain small loss note standard multiclass recalibration special case framework use raw score p single feature structured setting careful think choice classifier feature discuss choice feature calibration possible single constant feature eg x e sharpness depend strongly feature quality collapse point opposite label forecaster able separate sharp want informative feature afford recalibration set typically small compare calibration binary classification choice feature inform computational requirement informative feature require perform inference intractable model useful think feature belong type depend derive unstructured classifier train individually label map inference marginal inference section marginal inference produce sharp feature feature table propose feature follow guide principle illustrate computational tradeoff inherent structured prediction type map map recalibration margin map label length map gx margin mg margin p marginal recalibration margin label position label mp neighbor neighbor label mp label type map p mg margin table feature map recalibration map x marginal recalibration map consider type feature require unstructured map marginal inference generic function define score input order let arg let require define admissible classifier predict feature mp set lx ocr english word letter percentage map relative label forecaster recall calibration examine true probability event e condition forecaster prediction e p limit number different probability p output accurately estimate true probability p end let partition feature space range region r output probability p region r formally consider forecaster form e rr ix e r fraction point region r e e r event hold e note partition r depend recalibration set example forecaster neighbor knn decision tree let obtain additional insight performance forecaster function recalibration set size let denote recalibration set size use derive partition r probability estimate region r let tr e e r true event probability region r px r probability mass region r rewrite expect calibration error train random size draw p tr r rr classic biasvariance tradeoff small region low bias increase sharpness datum point region low variance calibration tr r bias variance r fixed partitioning independent bias variance empirical average fall n knn decision tree produce biased estimate tr region choose adaptively important achieve sharpness case ensure calibration error vanishe let p region grow uniformly large s e r e experiment test propose feature realworld task multiclass image classification task predict image label image set special case structured prediction framework improve exist multiclass recalibration strategy perform experiment cifar dataset image classification multiclass map accuracy raw svm raw fraction positive mean predict value ocr chain crf map recalibration accuracy use viterbi decode scene understand graph marginal accuracy use meanfield decode raw mean predict value raw mean predict value figure map recalibration multiclass chain crf setting leave middle marginal recalibration graph right legend include loss calibration radius black ball reflect number point forecast true probability consist color image different type animal class total train linear svm feature derive kmean cluster produce high accuracy dataset use feature high mutual information label drop performance negligible image use train calibration test optical character recognition task predict word sequence character sequence image figure calibrate ocr system useful automatic sorting setting demonstrate calibration tractable crf use dataset consist word human subject character binary image choose word training test remain word subsample way produce recalibration set scene understanding image divide set region task label region type eg person tree calibrate scene understanding important build autonomous agent try optimal action environment integrate uncertainty structured prediction setting inference intractable conduct experiment voc pascal dataset brief train graph crf predict joint labeling image image possible label input consist node feature crf edge connect adjacent use example train testing subsample remain example produce calibration set perform map inference use ad dual composition algorithm use mean field approximation compute marginal experimental setup perform map marginal calibration describe section use decision tree knn recalibration algorithm examine quality forecast base calibration sharpness section far discretize probability bucket size b report result use calibration curve test point let fi xi ei forecast probability ti true outcome bucket b compute average b fi nb b ti nb fi b number point bucket b calibration curve plot tb function perfect calibration correspond straight line figure example recalibration like demonstrate approach work box simple parameter single feature knn calibration set report result setting multiclass chain crf map recalibration margin feature figure leave middle graph crf marginal recalibration margin feature figure use calibration set respectively compare raw crf probability p e score character indicator mp marginal probability marginal probability mg feature ocr chain marginal recalibration accuracy use viterbi decode score length presence mp margin mp low marginal probability mg feature chain crf map recalibration accuracy use viterbi decode unstructured svm score map feature mp marginal feature scene understanding graph marginal recalibration accuracy use meanfield decode figure feature analysis map marginal recalibration chain crf leave resp marginal recalibration graph right calibration curve group feature table loss dot size indicate relative bucket size figure prediction green line setting set outperform exist approach individually classifier normalize probability estimate suggest specific event high scoring class estimate multiclass probability feature analysis investigate role feature figure consider structured setting setting evaluate performance use different set feature table describe progressively computationally demand feature main inexpensive feature naive expensive feature complementary help recalibration allow add global feature chain crf feature affect sharpness intractable graph crf setting figure observe pseudomarginal mp require map inference true marginal lack resolution mp augment additional feature mp capture label similar neighbor occur image resolve interaction feature appear marginal chain figure leave margin good class yield calibrate forecast slightly lack sharpness point confidence similarly small margin add feature improve calibration far differentiate low low confidence estimate similarly individual svm mp feature binary indicator character calibrate sharp accurately identify confidence set sufficient practice additional time compute add feature base marginal improve mp map crf recalibration figure middle simple feature mp mp sophisticated margin recall length word encode word map dictionary demonstrate recalibration let introduce new global feature s original crf dramatically improve calibration additional cost knn decision tree recalibration set size recalibration set size figure calibration error blue sharpness leave decision tree right function calibration set size marginal recalibration effect recalibration set size recalibration technique lastly figure compare knn decision tree chain crf marginal prediction use feature mg subsample calibration set size choose hyperparameter minimum leaf size decision tree knn fold crossvalidation try value increment figure method sharpness remain constant calibration error decrease quickly stabilize confirm datum decrease calibration error indicate crossvalidation successfully find good model finally find knn use continuous feature right column figure decision tree perform categorical feature previous work discussion calibration sharpness provide basis work idea connection l loss explore extensively statistic literature connection forecast evaluation exist generalization loss calibration online setting field starting point finally calibration explore extensively bayesian viewpoint start work recalibration study binary classification setting scale regression popular effective method method typically involve train predictor include extension rank loss combination estimator generalization structured prediction require develop notion event interest multiclass setting work estimate class probability useful typical structured prediction problem confidence estimation method play key role speech recognition require domain specific feature approach general apply graphical model include inference intractable use feature guarantee calibrate probability simple score correlate issue calibration arise time need assess confidence prediction importance discuss emphasize medicine natural language processing speech econometric psychology confidence measure calibrate probability formally tie objective frequency easy understand user eg patient undergo diagnosis researcher query probabilistic database modern ai system typically consist pipeline module setting calibrate probability important express uncertainty different potentially module hope extension structured prediction setting help calibration accessible easy apply complex diverse setting acknowledgement research support author sloan research fellowship second author code datum experiment paper available reference m confidence estimation automatic speech recognition hypothesis d e n normative expert system representation efficient knowledge acquisition inference method r h comparison approach online handwritten character recognition p taskar endtoend discriminative approach machine translation international conference computational linguistic association linguistic method learn structured prediction semantic segmentation natural image g verification forecast express term probability weather review h new vector partition probability score journal apply d p r asymptotic calibration t e probabilistic forecast calibration sharpness royal statistical society series statistical methodology reliability decomposition proper score royal society probabilistic output support vector machine comparison regularize method advance large margin classifier b elkan transform classifier score accurate multiclass probability estimate international conference knowledge discovery data mining page r predict good probability supervised learning proceeding international conference machine learn page d b t extra component score decomposition weather forecast krizhevsky learn multiple layer feature tiny image technical report coate learn feature representation kmean neural network trick trade second edition loss function binary class probability estimation classification structure application bayesian c elkan l predict accurate probability ranking loss international conference machine learning icml accurate probability calibration multiple classifier international conference artificial intelligence ijcai page d calibration confidence measure speech recognition m l calibrate predictive model estimate support medicine journal nguyen posterior calibration exploratory analysis natural language processing model empirical method natural language processing page d judgement uncertainty heuristic bias press
learning approximation application visual object recognition dept dept brain cognitive abstract recent study multiple learning effective object recognition lead popularity learning computer vision problem work develop efficient algorithm mlmkl assume class consideration share combination kernel function objective find optimal kernel combination benefit class algorithm develop mlmkl computational cost linear number class number class large challenge frequently encounter visual object recognition address computational challenge develop framework mlmkl combine worstcase analysis stochastic approximation analysis complexity algorithm m number class empirical study object recognition achieve similar classification accuracy propose method significantly efficient stateoftheart algorithm mlmkl introduction recent study promise performance kernel method object classification recognition localization choice kernel function significantly affect performance learn specifically multiple kernel learn attract considerable interest computer vision community work learn object recognition visual content image represent way depend method use keypoint detection extraction keypoint quantization representation lead different similarity measure image function related fusion problem cast mkl problem number algorithm develop mkl mkl formulate quadratically constraint quadratic program suggest algorithm base sequential minimization optimization improve efficiency formulate linear program solve efficiently use implementation order improve scalability order optimization method propose include subgradient method level method method base equivalence group lasso mkl propose regularize weight combination framework base maximum margin classification mkl formulate use discriminative analysis framework effort mkl focus binary classification problem recent study attempt extend mkl study assume similar kernel function use different related classification task study result significant improvement classification accuracy cost linear number class computationally expensive deal large number class object recognition problem involve object class number thousand important develop efficient learning sublinear number class work develop efficient algorithm mlmkl assume classifier share combination kernel note assumption significantly constrain choice function different class empirical study object recognition affect classification performance similar phenomenon observe naive implementation mlmkl share kernel combination lead computational cost linear number class alleviate computational challenge explore idea combine bad case stochastic approximation analysis reveal convergence rate propose algorithm significantly linear dependence m number class empirical study propose yield similar performance stateoftheart algorithm mlmkl significantly short run time suitable learning large number class rest paper organize follow section present propose algorithm convergence analysis section summarize experimental result object recognition section conclude work learning mlmkl denote collection training instance m number class introduce yk assignment kth class training instance assign kth class introduce x rd rd r function combine denote rnn s collection matrix datum point d xi introduce p p ps probability distribution combine kernel denote combine kernel matrix introduce domain p probability distribution p p p rs p goal learn training example optimal kernel combination p m class simple approach learn share kernel combination find optimal combination p minimize sum regularize loss function m class lead follow optimization problem m m pp max z reproduce kernel hilbert space p p x x regularize loss function kth class straightforward verify follow dual problem m kp max lp pp q q m m solve optimization problem eq view minimization problem lp follow subgradient descent approach compute gradient m p yk yk yk refer approach learning sum mlmklsum note approach similar propose main computational problem mlmklsum treat class equally iteration subgradient descent require solve svms large number class present formulation multilabel computational cost sublinear number class minimax framework order alleviate computational difficulty arise large number class search combine minimize bad classification error class max min differ eq replace main computational advantage use instead use appropriately design method able figure difficult class iteration spend computational cycle learn optimal combination difficult class way able achieve run time number class present optimization strategy eq base idea direct approach solve optimization problem eq dual form straightforward derive dual problem eq follow detail find supplementary document b b lp m m kp yk k m m challenge solve solution m domain b correlate impossible solve k independently svm solver descent approach develop optimize eq unable explore sparse structure k efficient stateoftheart solver order effectively explore power solver rewrite follow m kp m eq replace max advantage use eq resort svm solver efficiently find combination kernel kp develop subgradient descent approach solve optimization problem particular iteration subgradient descent compute gradient respect p follow m yk kp yk svm solution combine kernel kp follow descent method define potential function p p p pm follow equation update t t t normalization factor ensure p t t step size optimize p respectively unfortunately algorithm describe share shortcoming approach require solve m svm problem iteration computational complexity linear number class alleviate problem modify introduce stochastic approximation method particular iteration t instead compute gradient solve m svms sample classification task accord multinomial distribution m let index sample classification task use sample task estimate gradient respect denote t t follow t kp computation t t require need solve svm problem instead m svms key property estimate gradient eq expectation equal true gradient summarize proposition property key correctness proposition t t gap t et b stand expectation randomly sample task estimate gradient follow eq update p iteration t proportional t ensure norm t bound need smooth order smooth effect modify t sample directly m m small probability mass use smooth m m refer algorithm learning stochastic approximation mlmklsa short detailed description convergence analysis optimization problem introduce follow citation measure quality solution p lp min p p denote p optimal solution proposition follow property p p solution p p p p jointly convex p follow theorem convergence rate detailed proof find b b theorem run t iteration follow inequality solution obtain algorithm m e b m d c c t constant term e stand expectation sample task index iteration max max max z stand maximum eigenvalue matrix z input p step size kernel matrix assignment m different class training instance t number iteration smooth parameter initialization m p t sample classification task accord distribution m tm compute kp use svm solver compute estimate gradient t t use eq update t follow t t m t m end b b compute final solution p b b p p corollary m m run original paper iteration term need solve svm iteration computational complexity propose order sublinear number class m experiment section empirically evaluate propose multiple learn demonstrate efficiency effectiveness visual object recognition task datum set use benchmark datum set visual object recognition caltech pascal voc pascal voc caltech contain different object class addition background class use setting instance class use training instance test pascal voc datum set consist image distribute class use train pascal voc consist training image test image distribute class data set use default partition provide voc challenge caltech datum set image assign class image voc datum set assign multiple class simultaneously suitable code download table classification accuracy auc run time second mlmkl algorithm datum set sa gmkl sum simple vskl stand mlmklsa generalize mlmklsum variable sparsity learning average kernel respectively gmkl vskl sa mlmklsa training time coefficient gmkl coefficient voc gmkl mlmklsum vskl vskl figure evolution kernel weight time caltech datum set gmkl vskl curve display kernel weight average class different kernel combination learn class kernel extract kernel caltech datum set use software provide different feature extraction method use construction gb geometric blur descriptor apply keypoint use distance image compute average distance near descriptor pair image pair keypoint base dense sample descriptor quantize word spatial histogram x build generate feature use spatial histogram base visual word use form voc datum set different procedure base report voc challenge use construct multiple visual dictionary dictionary result different kernel obtain multiple visual dictionary deploy keypoint detector ie dense sampling keypoint descriptor ie sift spin different number visual word ie visual word iv different kernel function linear chisquare bandwidth calculate use procedure use variant visual dictionary construction construct kernel voc voc data set addition implementation apply hierarchical clustering algorithm descriptor quantization voc datum set lead kernel voc datum set baseline method compare propose algorithm mlmklsa follow mkl algorithm learn different combination class generalize multiple kernel learn method gmkl report promise result object learn kernel combination subgradient approach iii variable sparsity vskl descent base compare mlmklsa mlmklsum learn kernel combination share class describe section use optimization method implementation use implementation need experimental result evaluate effectiveness different algorithm learn compute area precisionrecall curve auc class report value auc average class auc auc mlmklsum mlmklsum caltech mlmklsum voc voc figure evolution classification accuracy time mlmklsa mlmklsum datum set auc number iteration number iteration figure classification accuracy auc propose mlmklsa caltech use different value p figure classification accuracy auc propose mlmklsa caltech use different value p evaluate efficiency algorithm running time train method code implement machine run ram operate propose method stop small state smoothing set simplicity p follow experiment step size choose caltech datum set voc datum set order achieve good computational efficiency b table summarize classification accuracy auc run time algorithm datum set note propose mkl method datum ie mlmklsa yield good performance method comparison justify assumption use kernel combination class note simple approach use average kernel yield reasonable performance classification accuracy significantly bad propose approach mlmklsa second observe average kernel method require learn kernel combination weight mlmklsa mlmklsum significantly efficient baseline approach surprising mlmklsa mlmklsum compute single kernel combination class compare mlmklsum observe mlmklsa overall efficient significantly efficient caltech datum set number class caltech significantly large voc challenge datum set result far confirm propose algorithm scalable data set large number class change kernel weight time propose method baseline method mlmklsum gmkl vskl caltech datum set observe overall mlmklsa share similar pattern gmkl vskl evolution curve kernel weight time fast baseline method mlmklsum significantly efficient gmkl vskl weight learn mlmklsum vary significantly particularly beginning learning process stable algorithm propose algorithm mlmklsa far compare mlmklsa fig classification accuracy change time method datum set observe unstable behavior mlmklsum classification accuracy mlmklsum vary significantly relatively short period time desirable method evaluate sensitivity propose method parameter conduct experiment varied value parameter fig classification accuracy auc propose algorithm change iteration caltech use different value observe final classification accuracy comparable different value demonstrate robustness propose method choice note extreme case ie bad performance indicate importance choose optimal value fig classification accuracy different value caltech datum set observe propose algorithm achieve similar classification accuracy set relatively small value result demonstrate propose algorithm insensitive choice step size conclusion future work paper present efficient optimization framework learning combine worstcase analysis stochastic approximation compare algorithm mlmkl key advantage propose algorithm computational cost sublinear number class suitable handle large number class verify effectiveness propose algorithm experiment object recognition benchmark datum set direction plan explore future aim far improve efficiency mlmkl reduce dependence number training example speed convergence rate second plan improve effectiveness efficiency explore correlation structure class acknowledgement work support national science foundation ii award office naval research onr opinion finding conclusion recommendation express material author necessarily reflect view aro onr research support world class university program research fund science technology r reference m zisserman pascal visual object class challenge voc result m statistical framework genomic datum fusion bioinformatics vol sun r jin learning proceeding neural information processing system cristianini p m learn matrix programming machine learn research vol pp o second order optimization parameter nip workshop learn automatic selection optimal kernel p s feature combination multiclass object classification proceeding ieee international conference computer vision p let kernel figure principled learning preprocesse classifier proceeding ieee conference computer vision pattern bach learn conic duality algorithm proceeding conference machine learn general efficient multiple learn proceeding neural information processing system pp bach machine learning pp jin m r extended level method efficient multiple kernel learning proceeding neural information processing system pp z jin m r simple efficient multiple kernel learning group lasso proceeding th international conference machine learn bach consistency group lasso multiple machine learning pp z jin m r smooth optimization effective multiple kernel learning proceeding conference artificial intelligence efficiency multiple learning proceeding conference machine learn m compare sparse multiple learning nip workshop understand multiple kernel learning method m p muller efficient accurate learn proceeding neural information processing system s learn unified kernel machine classification proceeding conference knowledge discovery datum mining p discriminant kernel regularization parameter learn programming proceeding international conference machine learn p learning proceeding th international conference machine learn multiple kernel learn multiple label proceeding conference intelligence multiple learn object categorization proceeding ieee international conference computer vision strongly learning proceeding ieee conference computer vision pattern nemirovski proxmethod rate convergence variational inequality continuous monotone operator smooth saddle point problem optimization vol pp m learn discriminative tradeoff proceeding conference computer vision m zisserman pascal visual object class challenge voc result m zisserman pascal visual object class challenge voc result open portable library computer vision shape matching object recognition use low distortion correspondence proceeding ieee conference computer vision pattern s c schmid p bag feature spatial pyramid match recognize natural scene category proceeding ieee conference computer vision pattern recognition e m match local image video proceeding ieee computer vision pattern schmid distinctive image feature scaleinvariant keypoint ieee transaction pattern analysis machine intelligence vol distinctive image feature scaleinvariant keypoint international computer vision vol c schmid p sparse texture representation use neighborhood proceeding ieee conference computer vision pattern m approximate near neighbor automatic algorithm configuration proceeding international conference computer vision theory application press k algorithmic application base learn formulation proceeding neural information processing system
extend unscented gaussian process new abstract present new method inference general nonlinear likelihood inference base variational framework assume likelihood linearize variational posterior mean use series expansion statistical linearization parameter update obtain equivalent state update equation iterative extended unscented kalman filter respectively refer algorithm extended unscented gps unscented gp treat likelihood require derivative inference apply likelihood model evaluate performance algorithm number synthetic inversion problem binary classification dataset introduction nonlinear inversion problem wish infer latent input system observation output system long history dynamical modeling estimation example inverse kinematic problem wish infer drive robot joint joint torque order place endeffector particular position measure position know forward kinematic arm exist algorithm estimate system input particular point time recursive manner extended unscented inversion problem continuous process smooth trajectory robot arm example nonparametric regression technique gaussian process applicable use linear inversion problem similarly gaussian process use learn inverse kinematic predict motion dynamical system robot arm human input torque system observable latent use train gps concern inference original latent input want find low dimensional representation high dimensional output prediction use gaussian process latent variable model paper introduce inference algorithm gps infer predict original latent input system explicitly train need infer latent input system desirable incorporate specific information algorithm term likelihood model specific task hand example nonparametric classification robust regression problem situation useful inference procedure require new likelihood model resort mcmc example present factorize likelihood model model expectation arise use arbitrary likelihood onedimensional easily evaluate use sample technique quadrature present alternative algorithm variational principle base linearize nonlinear likelihood model posterior mean method straightforwardly applicable likelihood retain computational efficiency require evaluation multidimensional intractable integral algorithm base statistical linearization require derivative likelihood model likelihood incorporate initially formulate model case linearization method general comparable exist algorithm fact derive update step iterative similar update iterative use variational inference procedure specifically derive likelihood model use framework use experiment variational inference nonlinear model linearization observable quantity rd likelihood model system interest situation desirable reason latent input system rd generate observation find input inversion problem probabilistic setting cast application baye rule follow form assume prior likelihood k rd rd nonlinear function forward model unfortunately marginal likelihood intractable nonlinear function likelihood prior posterior pf solution inverse problem intractable evaluate choose approximate posterior variational inference variational approximation use variational inference procedure lower bind likelihood use pf log log equality approximation true posterior pf lower bind refer free energy rewrite follow pf expectation respect variational posterior assume posterior gaussian form m c evaluate expectation kl term log log m m log c log pf expectation involve intractable method deal expectation present assume likelihood factorize observation provide alternative base linearize g posterior mean m parameter update find optimal posterior mean m need find derivative d gf m m term independent m drop place quadratic trace term component equation expectation represent augment d z s z m m s k solve m essentially nonlinear square problem expect posterior value expectation closed form solution use iterative newton method find m begin initial guess m proceed iteration m m m step length evaluate m intractable nonlinear term expectation equation linearize evaluate expectation b linearization matrix r dd intercept term r use m b m m m k substitute use identity derive iteration usually refer kalman gain term assume linearization ak intercept bk way dependent iteration find posterior covariance set s z log c analytic solution apply approximation k use identity converge value h point worth note relationship equation linearization need find expression linearization term method use expansion linearize calculation posterior mean mk linearize function way end optimization procedure find m coefficient substitute value equation jm h subscript construct converge posterior m remark single step iterate extend kalman filter correspond update variational framework use linearization nonlinear model posterior mean derive update variational framework proof trivial use equation iterative update statistical linearization method linearize statistical linearization find square fit g point advantage method require derivative obtain fit multiple observation forward model output different input point require key question evaluate forward model obtain representative sample carry linearization method obtain point unscented transform define point m m p mi m p d m free parameter refer column matrix square root follow use decomposition usual unscented transform use prior create point use posterior expectation equation use point define follow statistic d mi m d accord setting capture information high order moment distribution set yield uniform weight find linearization coefficient statistical linearization solve follow objective bk simply linear leastsquare solution c substitute b equation obtain evaluate use statistic kth iteration imply ak posterior covariance estimate iteration use form bk form equation respectively remark single step iterate unscented kalman filter approximation update statistically linearize variational framework equation equivalent equation single update iterate appear equation oppose term main difference derive update variational principle update similar regular recursive unscented kalman filter statistically linearize recursive square optimize posterior expectation involve arbitrary function equation analytical solution exist low bind marginal likelihood use approximation d log log log c log m m b trace term equation cancel trace term expected likelihood tr tr linearize substitute unfortunately approximation long lower bind log marginal likelihood general calculate approximation need optimize model hyperparameter gaussian process describe optimize m term dependent m linearization case m m maximum objective global convergence proof exist objective optimize procedure condition jacobian p guarantee exist statistical linearization monitor work practice experiment line search use select optimal value step length equation find set successively multiply number map objective decrease maximum number iteration exceed fast work practice maximum number iteration exceed diverge condition terminate search m return good value tend happen statistical linearization tend impact algorithm performance sure improve approximate f variational inference gaussian process model linearization present inference method model arbitrary nonlinear likelihood use framework present previously gaussian process model follow likelihood prior k rn n noisy observed value latent function latent function interested infer matrix element kxi result apply kernel function input pairwise manner important note likelihood noise model isotropic variance necessary condition use correlated noise likelihood model factorize likelihood case useful provide computational benefit approximation posterior m m m rn mean posterior latent function posterior covariance likelihood isotropic factorize n observation follow expectation variational inference log consequence linearization onedimensional fn use derive approximate gradient b m m m k diag likelihood obtain c k inverse posterior covariance prior inverse covariance modify diagonal mean use inverse parameterization gaussian use infer parameter instead obtain iterative step m straightforwardly m bk expression posterior covariance c value linearization method statistical mn nth diagonal element scalar version p equation point observation mn mn mn p refer linearize extended gp egp statistically linearize gp unscented gp ugp prediction distribution latent value query point require marginalization r predictive m c regular predictive gp m c m m k find predict observation evaluate onedimensional integral m c df use quadrature alternatively use ugp use application unscented transform approximate predictive distribution mi work practice figure demonstration learn linearize gps learn extended unscented gps consist inner outer loop laplace approximation binary gaussian process classifier inner loop learn posterior mean outer loop optimize likelihood parameter eg variance hyperparameter dominant computational cost learn parameter inversion equation computational complexity egp laplace gp approximation learn kernel hyperparameter use numerical technique find gradient algorithm approximate log log c log m k m specifically use optimization method library find fast effective advantage require knowledge high order derivative implicit gradient dependency experiment toy inversion problem experiment generate latent function datum k function use amplitude length scale lm r uniformly space build observation use test train gps generate point generate way use fold cross validation train point test point gps use standardized mean table negative log predictive density standardized mean square error test datum differentiable forward model low value measure predict report case mean egp gf b map trace learn m figure learn ugp forward model corresponding trace map objective function use learn m b optimization terminate divergence condition objective function value improve square error test prediction hold datum latent observe space use average negative log predictive density latent test datum calculate log mn gp method use covariance function hyperparameter initialize table result multiple differentiable forward model test egp ugp model use sample evaluate dimensional expectation number sample excessive simple problem goal competitive baseline algorithm test normal gp regression linear forward model figure result use forward model derivative exist crossing point objective function trace learn posterior mean use quadrature prediction observation space table unscented transform equation prediction figure interestingly difference performance egp ugp egp access derivative forward model ugp ugp egp consistently outperform term apart experiment inversion experiment ugp good performance egp outperform table classification performance usp dataset number low value negative log probability nlp error rate indicate performance learn signal variance length consistency nlp error rate laplace vb rbf logistic binary handwritten digit classification experiment evaluate egp ugp classification task interested probabilistic prediction class label value latent function use usp handwritten digit dataset task distinguish experiment logistic sigmoid use forward model g algorithm test laplace expectation propagation variational baye logistic gp classifier matlab toolbox support vector machine svm radial basis function probabilistic output logistic regression library square exponential kernel amplitude length scale use gps experiment initialize hyperparameter low bind initialize place lower bind egp ugp optimize value near value hyperparameter svm learn use grid search validation result summarize table report average negative nlp error rate learn hyperparameter value surprisingly ugp outperform classifier dataset classifier specifically formulate task conclusion discussion present variational inference framework linearization model nonlinear likelihood function use derive update extended unscented kalman filter generalize result develop inference algorithm process egp ugp ugp use derivative nonlinear forward model perform egp inversion classification problem method similar gp wish infer posterior latent function goal infer transformation nongaussian process observation space gp construct concern infer inverse function latent function model gp future work like create multitask ugp extend applicability inversion problem forward model multiple input output inverse kinematic dynamical system acknowledgment research support science industry fund big data knowledge discovery project thank nguyen helpful feedback fund australian government department communication australian research centre program reference algorithm leastsquare estimation nonlinear parameter society industrial apply mathematic vol filtering nonlinear estimation proceeding ieee vol mar e rasmussen process machine learn cambridge mit e joint inversion exploration resource proceeding international joint conference artificial intelligence pp k m learning robot inverse dynamic advance neural information processing system nip latent variable model high dimensional datum advance neural information processing system nip vol m process dynamical model neural information processing system nip vol p gaussian process dynamical model human motion pattern analysis machine intelligence ieee transaction vol pp m opper revisit computation vol b m iterate kalman filter update method ieee transaction automatic control vol pp l iterate point kalman filter application long range robotic science system vol m introduction variational method graphical model machine learning vol pp m statistically linearize recursive square machine learn signal processing ieee international workshop ieee pp nocedal s package online available output support vector machine comparison regularize likelihood method advance large margin classifier vol pp b r weiss d m m e machine learning machine learn research vol e c e rasmussen warp gaussian process nip
simple example dirichlet process mixture inconsistency number component division apply division apply abstract datum assume come finite mixture unknown number component common use dirichlet process mixture dpm density estimation inference number component typical approach use posterior distribution number cluster posterior number component represent observe datum turn posterior consistent concentrate true number component note elementary proof inconsistency simple possible set dpm normal component unit variance apply datum mixture standard normal component far example exhibit severe inconsistency instead posterior probability cluster converge probability introduction wellknown dirichlet process mixture dpm normal consistent density datum sufficiently regular density p posterior converge point mass p detail reference easy necessarily imply consistency number component example good estimate density include component small weight fact dpm infinitely component probability common apply dpm datum assume come finitely component population apply posterior number cluster word number component use process generate observed datum inference true number component prominent example course process closely resemble dpm model fine use posterior inference number cluster section example cite author evaluate performance method datum simulate fix finite number component population suggest find realistic dpm application important understand behavior posterior datum come finite mixture particular concentrate true number component note simple example dpm apply datum finite mixture posterior distribution number cluster concentrate true number component fact dpm exhibit type inconsistency general condition aim note brevity clarity end focus attention special case figure prior red estimate posterior blue o number cluster observed datum univariate normal dpm iid sample k dpm concentration parameter base measure mean precision b b estimate use collapse gibbs sampler sweep sample sweep running average use convergence diagnostic plot average independent run simple possible standard normal dpm dpm use univariate normal component unit variance standard normal base measure prior component mean rest paper organize follow section address question consider experimental evidence section formally define model consideration section elementary proof inconsistency case standard normal dpm datum component section standard normal datum standard normal dpm fact severely inconsistent discussion emphasize result way utility dirichlet process mixture flexible prior density ie bayesian density estimation addition widespread success empirical study dpm theoretical guarantee case posterior density concentrate true density rate logarithmic factor reference researcher eg empirically observe dpm posterior number cluster tend overestimate number component sense tend mass range value great equal true number figure illustrate effect univariate normal similar experiment different family component distribution yield similar result theoretical result section asymptotic nature experimental evidence suggest issue present small sample natural think fact prior number cluster diverge n log n rate main issue problem dpm strongly prefer tiny cluster introduce extra cluster need intuitive explanation case fact researcher observe presence tiny extra cluster reason previously understand incorrectly attribute difficulty detect component small weight tiny extra cluster especially clustering application deal ad way simply remove possible consistently estimate number component way remain open question natural solution follow number component unknown prior number component example draw number component probability mass function ps draw mix weight draw component parameter iid appropriate prior draw x iid s result mixture approach widely use certain condition posterior density concentrate true density rate logarithmic factor sufficiently regular true density strictly speak define model identifiable fairly straightforward modify identifiable choose representative equivalence class subject modification sort general condition datum finite mixture choose family model consistent number component mixing weight component parameter density interesting discussion estimate number component practical matter deal expect find datum come exactly finite mixture known family rare circumstance unfortunately model precede posterior number component typically highly sensitive likely order obtain robust estimator problem need reformulate researcher interested number component robustness issue think carefully need estimate number component measure suffice setup section define dirichlet process mixture model consideration dirichlet process mixture model dpm model introduce ferguson purpose bayesian density estimation practical effort author reference use p denote probability dpm model oppose probability distribution consider follow core dpm socalle chinese restaurant process crp define certain probability distribution partition let n denote set order partition t set word n ai crp concentration parameter define probability mass function t n set t ai t note t function t common distribution define term partition case t appear denominator purpose convenient use distribution order partition obtain uniformly affect prior posterior t consider hierarchical model t t ai t pxn t t t prior component parameter p family distribution component typically rd rk xi t t hierarchical model refer dirichlet process mixture dpm model prior number cluster t model t use t denote random variable represent number cluster distribution depend note distinguish term component cluster component mixture distribution mixture component p p cluster set index datum point come component dpm model cluster concern posterior distribution ptn t number cluster especially interested marginal distribution t pxn t pxn t t subset index denote let denote marginal d standard normal case note brevity clarity focus univariate normal case unit variance standard normal prior mean r r p straightforward calculation x p p p p density p refer result dpm standard normal dpm simple example inconsistency section prove following result exhibit simple example dpm inconsistent number component true number component datum posterior probability converge interestingly result apply identically equal constant c r simple set general result theorem r iid distribution probability standard normal dpm define converge proof let p let r n ai define sn use equation note s pn second factor equal convexity s sa factor equal sa p consequently pxn x pxn tn b p aa step follow apply equation numerator denominator use equation step b follow equation step c follow term sum nonnegative equation step d follow partition n d p r finite law large number n surely pxn pxn ptn p px px pxn exp exp x surely ptn converge severe inconsistency previous section converge standard normal dpm datum finite mean section prove fact converge standard normal datum illustrate use dpm way lead entirely result key step proof application hoeffding strong law large number theorem ptn standard normal dpm concentration parameter proof t define pxn t p method proof follow r word b r bound probability r word exist pxn tn r xn pr pxn ptn pxn tn r t pxn let r probability define s s marginal equation note s note p use fact density respect measure define ustatistic sum s k hoeffding strong law large number ustatistic k use equation nk log harmonic number inequality follow standard bound harmonic number log log r xn log surely follow easily r convergence probability imply sure convergence let equation pxn tn p r pn xi op conclude complete proof acknowledgment like thank raise question anonymous helpful suggestion improve quality manuscript research support national science foundation grant dms advanced project contract fa reference dirichlet process relate prior posterior asymptotic editor bayesian nonparametric page cambridge university press inference population structure dirichlet process model genetic m mixture model base clustering gene expression profile bioinformatic e nonparametric bayesian approach detect number regime switching model econometric review inference hierarchical dirichlet process mixture proceeding conference machine learn page p particle filter mixture model unknown number component statistic compute m inconsistency process mixture number component m muller hierarchical prior mixture model application regression density statistic m characterization bayesian genetic clustering base dirichlet process prior comparison bayesian clustering method bioinformatic bayesian analysis finite mixture distribution green bayesian analysis mixture unknown number component journal royal statistical society series green modeling dirichlet process statistic bayesian mixture unknown number component allocation sampler statistic compute adaptive bayesian density estimation electronic statistic p cluster bayesian analysis ferguson bayesian density estimation mixture normal distribution m d siegmund editor recent advance statistic page academic class bayesian nonparametric estimate density estimate annal statistic m compute nonparametric hierarchical model p muller d editor practical nonparametric semiparametric bayesian statistic page strong law large number statistic rl concrete mathematic
process model link analysis transfer learning laboratory abstract paper aim model relational datum edge network describe appropriate gaussian process gps direct undirected bipartite network edge effectively model adapt gp hyperparameter framework suggest connection prediction transfer learning traditionally separate research topic develop efficient learning handle large number observation experimental result realworld datum set verify superior learning capacity introduction scenario datum interest consist relational observation edge network typically finite collection relational datum represent m matrix partially observe element miss accompany attribute node edge important nature network highly interdependent condition know node edge attribute phenomenon extremely common realworld example graph datum represent relation different set object measurement pair heterogeneous condition notable example transfer learn know multitask learning jointly learn multiple related different predictive function base m observe label result function act set m datum example collaborative filtering important application transfer learning learn user interest large set item direct graph datum measurement existence strength type link set node graph collection observation m case m matrix symmetric asymmetric depend link undirected direct example include interaction social network citation network web link prediction aim recover miss measurement example predict unknown interaction base know interaction goal paper design gaussian process gp framework model dependence structure network contribute efficient algorithm learn predict relational datum explicitly construct series parametric model index dimensionality limit obtain nonparametric gp prior consistent dependence edgewise measurement matrix quadratic number edge computation cost cubic kernel size develop efficient algorithm reduce computational complexity demonstrate transfer learning connection link prediction method generalize recent transfer learn algorithm additionally learn taskspecific kernel directly express dependence task application learn network graph fairly recent work direction focus gps node graph target classification node paper regard edge develop general framework model dependence edgewise observation direct graph work extend build gps bipartite graph propose algorithm scaling number node contrast work general scale linearly number edge study promise careful treatment model nature edgewise observation offer promising tool link prediction gaussian process network datum modeling bipartite graph review edgewise gp bipartite observation measurement pair object different type pair condition formally let index set denote measurement edge context transfer learn pair involve data instance task denote label datum task probabilistic model assume noisy outcome realvalue function u r follow gaussian process characterize function b covariance kernel function edge kernel function v respectively result realization finite set m form matrix follow normal distribution equivalently normal distribution b k covariance mean kronecker product dependence structure edge decompose dependence node kernel notion similarity model express prior belief node similar node similar node essential learn kernel base partially observe order capture dependence structure network transfer learn mean learn kernel datum instance kernel task possible predict miss base know observation use let d k gp gp limit covariance pair theorem offer alternative view understand model edgewise function decompose product set intermediate nodewise function iid sample gp prior gp suggest gp bipartite relational datum generalization bayesian lowrank matrix b prior h nm finite element modeling direct undirected graph section model observation pair node set u case include direct undirected graph turn direct graph relatively easy handle derive gp prior undirected slightly nontrivial case direct graph let function u u r follow covariance function edge ci cj u u r kernel function node random function draw gp generally asymmetric symmetric direction edge model covariance function eq derive set independent set function sample gp prior c model situation nod behavior different statistically relate behavior receiver reasonable modeling assumption example paper cite common set paper likely cite common set paper case undirected graph need design gp ensure function symmetric follow construction gp symmetric calculation reveal bound limit theorem problem solve subtract grow quantity ci d suggest covariance function ci cj cj covariance function ensure symmetric covariance equal variance let d k tk ci gp limit covariance pair ci cj cj proof loss generality let base central limit theorem converge gaussian random variable d tk collection random variable independently follow distribution mean p ci j covariance function tk ci ci ci ci cj ci cj ci cj cj interestingly theorem recover theorem general connection let gp gp concatenate form function tk gp c covariance u ci j different set apply theorem lead d d tk bi d ci d d bi j ci cj cj theorem suggest general gp framework model direct undirected relationship connect heterogeneous type node basically learn nodewise covariance function edgewise covariance compose explain happen observation edge propose framework extend cope complex network datum example network contain undirected link direct link briefly discuss extension efficient learning consider regression case gaussian noise model later briefly discuss extension classification case let observational vector length corresponding quantity latent function k matrix edge observation compute observation edge generate bij k ij mean parametric form graph case let u analytically marginalize marginal distribution observation k parameter estimate minimize penalize negative loglikelihood l suitable regularization objective function form log c tr l m b bij optimization package apply find local optimum computation prohibitively high size measured edge big memory cost computational cost experiment thousand million slightly improved algorithm introduce complexity cubic size node algorithm employ nongaussian approximation base applicable bipartite reduce memory computational cost explore special structure discuss assume compose nodewise linear kernel hxi ci j hxi rl edgewise covariance graph hxi xi direct hxi xi hxi xi hxi xi turn problem optimize k problem optimize z z important note case form l matrix l apply identity u u dramatically reduce computational cost example bipartite graph case direct graph case respectively xi xi row u index undirected graph case rewrite hxi xi hxi xi hxi xi xi hxi xi h xi obtain simple form undirected graph case h xi overall computational cost ol empirically find algorithm efficient handle l o million gradient respect find calculation gradient respect z easily derive omit detail save space finally order predict miss measurement need estimate simple linear model bij incorporate additional attribute learn discrete observation different way incorporate node edge attribute model common practice let parametric function attribute choice function node edge attribute typically local information network global dependence structure network datum large pattern independent know predictor following example place bayesian prior u r describe flexible solution incorporate additional knowledge let covariance wish apriori close apply prior p z e use negative loglikelihood regularization log tr hyperparameter predetermine validation datum small number optimize energy function e relate divergence gp gp dirac kernel let linear kernel attribute normalize dimensionality derive likelihood dimension attribute random sample attribute nonlinear predictor set nonlinear kernel set corresponding attribute absent k set way observation discrete variable real value case appropriate likelihood function devise accordingly example probit function employ likelihood function binary label relate target cumulative normal bij preserve computationally tractability family inference technique laplace approximation apply find gaussian distribution approximate true likelihood marginal likelihood write explicit expression gradient derive analytically discussion relate work transfer learn suggest link prediction bipartite graph tight connection transfer learn clear let edgewise function u r consist nodewise function u r n fix dirac delta function assume function correspond learn task hierarchical assume multiple task share gp prior particular negative logarithm p log l l form close recent convex multitask learn regularization framework term replace trace term tr prove convex fj minimization convex jointly gp approach differ regularization approach aspect treat random variable marginalize need estimate regularization nonconvex term interestingly log trace norm envelope minimization problem similar thing framework introduce paper method introduce informative kernel task probabilistic modeling point view independence fj condition restrictive assumption incorrect taskspecific attribute mean taskspecific kernel transfer learning recently introduce increase computational complexity factor contribution paper transfer learning algorithm efficiently solve learn problem datum model learning generalization enforce model bipartite graph evidence equivalent form log ln tr l fully observe m matrix mean regularization assume column conditionally independent paper consider situation complex dependence edge network related work introduce link uncertainty framework relational model relational model popular aim find block structure link link prediction cast prediction statistical model base matrix factorization study work similar figure subset face datum contain people different view block indicate image test case righthand image row predictive image second row result row mmmf result fourth row bilinear result sense relation model multiplication nodewise factor recently multiplicative model generalize model encode relation numerical experiment set dimensionality model validation training datum case additional attribute node edge weak compare method matrix factorization mmmf use square loss similar singular value decomposition handle miss measurement demonstration face reconstruction subset face image size select illustrate algorithm consist people different view manually image test case present figure treat image vector lead matrix miss value column correspond view face train set l l matrix learn appearance relationship person identity pose image recover gp test case present second row figure right rmse result mmmf present row rmse employ bilinear model introduce handle miss datum matrix result row comparison quantitatively model offer generalization unseen view know person collaborative filtering collaborative filtering typical case bipartite graph rating measurement edge pair carry serial experiment eachmovie datum include user distinct rating movie randomly select user rating training use remain test case selection carry time independently comparison purpose evaluate predictive performance approach movie mean empirical mean rating movie use predictive value user rate movie user mean empirical mean rating user use predictive value user rate movie pearson score pearson correlation coefficient correspond dot product normalize rating vector compute matrix pearson score mean imputation movie user respectively principal component individual attribute try principal component attribute experiment carry square regression observed entry mmmf optimal rank decide validation table test result eachmovie datum number bracket indicate rank apply result average trial standard deviation evaluate accuracy utilize mean square error mean absolute error normalize mean square error ie rmse normalize standard deviation observation m m m m mmmf mmmf table test result datum classification accuracy rate average trial fold training fold test m ml l ink pca gp result approach report table average yield result average consistent finding previously report improvement use component pearson score significant generalization performance algorithm significant difference pvalue gp mmmf dimension term rmse worth highlight algorithm compact representation factor eachmovie datum factor represent thousand item individually train mmmf factor solution find accessible model mmmf fail achieve comparable performance case ie result mmmf trial number training sample program second accomplish update parameter use processor text categorization base content link use include paper data structure paper architecture machine learn ml programming language treat citation network direct graph model link existence binary label model apply probit likelihood learn nodewise covariance function c l compose edgewise covariance eq set prior covariance compute content attribute learn linear feature encode link content information use document classification compare method provide linear feature categorization use content bagofword feature link paper citation list pca component concatenation bagofword feature citation list paper choose dimensionality gp pca performance saturate dimensionality exceed report result base fold cross validation table gp clearly outperform method category main reason believe approach model behavior simultaneously paper conclusion extension paper propose gps model datum live link network describe solution handle direct undirected link link connect node work way future extension learn complex relational datum example model network contain direct undirected link let direct undirected base feature representation direct link eq undirected link ci cj cj indicate dependence direct link undirected link penalize compare dependence undirected link employ model multiple network involve multiple different type node type use nodewise covariance let covariance different type node obtain huge blockdiagonal nodewise covariance matrix block correspond type node big covariance matrix induce edgewise covariance link connect node different type near future promise apply model link prediction network completion problem reference e m m blei e p xe mixed membership stochastic block model relational datum application interaction annual meeting structure network learning nip workshop learn compare example t m multitask feature learn machine learn e learn use feature international conference artificial intelligence statistic j collaborative filtering factor analysis conference relational learning process processing system l e taskar probabilistic model text link structure classification workshop p multiplicative latent factor model description prediction social network appear computational mathematical organization theory p model stochastic equivalence symmetric relational datum appear neural processing system r hyperparameter learning base semisupervise classification neural processing system c j b t l griffith n learning system concept infinite relational model conference artificial intelligence latent variable model machine learning research c e rasmussen process machine learn srebro fast maximum margin matrix factorization collaborative prediction international conference machine learn b taskar m p abbeel link prediction relational datum processing system b tenenbaum freeman separate style content model computation z hide relational model international conference uncertainty artificial intelligence tresp learn gaussian process multiple task international conference machine learn tresp relational model discriminative link prediction neural processing system ghahramani semisupervise learn field university
optimal flow problem abstract present fast online solver large scale parametric problem occur optimization management computer vision logistic solve integer linear program online fashion exploit total constraint matrix lagrangian relaxation solve problem convex online game generate approximate solution problem perform stochastic gradient descent set flow apply algorithm optimize tier arrangement web page layered set cache serve incoming query stream optimally introduction parametric flow problem operation research receive significant contribution apply problem area database record segmentation energy minimization computer vision critical load factor determination system recently product selection word key technique estimation assignment problem unfortunately algorithm propose literature thousand million object billion common problem motivation solve parametric flow problem webpage tiere search engine index method entirely general apply range machine learning optimization problem focus webpage tiere illustrative example paper choose application firstly real problem search engine secondly provide large dataset introduce new problem machine learn community approach readily applicable large scale version problem describe specific problem provide run example assign webpage tier search engine cache time serve query minimize query search engine return number document typically time serve query depend document locate tier cache fast use hardware small retrieve document little latency single document locate tier delay considerably increase need search large slow tier desire document find goal assign popular document fast tier interaction document account tiere problem like allocate document d k tier storage let q query arrive search engine finite value probability query possibly weight relevance retrieve result set document retrieve query input structure store bipartite graph vertex q edge q e document retrieve query tier tier desirable costly form increase sequence capacity ct ct indicate page store tier t t loss generality assume d tier require hold document problem reduce finally t assume penalty incur level t know tier tier t access tier regardless set p convenience instance retrieve page tier incur total penalty p p background optimization index structure datum storage key problem build efficient search engine work build efficient index optimize query process paper deal issue optimize datum representation query index store general query particular address problem compute result scan entire invert list recently machine learning algorithm propose improve order collection basic invert indexing setup somewhat orthogonal strategy decompose collection webpage number disjoint tier order decrease level relevance document accord relevance answer query different tier typically increase size lead frequently retrieve relevant accord value query market parameter page tier small latency relegate frequently retrieve relevant page tier query carry sequentially search hierarchy tier improved ordering minimize latency improve user satisfaction reduce computation naive implementation approach simply assign value page index arrange frequently access page reside high level unfortunately approach suboptimal order answer query search engine typically return single page result return list r typically r page mean page find low tier need search retrieve page alternatively need sacrifice result relevance glance problem need correlation page induce user query account reason need design linear datum present ie number query storage requirement linear number page finally like obtain guarantee term performance assignment obtain problem r closely relate weight subgraph problem np hard optimization problem problem study somewhat general parametric flow problem derivation problem derive general version relegate proof appendix denote result set query q d d q g similarly set query seek document d q g document denote tier store d define max zd number cache level need traverse answer query q word document find bad tier determine cost access integrate optimization formulate tiere problem integer minimize zu subject k q d g t zd t ct t dd note replace maximization condition linear inequality preparation reformulation integer linear program obviously optimal z satisfy assume d exist optimal solution t ct d following address issue associate optimization problem eq integer program consequently discrete nonconvex exist reformulation problem b scale section present stochastic gradient descent procedure solve problem pass database datum accurate tier assignment page associate tail query address smooth estimator tier index page integer linear program replace selector variable zd binary variable code let subject xdt d t subject yqt index variable onetoone mapping xdt t z instance middle tier map x require good tier z correspond mapping analogous constraint simply rewrite coordinatewise p finally capacity constraint assume form ct number page allocate high tier d ct define remain capacity ct ct use variable transformation follow integer minimize subject xdt yqt yqt yqt xdt q d g p ct t k p p pk column vector matrix yqt advantage discrete linear constraint linear objective function problem variable need binary solution equivalent hardness discuss relaxation approximation algorithm review hardness problem consider tier case retrieve page query corresponding graph vertex d edge d d e display answer query case tiere problem reduce find subset vertex d induce subgraph large number possibly weight edge subject capacity constraint case page query simply assume page problem find good subset reduce case page query problem identical subgraph problem know np hard url ry ry url figure subgraph reduction vertex correspond url query correspond edge query serve corresponding url cache case induce subgraph contain edge program key idea solve relax capacity constraint tier render problem totally amenable solution linear program replace capacity constraint partial lagrangian ensure able meet capacity constraint exactly instead able state relaxed solution optimal observe capacity distribution able control capacity suitable choice associate lagrange multiplier linear program instead solve study program minimize subject xdt yqt yqt yqt xdt q d g yqt act lagrange multiplier t enforce capacity constraint denote column relate solution choice t linear program integral solution minimize xdt exist satisfy yqt solution solve succeed reduce complexity problem linear program need solve optimality accurate cache need adjust satisfy desire capacity constraint approximately denote value solution let t concave l maximize choice solution satisfie constraint note lemma provide guarantee associate integral solution exist set capacity constraint optimal capacity satisfy constraint find efficiently guarantee converse capacity constraint satisfied relaxation following example demonstrate example consider case tier drop index t single query q document set capacity constraint tier case impossible avoid cache miss lp relaxation optimal solution set xd partial lagrangian l maximize p p optimization problem solution p solution critical value combination value valid example optimal problem np hard possible design case tier assignment page highly ambiguous note integer programming problem capacity constraint allocate arbitrary pair page change objective function total cache feasibility page page query query s s t figure leave maximum flow problem problem page query minimum cut direct graph need page lead query alternatively need corresponding query incur penalty precisely tiere objective function case tier right query graph tier black node dash edge represent copy original graph additionally page original graph link corresponding query additional graph graph cut equivalence know case tier relax problem transformation work design bipartite graph query document document connect source s edge capacity query connect sink t capacity document retrieve query q connect q figure provide example graph source sink t tier slightly involved denote vertex associate document d tier denote vertex associate query q tier graph edge capacity edge document query pair infinite capacity edge t capacity simple cache problem need impose cut query edge incoming page edge cut key difference order benefit store page tier need guarantee page contain low tier variable reduction simplify relaxed problem far reduce number variable sacrifice solution step substitute xdt obtain optimization problem document minimize max xdt subject xdt t t note monotonicity condition yqt yqt t t automatically inherit solution integral problem equivalent integral solution scale t constant t t result solution new problem p unchanged essentially problem parameterize p yield solution form equivalence class consequently convenience solve assume t need consider original p evaluate objective use solution observed capacity relaxation reformulation extend integer linear program reasonable condition capacity constraint structure assume monotonically decrease t choice satisfy capacity constraint monotonically nonincrease tiere optimization initialize initialize n q learn rate increment update z q project max zd end end defer update observe current time read document compute update step repeat c large tier zd j change need reach tier zd partial step t zd step tier end update zd tier interpretation tier increasingly inexpensive optimal solution assign page fashion yield middle tier remain capacity ct strictly decrease monotonicity simplifie problem consequently exploit fact complete variable reduction define nonnegative virtue max note construction function clearly convex help describe tiere problem follow minimize max zd zd k use variable document constraint simple constraint simplifie convex projection need online programming solution equivalent online algorithm turn attention fast algorithm minimize greatly simplify relative remain problem billion variable key observation objective function write sum follow loss function max d q denote cardinality query set transformation suggest simple stochastic descent optimization traverse input stream query update value document d need tier order reduce service time query subsequently perform projection page vector set k ensure assign page tier proceed process input record stream comprise set page need display answer query specifically update tier preference page low tier score level decrement preference page apply result online optimization algorithm small number pass solution obtain converge rate log t t minimum value t number query process defer approximate update naive implementation infeasible require update d coordinate query possible defer update need directly key idea exploit update depend value update time section piecewise linear monotonically decrease path follow tiere problem appeal property solution increase form nest subset word relax capacity constraint promote page fact use design specialized solver work determine entire solution path problem alternatively simply advantage solution successive value determine approximate solution path use solution initialization strategy know context undesirable solve optimization particular value optimality instead simply solve approximately use small number pass property fact optimal solution binary total average solution entire path provide ordering page tier denote xd solution optimization problem value r denote average value range multiplier provide order sort document tier entire range practice choose finite number step nearoptimal solution yield path follow initialize refine variable use small number iteration end average variable sort document result total score zd fill order document tier tier experiment use synthetic datum feasible compute compare optimal lp solution pointwise value produce nearoptimal result case carry optimization procedure parameter simultaneously advantageous main cost sequential ram access cpu speed experiment examine efficacy algorithm test real datum major search engine result propose method compare max sum heuristic section perform experiment small synthetic datum tier tier able converge exact solution solver appendix solver slow feasible problem process log week contain result region include majority search engine user base simplify heavy processing involve collect massive datum set record particular result define query document pair appear result page session aggregate view count result use session value subset contain view document distinct query exclude result view yield final data set document simplicity experiment carry single cache system design parameter relative result readily extend probability measure set lagrangian value rk long positive weight value yield nest solution search result fix query vary variety reason database update approximate session graph treat query different result set different change figure leave experimental result real datum page query session miss rate online procedure max sum heuristic yaxis normalize point max heuristic optimal small cache size perform comparably online right online outperform max cache size large size prime tier cache ranking variant online pass datum consistently outperform max sum heuristic large span cache size figure direct comparison online procedure max sum heuristic induce ranking set document calculate session procedure cache size report relative improvement online algorithm ratio miss rate figure right optimizer fit ram value gb measure approximately second version small problem incur memory page fault problem readily fit gb ram computation value time implement version utilize cpu core performance improve memory disk bandwidth limit reach discussion large dense subset optimization problem solve efficiently relatively simple online optimization procedure extension appendix b come somewhat surprise max heuristic work nearly optimal solution experience correlation synthetic real datum believe possible prove approximation guarantee strategy satisfy certain powerlaw property reader question need static tiere solution datum theory different cache tier fly problem production system search engine large datum efficient reason eg different version ranking algorithm different version index different service level constraint transfer bandwidth addition problem restrict webpage occur product optimization resource constrain setting possible solve problem order magnitude large scale previously consider feasible acknowledgment thank provide computer fund australian government represent communication digital australian research centre program work carry lab optimization problem model accurate remove rare result maintain low count document large square root high session reference p bartlett e hazan adaptive online gradient descent s roweis editor nip cambridge m j mathematical technique efficient record segmentation large share database acm r combine fuzzy information multiple system acm symposium principle database system page r r maximal flow network mathematic predictive indexing fast search d d bengio l bottou editor nip page mit press d fast algorithm generalize parametric minimum cut problem application fast parametric e extension theorem h editor linear inequality relate system volume annal mathematic study source environment journal acm application parametric vision iccv confidence level solution stochastic programming technical report center operation economic l page pagerank citation rank bring order web technical report project h optimization algorithm complexity prenticehall m r filter document retrieval index m m architecture web search engine page ieee computer society critical load factor distribute system ieee t invert index compression query processing optimize document order editor conference world wide web page acm b simultaneous maximum flow algorithm selection report laboratory b simultaneous parametric find complete chain solution report laboratory b simultaneous parametric maximum flow algorithm vertex balance technical report laboratory
discriminative learning sumproduct network computer science wa abstract sumproduct network new deep architecture perform fast exact inference model generative method training spns propose date paper present discriminative train algorithm spns combine high accuracy representational power tractability class tractable discriminative spns broad class tractable generative propose efficient algorithm compute gradient conditional log likelihood standard gradient descent suffer diffusion problem network layer learn reliably use hard gradient descent marginal inference replace mpe inference ie infer probable state variable result update simple intuitive form test discriminative spns standard image classification task obtain good result date cifar dataset use feature prior method spn architecture learn local image structure discriminatively report high publish test accuracy stl use label portion dataset introduction probabilistic model play crucial role scientific real world application graphical model compactly represent joint distribution set variable product factor normalize partition function unfortunately inference graphical model generally intractable low treewidth ensure tractability restrictive condition particularly high practical treewidth usually sumproduct network spns overcome exploit contextspecific independence view new type deep architecture sum layer alternate product layer deep network layer hide variable greatly increase representational power single layer generally intractable add layer compound problem spns deep architecture probabilistic semantic inference guarantee tractable general condition derive poon domingo tractability spns expressive use solve difficult problem vision poon introduce algorithm generatively training spns generally observe discriminative training optimize p yx instead conditional random field retain joint inference dependent label variable allow flexible feature input unfortunately conditional partition function prone generative training reason low treewidth model chain tree commonly use research suggest approximate inference hard learn rich structured model paper discriminatively train spns allow combine flexible feature fast exact inference high treewidth model inference learn easily scale layer spns view type deep network exist deep network employ discriminative training backpropagation softmax layer support vector machine network variable network purely feedforward require approximate inference poon deep spns learn fast accurately deep belief network deep boltzmann machine generative image completion task paper contribute discriminative training use generative pretraine time combine advantage spns discriminative model paper review spns describe condition spn represent conditional partition function provide training algorithm demonstrate compute gradient conditional loglikelihood spn use backpropagation explore variation inference finally stateoftheart result spn achieve high accuracy svms deep model image classification task sumproduct network spns introduce aim identify expressive tractable representation possible foundation work lie network polynomial define unnormalized probability distribution vector boolean variable indicator function argument true distinguish random variable indicator variable use font vector variable denote p font respectively network polynomial define x product indicator state example network polynomial bayesian network p p x x x x p p x x x p p p p x compute p true false access corresponding term network polynomial set indicator rest find p true fix evidence set marginalize set notice reason set indicator evidence true case set marginalization general role determine term compatible variable state true include summation similarly x notation partition function z compute set indicator variable network polynomial size exponential number variable case represent compactly use sumproduct network definition poon domingo sumproduct network spn variable root direct graph leave indicator x d internal node sum product edge sum node nonnegative weight p value product node product value child value sum node child value value spn x d value root replace exponential sum variable state partition function linear evaluation network tractable example spn figure represent joint probability boolean variable p bayesian network use indicator sx x x p true sum joint state evaluate work total time instead set indicator network sum figure spn boolean variable indicator set compute sum state compatible evidence e true require evaluation spn property linear evaluation spn indicator set represent evidence equal exponential sum variable state consistent evidence spn valid definition poon domingo sumproduct network valid e evidence e paper poon prove condition sufficient validity completeness consistency definition poon domingo sumproduct network complete child sum node scope definition poon domingo sumproduct network consistent variable appear child product node poon domingo sumproduct network valid complete consistent scope node define set variable indicator node descendant appear child mean child descendant sum incomplete spn true marginal incomplete sum scope large child child nonzero state sum sx product node inconsistent spn marginal incorporate impossible state computation poon generatively train parameter spn method compute likelihood gradient optimize gradient descent use expectation maximization consider sum node marginalization hide variable find online use probable explanation mpe hard inference work good image completion task gradient diffusion key issue train deep model commonly observe neural network gradient propagate lower layer informative node network fractional error level node difficult parameter local poon effect use gradient descent train spns find online hard provide sparse strong learning signal synchronize effort upper low node note hard training exclusive section discriminatively train spns hard gradient descent discriminative learning spns define spn sy hx input disjoint set variable hide query denote setting indicator function sy x bold vector sum state variable discriminatively train spns instance treat constant mean ignore variable scope node consider completeness consistency add constant child product node product inconsistent variable child product node valid spn maintain completeness child sum node scope input set d instance variable label variable valid spn initialize parameter output spn learn weight repeat d d convergence early stop condition parameter spn learn use online procedure propose poon domingo dimension algorithm generative discriminative inference procedure weight update poon discuss generative descent marginal inference marginal mpe inference section derive discriminative gradient descent marginal mpe inference hard gradient descent use generative training typically use discriminative training require modification lower bind conditional likelihood close form mstep discriminative training marginal inference component gradient conditional log likelihood form log x h sy s sy x h summation separate bottomup evaluation spn indicator set sy s x respectively partial derivative spn respect weight compute detail perform bottomup evaluation spn partial derivative pass parent child follow chain rule describe form backpropagation present time linear number node spn product nod bounded number child gradient descent update follow direction partial derivative conditional log p yx gradient step optionally log likelihood learn rate renormalize weight sum node sum empirically find produce good result second spn evaluation marginalize reuse computation example model root sum node case value node equivalent evaluation architecture value node query variable indicator descendant input valid spn s sn denote value node bottomup output partial derivative spn respect node weight ij s initialize s s topdown order sum node sj s sj sj discriminative training mpe inference reason mpe inference appeal discriminatively training spns discuss hard inference crucial overcome gradient diffusion generatively training spns application goal predict probable structure sense use training finally common approximate summation maximization reason speed tractability summation spns fast exact mpe inference fast derive discriminative gradient descent use mpe inference figure positive negative term hard gradient root node sum variable sum nod left sum hidden variable h sum nod right sum circle denote input variable dash line indicate negative element gradient define network base network compactly represent polynomial compute convert spn replace sum node max node weight child retain gradient conditional log likelihood inference log log max h maximization compute m mpe inference consist bottomup evaluation follow topdown pass inference yield path spn complete include indicator assignment variable analogous viterbi decode path start root node max sum node travel child product nod path branch child define weight traverse path value form product number time appear partial derivative respect node weight compute modify accommodate m sum node body loop run child partial derivative logarithm respect weight form ci wj wj log m m m log m m m ci gradient conditional log likelihood inference ci ci ci ci difference number time traverse inference path m m respectively hard gradient update log hard gradient training instance illustrate figure expression complete travel mpe inference bold product node weight child appear gradient depict expression easily add regularization spn train l weight penalty familiar form w partial derivative add gradient appropriate optimization method l penalty use learn marginal inference dense spn architecture sparsity important spns nonzero weight impact inference time spns inference linear respect model size summary variation provide table generative hard gradient use place online dataset store inference result past epoch architecture high fanin sum nod soft inference able separate group mode fast hard inference alter child sum node time observe similarity update hard hard gradient descent particular reparameterize spn child sum node weight form consistent spn allow inference reach indicator branch path s product s weight s table inference procedure m kp m p p m m kp kp update disc m s si m table weight update soft p ci sx sx partial derivative log q ci log m m m wj mean hard gradient update weight wi ci resemble structured perceptron experiment apply discriminative training spns image classification benchmark cifar standard dataset deep network unsupervised feature learn class small image dataset achieve good result date task follow feature extraction pipeline coate et use recently learn pool function procedure consist extract pixel patch training set image whiten patch run kmean round normalize dictionary mean unit variance use dictionary extract feature pixel site image unit triangle encode max z kth item dictionary average image cifar example yield feature vector finally downsample maxpoole g g feature experiment simple architecture class allow discriminative learning local structure architecture train violate consistency inspire successful model construct net location work c class p class mixture component pattern image patch feature occur image arrangement e patch define curve dimension root spn sum child class multiply indicator state figure spn architecture experiment variable product p node variable indicator omit sum node t node hide variable h represent choice cluster mixture position respectively finally sum position j image logistic function variable dimension f overlap notice mixture model additional level spatial structure image patch feature learn kmean coate learn higherorder structure method learn structure discriminatively context model unsupervised group feature base correlation unable learn compare pool function et model independent translation patch feature architecture model nearby feature deep probabilistic architecture able model highlevel structure consider difficulty train model approximate inference hard use representational power model learn filter predefined image feature spn learn learn image feature model color detailed pattern generative spn architecture feature produce result generative training lead large number feature differentiate label generative spn paper continuous variable model univariate gaussian leave view sum node infinite child finite weight sum discriminative training continuous condition effectively fold weight network learn stochastic gradient descent regularize early stop find use marginal inference root mpe inference rest network work allow spn continue learn difference class correctly classify training instance fraction training set reserve validation cifar stl respectively learn rate p t choose base validation set performance result cifar cifar consist x pixel image training testing compare discriminative spns method vary size dictionary result figure fairly compare recent work set general observe spns achieve high performance use feature good approach learn pool function hypothesize spn allow discriminatively train large image structure capture large dictionary et pool function blur individual feature ie dictionary item classifier infer coordination image experiment fine grid dictionary item pool function information learn fine grid feasible method et number rectangular pooling function grow good test accuracy achieve p choose performance cifar accuracy discriminative learn pool white coate autoencoder raw coate whiten coate dictionary size figure impact dictionary size pool grid cifar test accuracy table test accuracy cifar logistic regression triangle layer learn rf learn pool discriminative spn dictionary grid x grid x grid grid accuracy table comparison average test accuracy fold stl layer vector quantization layer sparse code layer learn receptive field discriminative spn validation set performance architecture achieve publish test accuracy cifar dataset remarkably use fifth number feature good approach compare cifar result table highlight dictionary size system use feature extraction coate result stl stl large pixel image label datum training test cifar training set map predefine fold image experiment stl dataset manner similar cifar ignore item unlabeled datum model train fold test accuracy report average p t achieve standard deviation fold publish test accuracy write notably include approach use unlabeled training image coate architecture learn local relation different feature map spn able discriminatively learn latent mixture encode decision boundary linear classifier use work carry experiment report high accuracy unsupervised feature linear svm feature coate anticipate use spn instead svm beneficial learn spatial structure svm model conclusion sumproduct network new class probabilistic model inference remain tractable high treewidth hide layer paper introduce algorithm learn spns discriminatively use form backpropagation compute gradient discriminative training allow wide variety spn architecture generative training consistency maintain evidence variable propose soft hard gradient algorithm use marginal inference soft case mpe inference hard case successfully diffusion problem allow deep network learn experiment image classification benchmark illustrate power future research direction include apply discriminative learning paradigm spns method automatically learn spn structure apply discriminative spns variety structured prediction problem acknowledgment research partly fund aro grant contract nsf grant iis onr grant view conclusion contain document author interpret necessarily represent official policy express imply government reference m amer sumproduct network modeling activity stochastic structure cvpr junction tree advance neural information processing system bengio learn deep architecture foundation trend machine learn object recognition hierarchical kernel descriptor computer vision pattern recognition cvpr ieee conference page ieee l ren descriptor visual recognition advance neural information processing system l ren feature learn rgbd base object recognition c friedman m independence network proceeding twelfth conference uncertainty artificial intelligence page m probabilistic inference weight model count artificial intelligence efficient principled learning thin junction tree s roweis editor advance neural information processing system mit coate h analysis singlelayer network unsupervised feature learn aistat society artificial intelligence statistic coate ng importance encode training sparse coding vector quantization international conference machine learning volume page coate ng select receptive field deep network nip m collin discriminative training method hidden markov model theory experiment perceptron algorithm proceeding conference empirical method natural language processing page differential approach inference bayesian network acm modeling reasoning bayesian network cambridge university press bengio shallow deep sumproduct network proceeding th conference neural information processing system p m b rubin maximum likelihood incomplete datum royal statistical society series p d mcallester d discriminatively train multiscale model computer vision pattern cvpr ieee conference page hyvarinen e oja independent component analysis algorithm application neural network darrell spatial pyramid receptive field learn pooled image feature cvpr pereira structure learn approximate inference advance neural information processing system lafferty mccallum pereira conditional random field probabilistic model segment labeling datum proceeding international conference machine learn page poon p sumproduct network new deep architecture uncertainty artificial intelligence page mean covariance use factorize computer vision pattern recognition cvpr ieee conference page ieee expectation maximization algorithm conditional likelihood proceeding international conference machine learn page acm
unified view matrix completion general structural constraint twin city abstract matrix completion problem widely study special low dimensional structure low rank structure induce decomposable norm paper present unified analysis matrix completion general lowdimensional structural constraint induce norm regularization consider estimator general problem structured matrix completion provide unified upper bound sample complexity estimation error analysis rely generic establish intermediate result independent interest characterize size complexity low dimensional subset high dimensional ambient space certain partial complexity measure encounter analysis matrix completion problem characterize term understand complexity measure gaussian width b form restrict strong convexity hold matrix completion problem general norm regularization far provide nontrivial example structure include framework notably include recently propose spectral ksupport norm introduction task complete miss entry matrix incomplete subset potentially noisy entry encounter application include recommendation system datum imputation covariance matrix estimation sensor localization traditionally illpose high dimensional estimation problem number parameter estimate high number observation extensively study recent literature matrix completion problem particularly illpose observation limited high dimensional measurement extremely localized ie observation consist individual matrix localize measurement model contrast random measurement pose additional complication general high dimensional estimation wellpose estimation high dimensional problem include matrix completion low dimensional structural constraint impose target matrix completion special case lowrank constraint widely study exist work propose tractable estimator nearoptimal recovery guarantee approximate lowrank matrix completion recent work address extension structure decomposable norm regularization scope matrix completion extend low dimensional structure far simple decomposable norm structure paper consider unified statistical analysis matrix completion general set low dimensional structure induce suitable norm regularization provide statistical analysis generalize matrix completion estimator constrained norm minimizer generalize matrix selector section main result paper theorem b provide unify upper bound sample complexity estimation error estimator matrix completion norm regularization exist result matrix completion low rank decomposable structure obtain special case general result unified analysis sample complexity motivate recent work high dimensional estimation use global sub measurement key recovery analysis high dimensional estimation involve establish certain variation restrict isometry property rip measurement operator property satisfied measurement operator high probability unfortunately note cande et owe highly localize measurement condition satisfied matrix completion problem exist result base global sub measurement directly applicable fact key question consider limited measurement model matrix completion sample complexity estimation increase known sample complexity bound global measurement result upper bound sample complexity matrix completion log d factor large global sub measurement result know low rank matrix completion use nuclear norm minimization careful use generic log d factor suffice structure induce norm key intermediate result useful form restrict strong convexity hold localize measurement encounter matrix completion norm regularize structure result substantially generalize exist rsc result completion special case nuclear norm decomposable norm regularization analysis use tool generic chain characterize main result theorem term gaussian width definition certain error set gaussian provide powerful geometric characterization quantify complexity structured low dimensional subset high dimensional ambient space unified characterization term advantage numerous tool develop literature bound width structured set literature readily derive new recovery guarantee matrix completion suitable structural constraint appendix d addition theoretical unified framework identify useful potentially low dimensional structure significant practical interest broad class structure enforce symmetric convex body symmetric atomic set analyze paradigm section specialized structure potentially capture constraint certain application simple particular discuss detail nontrivial example spectral ksupport norm introduce summarize key contribution paper provide unify upper bound sample complexity estimation error matrix completion estimator use general norm regularization substantial generalization exist result matrix completion structural constraint theorem apply derive statistical result special case matrix completion spectral ksupport norm regularization intermediate result theorem norm regularization form restrict strong convexity rsc hold matrix completion set extremely localize measurement far certain partial measure complexity set encounter matrix completion analysis intermediate result theorem provide bound partial complexity measure term understand complexity measure gaussian width intermediate result independent interest scope notation preliminary index typically use index row column respectively matrix index k use index observation denote standard basis appropriate dimension notation use denote matrix vector respectively independent p e denote probability event expectation random variable respectively p integer let n euclidean norm vector space denote kxk xi matrix singular value p common norm include norm nuclear norm brevity omit explicit dependence dimension necessary spectral norm maximum norm kxk let sd rdd kxkf rdd kxkf finally norm define dual norm kxk k definition gaussian width set rdd widely study measure complexity subset high dimensional ambient space eg sup recall matrix independent standard gaussian variable key result discuss appendix d definition random variable subgaussian norm random variable kxk p p kxk b equivalently subgaussian follow condition satisfied constant p b p t px t et b e s b definition restrict strong convexity rsc function l satisfy restrict strong convexity rsc respect subset rsc parameter l l l d definition spikiness ratio rd measure spikiness sp definition norm compatibility constant compatibility constant norm r r closed convex cone v define follow rx r c sup structure matrix completion denote ground truth target matrix rdd let d noisy matrix completion observation consist individual entry observe additive noise list independently sample standard basis potential duplicate observation r h k r noise vector independent subgaussian variable definition scale variance noise note constant loss generality assume normalization uniform sampling assume entry draw independently uniformly e d define linear operator p rdd r follow r p p k hx structural constraint matrix completion low dimensional structural constraint necessary consider generalized constraint set lowdimensional model space m m enforce surrogate norm regularizer r assumption r norm low spikiness matrix completion uniform sample model restriction low dimensional structure require ensure informative entry matrix observe high probability early work assume stringent matrix incoherence condition lowrank completion preclude matrix recent work relax assumption intuitive restriction spikiness ratio define relaxation approximate recovery typically guarantee regime oppose exact recovery incoherence assumption assumption spikiness ratio exist d special case application briefly introduce interesting example structural constraint practical application example low rank decomposable norm common structure use matrix estimation problem include collaborative filtering pca spectral clustering estimator use nuclear norm widely study statistically recent work extend analysis low rank matrix completion general decomposable norm ie r m m example spectral ksupport norm nontrivial significant example norm regularization decomposable spectral ksupport norm recently introduce spectral ksupport norm essentially vector ksupport norm apply singular value matrix rdd loss generality let d g set subset cardinality denote set let d d spectral ksupport norm spectral ksupport norm special case cluster norm far multitask learning task column assume cluster dense group cluster norm provide tradeoff inverse norm task vector demonstrate superior empirical performance cluster norm ksupport norm traditional trace norm spectral elastic net minimization mark matrix completion multitask learn dataset known work statistical analysis matrix completion spectral ksupport norm regularization section discuss consequence main theorem nontrivial special case example additive decomposition elementwise sparsity common structure assume highdimensional estimation problem matrix completion sparsity conflict assumption traditional incoherence assumption easy high probability uniformly sample observation inform prediction infeasible p sparse structure use additive decomposition framework component turn structure low use robust structure scope recover sparse component observed index assume sparse far sparsity assumption conflict spikiness assumption case consistent matrix completion feasible additional regularity assumption matrix candidate norm regularizer structure weight infimum convolution individual structure induce norm example application potential application include cut matrix structure induce compact convex set norm induce structured sparsity assumption spectrum handle paradigm paper structure matrix estimator let r norm surrogate structural constraint r denote dual norm propose analyze estimator task structured matrix completion constrain norm minimizer argmin r d generalize matrix r d d r p p p r rdd linear adjoint p hx consistency result respectively certain condition parameter particular condition assume knowledge noise variance spikiness ratio practice sp typically unknown parameter tune validate hold datum main result define follow restrict error cone subset tr tr cone r tr sd d cn b ds estimate respectively choose let b cn belong feasible set respectively error matrix b contain tr theorem constrained norm minimizer problem setup section let b cn estimate large c log exist rsc parameter constant c c probability great expc expc log log d c max d c d b selector problem setup section let d d b ds estimate r p large c c log d exist rsc parameter constant c probability great expc e log d r width subspace compatibility constant r respectively remark r tr r r log d use bound recover nearoptimal result low rank matrix completion spikiness assumption estimator upper bind sample complexity dominate square consider effective dimension subset high dimensional space play key role high dimensional estimation result independent r upper bind sample complexity consistent matrix completion highly localize measurement log d factor known sample complexity estimation measurement term estimation error bound scale observation noise variance constant second term upper bind error arise certain radius spikiness constraint contrast exact recovery use stringent matrix incoherence condition cn comparable result cande low rank bind matrix completion regime term dominate high dimensional estimation gaussian measurement bind easy specialize result new structural constraint bind potentially loose asymptotically converge constant error proportional noise variance estimation error bind typically sharp specific structure use application require additional bound p r tr d partial complexity measure recall e gi g standard normal vector definition partial complexity measure randomly sample collection rdd random vector r partial complexity measure e sup p special case vector standard standard rademacher variable particular interest case symmetric e p later expression use ignore constant term theorem partial gaussian complexity let let sample accord universal constant r sp s k s min sup sup d d d d far center r k note d second term consequence localized measurement spectral ksupport norm introduce spectral ksupport norm section estimator spectral ksupport norm efficiently solve proximal method use proximal operator derive interested statistical guarantee matrix completion use spectral ksupport norm regularization extend analysis upper bounding gaussian width descent cone vector ksupport norm case spectral ksupport norm let rd vector singular value sort order let d pp let r unique integer satisfy denote r r k r c finally rank error set r proof provide appendix combine obtain recovery guarantee matrix completion spectral ksupport norm discussion related work sample complexity consistent recovery high dimensional convex estimation desirable descent cone target parameter small relative feasible set enforce observation estimator measure error cone crucial establish sample complexity estimation error bound result paper largely characterize term widely use complexity measure gaussian width compare literature estimation error bound theorem provide estimation error bound depend width descent cone regime result comparable analogous result constrain norm minimization bind potentially loose owe term use square loss asymptotically converge constant error proportional noise variance tight analysis estimation error obtain matrix selector application require compute high probability upper bind r p literature norm random matrix exploit derive bound special case use obtain asymptotically consistent result finally second term result theorem dominate bound weak owe relaxation strong incoherence assumption relate work future direction close relate work result consistency matrix completion decomposable norm regularization result paper strict generalization general norm regularize necessarily decomposable matrix completion provide nontrivial example application structure enforce norm interest far contrast result base parameter depend modify complexity measure r definition advantage result base application special case greatly benefit numerous tool literature computation closely related line work analysis high dimensional estimation measurement analysis literature rely variant rip measurement ensemble satisfied extremely localize measurement encounter matrix completion intermediate result establish form rsc matrix completion general norm regularization result previously know nuclear norm decomposable norm future work interest derive matching low bound estimation error completion general low dimensional structure line explore special case application result paper plan derive explicit characterization term gaussian width unit ball exploit generic chain result general space proof sketch proof lemma provide appendix proof define follow set matrix constant c theorem d sp x kxkf define c c s e log case error matrix error matrix large spikiness ratio b d d follow bind error immediate use proposition ac error matrix constant sp e log r analogous result hold b ds b cn case error matrix let restrict strong convexity rsc recall tr significant step proof involve useful subset tr form rsc satisfied squared loss penalty restrict strong convexity let c log d large constant far subsample excess sample log exist rsc following hold great expc tr d c kxkf proof appendix empirical process tool theorem recall r consist independent variable definition constrain norm minimizer condition let c p constant universal constant c probability great tr kp p cn use theorem use kp d d c c matrix selector d r p b ds tr d d b r p p b ds triangle inequality result follow optimality d b d d r p r recall norm compatibility constant r tr finally use theorem r tr kp d d proof let entry sample recall r standard normal vector compact rdd suffice prove theorem dense subset s subset define follow random process p k start key lemma proof theorem proof lemma provide appendix b use tool broad topic generic develop recent work d constant r s e sup k s k e sup exist constant k d sp e sup kp sup d follow combine simplify use b triangle inequality appendix b statement partial complexity follow standard result empirical process acknowledgment thank anonymous reviewer helpful comment suggestion acknowledge funding funding nsf grant ii banerjee acknowledge nsf grant grant reference m m live edge geometric theory transition optimization inform inference r srebro sparse prediction ksupport norm nip estimation norm regularization nip banerjee s dhillon clustering bregman divergence jmlr t local rate convergence linear inverse problem preprint e cande robust principal component analysis acm e cande plan matrix completion noise proceeding ieee e cande exact matrix completion optimization cande decode linear programming information theory ieee transaction chandrasekaran b p s willsky geometry linear inverse problem foundation computational mathematic m plan e m bit matrix completion inform inference r m dudley size compact subset hilbert space continuity functional analysis eigenvalue condition number random matrix matrix analysis application m fazel s minimization heuristic application minimum order system approximation m expect instantaneous loss bound journal computer system science s p ravikumar family matrix completion structural constraint icml l vert r bach cluster multitask learn convex formulation nip r h s matrix completion entry ieee tran r h s matrix completion noisy entry jmlr noisy lowrank matrix completion general sample distribution matrix completion singular value thresholde sharp bound koltchinskii alexandre b tsybakov penalization optimal rate noisy lowrank matrix completion annal statistic m m talagrand probability banach space process springer e small singular value random matrix geometry random advance new perspective ksupport cluster norm preprint s m wainwright restrict strong convexity weight matrix completion optimal bound noise jmlr b m p ravikumar unified framework analysis decomposable regularizer nip simple approach matrix completion jmlr vert tight convex relaxation sparse matrix factorization eprint srebro rank learn theory springer m talagrand measure generic chain annal probability m talagrand measure measure annal probability m talagrand upper low bound stochastic process springer tail bound sum random matrix foundation mathematic recovery structured signal independent random linear measurement preprint r introduction analysis random matrix compress sense page r estimation high dimension geometric perspective arxiv eprint characterization subdifferential matrix norm linear application p ravikumar statistical model nip
analysis electrical computer abstract nonparametric bayesian method develop analysis spiketrain datum feature learning spike sort perform jointly feature learning sorting perform simultaneously channel dictionary learning implement betabernoulli process spike sort perform dynamic hierarchical dirichlet process dhdp model couple dhdp augment eliminate violation allow appearance neuron time model smooth variation spike statistic introduction analysis action potential spike device problem interest reference research typically interested cluster sort spike goal link cluster particular neuron technology interest brainmachine interface gain insight property neural circuit research typically filter raw sensor perform thresholde detect spike iii map detect spike feature vector cluster feature vector principal component analysis popular choice feature mapping perform sort typically v search refractorytime violation occur spike sufficiently associate impossible time delay require neuron spike recent research combine iv single model method develop recently address perform early method spike sort base classical clustering technique kmean fix number mixture recently bayesian method develop account modeling example author employ modification chinese restaurant formulation dirichlet process automatically infer number cluster neuron present allow statistical drift feature statistic permit neuron time automatically account refractorytime requirement clustering step assume feature provide principal component feature learning spike sort perform jointly mixture factor formulation model selection perform number feature number neuron maximum likelihood ml point estimate constitute model parameter fix number cluster infer model directly allow neuron temporal dependence increase interest develop neural device c recording channel produce separate electrical recording neural activity recent research increase system performance large c research spike sort ground truth pc know neuron pc hdpdl gmm pc kmean pc pc b pc c pc d figure comparison spike sort real datum ground truth b kmean cluster principal component gmm cluster principal component propose method label use arrow example kmean gmm miss propose method properly sort perform single channel multiple channel present typically analyze isolation c channel consider assume spike occur time nearly time channel feature channel concatenate effectively reduce analysis c assumption neuron observe simultaneously channel typically inappropriate fact diversity sense device desire enhance functionality paper address multichannel problem condition concatenation inappropriate propose model generalize formulation hierarchical dp hdp formulation formulation statistical strength share channel assume neuron simultaneously view channel far model generalize hdp dynamic hdp allow neuron allow smooth change statistic neuron far explicitly account refractory time perform joint feature learn clustering use mixture factor construction fully set additionally account timevarye statistic learn factor loading find similar wavelet match property neuron spike contrast previous feature extraction spike base orthogonal wavelet necessarily match property result provide sense importance feature learning relative mapping datum pca feature learn offline figure comparison cluster result channel datum case figure datum depict visualization propose method learn number feature composition simultaneously perform cluster result b c correspond respectively widely employ kmean gmm analysis base use case analysis employ space approach figure b c observe kmean gmm work constrain feature space incorrectly classify spike mark arrow propose model figure d incorporate dictionary learn sort infer appropriate feature space effectively cluster neuron detail model include multichannel extension discuss detail model dictionary learn initially assume spike detection perform channel vector rd define time sample spike center peak detect signal spike channel c datum channel represent term dictionary upper bind number need dictionary element column d model infer subset dictionary element need represent datum represent d sn c diag b b k bk diagonal matrix b bk t define kth column let represent d identity matrix prior model parameter d c c c diag d represent truncated positive normal distribution gamma prior detail present result place d binary vector b impose prior bk k imply number nonzero component b draw aa correspond limit parameter b set favor sparse c model impose draw linear subspace define column corresponding nonzero component b linear subspace share channel c strength column contribute depend channel c define c concern c explicitly impose sparse diagonal draw shrinkage prior employ ck draw gamma prior favor large ck encourage diagonal element c small typically exactly test model perform similarly shrinkage prior use c relative explicit sparseness result base construction multichannel dynamic hierarchical dirichlet process c sort spike channel cluster sn sense feature design learn sort perform simultaneously discuss perform hierarchical dirichlet process hdp construction extend dynamic consider multiple channel hdp construction sn model draw g p q draw example construct vi beta g unit point measure p c p c form share probability use element gamma employ c context model develop section density function correspond gaussian parameter correspond mean precision matrix distribution propose model view mixture factor apply channel addition sharing statistical strength c channel hdp sharing manifest form share linear subspace define column d hierarchical cluster hdp relative weighting sn test use find critical model success compare employ single share channel p c hdp construction assume imply c probability draw time invariant way assumption violate refractory time imply minimum delay consecutive fire neuron effect address relatively straightforward manner discuss section second issue correspond appearance neuron characterize increase value component characterize component desirable augment model address objective achieve application dhdp construction develop divide time axis contiguous nonoverlapping temporal block correspond spike observe time j consider block index spike channel c block denote represent number spike block channel dhdp construction c c c c c c wj gj c c c gj wj c c expression wj c gj probability c c wj c control probability draw c c wj parameter draw cumulative mixture model wj support arbitrary level variation block block spiketrain analysis small probability observe particular type change significantly block wj mixture probability change quickly neuron wj extreme probability observe particular neuron change consecutive block model allow significant degree flexibility change account refractory time drift demonstrate explicitly account refractorytime condition model assume time difference spike refractory time spike temporal separation great refractory time consider spike type notational convenience basic formulation readily extend spike type wish impose associate c model unchanged n c assume p new conditional generative construction c p l c indicator function equal argument true construction impose model preserve element note time associate draw relative probability consistent spike assume know detection ie covariate know spike adjustment model c representation constitute proper generative construction presence spike refractory time complicate inference specifically recall p beta original construction refractorytime violation account gibbs update equation analytic model conjugacy conjugacy lose metropolishasting step require draw random variable markov analysis add complexity unnecessary number refractorytime event typically small relative total number spike sort successfully implement follow approximation construction draw assign member avoid refractorytime violation update equation execute step word construction use assign element spike step update equation implement original conjugate model essentially approach employ term stickbreaking crp construction dhdp find yield encouraging result eg refractorytime violation sort good agreement truth available finally author consider drift atom associate dp correspond drift atom associate dhdp construction draw g draw g block time simple gaussian autoregressive model employ allow drift small consecutive block specifically represent atom block impose large examine context model propose datum consider section add model complexity change result significantly consider add complexity present result observe impose drift likely fact c draw gaussian change datum block model allow drift variation draw infer variance thereof inference computation online sorting spike chinese restaurant process crp formulation desirable propose model implement generalization crp general form model section independent specific way inference perform crp construction chinese restaurant crf invoke model section yield dynamic crf associate particular channel hierarchical form include component section fully conjugate implement gibbs sampler hint construction employ stickbreake construction model analogous form inference employ employ retrospective stickbreake construction g number term use construct unbounded adapt datum use construction model able adapt number neuron present add delete cluster need sense stickbreake construction consider online implementation far model parameter gibb sampling follow inference data block come sequentially parameter block depend previous new component online implementation principal focus execute propose model implement crf implementation truncation inference method stickbreaking crf implementation similar result paper focus online implementation context consider online evolve learning dictionary d recent research online dictionary learning adapt use recent extension formalism example allow linear subspace spike shape reside datum block example result experiment use truncation level element learn hyperparameter gamma prior c p set c bc hdp set c p set order encourage group share set prior parameter optimize analogous setting yield similar result use sample collection sample gibbs sampler choose collection sample table summary result simulated method gmm kmean gmm hdpdl channel channel channel average maximum likelihood present example clustering kmean gmm set cluster level simulated datum cluster real datum simulate datum neural spike train difficult ground truth information testing verification initially consider simulated datum know ground truth generate datum draw model c sn define rk construct datum primary dictionary element length c channel dictionary element randomly draw vary c channel channel generate feature strength accord n sn mean neuron channel define rk mean feature space neuron shift neuron mean channel result associate cluster neuron determine percentage spike correct cluster result table dirichlet process dictionary learn similar result gmm principal component learn appropriate number cluster dictionary element model expect perform similarly require knowledge number dictionary element cluster priori hdpdl allow share global cluster dictionary element channel improve result figure sample posterior peak true value use global cluster layer hdp use dictionary element additionally information channel help cluster accuracy fact spike time typically draw global cluster independent local cluster global cluster channel figure b determine global spike time point channel index global cluster probability number dictionary element channel spike index spike index spike index probability number global cluster c figure posterior information hdpdl simulate datum approximate posterior distribution number use dictionary element ie b example collection sample global cluster usage local cluster map correspond global index c approximate posterior distribution number global cluster use table result test data represent kalman filter mixture method method gmm kmean gmm hdpdl real datum partial ground truth use publicly available dataset datum consist extracellular recording intracellular recording nearby neuron rat intracellular recording clean signal spike train specific neuron accurate time neuron detect spike nearby extracellular recording close time period intracellular spike assume spike detect extracellular recording correspond know neuron spike allow know partial ground truth allow test method compare known information accuracy analysis determine cluster correspond know neuron consider spike correctly sort known spike known cluster unknown spike unknown cluster order fair comparison method consider widely use data d use preprocessing data consist channel extracellular recording intracellular recording use detect spike spike come know neuron result figure result learn feature space instead use pca component increase sort accuracy phenomenon figure impossible accurately resolve cluster space base principal component kmean gmm jointly learn suitable feature space clustering able separate unknown know neuron cluster accurately hdp model advantage clear global accuracy achieve use use dictionary learn addition learn appropriate feature space hdpdl infer appropriate number cluster allow datum define number neuron posterior distribution number global cluster number factor dictionary element use figure b use element learn dictionary figure dictionary element shape similar neuron spike figure d wavelet nature learn dictionary factor similar use discrete transform cluster choose use wavelet nature select orthogonal wavelet basis learn dictionary typically orthogonal use d data consist extracellular recording intracellular recording spike detection filter datum hz detect spike voltage level pass positive negative threshold choose datum know neuron display dynamic property period activity intracellular recording figure know neuron active brief section record signal inactive rest signal pass extracellular spike train detect spike use detect spike include spike know cluster order model dynamic property bin datum subgroup spike use multichannel dynamic hdp result available probability probability number global cluster number dictionary element b c d figure result hdpdl d datum approximate posterior probability number global cluster channel approximate posterior distribution number dictionary element use dictionary element example typical spike datum method gmm kmean gmm table result channel channel channel average record signal time s spike index c probability change index mixture distribution table model adapt nonstationary spike dynamic learn parameter model dynamic property block indicate dhdp detect change characteristic spike know neuron inactive model likely draw new local cluster point reflect nonstationary datum additionally figure global cluster usage dramatic change time block cluster model inactive time know neuron inactive dynamic model map dynamic property result improve use model additionally obtain global accuracy channel use hdpdl global accuracy use multichannel dynamic hdpdl try datum unable satisfactory result additionally calculate true positive false positive number evaluate method limited space result supplementary material probability introduce new component block index d figure result multichannel dhdp d second intracellular recording local cluster usage spike d data channel global cluster usage different time block data sharing weight wj time block fourth channel spike occur know neuron inactive conclusion present new method perform spike sort underlie feature dictionary element sorting perform jointly allow timeevolve variation statistic model adaptively learn dictionary element nature orthogonal characteristic shape spike encouraging result present simulated real datum author like thank provide code process datum acknowledgement research report support program reference e spike sort bayesian clustering datum neuroscience method l kalman filter mixture model spike sorting datum neuroscience method t ferguson bayesian analysis nonparametric problem annal statistic m black e arm motion neural activity motor advance nip d dependent dirichlet process spike sort advance neural information processing system rasmussen modelling spike factor pattern recognition intracellular predict extracellular recording vivo neurophysiology sort hide markov model neuroscience method hoffman dm bach online learning latent dirichlet allocation nip gibb sample method stickbreake prior p p spike sort base discrete wavelet transform coefficient neuroscience method m s review method spike sort detection classification network computation neural system j learning matrix factorization sparse code machine learn brainmachine interface restore motor function probe neural circuit nature review neuroscience retrospective markov dirichlet process model biometrika m improve model firing statistic spike amplitude markov approach neurophysiology ren d l dynamic hierarchical dirichlet process international conference machine learn interface nature sethuraman constructive definition dirichlet prior teh m m blei hierarchical dirichlet process roth m black modeling neural population spiking activity distribution advance neural information processing system black m decode motion use kalman filter advance nip
metric learning temporal sequence alignment abstract paper propose learn mahalanobis distance perform alignment learn example task time series true alignment know cast alignment problem structured prediction task propose realistic loss alignment optimization tractable provide experiment real datum context learning similarity measure lead improvement performance alignment task propose use metric learning framework perform feature selection basic audio feature build combination alignment performance introduction problem align temporal sequence application range bioinformatics audio processing goal align similar time series global structure local temporal difference alignment algorithm rely similarity measure good metric crucial especially highdimensional setting feature signal irrelevant alignment task goal paper learn similarity measure annotate example order improve relevance alignment example context music alignment use different case alignment alignment case goal match audio interpretation piece potentially different alignment focus match audio signal symbolic representation score second case attempt learn annotate datum measure perform alignment propose fit generative model context learn measure discriminative set similarly use discriminative loss learn measure work focus alignment context set alignment large explicitly cast problem structured prediction task solve use optimization technique proper significant adjustment particular term loss idea alignment relevant community speech recognition work contribute equally en need metric learning far unsupervised partitioning problem weinberger propose framework learn metric base set constraint propose use large margin framework learn mahalanobis metric context partition problem structured propose taskar successfully use solve learn problem instance learn weight graph match metric rank task use learn graph structure use graph cut follow contribution cast learning mahalanobis metric context alignment structured prediction problem real musical dataset metric improve performance alignment algorithm use highlevel feature propose use metric learning framework learn combination basic audio feature good alignment performance experimentally standard hamming loss tractable permit learn relevant similarity measure real world setting propose new loss close true evaluation loss alignment lead tractable learn task derive efficient algorithm deal new loss loss solve issue encounter hamming loss formulation alignment problem notation paper consider alignment problem multivariate time series share dimension p possibly different length p p refer row b b column vector denote pair signal let arbitrary pairwise affinity matrix associate pair encode affinity bj note framework extend case b multivariate signal different dimension long welldefine goal alignment task find sequence index length match index time series time index time series b way maximal satisfie matching beginning u u matching type define binary matrix denote set matrix uniquely determine example fig vertical matrix mean signal b wait horizontal mean wait b diagonal mean sense time reference warp know alignment task cast follow linear program lp set max goal learn form affinity matrix learn alignment obtain optimization problem eq refer model dynamic time warp affinity matrix cx associate pair signal b find alignment solve lp eq efficiently use figure example valid alignment encode matrix red upper triangle j blue low j correspond area loss ab dynamic programming algorithm refer dynamic time warp describe supplementary material additional constraint use dynamic time warp easily add alg cardinality set huge correspond number path rectangular grid corner vertical horizontal diagonal allow definition number note infinity t mahalanobis metric application pair b affinity matrix compute bij bjk k paper propose learn metric compare ai bj instead use plain euclidean metric cx parametrize matrix set semidefinite positive matrix use correspond mahalanobis metric compute pairwise affinity bj ij ai bj note eq maximization linear function max y define joint feature map x ai learn metric assume pair training instance y ai b p p tb goal find matrix predict alignment close groundtruth example unseen example define loss alignment order quantify proximity alignment necessary fully label instance mean pair need exact alignment partial alignment deal alternate learn constrain alignment loss alignment framework alignment encode matrix p interested tion r frobenius norm define km ij loss simple loss matrix frobenius norm difference turn unnormalized hamming loss value matrix matrix define h try try try try tb try try t vector r coordinate equal line eq come fact value hamming loss affine loss use structured prediction task set use modify version loss average number time difference alignment great fix threshold loss easy optimize linear parametrization problem optimal alignment major drawback hamming loss alignment fix length depend number crossing alignment path easily find close fig important notice case length signal grow area loss natural loss compute mean distance path depict matrix loss correspond area path matrix represent formally fig t tb area loss mean t audio literature loss mean absolute deviation loss note unfortunately general alignment problem ab linear matrix context alignment sequence different nature signal reference index sequence define increase alignment problem loss linear argument precisely introduce matrix low triangular include diagonal write loss prove loss correspond area loss special case let alignment easy lta k lta ik k vertical unique lta kj kj exactly area curve determine path experiment use ab evaluation training approximation area loss symmetrize area loss real world application meaningful loss assess quality alignment area loss experiment hamming loss sufficient simple situation allow learn metric lead good alignment performance term area loss challenging dataset work fact alignment close term area loss suffer big hamming loss cf natural extend formulation matrix yx start symmetrize formulation eq overcome problem vertical horizontal define couple binary matrix try lta try lta lta try l try tb try violate constraint hamming loss violate constraint alignment tb figure real world bach chorale dataset represent groundtruth alignment term hamming loss alignment far area loss structured prediction setting describe depict alignment socalled violate constraint output loss augment decode step propose loss concave hull yx denote let introduce max large eigenvalue binary matrix y try lta try l try try try try concave function coincide empirical loss recall alignment example fix loss goal solve follow minimization problem min argmax b convex regularizer prevent overfitte large margin approach section describe large margin approach solve surrogate problem eq decode task maximum linear function parameter aim predict output large discrete space potential alignment respect constraint learn fall structured prediction framework define hinge loss convex surrogate completeness experiment try set matrix minimal trace dominate solve semidefinite program sdp report associate result fig note matrix choose particular matrix pointwise positive matrix loss concave evaluation l usually refer decode define argmax eq elementary computation argmin tb r aim solve following problem problem min max hamming loss case notice joint feature map linear loss linear argument instance hamming loss maximization linear function space solve efficiently use dynamic programming algorithm supplementary material way plug hamming loss eq eq lead convex structure prediction problem problem solve use standard technique cut plane method descent dual note adapt standard unconstrained optimization method setting w optimization use symmetrize area loss symmetrize area loss concave argument problem eq minmax form derive dual straightforward detail find supplementary material plug symmetrize area loss define eq problem dual follow form pn pn p s z z z bk z denote yx hull set product training example set note recover similar result loss concave aforementioned problem problem quadratic program compact set use note similar propose additional term loss experiment apply method task learn good similarity measure align audio signal field researcher spend lot effort design meaningful feature problem combine feature align temporal sequence challenge simplicity w diagonal experiment dataset description apply method dataset dataset pair align example ai b create stretch original audio signal way groundtruth alignment know datum fall set precise description dataset find pair stretch different curve signal s music divide frame lead typical length signal t setting p feature simple implement know perform alignment task mfcc label m m fig spectral spectral spectral spread maximum max power level frame detail computation feature normalize feature subtract median value divide standard deviation median audio datum subject outlier m m m max ss m m figure comparison performance individual feature learn metric error bar performance learn metric determine good bad performance different experiment denote learn combination use method m good mfcc combination experiment conduct following experiment individual feature perform alignment use dynamic time warp evaluate performance single feature term loss typically use performance set fig report result experiment plug datum method use hamming loss learn linear positive combination feature result report fig combine feature dataset yield performance consider single feature completeness conduct experiment use standard coefficient second order derivative feature result compete good learn combination handcraft feature term loss perform second note result slightly bad good single handcraft feature good mfcc coefficient use feature baseline compare uniform combination handcraft feature metric identity matrix result fig ab second individual value range second second dataset bach dataset consist chorale small piece chorale reference file correspond score basically representation partition alignment file audio file convert file audio follow alignment way fall framework technique apply piece music approximately s long lead similar signal length experiment use feature depict fig optimization hamming loss perform poorly dataset fact good individual feature performance far performance learn metric learning practical hamming loss perform bad good single feature conduct learning experiment area loss result learn parameter far learn use hamming loss performance similar good feature note feature handcraft reach performance hard task training instance challenge figure performance algorithm chorale dataset left right good single feature learn combination feature use symmetrize area loss good combination mfcc use obtain sdp section good combination mfcc derivative learn good combination derivative learn hamming loss good combination feature use fig depict result learn parameter loss augment decode perform use area know structured represent violate constraint violate constraint hamming loss lead alignment totally groundtruth alignment symmetrize area loss far close discriminative feature selection conduct feature selection experiment dataset start low level feature lead coefficient derivative learn linear combination achieve good alignment performance term area loss note little musical prior knowledge improve good handcraft feature dataset perform similarly dataset performance learn combination handcraft feature perform similarly combination coefficient conclusion paper present structured prediction framework learn metric temporal alignment problem able combine feature build automatically new stateoftheart feature basic lowlevel information little expert knowledge technically possible consider loss usual hamming loss typically use practical structured prediction framework linear output present work extend way main consider case partial information alignment available case music bioinformatics application note similarly et simple alternate optimization metric learning constrained alignment provide simple solution probably improve acknowledgement author acknowledge support project project fund program foundation phd fellowship thank helpful discussion thank share knowledge audio processing alexander dataset reference j m align gene expression time series time warp bioinformatic c number statistical planning inference learning graph match d evaluation realtime alignment m kernel time series base global alignment proc volume page match music alignment tool proc page m algorithm quadratic programming naval research logistic b speech audio signal processing processing perception speech music son r hamming error error code r b audio matching alignment computer science department page t joachim training structural svms machine learn learn optimal feature audio speech language processing shalevshwartz singer large margin algorithm alignment ieee transaction audio speech language processing h evaluation feature alignment new music research s m m optimization structural svms proc icml r f bach metric learning constrained partitioning problem proc icml b g learning rank proc icml page m music motion springer dynamic programming optimization speak word recognition acoustic speech signal processing ieee transaction shalevshwartz singer srebro primal estimate subgradient solver svm mathematical programming m learning crf use graph cut proc cvpr taskar markov network adv nip benchmark alignment database evaluation multiple alignment program bioinformatic exact formula number alignment joachim singer large margin method structured interdependent output variable machine learn research q weinberger metric learning large margin near neighbor machine learn research
changepoint analysis project abstract introduce kernelbase method changepoint analysis sequence temporal observation changepoint analysis sample observation consist test change distribution occur sample second change occur estimate changepoint instant distribution observation switch distribution different distribution propose test statistic base discriminant ratio measure homogeneity segment derive limit distribution null hypothesis change occur establish consistency alternative hypothesis change occur allow build statistical hypothesis testing procedure test presence changepoint prescribed falsealarm probability detection probability tend largesample set change actually occur test statistic yield estimator changepoint location promise experimental result temporal segmentation mental task datum pop song present introduction need partition sequence observation homogeneous segment arise application range speaker segmentation pop song far task deal use probabilistic sequence model hide markov model discriminative counterpart conditional random field probabilistic model require sound knowledge transition structure segment demand careful train yield competitive performance datum acquire online inference model straightforward model essentially perform multiple changepoint estimation interested meaningful quantitative measure detection changepoint sample parametric model available model distribution change comprehensive literature changepoint analysis develop provide optimal criterion maximum likelihood framework describe nonparametric procedure propose review limit univariate datum simple setting online counterpart propose build cumulative sum scheme extensive reference far extension case distribution change know distribution change know remain open problem bring light need develop statistically ground changepoint analysis work multivariate highdimensional structure datum propose regularize kernelbase test statistic allow simultaneously provide quantitative answer question changepoint sample prove test statistic changepoint analysis falsealarm probability tend detection probability tend number observation tend infinity test statistic directly provide accurate estimate changepoint instant method readily extend multiple changepoint setting perform sequence changepoint analysis slide window run signal usually physical consideration allow set sufficiently small length guarantee changepoint occur window sufficiently large length decision rule statistically significant typically section set framework changepoint analysis section describe devise regularize kernelbase approach changepoint problem section section respectively derive limit distribution test statistic null hypothesis h change occur establish consistency power alternative change occur theoretical result allow build test statistic provably falsealarm probability tend prescribed level detection probability tend number observation tend infinity finally section display performance respectively segmentation mental task datum temporal segmentation pop song changepoint analysis section outline changepoint problem describe formally strategy build changepoint analysis test statistic changepoint problem let time series independent random variable changepoint analysis sample consist follow step decide px pxn exist px pxn estimate sample true share similarity usual machine learn problem changepoint problem different statistical hypothesis test important aspect formulation changepoint problem natural embed statistical hypothesis testing framework let recall briefly main concept statistical hypothesis testing order changepoint problem framework goal build decision rule answer question changepoint problem state set falsealarm probability level type error purpose theoretically guarantee true close actually changepoint sample like miss detection probability power equal type close purpose section theoretical guarantee practical requirement largesample setting number observation tend infinity run maximum partition strategy efficient strategy build changepoint analysis procedure select partition sample yield maximum segment sample candidate change point assume compute measure nk segment xk hand hand run maximum partition strategy consist use building block changepoint analysis figure use test presence changepoint p p r figure run maximum strategy changepoint analysis test statistic changepoint analysis run candidate changepoint sequence observation hope true changepoint k overall homogeneity sample nk provide sensible estimator true changepoint instant changepoint analysis section describe discriminant ratio prove relevant measure homogeneity sample embed run maximum partition strategy provide powerful test statistic coin kcpa analysis address changepoint problem describe framework develop statistical analysis kernelbase learning testing algorithm reproduce let d separable measurable metric space let x value random variable probability measure p expectation respect p denote e covariance cov consider reproduce kernel rkh function r point correspond element h r positive definite kernel following exclusively work assume separable hilbert space note case separable metric space kernel continuous follow assumption kernel satisfied particular bound probability distribution p rkh associate dense ratio consider sequence independent observation define corresponding empirical mean element covariance operator follow quantity obvious population counterpart population element population covariance operator define probability measure p h h discriminant ratio define xn h note merge label sample single sample recover test statistic consider test homogeneity sample follow follow assumption covariance operator consider p paper b eigenvalue p p satisfy p p b infinitely strictly positive eigenvalue p changepoint analysis apply strategy describe obtain main building block test statistic changepoint analysis define test statistic k kn quantity define respectively tr tr nk act constant tend infinity standard statistical transformation know maximum search interval restriction order prevent test statistic behaviour neighborhood interval boundary standard practice set note input space euclidean instance rd kernel interpret regularize version classical maximumlikelihood multivariate test statistic use test change mean covariance assumption normal observation describe section test statistic truly nonparametric largesample property require feature assumption practice compute thank trick adapt discriminant analysis outline chapter falsealarm detection probability order build principled testing procedure proper theoretical analysis statistical point view necessary section prescribed build procedure tend infinity falsealarm probability null hypothesis sample completely homogeneous contain sample actually contain changepoint prove test statistic able probability tend infinity largesample set sake generality describe largesample set changepoint problem alternative hypothesis essentially correspond normalize signal sample interval let resolution increase observe datum point assume px pxn define define p probability distribution leave segment length probability distribution right segment length want study happen observation p change observation pr change large fix limit distribution null hypothesis section work null hypothesis pxn result limit distribution number observation tend infinity state theoretical result let describe approach prove use result similar sufficient study largesample behaviour d h respectively population rescale quantity covariance operator note remain stochastic term let expand p ep p operator follow x k ep p p pk define p rewrite infinitedimensional quadratic form partial sum nk yield idea view stochastic process random function p probability space invoke socalled invariance principle distribution realize random sum linearly interpolate value point behave asymptotically tend infinity brownian motion wiener component define brownian brownian t yield continuous approximation distribution correspond nk proof omit space limitation consist derive functional limit theorem apply continuous mapping argument assume b hold pxi p assume addition regularization parameter hold fix tend infinity u n tend infinity p d sup p t p p sequence eigenvalue overall covariance operator sequence independent brownian bridge define t quantile compute t simulation describe bootstrap resample null hypothesis result prove use limit distribution null state build test statistic prescribed falsealarm probability large corollary test falsealarm probability tend infinity sequence regularization parameter decrease slowly particular slow test statistic turn asymptotically tend infinity proof hinge martingale functional limit theorem point replace limit null distribution correctly normalize variance proposition assume hold pxi p assume addition regularization parameter n u n tend infinity d max sup p t remark close look proposition bring light t square brownian bridge component crucial test statistic segment length alternative far t simply consider corresponding test statistic yield loss power alternative far section allow level test statistic changepoint problem look behaviour test statistic null hypothesis h step prove test statistic consistent power detection probability tend n tend infinity alternative hypothesis consistency power section alternative hypothesis hold test statistic able detect presence change probability largesample set proposition prove framework consider previous section component split random sum largesample behaviour project random sum brownian motion assume aa hold exist pxn pxn assume addition regularization parameter hold fix tend infinity u n v t n extension related work extension note build similar procedure maximum mean discrepancy test statistic propose note instead regularization covariance operator regularization scheme apply spectral truncation regularization covariance operator equivalent preprocesse center kernel principal component analysis use instance relate work relate problem abrupt change detection problem explore naturally encompass framework interested early detection change distribution distribution procedure consist run detection algorithm use oneclass support machine train respectively left right window compare set obtain weight approach differ point limit null distribution kcpa allow compute decision threshold principled way second test statistic incorporate power alternative segment experiment computational consideration experiment set isotropic bandwidth set rule use density estimation second test statistic change correspond account change labelling motivate efficient strategy computation test statistic compute matrix inversion regularize gram cost compute value test statistic partition matrix compute decision threshold t use bootstrap resample calibration run base calibration procedure possible leave future research kcpa subject subject subject table average classification accuracy subject signal acquire experiment naturally exhibit temporal structure consider dataset propose competition acquire session normal subject subject ask perform different task time subject switch task mental task segmentation usually tackle supervised classification algorithm require label datum acquire standard supervise classification algorithm yield poor performance perform sequence changepoint analysis slide window overlap signal provide way measure performance method figure leave empirical test statistic average signal hand test statistic yield competitive performance test presence changepoint compare standard parametric multivariate procedure second table experimental result term classification accuracy prove reach performance supervised multiclass classification algorithm svm completely unsupervised kernel changepoint analysis segment consider sample class classification accuracy correspond proportion correctly assign point end segmentation process clearly accurate estimate changepoint changepoint estimation error directly measure power power curve level level figure comparison curve task segmentation datum leave pop song segmentation pop song segmentation music signal aim provide temporal segmentation section different dynamic characteristic investigate performance kcpa database pop music signal manual segmentation available figure provide respective kcpa approach competitive context conclusion propose principled approach changepoint analysis timeserie independent observation provide powerful testing procedure test presence change distribution sample experiment allow accurately estimate changepoint change occur currently explore extension kcpa experimental result promise real datum assumption independence analyze effect dependence largesample behaviour test statistic explain test statistic remain powerful weakly dependent datum investigate adaptive version changepoint analysis parameter reproduce kernel learn datum acknowledgment work support contract reference temporal segmentation facial behavior iccv lafferty mccallum pereira conditional random field probabilistic model segment label sequence datum proc icml e mouline inference hide markov model springer parametric statistical changepoint analysis m l limit theorem changepoint analysis son m detection abrupt change prenticehall sequential analysis classical problem new challenge e test statistical hypothesis harchaoui f bach e mouline test homogeneity discriminant analysis nip o statistical property principal component analysis machine learn statistical convergence canonical correlation analysis c smooth spline explicit description rkh inform b scholkopf hilbert space embedding probability measure colt d siegmund test changepoint biometrika shawetaylor cristianini kernel method pattern analysis p convergence probability measure method financial engineering kernel method twosample problem nip b scholkopf smola learning projection classification statistical learn ieee transaction information theory z harchaoui regularize kernelbase approach unsupervised audio segmentation m davy online kernel change detection signal processing harchaoui retrospective multiple changepoint estimation kernel ieee workshop statistical signal processing
moment match discrete abstract consider moment match technique estimation latent dirichlet allocation lda draw explicit link discrete version independent component analysis derive new set tensor improved sample complexity reuse ica technique joint diagonalization tensor improve exist method base tensor power method extensive set experiment synthetic real dataset new combination tensor orthogonal joint diagonalization technique outperform exist moment match method introduction topic model emerge flexible important tool early work focus approximate inference technique variational inference gibb sample moment match technique recently emerge strong competitor computational speed theoretical guarantee paper draw explicit link independent component analysis reference strong relationship latent dirichlet allocation lda ica reuse standard ica technique result derive new tensor sample complexity new algorithm base joint diagonalization lda discrete pca discrete notation follow text modeling terminology define collection document document collection wn ln token convenient represent token nth document encoding indicator vector m m vocabulary size document p nonzero m wn r representation length document ln m use index refer topic index refer document index m m refer word vocabulary index refer token nth document plate diagram model section present latent dirichlet allocation generative probabilistic model discrete datum accordance model nth document model vocabulary m word k latent topic specifically latent variable sample dirichlet distribution represent topic mixture proportion topic nth document topic choice th token sample multinomial distribution probability vector token sample multinomial distribution probability vector index nonzero element vector kth topic vector probability word vocabulary subject simplex constraint m m rm m dm generative process document index omit simplicity summarize multinomial dz think latent variable auxiliary variable introduce fact marginalize lead follow model d lda model rm k topic matrix kth column equal kth topic c rk vector parameter dirichlet distribution document represent set token w formulation formulation instead compactly represent document count vector representation equivalent focus second paper refer importantly model model length document original propose model document length poisson use practice particular parameter learn way lda model typically use provide complete generative process document rule sample l paper fact important need model document length order link pca lda model discretization principal component analysis pca replacement normal likelihood multinomial adjust prior follow probabilistic pca model normal ik m rm transformation matrix interestingly small extension model allow interpretation discrete independent component analysis model extension naturally arise document length lda model model random variable mixture equivalent negative random variable l poisson b shape parameter b rate parameter document length equivalent appendix b b model mutually independent parameter ck coincide model free parameter b appendix b scaling parameter document length prescribe model introduce later discrete ica model natural model gp model model mutually independent discrete difference standard model additive noise presence poisson noise enforce discrete instead continuous value note discrete model semiparametric model adapt distribution topic gp model particular case lda model thank close connection reuse standard technique derive new efficient algorithm topic model moment match topic modeling method moment estimate latent parameter probabilistic model match theoretical expression moment sample estimate recently method moment apply different latent variable model include lda result computationally fast learn algorithm theoretical guarantee construct lda moment particular diagonal structure develop algorithm estimate parameter model exploit diagonal structure paper introduce cumulant similar moment structure structure allow algorithm estimation model parameter theoretical guarantee consider algorithm applicable lda moment cumulant model section derive analyze novel cumulant model gp model particular case model result section extend model cumulant tensor random vector define follow e ex e ex denote tensor product property cumulant essential property cumulant hold moment use paper cumulant tensor random vector independent component diagonal let d poisson random variable expectation law total expectation linearity expectation expectation follow form far variance poisson random conditionally independent covariance matrix diagonal law total covariance covariance form e diag diag equality follow property cumulant c term rhs define diag ex independence follow diagonal structure var d analogy second order case use law total property cumulant independence derive appendix c expression similar cumulant term expression define tensor t follow element m m m m m m m m m m m m kronecker delta analogy appendix c diagonal structure dk appendix e recall notation matrix s tensor analogue matrix tensor model slightly abuse terminology refer matrix tensor moment matrix tensor cumulant diagonal structure moment similar diagonal structure arise slightly different argument discuss end appendix e importantly similarity algorithmic framework cumulant moment coincide follow sample complexity result apply sample estimate cumulant proposition gp model expect error sample estimator r b e e max l c l min l high probability bind derive use concentration inequality variable expectation right order magnitude error example inequality expression unbiased finite sample estimate expression unbiased finite sample estimate t define sketch proof proposition find appendix d follow similar analysis topic recovery error term error gp cumulant importantly whiten transformation introduce section scale appendix detail mean error l contribution recovery error scale max l small small present exact expression l expect square error estimator t similar structure derivation expect analogous bind t kf max current sample complexity result summarize n proof find supplementary material analyze case finite sample estimate moment construct document w ustatistic average multiple dependent document practical expression careful compare upper bound compare bound current theoretical result lda moment sample complexity contain norm column topic matrix d numerator oppose coefficient lda moment norm significantly small vector sparse topic suggest cumulant finite sample convergence property lda moment experimental result section consistent statement cumulant somewhat intuitive derivation lda moment express count vector sufficient statistic model token note construction moment depend unknown parameter unsupervised setting evaluation difficult task set parameter nontrivial observe experimentally lda moment somewhat sensitive choice diagonalization algorithm diagonal structure t helpful estimation model parameter question investigate signal processing reference machine learning reference literature review approach section similar diagonal structure algorithm section apply lda moment simplicity let rewrite expression s t follow note expect square error cumulant similar expression compact general depend prior completeness present sample estimate t moment consistent suggest introduce rescaled topic follow assumption topic vector rewrite d e rank compute whiten matrix linearly d matrix ik identity matrix detail result vector form orthonormal set vector far let define projection t tensor vector u rk apply transformation definition project result tensor t vector u rk obtain tk rescaling topic h stand inner product vector orthonormal pair tk ui matrix uniquely define eigenvalue different unique recover lda model parameter e k ui procedure refer spectral algorithm fourthorder identification algorithm expect sample estimate possess approximately diagonal structure reasoning apply assume effect sample error control spectral know unstable practice overcome problem algorithm propose notable probably appropriate choice contrast function estimate iteratively topic use orthonormal structure perform procedure step recently introduce tensor method tpm lda model close alternatively modify spectral algorithm perform multiple projection jointly result matrix orthogonal matrix spectral algorithm special case orthogonal joint diagonalization projection choose importantly fast implementation orthogonal joint diagonalization algorithm propose base closedform iterative update later practice orthogonal joint diagonalization robust p spectral algorithm application learning topic model mention literature implement practice paper apply diagonalization moment describe note choice projection c vector rk important correspond vector rm obtain c mode importantly transformation joint diagonalization routine perform p matrix size number topic usually big algorithm computationally fast true spectral algorithm tpm section compare experimentally performance spectral tpm algorithm estimation parameter lda model aware experimental comparison algorithm lda context work manuscript independently analyze context factorization general latent variable model focus comparison approach tensor factorization stability property brief experiment use latent variable model relate equivalent lda community detection contrast provide detailed experimental comparison context paper novel estimator space restriction estimation topic matrix d parameter appendix discussion order joint diagonalization lda moment input n p number random projection c moment b rm m appendix compute estimate whiten matrix option choose vector u u rk uniformly random unit c rm p sphere set p yield spectral option b choose vector u u rk canonical basis e e c rm rk set c compute c c p perform orthogonal joint diagonalization matrix find orthogonal matrix r vector rk c p value p estimate joint diagonalization matrix output estimate d c describe experiment section compare experimentally cumulant lda moment spectral algorithm tensor power method joint diagonalization variational inference real datum dataset d web page document b nip m vocabulary word average document length l dataset dataset nip word l uci repository document word datum construct analogy lda parameter d c learn real dataset variational inference toy datum sample model interest parameter d provide ground truth parameter set datum sample time result average plot error bar minimum maximum value datum k topic learn nip data topic learn large obtain topic matrix violate identifiability condition topic recovery use moment match technique document token resample sample technique sample model parameter set c c learned c real dataset variational parameter vary gp datum sample model b c l expect document length l appendix datum sample document length fix l datum sample follow portion document sample model document length l portion document sample model document length evaluation evaluation topic recovery datum perform b true topic matrix good permutation column error recover d p b b err d minimization possible b efficiently obtain permutation column algorithm bipartite matching evaluation topic recovery real datum case use approximation loglikelihood hold document metric detail tpm error error number number figure comparison diagonalization algorithm topic matrix d dirichlet parameter c learn c scale sum set fit expect document length b dataset sample gp number document vary leave lda moment note small value error use matlab implementation moment diagonalization algorithm dataset code reproduce experiment available online appendix discuss implementation complexity algorithm explain initialize parameter lda moment comparison diagonalization algorithm figure compare diagonalization algorithm dataset k use gp sample compare tensor power method tpm spectral orthogonal joint diagonalization describe different option choose random projection k vector sample uniformly unit sphere rk select option select basis e ek rk set ep option b choose canonical basis rm projection vector computationally expensive lda moment setup moment slow finite sample convergence large estimation error value n expect spectral algorithm slightly inferior joint diagonalization algorithm cumulant estimation error low demonstrate good performance expectation tpm perfect performance case leave significantly deteriorate moment right explain large estimation error moment lack robustness tpm running time discuss overall orthogonal joint diagonalization initialization random projection multiply canonical basis rk computationally efficient fast comparison lda moment figure sample gp model leave lda moment specify imply approximation error error infinite number document low cumulant achieve low value estimation error n document independently number topic convergence slow lda moment sample model right cumulant approximation error high estimation error low fast finite sample convergence reason poor performance cumulant case absence variance document length document different length mix sample leave cumulant performance improve experiment change fraction document right nonzero variance length improve performance cumulant real usually nonzero variance document length bad scenario likely happen error number error error number fraction length number loglikelihood bit loglikelihood bit figure comparison lda moment topic matrix parameter c learn nip dataset c c scale sum c corpus different size leave b set fit expect document length b sample gp model right sampling model leave sample model right number document fix sample model vary value fraction step note small value error topic topic figure experiment real datum leave dataset right dataset note high value loglikelihood real datum experiment figure compare variational inference vi variational inference initialize output measure hold loglikelihood token g detail experimental setup orthogonal joint diagonalization demonstrate promise performance particular significantly outperform moment variational inference perform restart variational inference output systematically lead result similar behavior observe conclusion paper propose new set tensor discrete ica model relate word count directly model moment assumption regard distribution theoretically empirically robust previously propose tensor synthetic real datum follow literature joint diagonalization procedure robust topic matrix estimate semiparametric way topic intensity leave interesting learn unknown distribution independent topic acknowledgment work partially support joint center author like thank helpful discussion reference dm blei dirichlet allocation learn griffith gibb sample generative model latent dirichlet allocation hsu kakade spectral algorithm latent dirichlet allocation nip kakade m tensor decomposition learn latent variable model learn p handbook blind source separation independent component analysis application thesis c j blind separation source adaptive algorithm base architecture signal process p independent component analysis new concept signal process variational extension multinomial pca ecml cm bishop probabilistic principal component analysis r stat soc roweis algorithm nip j gap factor model discrete datum sigir apply discrete pca datum analysis p concentration theory hsu kakade spectral algorithm latent dirichlet allocation wallach murray r salakhutdinov d evaluation method topic model icml cardoso source separation use high order moment cardoso fourthorder cumulant tensor application blind source separation problem cardoso p independent component analysis survey algebraic method hyvarinen fast robust fixedpoint algorithm independent component analysis cardoso blind non gaussian signal contrast independent component analysis neural cardoso angle simultaneous diagonalization anal r numerical method simultaneous diagonalization anal nocedal sj bach independent component analysis learn p tensor factorization matrix factorization aistat pereira tishby euclidean embed cooccurrence datum learn d sontag m practical algorithm topic model provable guarantee icml cohen m collin provably correct learn
neural network exploration use optimal experiment design dept brain cognitive abstract consider problem learn inputoutput mapping exploration learn kinematic dynamic robotic manipulator action expensive computation cheap explore select trajectory input space information number step discuss result field optimal experiment design use guide exploration demonstrate use simple kinematic problem introduction machine learn research treat learner passive datum process approach ignore fact situation learner able require act environment gather learn control inherently involve active controller act order learn result action train neural network control robotic arm explore allow controller length time arm random coordinate space build datum build model feasible action expensive conserve situation choose training trajectory information limited number step manually design trajectory slow process intuitively good trajectory fail sufficiently explore state space cohn paper discuss alternative exploration automatic incremental generation training trajectory use result optimal experiment design study optimal experiment design oed concern design experiment expect minimize variance parameterized model view action experiment state space use technique oed design training trajectory optimal experiment design usually maximize confidence model minimize parameter variance system identification minimize model output variance use form oed identify link moment robot arm find automatically generate training trajectory provide significant improvement trajectory automatic exploration strategy try neural network moore use oed neural network community limit white successfully use filter data set maximally informative point application select new datum propose demonstrate follow section brief description relevant result optimal experiment design section describe result adapt guide network exploration section present experimental result implement adaptation finally section discuss implication result extension current experiment optimal experiment design optimal experiment design draw heavily technique maximum likelihood estimation mle set assumption learner architecture source noise output mle provide statistical basis learn specific mle technique use hold exactly linear model certain computational approximation allow use nonlinear system neural network begin training set inputoutput pair learner define learner output input x weight assumption additive gaussian maximum likelihood estimate weight vector minimize sum square error estimate estimate output novel input figure mle allow compute variance weight output estimate write output sensitivity covariance approximation assume local linearity brevity output sensitivity rest paper exploration use optimal experiment design figure set training example classification problem network fit data maximum likelihood estimate network output variance problem reference input estimate output variance r gx r output variance correspond model estimate expect square distance output unknown true output output correspond model estimate mean square error mse figure b estimate accurate minimize output variance correspond minimize network optimal experiment design estimate add new training example expect change compute variance novel use oed predict effect add training set assumption t correspond assume current model fairly good base assumption new parameter variance combine equation predict new example change output variance reference input minimize expect value r select maximize right equation interesting oed measure cohn adapt oed exploration build world model learner try build mapping eg joint angle coordinate stateaction pair state allow select arbitrary joint angle input successive time step problem select query cohn exploration choice input constrain current input state space choose input available time step approach select input use selective sampling evaluate number possible random input choose high expect gain highdimensional action space inefficient approach follow gradient search differentiate equation hillclimbe r note equation expect change variance single point r wish minimize average variance entire domain explicitly integrate domain intractable approximation propose use fix set reference point measure expect change variance produce local reference point undesirable effect arbitrarily quantize input space approach iteratively draw reference point random uniformly accord distribution interest compute stochastic approximation var stochastically gradient convergence horizon available input settle locally optimal decrease expect variance experimental result section describe set experiment attempt confirm gain predict optimal experiment design actually realize practice second study application oed simple learning task expect actual gain emphasize gain predict oed expect gain expectation base relatively strong assumption mle strictly hold order expect gain bridge cross expect decrease model variance realize actual decrease variance second actual decrease model variance translate actual decrease model mse expect decrease variance actual decrease variance translation expect actual change variance require coordination exploration strategy learn predict variance weight change new piece datum predictor know weight neighboring weight change use black network exploration use optimal experiment design vl ct c t e d d e lta var e lta r figure correlation expect change output variance actual change output variance b correlation actual change output variance change mean squared error correlation plot network train example arm kinematic task box backpropagation update weight guarantee expected actual decrease experiment indicate correlation predict actual change variance relatively good figure decrease variance decrease translation model variance model correctness highly nonlinear nature neural network leave situation model confident entirely wrong high confidence learner reject action reduce mean square error explore area model correct low confidence evidence behavior low right corner figure b action produce large decrease variance little effect network decrease utility oed discuss possible solution problem end paper learn kinematic use stochastic approximation var guide exploration simple task involve classification regression detail experiment involve exploration kinematic simple twodimensional arm task learn forward model x y exploration use build controller follow model learn feedforward network sigmoid transfer function use single hidden layer hide unit figure learn arm kinematic hide unit geometry d arm b sample trajectory use greedy exploration time step learner allow select input tip position incorporate training set hillclimbe find limit movement maximize approximation var time step limit change respectively simulation perform simulator available variance gradient step randomly draw point sample tip trajectory illustrate figure b compare performance onestep optimal greedy learner term mean squared error identical learner explore randomly surprisingly improvement greedy exploration exploration significant figure b asymptotic performance learner random learner reach step discussion experiment describe paper indicate optimal experiment design promising tool guide neural network exploration require arbitrary discretization state action space amenable gradient search technique high computational cost discuss section lead model settle local minimum alternative greedy oed greedy approach prone box corner leave important domain heuristic avoid local minima network exploration use optimal experiment design o o e number step number step figure learn arm kinematic mse single exploration trajectory hidden unit b plot mse random greedy exploration number training example average run hide unit check expect gain input space promise great gain greedy step theoretically correct computationally expensive approach optimize entire trajectory trajectory optimization entail start initial trajectory compute expect gain iteratively perturb point trajectory optimal expect gain subject point trajectory explore experiment currently determine improvement realize trajectory optimization unclear improvement greedy approach worth add computational cost computational cost computational cost greedy oed great select action require computation inversion hessian time action select new datum incorporate training set learner retrain comparison use strategy fix trajectory datum gather little computation learner train batch light cost datum great cost computation optimal experiment design preferable strategy approximation significantly bring cost oed consider covariance weight lead neuron hessian reduce block diagonal form neuron compute simple covariance parallel extreme away covariance entirely rely individual weight variance computation simple token incorporate new example small batch retrain step suboptimal datum gathering perspective appear outperform random exploration cheap optimization cohn alternative architecture able bring computational cost improve performance use different architecture learner standard feedforward network repeat variance expensive fail yield estimate suitable use confidence interval section solution problem lie selection amenable architecture learn architecture output variance direct role estimation mixture gaussian efficiently train use expect line future research fruitful acknowledgement help research possible work fund human information processing laboratory research nsf grant find trajectory identification experiment int robotic research e construct hide unit use example query neural information process system r training connectionist network query selective sampling editor advance neural information processing system theory optimal experiment academic m supervise learn incomplete datum approach volume m rumelhart forward model supervise learn teacher cognitive science objective function active datum selection neural computation moore variable resolution reinforcement learn multidimensional statespace volume m white select training set ieee neural network r element statistical computing chapman thrun k active exploration dynamic environment ai editor advance neural information processing system
logistic regression support vector machine svm know good performance binary classification extension multiclass classification ongoing research issue paper propose new approach classification vector machine ivm build logistic regression ivm perform svm binary classification naturally generalize multiclass case furthermore provide estimate underlying probability similar support point model use fraction training datum basis function typically small fraction computational advantage svm especially size training datum set large introduction standard classification problem set training datum output qualitative assume value finite set wish find rule training datum new input assign class usually assume training datum independently identically distribute sample unknown probability distribution support vector machine svm work binary classification ie appropriate extension multiclass case ongoing research issue weakness svm estimate probability interest conditional probability point class paper propose new approach machine address classification problem perform svm binary classification naturally generalize multiclass case furthermore provide estimate probability similar support point svm model use fraction training datum index basis function training datum e cost svm import point computational cost ivm b number import point tend increase increase ivm fast svm especially large training datum set empirical result number import point usually number support point section briefly review result svm binary classification compare logistic regression section propose section simulation result section generalize ivm multiclass case support vector machine logistic regression standard svm produce nonlinear classification boundary original input space construct linear boundary transform version original input space dimension transform space large infinite case seemingly computation achieve positive definite inner product transform people note relationship svm regularize function estimation reproduce kernel hilbert space rkh overview find hastie wahba fit svm equivalent minimize c classification rule rkh generate kernel optimal form happen fraction value consequence truncation property criterion attractive property point wrong classification boundary right boundary influence determine position boundary nonzero correspond s support point notice form loss function plot figure traditional loss function negative distribution similar shape svm replace nll distribution problem problem expect fit function perform similarly binary immediate advantage replacement classification rule offer natural probability svm estimate naturally generalize multiclass case regression case svm compromise hinge loss function svm long support point property word s study problem wahba reference hastie square support vector figure loss function computational cost b save computational cost find submodel approximate model submodel form subset training datum datum import point advantage submodel computational cost reduce especially large training datum set performance classification researcher investigate technique select subset divide training datum cluster randomly select representative smola et greedy technique e span select column matrix e column approximate span propose randomly select point training datum use method approximate matrix expand result dimension use output select procedure involve use output input select subset way result fit approximate model vector machine follow logistic regression let rest paper notational simplicity constant term fit function ignore want minimize equivalent finite dimensional form tion matrix matrix find set derivative respect equal use method iteratively solve score equation step weight square step step matrix weight mention section want find subset submodel good approximation model impossible search subset use follow greedy forward strategy basic algorithm let minimize let repeat step point let find let converge import point revise algorithm computationally feasible step need find iteratively number import point large computation expensive reduce computation use approximation instead iteratively compute converge onestep iteration use approximation good approximation advantage fit result current optimal submodel use initial value onestep update similar score test e generalize linear model penalty term update formula allow weight regression compute time revise step basic algorithm column column row use formula find compute stop rule add point step basic algorithm need decide converge sequence natural stop rule look regularize let regularize nll obtain step step compare small integer example ratio small number example stop add new point choose regularization far assume regularization parameter fix practice need choose optimal randomly split datum training set tune set use misclassification error tuning set criterion choose reduce computation advantage fact regularize converge fast large instead run entire revise algorithm procedure combine add import point propose follow choose optimal start large regularization parameter let run step revise stop way compute satisfied error tuning set decrease small value repeat step start choose optimal correspond minimum misclassification error tuning set section use simulation illustrate ivm method datum class generate mixture hastie simulation result figure remark support point svm close classification boundary misclassifie usually large weight import point decrease regularize close far classification boundary natural svm concerned classification focus unknown probability point away classification boundary contribute determine position ofthe classification boundary contribute estimate unknown probability figure comparison total computational cost svm b computational cost ivm method b number import point misclassification rate different regularize optimal regularize different import point add import point add figure radial kernel use optimal rate find correspond stop criterion satisfied import point add left middle panel illustrate choose decrease minimum misclassification right panel optimal tend increase increase computational cost ivm small svm especially large training datum set multiclass case section briefly describe generalization ivm multiclass classification suppose class write response vector component indicate class observation indicate response th class indicate response class use class basis write baye classification rule argmax use index observation index class regularize negative loglikelihood use th element minimize form svm support point import point error train test error baye error test error baye error oo o ooo o o oo o oo oo o ooo o o oo o oo figure solid line classification boundary dotted line baye rule boundary svm dash line edge margin dash line line case th row define way binary procedure similar binary case computational cost figure simulation datum class generate mixture hastie import point training error test error ooo oo ooo o ooo o error figure radial kernel use conclusion discuss machine ivm method binary multiclass e classification perform svm provide e ivm estimate probability cost binary case multiclass case number point acknowledgment thank helpful comment partially support hastie partially support grant dms national science foundation grant national institute health thank point interesting important reference want thank anonymous nip help improve paper reference burge support vector machine pattern recognition mining knowledge m poggio t regularization network support vector machine b scholkopf c editor advance large margin classifier mit green p semiparametric generalize linear model proceeding international conference lecture note statistic generalize additive model chapman tibshirani friedman element statistical learning smooth spline model large datum set observation randomized technical report department wahba result spline function anal smola scholkopf sparse greedy matrix approximation machine learn proceeding international conference machine learn wahba support vector machine reproduce hilbert space randomized technical report wahba r soft classification risk estimation penalize log likelihood smooth spline analysis variance editor mathematic study science seeger m use nystrom method speed tresp editor advance neural information process system mit press
dynamical andor graph learn object shape modeling paper study novel discriminative model represent recognize object shape andor graph define model consist layer leafnode collaborative edge localize local ornode specify switch leafnode rootnode encode global verification discriminative learning extend cccp propose train model dynamical manner model structure configuration leafnode associate ornode automatically determined optimize multilayer parameter iteration advantage method model enable handle large variance background clutter object shape detection image propose learning able obtain andor graph representation require elaborate initialization validate propose method challenge database uiucpeople outperform stateoftheart approach introduction hierarchical representation widely study computer vision lead elegant framework complex object detection recognition method address hierarchical decomposition treestructure model structural switch hierarchy key handle large variance object detection addition interaction omit learning detection andor graph model recently explore hierarchically model object category ornode represent respectively composition structural variation main limitation learning process strongly supervise model structure need manually annotate key contribution work novel andor graph model parameter structure jointly learn weakly supervised manner achieve superior performance task detect localize shape background compare stateoftheart approach fig illustrate propose andor graph model consist layer describe follow leafnode layer represent batch local classifier contour fragment provide partial matching scheme recognize accurate contour deal correspond author work support national natural science foundation fundamental research fund central university natural science foundation work partially fund high performance compute typical application project problem true contour object connect background clutter unreliable edge extraction ornode middle layer switch variable specify activation child leafnode utilize ornode account alternate way composition define multilayer detector handle variance inconsistency cause unreliable edge detection ornode use select contour candidate detect associate leafnode layer detection location allow ornode tackle deformation rootnode layer global classifier capture holistic deformation object contour select ornode far verify order detection robust background clutter collaborative edge leafnode define probabilistic cooccurrence local classifier relax conditional independence assumption commonly use previous tree structure model model allow nearby contour interact key problem train andor graph model automatic structure determination propose novel learn dynamic cccp extend procedure cccp embed structural reconfiguration iterate dynamically determine production leafnode associate ornode simplify manually fix previous method structure attribute layout ornode activation leafnode implicitly infer latent variable relate work remarkable progress object detection employ shape descriptor match scheme work represent recognize object shape loose collection local contour example use codebook pairwise adjacent segment localize object interest propose maximum margin hough vote hypothesis region combine intersection kernel verification construct shape model fully connect graph form learning detect object particle filter pf framework recently tree structure latent model provide significant improvement object detection base method train detector use learning song integrate context information learn far combine latent discriminative learn conditional random field use multiple feature knowledge representation graph introduce model visual pattern general idea ie use graph structure node apply object scene parse action classification andor graph representation object shape andor graph model define e v represent type node e graph edge fig illustrate square rootnode represent complete object instance dash circle derive root ornode arrange layout b block represent object ornode comprise number leafnode denote solid circle leafnode allow dynamically create remove learning simplicity set maximum number m leafnode ornode parameter leafnode maximum number node model z z m use index root node ornode j leafnode define index child node node horizontal graph edge collaborative edge define leafnode associate different ornode order encode compatibility object definition present follow leafnode local classifier contour placement decide parent ornode localized block suppose contour fragment c edge map capture block locate pxi pyi input classifier denote pi c feature vector use shape context descriptor classifier fall block account set c entirely response location pi edge map x define max l pi c cx parameter vector set corresponding leafnode detect contour edge map classifier cj l pi c ornode ornode ui propose specify proper contour set candidate detect child leafnode note consider ornode activate leafnode ornode allow perturb slightly respect root ui define deformation feature p pi dy dy dy ornode position pi expect position p determine rootnode cost locate ui s p pi dimensional parameter vector correspond p pi method ornode contain m leafnode activate inference associate ui introduce indicator variable represent activate derive auxiliary switch vector ui response ornode ui define p pi p pi collaborative edge pair leafnode respectively associate different ornode define collaborative edge accord contextual cooccurrence likely object contain contour detect leafnode response pairwise potential parameterize vj vj define neighbor leafnode ornode adjacent spatial direction lj v joint vector indicate compatibility rootnode represent global classifier verify ensemble contour fragment r c cz propose ornode response rootnode parameterize r r r c r r c r feature vector r r corresponding parameter overall response andor graph p p pi r l cj p pi r r vj c p p p vector position ornode understanding refer p latent variable inference p imply deformation represent ornode v imply discrete distribution leafnode leafnode activate detection eq far simplify h include complete parameter andor graph h feature e z nl r h p cz p p s p r c r l l figure illustration dynamical structure learn model ornode u u visualize intermediate step initial structure regular layout object new structure dynamically generate iteration b leafnode associate remove new leafnode create assign u inference inference task localize optimal contour fragment detection window scale position edge map assume rootnode locate object shape localize maximize x h define sp max x h h inference procedure integrate bottomup testing topdown bottomup testing ornode ui child leafnode ie local classifier utilize detect contour fragment edge map assume leafnode associate ui activate optimal contour fragment cj localize maximize response eq optimal location pij determine generate set candidate ornode cj pij detect contour fragment leafnode set candidate pass topdown step activation vi ui far validate calculate response bottomup step p pi denote hypothesis leafnode activation ornode practice far prune candidate contour set threshold v select ensemble contour c r c cz detect activate topdown verification ensemble contour r apply global classifier rootnode verify c r eq accumulate pairwise potential collaborative edge define incorporate bottomup topdown step obtain response andor graph final detection acquire select maximum score eq discriminative learning graph formulate learning graph model joint optimization task model structure parameter solve iterative method extend framework algorithm iterate determine andor graph structure dynamical manner infer latent variable p step leafnode automatically create remove generate new structural configuration specific new leafnode encourage create local detector contour handle current leafnode encourage remove similar discriminative ability b procedure dynamical cccp optimization suppose set positive negative training sample edge map label indicate positive negative sample assume sample index positive sample feature vector sample h latent variable rewrite discriminative function optimization function solve use structural svm latent variable max xk lyk max xk penalty empirically lyk loss function define lyk y method optimization target equation nonconvex recently utilize provide local optimum solution iteratively solve latent variable model parameter cccp address ornode ie assume configuration structure fix following propose embed structural reconfiguration step optimization dynamic cccp follow original cccp framework convert function eq convex concave form min max xk lyk d max yk h n represent term represent term original cccp include iterative step fix model parameter estimate latent variable positive sample compute model parameter traditional structural method method infer need far determine graph configuration production leafnode associate ornode obtain complete structure insert step original perform structure reconfiguration iterative step present follow optimization find hyperplane qt bind concave qt t include model parameter obtain previous iteration construct qt calculate optimal latent variable t xk h positive training sample account computation hyperplane construct qt d step adjust model structure leafnode model map feature dimension x process reconfiguration equivalent feature vector x accordingly change lead learn operate guide principal component allow adjustment nonprincipal component dimension term preserve significant information h result assume step model reconfiguration divide feature guide positive sample apply xk number eigenvector ei eigenvector parameter set large number u ei feature c figure toy example structural clustering consider sample train structure ui feature vector sample associate ui intensity feature bin indicate feature value red green bounding box vector indicate nonprincipal feature represent detect contour fragment different leafnode b illustrate clustering perform vector group right cluster left c adjust feature vector accord clustering note clustering result structural reconfiguration discuss text figure encourage view electronic version vector consider nonprincipal ei u experiment ornode ui set detect contour fragment ci ci obtain positive sample feature vector contour generate l pi ci l pk ci map different complete feature specifically select h feature vector principal different vector feature select nonprincipal bin form new vector feature vector contour pi ci ci structural reconfiguration cluster trigger structural reconfiguration ornode ui perform clustering detect contour fragment represent newly form feature vector group contour detect leafnode cluster partition perform apply euclidean distance close contour group cluster accord new partition feature vector represent similar contour bin complete feature vector recall vector contour present toy example illustration fig select feature vector nonprincipal ci group cluster compare observe feature vector accordingly leafnode correspond cluster contour typical state new leafnode create cluster generate previous parameter learn base feature vector contour cluster leafnode remove feature bin relate imply contour detect leafnode group practice constrain extent structural reconfiguration leafnode create remove ornode iteration structural reconfiguration denote feature vector xk adjust xk yk new hyperplane generate d k xk newly generate model structure represent feature vector xk learn model parameter solve argmin substitute upper bind hyperplane optimization task eq rewrite max xk lyk xk yk standard structural svm problem solution present b iteration figure train andor graph model uiucpeople dataset visualize layer model image imply verification rootnode b exhibit leafnode associate ornode u practical detection activate leafnode highlight average precision result generate tree model andor graph model d xk yk h calculate maximize max lyk h xk h dual problem standard svm solve apply cut plane method sequential minimal optimization obtain update parameter t continue step iteration function converge initialization beginning learn andor graph model initialize follow training sample contour extract partition regular layout block correspond ornode contour fall block treat input learn contour block select large length leafnode generate cluster select contour constraint obtain initial feature vector d sample experiment evaluate method object shape detection use benchmark dataset uiucpeople implementation set fix number ornode andor model uiucpeople dataset experiment initial layout regular partition block uiucpeople dataset m leafnode ornode positive sample extract object contour negative sample compute edge map use pb edge detector edge link method convergence learning iteration detection edge map test image extract negative training sample object search different scale contour input leafnode sample point compute shape context descriptor point descriptor quantize polar angle radial bin adopt testing criterion define pascal voc challenge detection count correct intersection union groundtruth experiment uiucpeople dataset contain image train test image contain person play fig b train andor ornode associate leafnode evaluate benefit collaborative edge degenerate model andor tree remove collaborative edge fig c illustrate average precision detection apply respectively compare model stateoftheart detector use manually label model follow accuracy method average b table comparison detection accuracy uiucpeople dataset b comparison average precision dataset mention calculate detection accuracy consider detection high score image method table report method outperform approach horse vote b c d figure result measurement database object shape detection apply method dataset false positive annotate blue dataset consist horse image image horse positive example negative example use training remain image test report plot false positive image recall system substantially outperform recent method model achieve detection rate respectively contrast result compete method experiment iii test method object category dataset category include image half image randomly select positive example negative example obtain category background train model category test remain image table b report result evaluate mean average precision compare current method model achieve competitive result result visualize experiment iii respectively conclusion paper propose discriminative object model andor graph representation model train dynamical manner model structure automatically determine iteration parameter method achieve object shape detection challenge dataset reference hide markov support vector machine icml m roth structure revisit people detection articulated pose estimation cvpr s shape matching object recognition use shape l detect people use mutually consistent activation eccv p r mcallester d object detection discriminatively train model c schmid image shape model object detection computer vision c schmid group adjacent contour segment detection dimension reduction local principal component analysis neural computation shape guide contour group particle filter iccv partial shape match local deformation robust global shape similarity object detection cvpr object detection use hough transform cvpr r c learn detect natural image boundary use local color texture cue pami use analytic qp sparseness speed training support vector machine advance neural information processing system page p roth discriminative structure learning hierarchical representation object detection cvpr z song object detection classification cvpr p q match describe discriminate object shape cvpr tran improve human relational model tran z learn hierarchical human cvpr weakly supervise shape base object detection filter eccv b l classify action measure action similarity model mutual context object human pose icml p voting group dependent eccv joachim learn structural svms latent variable icml yuille nip page image parse stochastic grammar nip l freeman latent hierarchical structural learning detection cvpr l yuille max margin andor graph learn parse human body cvpr stochastic grammar image foundation trend computer graphic vision
sejnowski combine visual acoustic speech signal neural network improve electrical apply recognition degrade presence noise information available visual speech signal speaker previous attempt use visual speech signal improve automatic speech recognition system combine acoustic visual speech information symbolic level use heuristic rule paper demonstrate alternative approach fuse visual acoustic information train feedforward neural network map visual signal corresponding shortterm spectral amplitude acoustic signal information directly combine degrade acoustic significant improvement demonstrate vowel recognition acoustic signal result compare performance human pattern matching estimation introduction current automatic speech recognition system rely exclusively acoustic speech signal consequence system perform poorly noisy combine visual acoustic speech signal environment compensate acoustic signal attempt remove noise acoustic supplement acoustic signal source speech information source visible movement human visual speech signal improve speech perception acoustic signal degrade noise serve source speech information acoustic signal completely absent visual speech signal use improve automatic recognition speech recognition system extensively use visual speech signal develop limited vocabulary demonstrate visual speech signal use significantly improve automatic speech recognition compare acoustic recognition system rely codebook image use translate incoming image corresponding symbol symbol string compare store sequence represent different word vocabulary categorical treatment signal require computational limitation currently available digital paper propose alternative method process visual speech signal base analog computation distribute network architecture use interconnect processor work parallel large datum handle concurrently addition speed computation approach require segmentation early stage processing analog signal visual auditory pathway flow network real time combine directly result present series experiment use neural network process visual speech signal preliminary experiment result limit static image vowel demonstrate network able extract speech information visual image information use improve automatic vowel recognition visual acoustic speech signal acoustic speech signal model response vocal tract filter sound source resonance vocal tract appear peak shortterm power spectrum sufficient identify individual vowel overall shape spectra important general speech perception configuration articulator define shape vocal tract corresponding resonance characteristic filter articulator visible face speaker lip tip tongue contribution visible articulator result speech sound susceptible noise distortion contribution hide articulator visual speech signal tend complement acoustic sejnowski example distinct speech sound pair present presence noise complementary structure perception speech noise greatly improve speech signal present level speech signal combine previous attempt use visual speech signal information visual signal incorporate recognition system signal categorize approach visual signal use resolve ambiguity acoustic signal categorize combine source information early stage processing possible reduce number erroneous decision increase information pass later stage process additional information provide visual signal serve constrain possible interpretation ambiguous acoustic signal serve alternative source speech information signal heavily case massive computation perform raw datum new architecture base neural network new training procedure approach feasible interpret visual signal approach visual signal map directly acoustic representation closely relate vocal tract transfer function representation allow visual signal fuse acoustic signal symbolic encoding visual signal provide partial description vocal tract transfer function description usually ambiguous visual signal possible configuration vocal tract consequently possible correspond acoustic signal goal define good estimate acoustic signal visual signal use estimate conjunction residual acoustic information speech signal use experiment obtain male speaker video face camera condition visual acoustic signal transfer store laser disc allow access individual video frame correspond sound track video standard base frame second word preserve series frame laser disc data set construct example different vowel reduce image automatically define center result sample produce topographically accurate image pixel serve represent speech signal efficient encoding use parallel approach computation represent observe array sensor combine visual acoustic speech signal video frame laser disc speech representation choose acoustic output structure spectral amplitude acoustic signal essential recognition closely relate vocal tract transfer function calculate shortterm power spectrum acoustic signal signal sample analysis use produce smooth envelope original power spectrum sample frequency figure typical lip image present network feedforward network nonlinear unit use perform map lip image present input unit estimate produce output unit network hide unit find provide necessary bandwidth minimize effect standard backpropagation technique use compute error gradient train network instead use update weight error gradient use algorithm weight change training pattern present integrate visual acoustic signal evaluate spectral estimate feedforward network train recognize vowel stsae noise present train network correctly categorize training set serve perfect recognizer datum vowel present speech information channel fig path represent information obtain acoustic signal path provide information obtain correspond visual speech assess performance recognizer noise clean spectral envelope systematically degrade noise present recognizer particular condition visual input network noise introduce add normalize random vector noise corrupt vector produce interval step different vector produce performance report average fig recognition rate function noise ratio ratio recognizer operate chance network train estimate spectral envelope image use sejnowski provide independent stsae input recognizer fig network train datum use train vowel task remain combine estimate visual combine stsae stsae noise figure vowel recognizer integrate acoustic visual speech signal consider different way combine estimate obtain visual signal noise degrade acoustic envelope approach simply average envelope prove optimal able identify stsae estimate visual signal visual estimate combine noise degrade acoustic capable sin similarly high signaltonoise ratio combine input produce poor result signal provide correct input need weight accord relative information available source weighting factor introduce function optimal value parameter find empirically vary linearly ratio value range approximately sin result obtain use weighted average method use fuse stsae secondorder network network train acoustic envelope estimate envelope correspond visual speech signal network able great accuracy method measure square error increase accuracy translate improve recognition rate combine visual acoustic speech signal l l sin figure visual contribution speech recognition noise low curve performance recognizer vary signaltonoise condition use acoustic channel curve final improvement channel combine use weighted average compare performance performance network compare traditional technique neighbor comparison estimate stsae obtain use neighbor approach image training set store correspond stsae calculate acoustic signal image serve data base store template individual image test set correlate store image close image select acoustic correspond select image average produce estimate stsae correspond test image use procedure value average mse calculate test set procedure repeat test training set reverse value neighbor estimator able produce estimate approximately accuracy neural network network evaluate training epoch produce estimate error knn approach weight correspond network good performance define produce estimate error principal component analysis second method comparison obtain estimate use combination optimal linear technique step encode image use transform produce optimal encoding image respect error encode image compute sejnowski normalize image use mean image transformation matrix row large eigenvector covariance matrix image represent image hidden unit neural network second step find mapping encode image vector yi correspond spectral envelope si use linear leastsquare fit calculate b find provide good estimate desire si think matrix correspond weight input layer hide unit map hide unit output unit network train produce estimate far superior obtain use coefficient b true training datum b calculate test datum set compare network train epoch network produce estimate stsae training set test set conclusion human capable combine information receive distinct sensory channel great speed ease combine use visual acoustic speech signal example integrate information modality pollack relative improvement provide visual signal vary signaltonoise ratio acoustic signal combine speech information available speech signal categorize obtain performance comparable demonstrate human visual acoustic speech information effectively fuse require categorical preprocesse lowlevel integration speech signal particularly useful signaltonoise ratio range combine signal recognize great accuracy component signal contrast independent categorical decision channel require additional information form ad rule produce level performance lip read research traditionally focus identification evaluation visual feature reduce original speech signal finite set predefined parameter discrete symbol waste information automatic recognition system information prove useful later stage processing approach speech information visual signal access require discrete feature analysis categorical decision bin visual acoustic speech signal line research consequence problem target identification base multiple sensor example problem arise design system combine radar datum mapping common representation use neural network model apply problem domain key insight combine information stage prior network learning procedure allow system construct perform mapping long sufficient datum available train network acknowledgement research support grant scientific research apply physics laboratory reference sp ed perception production speech acoustic theory speech publisher pl physical characteristic lip underlie vowel soc improved automatic system enhance report tm control method use study vowel soc learn internal representation error propagation parallel distribute processing cognition vol foundation mit pollack visual contribution speech noise soc q preliminary comprehensive account speech perception r ed hearing eye sejnowski integration acoustic visual speech signal use neural network ieee
layer dynamic texture electrical computer abstract dynamic texture video model treat video sample spatiotemporal stochastic process specifically linear dynamical system problem associate dynamic texture model video multiple region distinct motion work introduce layer dynamic texture model address problem introduce variant model present learn model finally demonstrate efficacy propose model task segmentation synthesis video introduction traditional motion representation base optical flow inherently local significant difficulty face problem noise classical solution problem regularize optical flow field introduce undesirable smooth motion edge region motion definition smooth eg vegetation scene recently attempt model video superposition layer subject homogeneous motion layer representation exhibit significant promise term combine advantage regularization use global cue determine local motion flexibility local representation little smoothing potential far fully main limitation dependence parametric motion model affine transform assume piecewise planar world rarely hold practice fact layer usually formulate model world warp transformation form frame video stream severely limit type video synthesize layer promise model scene compose ensemble object subject homogeneous motion leave bird highway traffic little progress far demonstrate actually model scene recently success model complex scene dynamic texture precisely sample stochastic process define space time work demonstrate model dynamic appearance video stochastic quantity lead powerful generative model video figure subject parametric motion fact dynamic texture model surprising ability abstract wide variety complex pattern motion appearance simple spatiotemporal model major current limitation dynamic texture framework inability account visual process consist multiple dynamic texture example bird fly water highway traffic different speed video contain tree background people case exist dynamic texture model inherently incorrect represent multiple motion field single dynamic process work address limitation introduce new generative model video denote layer dynamic texture ldt consist augment dynamic texture discrete hide variable enable assignment different dynamic different region video condition state hide variable video model simple dynamic texture introduce share dynamic representation pixel region new model layered representation compare traditional layered model replace process layer formation base figure base sample generative model dynamic appearance provide dynamic texture enable rich video representation layer dynamic texture model dynamic texture capable assign different dynamic appearance different image region consider model ldt differ way enforce consistency layer dynamic model enforce strong consistency closedform solution parameter estimate require sample second enforce weak consistency simple learn model apply segmentation synthesis sequence challenge traditional vision representation strong consistency lead superior performance demonstrate benefit sophisticated layered representation paper organize follow section introduce layer dynamic texture model section present learn model training datum finally section present experimental evaluation context segmentation synthesis layer dynamic texture start brief review dynamic texture introduce layer dynamic texture model dynamic texture dynamic texture generative model video base linear dynamical system basic idea separate visual component underlie dynamic process dynamic represent timeevolve state process r appearance frame yt r linear function current state vector observation noise formally system describe yt r transition matrix c r transformation matrix q state observation noise process r r r initial state r constant interpretation dynamic texture model column principal component video frame state vector coefficient video frame case model learn method figure layer dynamic texture leave approximate layer dynamic texture right observed pixel time hidden state process collection layer assignment variable assign pixel state process alternative interpretation consider single pixel evolve time coordinate define onedimensional random trajectory time represent weighted sum random trajectory weighting coefficient contain corresponding row c analogous discrete fouri transform signal processing signal represent weighted sum complex exponential dynamic texture trajectory necessarily orthogonal interpretation illustrate ability dynamic texture model motion different intensity level eg car shade simply scale row c regardless interpretation simple dynamic texture model state process restrict efficacy model video motion homogeneous layer dynamic texture introduce layer dynamic texture ldt figure leave model address limitation dynamic texture rely set state process j model different video dynamic layer assignment variable zi assign state process layer condition layer assignment pixel layer model dynamic texture addition collection layer assignment model ensure spatial layer consistency linear system equation layer dynamic texture ci wit j r transformation hidden state observed pixel domain pixel layer noise parameter r r noise process wit initial state draw generative model layer dynamic texture assume state process layer assignment independent layer motion independent layer location vice versa section intractable compute address issue consider slightly different model approximate layer dynamic texture consider different model approximate layer dynamic texture figure right associate state process different dynamic texture define dynamic texture associate layer share set dynamic parameter assign layer assignment variable collection layer assignment model mrf model conditioning layer assignment pixel independent model describe follow linear system equation ci wit noise process wit n initial state model video extension popular image mrf model class variable pixel form grid class pixel segment classconditional distribution case linear dynamical system main difference propose model consistency dynamic layer ldt consistency dynamic strongly enforce require pixel layer associate state process hand aldt consistency layer weakly enforce allow pixel associate instantiation state process instantiation associate layer share dynamic parameter weak dependency structure enable efficient learning model layer assignment mrf determine layer assignment follow distribution z ije e set edge mrf grid normalization constant partition function potential function form z zi potential function define prior likelihood layer ij attribute high probability configuration neighboring pixel layer parameter potential function learn model instead treat constant estimate database manually segment training video parameter parameter model learn use expectationmaximization iterate estimate hide state variable hide layer assignment current parameter update parameter current hide variable estimate iteration contain follow step e estep q log px argmax q mstep remainder section briefly describe algorithm propose model limited space available refer reader report detail layer dynamic texture estep layer dynamic texture compute conditional mean observe video expectation intractable compute closedform know state process pixel assign necessary marginalize configuration z problem appear computation posterior layer assignment probability method approximate expectation currently adopt simply average draw posterior px use gibbs sampler approximation variational method belief propagation use plan consider future expectation know mstep parameter update analogous require learn regular linear dynamical system minor modification update transformation matrix ci detail approximate layer dynamic texture aldt model similar mixture dynamic texture video cluster model treat collection video sample collection dynamic texture aldt model pixel sample set onedimensional dynamic texture similar mixture dynamic texture difference estep compute posterior assignment probability observed datum condition single data point pz posterior approximate sample posterior use monte method loopy belief second transformation matrix ci different pixel e m step modify accordingly detail available experiment section efficacy propose model segmentation synthesis video multiple region distinct motion figure video sequence use test composite distinct video texture water fire second middle laundry laundry leave video spin place circular motion laundry outside spin fast final video highway traffic lane travel different speed second fourth lane leave right fast fifth video multiple region motion properly model model propose paper regular dynamic texture variation video model fit video model layer dynamic texture approximate layer dynamic texture model aldt model case layer assignment zi distribute multinomial experiment dimension state space base system clique size parameter potential function expectation require approximate use gibb sample ldt model mcmc aldt model present segmentation result model effectively separate layer different dynamic discuss result relative video synthesis learn model segmentation video segment assign pixel probable layer condition observe video argmax possibility assign pixel maximize posterior pixel maximize true posterior practice obtain similar result method method choose individual posterior distribution compute estep column figure segmentation result obtain model column b aldt column video available segmentation produce model conclude composite laundry video reasonably segment mrf prior confirm intuition video region contain distinct dynamic model separate state process pixel randomly assign layer uniformly assign segmentation traffic video use poor dynamic different difference significantly subtle segmentation require strong layer consistency general segmentation use weak form layer consistency impose aldt model deficiency offset introduction mrf prior strong consistency enforce ldt model result segmentation illustrate need design sophisticated layer representation goal model video subtle variation expect introduction mrf prior improve segmentation produce model example composite sequence erroneous segment water region remove traffic sequence segmentation disappear term overall segmentation quality able segment composite video perfectly segmentation laundry video model plausible laundry edge fast spin place model produce reasonable segmentation traffic video segment roughly correspond different lane traffic error correspond region contain motion region lane motion eg corner lane error eliminate filter video segmentation attempt pre finally note laundry traffic video trivial segment standard computer vision technique method base optical flow particularly true case traffic video straight line flat region compute correct optical flow difficult problem synthesis layer dynamic texture generative model video synthesize draw sample learn model synthesize composite video use ldt normal dynamic texture find model video multiple motion regular dynamic texture average different dynamic figure frame test video sequence composite water fire video texture middle laundry highway traffic lane travel different speed b c d figure segmentation result test video use layer dynamic texture b layered dynamic texture approximate layer dynamic texture approximate ldt synthesize video fire region speed original video furthermore motion different region couple fire begin fast water region smoothly contrast video synthesize layer dynamic texture realistic fire region correct speed different region follow motion pattern video synthesize appear noisy pixel evolve different instantiation state process illustrate need sophisticated layer model reference b determine optical flow artificial intelligence vol b iterative image technique application darpa image understanding workshop j barron s performance optical flow technique computer vision vol e represent image layer ieee tran image processing vol estimate mixture model image infer spatial transformation use ieee conference computer vision pattern s dynamic texture international computer vision vol pp g d p s dynamic texture segmentation international conference computer vision vol p s dynamic texture recognition computer vision pattern recognition proceeding vol pp probabilistic kernel classification autoregressive visual process ieee conference computer vision pattern recognition vol geman geman stochastic relaxation gibb distribution bayesian image ieee transaction pattern analysis machine intelligence vol pp p m b rubin maximum likelihood incomplete datum royal statistical society b vol pp layered dynamic texture mixture dynamic texture ieee international conference computer vision vol r h approach time series smoothing forecasting use time analysis vol roweis ghahramani unifying review linear neural computation vol
learn hierarchical structure linear relational embed computational neuroscience unit abstract present linear relational embed new method learn distribute representation concept datum consist instance relation concept final goal able generalize ie infer new instance relation concept task involve family relationship generalize previously publish method use effectively find compact distribute representation recursive datum structure tree list linear relational embed aim large set fact domain express tuple arbitrary symbol simple syntactic able infer fact prior knowledge domain let imagine situation set concept set relation concept datum consist instance relation hold concept want able infer instance relation example concept people certain family relation relation fact like able infer approach learn appropriate distribute representation entity datum exploit generalization property distribute representation inference paper present method linear relational embed lre learn distribute representation concept embed space relation concept linear transformation distribute representation let consider case relation binary ie involve concept case data consist triplet problem try solve infer missing triplet infer triplet equivalent able complete come element try complete element triplet lre represent concept datum learned vector method analogous present use complete element triplet find euclidean space relationship concept learn matrix map concept approximation second concept let assume data consist triplet contain distinct concept binary relation denote set dimensional set triplet vector correspond concept set matrix correspond relation need indicate vector matrix correspond concept relation certain triplet case denote vector correspond concept vector correspond second concept matrix correspond relation write triplet operation relate pair vector matrixvector multiplication produce approximation triplet think noisy version concept vector way learn embed maximize probability noisy version correct completion imagine concept average location space observation concept noisy realization average location assume spherical noise variance dimension discriminative function correspond log probability right completion sum train triplet d number triplet term equal differ term learn base maximize respect vector matrix component good result prove successful generalization learn embedding maximize use exactly information triplet triplet vector represent correct completion probable concept vector triplet state equal l numerator exactly denominator necessary order stay away trivial m solution notice denominator critical beginning learning vector matrix differentiate gradually lift o real goal learning allow modify discriminative function include parameter t anneal learn wj like system assign equal probability correct completion m discrete probability distribution want n system implement discrete delta function probability distribution normalization factor x factor ensure minimize kullbackleibler divergence approach obvious embedding minimize sum distance find triplet respect vector matrix component unfortunately minimization cause vector matrix collapse trivial solution relation decrease value way cause concept vector way equal different vector collapse unique vector learn function maximize respect vector matrix component generalization performance obtain maximize result present section obtain maximize use gradient ascent vector matrix component update simultaneously iteration effective method perform optimization conjugate gradient learning fast usually require update worth point general different initial configuration optimization cause system arrive different solution solution similar term generalization performance lre result present result obtain apply lre family tree problem problem datum consist people relation people belong family english fig leave information tree represent simple proposition form use relation triplet tree fig right embedding obtain train notice linearly separable english people member family symmetric correspond member family sign component vector feature test generalization figure leave family tree symbol mean right layout vector represent people obtain family tree problem vector endpoint indicate family tree connect triplet use train right diagram d vector vector person column order accord tree diagram left performance triplet test set choose completion concept accord probability system generally able complete correctly triplet pick leave train result family tree problem obtain use method problem generalize triplet oreilly error test case hold train family alter originally use match author family problem exist triplet complete case example family tree problem argue sufficient test generalization merely test completion triplet use train proper test generalization system complete triplet kind range concept r relation assume knowledge triplet admit completion knowledge issue analyze handle problem correctly system need way indicate triplet admit completion maximization terminate build new probabilistic model solution find new model constitute relation mixture identical spherical gaussian center concept vector uniform distribution uniform distribution care know answer compete gaussian represent concept vector relation different variance uniform different height parameter probabilistic model relation variance relative density uniform distribution write parameter learn use validation set union set positive triplet set pair complete negative indicate fact result apply relation belong maximize follow discriminative function validation set p d p d p d p p respect fix parameter compute learn parameter order complete triplet probability distribution gaussian uniform distribution system choose vector know answer accord tie completion triplet use method family tree problem use train test validation set build follow way test set contain positive triplet choose triplet relation validation set contain group positive group negative triplet choose random group triplet relation train set contain remain positive triplet learn distribute representation entity datum maximize training set learn parameter probabilistic model maximize validation set result system able correctly complete possible triplet figure distribution probability complete triplet test set lre scale problem big size use big version family tree problem family tree branch real family people generation use set tree author contain relation use family tree problem total positive triplet learn use training set positive triplet validation set constitute figure distribution probability assign concept leave right triplet write diagram triplet correct completion triplet use train black bar probability people order accord fig bar right probability know answer positive negative triplet system able complete correctly possible triplet completion correct high probability assign case probability assign wrong completion generalization error specific form system appear believe mean parent fail model extra restriction people hand data specify restriction use lre represent recursive datum structure section use effectively find compact distribute representation recursive datum structure tree list discuss binary tree reasoning apply tree approach inspire pollack architecture raam autoencoder train use backpropagation figure architecture network tree system think compose network l r l r r r r r c phrase r l r l r r r r c figure leave architecture raam binary tree layer fully connect adapt center use learn representation binary tree fashion right binary tree structure sentence use experiment encode pattern single pattern size second decode compressed pattern determine far decode encode tree network learn total number nonterminal node tree code terminal node supply network learn suitable code nod decode procedure decide vector represent terminal node internal node far decode use binary code terminal symbol fix threshold use check decode approach cast lre problem concept tree subtree leave pair tree subtree leave exist relationship implement jointly implement fig learn representation tree matrix maximize eq formulation hierarchical lre solve problem encounter raam need supply code leave tree learn appropriate distribute representation secondly learn datum stop decode process fact problem recognize node need far decode similar problem recognize certain triplet admit completion solve previous section build outlier model know answer build nonterminal node learn appropriate value relation maximize eq set triplet leaf tree play role set appear eq apply method problem encode binary tree correspond sentence word small vocabulary sentence fix structure phrase constitute adjective noun follow verb phrase verb noun fig sentence fix grammatical structure add extra semantic structure follow way word grammatical category divide disjoint set noun girl woman scientist adjective pretty young old verb help training set type type suffix indicate set word type belong way sentence kind pretty girl scientist allow training set possible sentence satisfy constraint implicit training set use learn distribute representation node tree maximize use sentence training set d build outlier model nonterminal symbol root internal node system reconstruct child nonterminal symbol far decode decode process provide correct reconstruction sentence training set row fig distribute representation find word vocabulary notice set adjective verb symmetric respect origin difference set evident noun fact exist restriction noun use position restriction noun appear position training sentence fig test system generalize training set use procedure use pollack enumerate set tree raam able represent pair pattern tree encode pattern new high level tree decode tree pattern subtree norm difference original tree reconstructed subtree tolerance set consider form system impressive generalization performance training use sentence sentence generate form sentence generate sentence wrong dog old girl sentence violate semantic constraint pretty girl scientist compare poor generalization performance obtain raam similar problem recognize u u u adjective verb girl girl girl girl r c c noun r c c adjective r c c noun r c c verb figure diagram multiple row row relate word follow order adjective pretty young old verb help low row bar separate representation word sentence find learn different contribution root tree word girl place position tree row contribution leaf reconstruction adjective verb noun apply position respectively pollack certainly fact raam representation leave similar problem formulation solve learn distribute representation let try explain generalize matrix decompose child node denote concatenation operator pair matrix associate link graph system learn embed find distribute representation tree multiply representation leave matrix find path leave root add matrix multiplication sequence word leave generate different representation root node second row fig point clear different contribution root tree word girl depend position sentence tree root leave multiply distribute representation use matrix analyze particular leaf reconstructed leaf example reconstruct row fig contribution leaf reconstruction adjective verb noun place leave respectively contribution adjective match closely actual distribute representation contribution noun position negligible mean adjective place tend reconstruct correctly reconstruction independent noun position hand contribution noun verb position notice word belong subset symmetric word subset way system able enforce semantic agreement word position finally reconstruction adjective verb noun place leave respectively assign low probability word system generate sentence form linear relational embed new method learn distribute representation concept relation datum consist instance relation concept find mapping concept impose constraint relation model linear operation lre excellent generalization performance result family tree problem far obtain previously publish method result problem similar learn distribute representation set concept relation lre easily modify representation incorporate new concept relation possible extract rule solution couple lre learning fast lre rarely converge solution poor generalization begin introduce lre binary relation idea easily extend high relation simply concatenate concept vector use rectangular matrix relation binary tree relation tree high relation high use find distribute representation hierarchical structure generalization performance obtain use raam similar problem easy prove relation binary sufficient number dimension exist solution satisfy set triplet linearity lre represent relation great limitation overcome add extra layer nonlinear unit represent relation new method nonlinear relational embed represent relation good generalization result reference geoffrey e hinton learn distribute representation concept proceeding eighth annual conference cognitive science society page geoffrey e distribute representation research group editor parallel distribute processing volume page mit press oreilly model neural interaction learn psychology learn distribute representation relational datum use linear relational embed phd thesis computer science geoffrey e hinton learn distribute representation map concept relation linear space proceeding icml page recursive distribute representation artificial intelligence r learn definition relation machine learn
local smoothness variance reduce optimization dept operation research financial dept abstract propose family nonuniform sample strategy provably speed class stochastic optimization algorithm linear convergence include stochastic reduce gradient svrg stochastic dual large family penalize empirical risk minimization problem method exploit datum dependent local smoothness loss function optimum maintain convergence guarantee bound quantify advantage gain local smoothness significant problem significantly empirically provide thorough numerical result theory additionally present algorithm exploit local smoothness aggressive way perform practice introduction consider minimization function form p r w convex correspond loss w data r convex regularizer p strongly convex p p p wk addition assume smooth general flat region example include regression absolute error insensitive loss smooth approximation logistic regression stochastic optimization algorithm consider loss time choose accord distribution pt change time recent algorithm combine information previously loss accelerate process achieve linear convergence rate include stochastic reduce gradient svrg stochastic average gradient stochastic dual coordinate expect number iteration require form l log l lipschitz constant loss gradient measure smoothness difficult problem condition number l large n ill condition motivate development algorithm algorithm adapt allow importance sampling uniform effect convergence bound replace uniform bind describe average loss specific lipschitz bound practice important class problem large proportion need sample time example instance smooth n l solve figure observe decay upper bind update possible different sample choose sample white produce update large majority figure white indicate waste effort loss algorithm capture relevant information visit non white nearly constant time detect focus important loss possible represent success significant room improvement focus effort active loss increase effectiveness factor similar phenomenon occur svrg algorithm phenomenon specific single problem general problem expect set useful loss small near constant figure sdca smoothed dual residual upper bind sdca update size indicate waste effort dual residual quickly sparse support stable allow change time phenomenon describe exploit figure significant speedup obtain variant svrg sdca comparison dataset section mechanism speed obtain specific algorithm underlie phenomenon exploit problem smooth locally globally consider single smoothed hinge loss use smoothed smooth parameter hinge loss spread interval length illustrate figure svrg solve smoothed hinge loss svm loss gradient e lip smooth e strong convexity uniform sample global smoothness sample local svrg empirical affinity svrg effective pass datum duality gapsuboptimality duality gapsuboptimality lipschitz constant enter global estimate condition number kxi approximate hinge loss precisely small problem strictly ill condition interval length locally approximate affine constant gradient correct expression local conditioning interval b figure contribute small problem locally condition set loss constant gradient subset hypothesis space summarize purpose optimization single affine sdca solve smoothed hinge loss svm loss gradient e lip smooth e strong convexity uniform sample global smoothness sampling alg empirical sdca alg effective pass datum figure left variant svrg l right variant sdca figure loss flat hessian vanishe constant gradient ball b r b radius r kxi induce euclidean ball hypothesis wt r prove include loss contribute curvature region interest affine model sum replace sample find r algorithm combine strong convexity quantity duality gap gradient norm function sample necessary happen svrg naturally modeling need light modification realize significant gain provide detail svrg section case similar sdca section loss affine locally smooth logistic regression loss gradient local lipschitz constant decay exponentially distance hyperplane dependent loss sample obtain bound benefit local smoothness svrg variant define formally relevant geometric property optimization problem relate provable convergence improvement exist generic bound detailed bound sequel b c r euclidean ball radius r c definition denote lir r uniform lipschitz coefficient hold distance r ir remark algorithm use similar quantity dependent know w known definition define average ball smoothness function s r problem r require stochastic gradient sample reduce loss suboptimality constant factor svrg importance sampling accord global smoothness certify optimum r current iterate use r time stochastic gradient step measure similarly increase loss affine ball optimum definition define ball affinity function s r problem lir r similarly require access reduce duality gap sdca importance sampling accord global smoothness certify optimum distance r current primal iterate access r time case local smoothness affinity enable focus constant portion sample effort loss challenge optimum ratio algorithmic advantage large obtain provable speedup fast use local smoothness certify non smooth loss absolute loss regression similarly ignore irrelevant loss lead significant practical improvement current theory loss quantify speed smooth loss obtain algorithm simple fast use qualitative observation iterate tend optimum set relevant loss generally stable shrink estimate set relevant loss directly quantity observe perform stochastic iteration estimate r previous work general direction paper work combine nonuniform sampling empirical estimation loss smoothness note excellent empirical performance variant theory ensure convergence provide similarly fast bind free variant sdca svrg section dynamic importance sample variant report relation local smoothness discuss connection section local smoothness gradient descent section describe svrg contrast classical stochastic gradient descent naturally expose local smoothness loss present variant svrg realize gain begin consider single loss close optimum simplicity assume r assume small ball b w r current estimate include optimum b contain flat region hold large proportion loss descendent svrg importance sampling use update form pi unbiased estimator gradient loss term xi svrg use t reference point advantage variance vanishe point addition b constant b effect sample cancel particular set loss generally xi constant small lir difference sample value small similarly small formalize section localize exist theory apply importance sample adapt svrg loss varied global smoothness local svrg suboptimality solution use svrg compute exact gradient reference point perform stochastic gradient descent step sample distribution step size number iteration determine smoothness loss replace global bound gradient change local lir valid restrict iteration small ball certify contain optimum allow leverage previous algorithm analysis maintain previous guarantee improve r large section assume p initial version svrg incorporate smooth regularizer different way explain later allow apply exist theory instead use proximal operator fixed regularization use localize projection stochastic descent ball b reference point theory develop importance sampling global smoothness apply sharp local smoothness estimate hold ignore affine special case allow stochastic iteration use large stepsize obtain speedup problem dependent large late stage figure formalize follow local svrg application dependent regularization portion reduce suboptimality constant factor apply iteratively minimize loss compute b r define r r strong convexity r compute l x w define probability distribution pi ir step size maxi l l p ir weight lipschitz constant p apply inner loop set b choose p compute return m let initial solution certify b b find ef w f w use o m time m lir p remark difficult case ill condition locally term negligible ratio complexity svrg use global smoothness approach r ir lir apply single proof initial pass datum compute w r round algorithm regularizer r localize ir instead global reference point apply theorem local require general proximal operator allow use correspond large stepsize ir remark use projection restriction smooth regularization necessary local smoothness restrict b b large step size compromise convergence entirely difficult control theory practice skip projection entirely convergence step far b require consistently unlikely event remark theory require m stochastic step exact gradient guarantee improvement ill condition problem practice stochastic step exact gradient provide benefit heuristic scenario sample distribution large step size enlarge step size accompany theory gain corresponding speed certain precision risk non convergence frequently incorporate smooth r add loss function reduce smooth inherent loss reduce benefit approach instead increase l propose add single loss function define form pose real difficulty depend loss gradient smoothness main difficulty approach section early stage r large small common choice lead loose bound ir case speed obtain precision satisfactory consider conservative scheme section empirical affinity svrg rely local smoothness certify ti small contrast empirical affinity svrg ti t evidence loss active time evidence local affinity loss sample strategy locally affine loss r large certify focus work relevant loss early half time sample global bound estimate current bound variance ti increase positive benefit use observe sample additional work pseudo code slightly long algorithm supplementary material space reason stochastic dual coordinate solve p dual problem d r xi iteration choose random accord update correspond loss increase d scheme use particular loss analyze obtain linear rate general smooth loss uniform sampling l regularization recently generalize regularizer general sample distribution particular improve bound performance adapt global loss use distribution pi smoothness pn l l log iteration obtain expect suffice perform gap sdca different gradient descent method share property current state algorithm form match derivative information update require skip ve figure loss converge quickly local affinity sufficient condition algorithmic approach exploit locally affine loss sdca different gradient descent style algorithm affine loss certify early final form henceforth ignore apply locally affine smooth loss require modify algorithm explicit localization use reduction obtain improved rate reuse theory remain point result state squared euclidean regularization hold strongly r s let t r let gi r word t affine r include compute optimal value proof state section xi constant singleton wt r contain particular enable ignore grow proportion loss overall convergence enable follow adapt locally affine approximately r w m compute r p m s b compute r x c s e t m m choose p compute tit sit x t duality gap achieve expect duality gap log iteration p remark assume l simplicity recall r find number iteration reduce factor r compare use contrast cost step add factor o m m drive choice m recent work modify sdca dynamic importance sample dependent dual residual refer derivative exhibit practical improvement convergence especially smooth svm theoretical speed sparse impractical version algorithm tell precondition hold magnitude expect benefit term property problem oppose state context locally flat loss smooth svm answer question local smoothness lemma tend loss locally affine ball optimum practical algorithm realize benefit come play quantify term r empirical sdca use local affinity small duality gap certify optimality avoid calculate naturally r small late process instead half sample proportion magnitude recent half choose uniformly figure illustrate approach lead significant speed early approach base duality gap local affinity clear prove bind strictly improve worth note probably rare update factor empirical algorithm quickly detect locally affine loss obtain speed certify addition naturally adapt expect small update locally smooth loss note closely relate current algorithm differ significantly quantity use guide sample empirical sdca p p p p ai b t m m choose p compute tit sit x empirical evaluation duality gapsuboptimality sdca solve smoothed hinge loss svm loss gradient e lip smooth e strong convexity uniform sample global smoothness sampling empirical sdca alg effective pass datum sdca solve smoothed hinge loss svm loss gradient e lip smooth e strong convexity uniform sample global smoothness sampling alg empirical sdca effective pass datum duality gapsuboptimality duality gapsuboptimality sdca solve smoothed hinge loss svm loss gradient e lip smooth e strong convexity uniform sample global smoothness sampling alg empirical sdca effective pass datum duality gapsuboptimality apply algorithm parameter additional classification dataset demonstrate impact variant widely result sdca figure svrg figure section supplementary material lack space sdca solve smoothed hinge loss svm loss gradient e lip smooth e strong convexity uniform sample global smoothness sampling empirical sdca alg effective pass datum figure sdca variant result additional dataset advantage use local smoothness significant hard dataset reference dual coordinate ascent probability accelerate stochastic gradient descent use predictive variance reduction advance neural information processing system page new dataset svrg ratio stepsize aggressive theory suggest stop converge change run use parameter change adapt accelerate proximal coordinate gradient method application regularize empirical risk minimization minimize finite sum stochastic average shalevshwartz accelerate proximal stochastic dual coordinate regularize mathematical programming page shalevshwartz dual coordinate ascent method regularize loss journal machine learn research proximal stochastic gradient method optimization coordinate method regularize empirical risk minimization optimization importance sample optimization importance sampling regularize loss minimization proceeding international conference machine learn
estimate class prior posterior positive unlabeled datum computer abstract develop classification algorithm estimate posterior distribution positiveunlabele datum robust noise positive label effective highdimensional datum recent year algorithm propose learn positiveunlabele datum contribution remain theoretical perform poorly real highdimensional datum typically contaminate noise build previous work develop practical classification algorithm explicitly model noise positive label utilize univariate transform build discriminative classifier prove univariate transform preserve class prior enable estimation univariate space avoid kernel density estimation highdimensional datum theoretical development parametric nonparametric algorithm propose constitute important step widespread use robust classification algorithm data introduction access positive negative unlabeled example standard assumption semisupervised binary classification technique domain sample class negative available lead setting learn positive unlabeled datum positiveunlabele learning emerge science observation positive example protein social network user like particular product usually reliable absence positive observation interpret negative example molecular biology attempt label data point positive protein enzyme variety experimental biological reason social network explicit product possible scenario lead situation negative example actively collect fortunately absence label example tackle incorporate unlabeled example negative lead development nontraditional classifier traditional classifier predict example positive negative nontraditional classifier predict example positive unlabeled elkan positive unlabeled training reasonable class posterior optimum scoring function proper loss traditional setting monotonically relate posterior nontraditional setting true posterior fully recover nontraditional posterior know class prior proportion positive unlabeled knowledge class prior necessary estimation performance criterion error rate balanced error rate fmeasure find classifier optimize criterion obtain thresholde nontraditional scoring function prior estimation nonparametric setting actively research past decade offer extensive theory identifiability practical solution elkan jain application algorithm real datum limit propose algorithm simultaneously deal noise label practical estimation highdimensional datum theory learn class prior rely assumption distribution positive know positive sample clean practice label data set contain noise negative example contaminate positive sample realistic scenario experimental science advance enable generation datum cost error example come study protein use analytical technology mass example process identification bioinformatics method usually set report result specified false discovery rate threshold eg unfortunately statistical assumption experiment violate lead substantial noise report result case identify protein modification similar noise appear social network user select actually like particular post far approach consider similar noise scott require density estimation know problematic highdimensional datum work propose classification algorithm class prior estimation design particularly highdimensional datum noise labeling positive datum formalize problem class prior estimation noisy positive unlabeled datum extend exist identifiability theory class prior estimation positiveunlabele datum noise set practically estimate class prior posterior distribution transform input space univariate space density estimation reliable prove transformation preserve class prior correspond train nontraditional classifier derive parametric algorithm nonparametric algorithm learn class prior finally carry experiment synthetic reallife datum provide evidence new approach sound effective problem formulation consider binary classification problem map input space output space let true distribution input represent follow mixture x distribution positive negative class respectively class prior proportion positive example refer sample unlabeled let distribution input label datum label sample contain example corresponding distribution mixture small proportion x observe mixture component different mix proportion simple scenario mix component correspond classconditional distribution pxy respectively approach permit transformation input space result general setup objective work study estimation class prior propose practical algorithm estimate efficacy estimation clearly tie small noise positive label large discuss identifiability practical algorithm estimate use result estimate posterior distribution class variable pyx fact label set contain negative example identifiability class prior identifiable unique class prior pair identifiability characterization section consider case section relate work result aim introduce require notation highlight important result late include missing result need approach proof technique different interest include appendix typically aspect address identifiability need determine problem identifiable second propose canonical form identifiable section class prior identifiable general mixture contain vice versa ensure identifiability necessary choose canonical form prefer class prior component different possible canonical form introduce mutual condition scott relate proper novelty distribution form jain discuss identifiability term measure let probability measure define algebra correspond respectively follow consider family pair mixture component set pair probability measure define family parametrize condition mean great proportion compare consistent assumption label sample mainly contain positive general choice p p p p set probability measure define p set pair equal distribution remove equal pair prevent identical define maximum proportion component mixture use result specify criterion enable identifiability specifically max p particular interest case read mixture contain finally define set possible generate vary singleton set identifiable general choice lead fortunately choose restrict set obtain identifiability theorem word contain pair distribution distribution pair express mixture contain proof result pair mixture generate follow fall let parameter onetoone relation expression righthand equation define probability measure fall uniquely determined fall individually uniquely determine observe definition imply consequently satisfie expect fall let let follow generate consequently fre contain pair mixture fall fre identifiable uniquely determined refer expression mixture component form pick form enforce mixture contain lead maximum separation generate pair distribution fre represent form identifiability fre occur precisely ie pair mix proportion appear form statement statement imply form unique completely specify follow equation use fre model unlabeled label datum distribution estimation class prior wellpose problem statement loss modeling capability use fre instead fall overall identifiability absence loss modeling capability maximum separation combine justify estimate class prior univariate transformation theory algorithm class prior estimation agnostic dimensionality datum practice dimensionality important consequence mixture model train expectationmaximization know strongly suffer highdimensional datum density estimation know issue practice address curse dimensionality transform datum single dimension transformation r surprisingly simply output nontraditional classifier train separate label sample l unlabeled sample transform similar require calibrate posterior distribution good ranking function sufficient introduce notation formalize data generation step figure let random variable value capture true distribution input unobserved random variable value true class input follow distribute accord respectively let selection random variable value determine sample input add figure s add noisy label sample s add unlabeled sample s add sample follow input select label unlabeled success label noisy positive drop figure labeling procedure value step sample randomly select attempt labeling probability independent select add unlabeled set select labeling attempt true label probability labeling succeed add noisy positive add drop set true label attempt labeling likely fail noise succeed attempt label succeed probability add noisy positive actually negative instance lead noise case noise increase increase proportion positive noisy positive l distribute accord respectively follow assumption consistent statement s assumption state proportion positive unlabeled sample label sample match true proportion respectively assumption state distribution positive input negative input unlabeled label sample equal unbiased lemma implication assumption statement particularly interesting state nonzero probability input need drop let random variable value s respectively l measure satisfy equation let distribution respectively satisfy assumption independent l distribute accord respectively proof appendix highlight condition score function preserve observe s serve pseudo class label label unlabeled classification expression posterior p s theorem transform let random variable x u l measure define let p posterior define equation p function composition operator assume l continuous density respectively measure correspond l respectively transformation p use compute true posterior probability p proof appendix theorem original datum transform datum transformation function express composition p onetoone function h define trivially p function emphasize preservation limit efficacy calibration scoring rank input p preserve theorem far demonstrate true recover p plug estimate p equation posterior probability p estimate directly use probabilistic classifier calibrate classifier score l serve estimate section parametric nonparametric approach estimation algorithm section derive parametric nonparametric algorithm estimate unlabeled sample noisy positive sample l theory approach handle multivariate sample practice circumvent curse dimensionality exploit theory univariate transform transform sample parametric approach parametric approach derive model sample component gaussian mixture share component different mixing proportion u u rd set dd positive definite matrix algorithm extension approach model instead estimate parameter single mixture parameter mixture u estimate simultaneously maximize likelihood l approach refer gmm msgmm exploit constraint mixture share component update rule derivation nonparametric approach nonparametric strategy directly exploit result direct connection twocomponent mixture sample m sample component c require algorithm estimate maximum proportion m purpose use briefly summarize appendix specifically twostep approach estimate follow estimate output respectively estimate estimate apply equation refer nonparametric alphamaxn empirical investigation section systematically evaluate new algorithm control synthetic setting variety data set uci machine learn experiment synthetic datum start evaluate algorithm univariate set mix proportion know generate gaussian iid sample explore impact mix proportion size component sample separation overlap mix component accuracy estimation class prior varied noise component size label sample l varied size unlabeled sample fix experiment reallife datum consider reallife datum set machine learn repository adjust datum problem categorical feature transform numerical use sparse binary representation regression datum set transform classification base mean target variable multiclass classification problem convert binary problem combine class datum set subset positive negative example randomly select provide label sample remain datum class label use unlabeled datum size label sample small datum set maximum size unlabeled datum set algorithm compare alphamaxn msgmm algorithm noiseless version version estimator use underlie classifier use e alternative estimator combine ensemble twolayer feedforward network hide unit score classifier use transformation create input algorithm important mention develop handle noisy label datum addition theory estimator restrict use classconditional distribution nonoverlapping support algorithm minimize objective estimator implement evaluation experiment repeat time able draw conclusion statistical significance reallife datum label sample create randomly choose appropriate number positive negative example satisfy condition size label sample remain datum use unlabeled sample class prior unlabeled data vary selection noise parameter mean absolute difference true estimate class prior use performance measure good perform datum set determine multiple hypothesis testing use pvalue correction result comprehensive result synthetic datum draw univariate distribution appendix table experiment transformation apply prior run algorithm expect result excellent performance msgmm model gaussian datum result significantly degrade datum suggest sensitivity underlie assumption hand accurate datum set robust noise result suggest new parametric nonparametric algorithm perform control setting table result real datum set alphamaxn demonstrate significant robustness noise parametric version msgmm competitive case hand degrade noise finally investigate practical usefulness preserve transform table appendix result alphamaxn msgmm real datum set use transform computational numerical issue reduce dimensionality use principal component analysis original datum cause matrix issue msgmm density estimation issue alphamaxn msgmm deteriorate significantly transform alphamaxn preserve signal class prior alphamaxn transform superior performance datum set relate work class prior estimation semisupervised setting include positiveunlabele learning extensively discuss previously reference recently general setting label noise introduce mutual model aim model estimate multiple unknown base distribution use multiple random sample compose different combination base distribution setting label noise subset general set treat general condition previously investigate restrictive setting cotraining blum natural approach use robust estimation learn presence class noise strategy theoretically long servedio empirically indicate need explicitly model noise generative mixture model approach develop explicitly model noise scholkopf algorithm assume label datum class relate work explicitly treat learning noisy positive formulation incorporate setting use theoretical algorithmic treatment different focus identifiability analyze convergence rate statistical property assume access function obtain proportion table mean absolute difference true mixing proportion datum set machine learn repository statistical significance evaluate compare gmm apply transform bold font type indicate winner indicate statistical significance datum set true mix proportion true proportion positive label sample sample dimensionality number positive example total number example area curve auc model train label unlabeled datum auc concrete gas sample explicitly address issue highdimensional datum focus algorithm obtain contrast focus primarily univariate transformation handle highdimensional datum practical algorithm estimate supervised learning use class transformation provide rich set technique address highdimensional data conclusion paper develop practical algorithm classification positiveunlabele datum noise label datum set particular focus strategy highdimensional datum provide univariate transform reduce dimension datum preserve class prior estimation reduce space remain valid far useful classification approach provide simple algorithm simultaneously improve estimation class prior provide result classifier derive parametric nonparametric version evaluate performance wide variety learn scenario datum set good knowledge algorithm represent practical approach learn highdimensional positiveunlabele datum noise label acknowledgement thank helpful comment grant support precision health reference e m high mixture discriminant analysis anal scott semisupervise novelty detection learn blum mitchell combine label unlabeled datum cotraine colt page c robust supervise classification mixture model learn datum uncertain label pattern c m sample selection bias correction theory page r learn positive unlabeled example m m class prior estimation positive unlabeled data tran learn classifier positive unlabeled data page d m linear discriminant analysis m white m w p nonparametric semisupervise learning class proportion preprint url httparxivorgab mutual analysis mixed membership partial label model url httparxivorgab d scholkopf estimate discriminant presence label noise icml page m machine learn repository l sparse nonparametric density estimation high dimension use aistat page p m long r servedio random classification noise convex potential booster mach learn p noise tolerance risk minimization ieee c williamson learn corrupt binary label estimation icml page r obtain calibrate probability boost page probabilistic output support vector machine comparison regularize likelihood method page mit scott tewari mixture proportion estimation embed distribution preprint url m r williamson composite binary loss mach learn m p adjust output classifier new priori probability simple procedure neural comput t class proportion estimation application rejection aistat page g classification consistency maximal denoise mach learn curse dimensionality dimension reduction multivariate density estimation theory practice visualization page m sequence cell datum
linear concept hide variable empirical study computer abstract learning technique classification task work indirectly try fit probabilistic model observed datum good idea depend robustness respect deviation model study question experimentally restricted nontrivial interesting case consider conditionally independent attribute postulate single hidden variable z attribute ie target observable depend model find likely value variable know value reduce test linear function observed value learn technique standard new develop base covariance compare control fashion algorithm version winnow attempt find good linear classifier directly conclusion help use model classification datum depart model performance quickly degrade drop linear introduction consider classic task predict binary target variable base value binary variable zi distinguish style learn approach task parametric algorithm postulate form probabilistic model underlie datum try fit model parameter classify example compute conditional probability distribution value know variable predict probable value nonparametric assume training datum particular form instead search directly space possible classification function attempt find small error training set example important advantage parametric approach induce model use support wide range inference specify classification task hand postulate particular form probabilistic model strong assumption partly support onr grant visit concept hide variable empirical study important understand robust method real world deviate model paper report experiment test issue consider specific case conditionally independent attribute zi single unobserved variable assume binary value zi depend henceforth binary model section fact model plausible domain instance language interpretation task fit parameter model use wellknown expectationmaximization technique new algorithm develop base estimate covariance section nonparametric case simply search good linear separator optimal predictor binary model ie predict variable know value rest linear mean comparison fair sense strategy choose classifier expressive power representative nonparametric class algorithm use winnow light modification section winnow work directly find good linear separator guarantee find perfect separator exist empirically fairly successful perfect separator fast experimental methodology generate synthetic datum true model test performance study deviation model interesting issue involve construct good experiment include control inherent difficulty learn model characterize entire space consider deviation datum draw model hide variable value note optimal classifier generally linear case observation qualitatively surprising assume model correct performance degrade world depart model discuss find surprising model compare algorithm winnow data linearly separable expect direct learning technique degrade performance related approach far main contribution work result shed light specific tradeoff fit parameter probabilistic model direct search good classifier specifically illustrate predict use model slightly simple distribution actually generate datum relative robustness directly search good predictor important practical issue highlight need theoretical understanding notion conditionally independent attribute assume example binary vector z e nl example generate independently random accord unknown distribution nl use xi denote attribute consider random variable denote value xi conditionally independent attribute model example generate follow postulate hide variable ie value value z z ie probability e ie independent parameter randomly choose value z hidden variable choose value observable xi value probability p p e attribute value choose independently remain fix note e probability parameter p following let l denote set n e ie parameter model point section assume ie case simplify notation write ai p pi p qi expectationmaximization algorithm traditional unsupervised approach learn parameter model find maximumlikelihood parameter distribution datum attempt find set parameter maximize probability datum observe find maximum likelihood parameterization analytically appear difficult problem simple set practical approach use wellknown expectationmaximization iterative approach converge local maximum likelihood function set procedure follow simply begin randomly choose parameterization p iterate apparent convergence expectation zi compute z z maximization reestimate p follow write ui ei ei convergence detect know likelihood function repeat process different restart experiment extremely conservative concern stop criterion iteration number iteration try practice sure true optimum locate approach partly response concern express develop heuristic technique learn p algorithm base measure covariance pair attribute attribute appear correlate fact model correct easy covariance define expectation respectively di denote pi qi know expect value furthermore able accurate estimate observe proportion sample zi estimate di trivial solve estimate pi estimate suppose compute pairwise covariance use datum use denote estimate distinct clearly l estimate d use equation estimate o consider pair average individual estimate individual estimate equally good small reliable expect estimate limit perfectly uncorrelated valid estimate suggest use weighted average weight proportional use weight lead equation determine simplification ei substitute estimate estimate estimate compute linear time determination quadratic depend compute time total estimate remain estimate sign briefly determine sign positive sign choose maximization phase work estimate parameter average base weight label data le sample point datum weight z weight point concept hide variable empirical study principle sign equal sign estimate practice statistically unreliable small sample size use involved voting procedure detail omit finally estimate q find method simply search optimal value use likelihood search criterion search turn efficient practice linear separator fully parameterize model interested predict value variable know value remain variable fact optimal prediction region linear separator variable omit detail suggest obvious learning strategy simply try find line minimize loss training set unfortunately task find linear separator minimize disagreement collection example know nphard instead use algorithm know produce good result linear separator exist certain relaxed assumption appear effective practice learning use basic version winnow ndimensional vector positive weight le weight associate ith feature update mistake initially weight vector typically set assign equal positive weight feature algorithm parameter parameter q parameter threshold instance predict predict label positive example weight correspond active attribute weight replace large weight q wi conversely predict receive label weight correspond active feature factor allow negative weight follow example rewrite example n variable apply winnow learn positive weight weight associate weight associate prediction rule simply compare threshold wi experiment describe significant modification basic algorithm reduce variance final classifier weighted average classifier train use sub training set weight base base sample second bias algorithm look thick classifier understand consider case data perfectly linearly separable generally linear concept separate training datum actually plausible chance unseen test datum choose linear concept separate positive negative training example widely possible idea wide separation clear perfect separator appeal basic intuition bias search thick separator change train rule somewhat new margin parameter t update current hypothesis mistake update prediction correct experiment find performance use version winnow basic algorithm paper present result derivation slightly different case predict find choice use number attribute size experiment differ dimension type process generate datum interested deviation difficulty problem feature determine data model use distribution ft use generate datum set experiment consider case datum draw binary distribution associate distribution difficulty parameter b accuracy predict value z actually know correct model course knowledge correct model expect accuracy ability control b allow select study model different qualitative characteristic particular allow concentrate experiment fairly hard instance compare trial differ number attribute denote class datum model binary distribution variable difficulty family data model use model use value hide variable denote family use ie value b ie complex correlation pattern xi furthermore optimal predictor necessarily linear specific result discuss section concentrate case set parameter include particular class data model experiment design goal good statistical accuracy repeatedly typically time choose data model random choose class choose sample appropriate size model run algorithm produce linear hypothesis measure success rate proportion time hypothesis correct prediction draw random sample data model use test phase draw new sample confidence interval result single model width use value construct normalize measure performance denote t follow let good possible accuracy ie accuracy achieve actual model generate datum let denote performance possible constant prediction rule ie rule predict likely value note vary model model model compute normalize statistic t average value good thought measure percentage possible predictive power plausible baseline algorithm achieve result report small representative selection experiment detail instance consider value range graph learning curve n note display t statistic error bar standard error provide accuracy surprisingly datum model binary note simply choose parameter model independently random examine difficulty model adjust n trivial problem easy predict nearly accuracy predict optimally nontrivial efficiently select random model class briefly scheme choose parameter model independently symmetric beta distribution model parameter expect value choose parameter beta distribution determine concentration average b value model generate equal finally use rejection sample find model b value exactly compute observed standard deviation divide square root number trial linear concept hide variable empirical study extremely learn significantly fast winnow depart binary assumption performance quickly degrade z t j m m figure figure l m l m figure figure j j r m odd figure figure ie performance similar winnow superior significantly uniformly n fix ie difference somewhat dramatic increase figure winnow obviously dominant fact sample size reach mean sample attribute performance attribute degradation binary assumption question reason consider covariance algorithm result generally similar support belief phenomenon property inherent model specific algorithm use similarly result omit try algorithm try find good linear separator directly include classic perceptron algorithm mp version winnow good experiment try conjecture performance somewhat possible approach comparison illustrate little qualitative differ roth phenomenon observe number attribute increase grow winnow need example performance surpass algorithm fix k note simply noisy nature region reason believe partially artifact way select model previously note experiment vary difficulty b level omit corresponding figure mention main difference winnow little fast surpass datum deviate model data model binary converge fast optimal performance pattern try compare approach real datum use datum originate problem assume hidden context variable somewhat plausible data correction domain use datum set use example sentence word pass past appear task determine occurrence task model think context hidden variable sense try learn case model binary value hide variable result predict likely classification winnow contrast perform extremely exceed task read limited experiment note far consistent careful experiment synthetic datum conclusion restrict binary hide variable able consider fair comparison probabilistic model construction traditional algorithm directly learn sense expressive power conclusion concern surprising believe importance problem valuable idea true significance effect indicate realworld case model sort consider plausible impossible specific characterization probabilistic model result exhibit important use correct model sensitive result deviation attempt learn use model construction purpose paper practice use winnow binary exactly form consider rich probabilistic model use model selection phase study problem restricted control environment crucial understand nature significance fundamental problem reference blum empirical support winnow weight majority base result domain machine learn p m b rubin maximum likelihood incomplete datum royal statistical society b r d roth apply winnow international conference machine learn page robust single neuron learn theory page press light littlestone learn quickly irrelevant attribute new learn light littlestone redundant noisy attribute attribute error linear threshold learn use winnow comput learn theory page mp m l s perceptron mit
stochastic expectation propagation abstract expectation propagation deterministic approximation algorithm use perform approximate bayesian parameter learn approximate intractable posterior distribution set local approximation iteratively refine datapoint offer analytic computational advantage approximation variational inference vi method choice number model local nature appear ideal candidate perform bayesian learn large model setting crucial limitation context number approximate factor need increase number datapoint n entail prohibitively large memory paper present extension stochastic expectation propagation maintain global posterior approximation vi update local way experiment number canonical learning problem use synthetic realworld dataset indicate perform reduce memory consumption factor sep ideally suit perform approximate bayesian learn large model large dataset set introduction recently number method develop apply bayesian learn large dataset example include sample approximation distributional approximation include stochastic variational inference svi assume density filtering approach mix distributional sample approximation family approximation method attention regard expectation propagation construct posterior approximation iterate simple local computation refine factor approximate posterior contribution datapoint appear suited problem locality computation simple distribute good practical performance range small datum application suggest accurate local computation buy price memory overhead grow number datapoint n local approximate factor need maintain datapoint typically incur memory overhead global approximation exist broad class power algorithm include variational message pass contrast variational inference vi method global approximation refine directly prevent overhead scale case prefer method large datum believe certainly provide significantly accurate approximation know variational approach biased severely particular model variational objective nonsmooth likelihood function second fact truly local factor posterior likelihood mean afford different opportunity tractable design update simple approximate appear method choice application researcher attempt push scale approach large computational simply use large datum structure store approximate factor approach push far second approach use simple variant require global approximation maintain memory adf provide poorly calibrate uncertainty estimate main reason develop place idea complementary describe use approximating factor simple structure low rank reduce memory consumption factor d stop scaling idea use dataset use approximate factor collection datapoint result local update method use compute spirit extend sample method large dataset ep good world accurate global approximation derive truly local computation address question develop algorithm base standard algorithm maintain global approximation update local way class stochastic expectation propagation update global approximation stochastic estimate data subsample analogous way svi generalisation algorithm set directly relate svi importantly reduce memory factor n compare far extend method control granularity approximation treat model latent variable compromise accuracy unnecessary memory demand finally demonstrate scalability accuracy method number real world synthetic dataset expectation propagation assume density filtering begin briefly review ep adf algorithm new method base consider simplicity observe dataset iid sample probabilistic model px unknown ddimensional vector draw prior p exact bayesian inference involve compute typically intractable posterior distribution parameter datum pd p pxn p simple tractable approximating distribution refine goal refine approximate factor capture contribution likelihood term posterior pxn spirit approach find approximate factor minimise kullbackleibler divergence posterior distribution form replace likelihood correspond approximate factor pxn unfortunately update intractable involve compute posterior instead approximate procedure replace exact leaveoneout posterior approximate leaveoneout posterior cavity distribution couple update approximate factor update iterate detail iterate simple step factor select update remove approximation produce cavity distribution second corresponding likelihood include produce tilt distribution pxn update approximate factor minimise hope contribution posterior similar effect likelihood tilt distribution approximating distribution exponential family case reduce moment match step denote finally update factor include approximating distribution update procedure single factor critically approximation step involve local computation likelihood term treat time assumption choose factor fn refine compute cavity distribution compute tilt distribution pxn moment match inclusion q adf choose datapoint d compute cavity distribution compute tilt distribution pxn moment match inclusion q sep choose datapoint d compute cavity compute tilt distribution pxn moment match pn inclusion q implicit update fn figure compare expectation propagation assume density filtering adf stochastic expectation propagation sep update step typically algorithm use p appropriate local computation possibly require approximation far simple handle compare posterior pd practice perform update use approximate factor group datapoint run additional approximate inference algorithm perform update include datum suitable distribute approximate complicate deployment scale computation cavity distribution require removal current approximating factor mean implementation store explicitly necessitate memory option simply ignore removal step replace cavity distribution approximation result adf need maintain global approximation memory moment match step underlie approximating factor consider new form objective variance approximation shrink multiple pass dataset early stopping require prevent overfitting generally speak adf return uncertainty posterior section introduce new algorithm ep large memory demand avoid behaviour adf stochastic expectation propagation section introduce new algorithm inspire stochastic expectation propagation sep combine benefit local approximation tractability update global approximation reduce memory demand algorithm interpret version approximate factor tie alternatively correct version adf prevent overfitte key idea convergence approximate factor interpret parameterise global factor capture average effect likelihood posterior pxn spirit new employ direct iterative refinement global approximation comprise prior copy single approximating factor use update analogous order refine way capture average effect likelihood function posterior cavity distribution form remove copy factor second corresponding likelihood include produce tilt distribution q pxn find intermediate factor approximation moment match finally update factor include approximating distribution important update capture effect single likelihood function pxn instead damp employ partial update natural choice use interpret minimise pn p moment update choice appropriate include decrease accord condition cavity form divide capture average affect likelihood prevent posterior collapse maintain global approximation q p gaussian approximating factor use example reduce storage requirement od substantial saving enable model parameter apply large dataset algorithmic extension sep theoretical result motivate practical perspective limitation inherent section extend sep orthogonal direction relate svi describe figure detail supplementary material parallel sep relate fix point sep outline approximate likelihood time computationally slow simple update follow consider minibatch comprise m datapoint parallel batch update use m form cavity distribution likelihood identical parallel compute m intermediate factor intermediate factor tion update p nm fn m fm sep update use approximating distribution q p fold m m approximate factor fold m way understand parallel sep double loop inner loop produce intermediate approximation q combine outer loop q arg m m m parallel sep reduce original sep algorithm m parallel sep equivalent socalled average propose theoretical tool study convergence property normal ep work fairly restrictive condition likelihood function vary slowly function parameter converge fix point large datum limit connection sep approximating factor converge geometric average intermediate factor sep converge fix point learning rate satisfy condition certain condition fix point open question direct relationship stochastic power relationship variational method relationship variational inference stochastic variational inference mirror relationship relationship formal moment projection step replace natural parameter matching step result equivalent variational message pass supplementary material fix point variational inference minimise local variational kl divergence equivalent minimise global result carry new algorithm minor modification specifically vmp transform replace vmp local approximation global form employ supplementary material algorithm instance standard svi fix point vi satisfy condition generally procedure apply member power family algorithm replace moment projection step minimization b relationship xe point relationship vi vmp divergence update approximate factor stochastic method sep m average average expectation propagation large datum limit condition apply parallel update power parallel update parallel update parallel update variational variational message pass figure relationship note care need interpret supplementary material care limit case supplementary result weight view natural stochastic generalisation distribute sep control granularity approximation use approximation comprise single factor likelihood hand use approximation comprise signal global factor approximate average effect likelihood term sep approximation severe dataset contain set datapoint different likelihood contribution handwritten digit classification consider affect posterior sensible case partition dataset disjoint piece nk use approximating factor partition normal update perform subset treat single true factor approximate arrive distribute update challenge multiple likelihood term include update necessitate additional approximation simple alternative use partition imply posterior approximation form p k nk nk approximate limit case recover sep respectively sep latent variable application involve latent variable model main focus paper applicable case scale memory consider model contain hide variable associate observation pxn draw prior p goal q approximate true posterior parameter hide variable p d p p pxn typically approximate effect intractable term pxn p instead sep tie approximate parameter factor pxn p yield q p critically prove supplementary local factor need maintain memory mean advantage carry complex model involve latent variable potentially increase computation time case update analytic scratch update experiment purpose experiment evaluate number dataset synthetic realworld small large number model probit regression mixture network bayesian probit regression experiment consider simple bayesian classification problem investigate stability quality relation effect use minibatche vary granularity approximation model comprise probit likelihood function p gaussian prior hyperplane parameter p synthetic datum comprise datapoint dimensional sample single gaussian distribution fig mixture gaussian component fig b investigate sensitivity method homogeneity dataset label produce sample generative model follow measure performance compute approximation replace gaussian mean covariance sample draw posterior use sampler quantify calibration uncertainty estimation result fig indicate good perform method adf collapse delta function sep converge solution appear similar quality obtain dataset contain gaussian input slightly bad use use large minibatche fluctuate typically long converge small minibatche effect clear utility approximation depend homogeneity datum second dataset contain input fig b approximation find advantageous datapoint mixture component assign approximate factor generally find advantage retain approximate factor cluster dataset verify conclusion granularity approximation hold real dataset sample datapoint digit perform classification digit class assign global approximating factor compare loglikelihood test set use adf figure significantly outperform slightly bad initially reduce memory ep lose accuracy substantially increase end learning slightly empirical comparison report supplementary summary method likelihood function similar contribution posterior finally test sep performance small binary classification dataset uci machine learn repository consider effect minibatche granularity approximation use m run test damp stop learn convergence monitor update approximate factor classification result table adf perform reasonably mean classification error metric presumably tend learn good approximation posterior mode posterior variance poorly approximate return poor test loglikelihood score achieve significantly high test loglikelihood indicate superior approximation posterior variance attain sep perform similarly imply accurate alternative refine cheap global posterior approximation mixture gaussian cluster small scale experiment probit regression indicate perform probabilistic model main focus paper seek test flexibility method apply latent variable model specifically mixture gaussian synthetic dataset contain datapoint construct comprise b c figure bayesian logistic regression experiment panel b synthetic datum experiment panel c result mnist text detail table average test result method probit regression method appear capture posterior mode outperform adf term test loglikelihood dataset sep perform similarly australian mean error test mean sample gaussian distribution m cluster identity variable sample uniform categorical distribution mixture component isotropic pxn ep perform approximate joint posterior cluster mean cluster identity variable parameter assume known figure approximate posterior iteration method return good estimate mean collapse point estimate expect capture uncertainty return nearly identical approximation accuracy method quantify fig b compare approximate posterior obtain case approximate kldivergence measure analytically intractable instead use difference gaussian parameter fit method measure confirm approximate case probabilistic backpropagation final set test consider complicated model large dataset specifically evaluate method probabilistic backpropagation recent stateoftheart method scalable bayesian learn neural network model previous implementation perform iteration adf training datum moment match operation require intractable approximate propagate uncertainty synaptic weight forward network sequential way compute gradient marginal likelihood backpropagation adf use reduce large memory cost require available datum large perform experiment assess accuracy different implementation base adf ep regression dataset follow experimental protocol supplementary material consider neural network hide unit year protein use table average test rmse test loglikelihood method interestingly sep outperform setting possibly stochasticity enable find solution typically perform similarly memory reduction use b figure posterior approximation mean component posterior approximation cluster mean confidence level dot indicate true label topleft infer cluster assignment rest b error approximate gaussian mean covariance table average test result method dataset uci machine learn year rmse adf test sep instead large mb protein dataset mb year supplementary surprisingly adf outperform result present use nearoptimal number sweep iteration generally degrade performance good performance likely interaction additional moment approximation require accurate number factor increase conclusion future work paper present stochastic expectation propagation method reduce ep large memory consumption large dataset connect new number exist method include assume density filter variational message pass variational variational inference average experiment logistic regression synthetic real world mixture gaussian cluster indicate new method accuracy competitive experiment probabilistic backpropagation large real world regression dataset sep comparably reduce memory future experimental work focus develop method leverage approximation promise experimental performance minibatch update need theoretical understanding algorithm theoretical work study convergence property new algorithm limit result present systematic comparison algorithm variational method guide choose appropriate scheme application acknowledgement thank reviewer valuable comment thank foundation future fellowship support phd study acknowledge support foundation thank grant reference max distribute stochastic gradient mcmc proceeding conference machine learning icml page doucet scale markov adaptive approach proceeding conference machine learn icml page matthew d hoffman machine learn research probabilistic backpropagation scalable learn network expectation propagation way life teh distribute posterior sampling moment sharing nip minka expectation propagation approximate bayesian inference uncertainty artificial intelligence volume page manfre opper expectation approximate inference learn research rasmussen assess approximate inference binary machine learn research expectation propagation inference gaussian probability expectation propagation cambridge m variational message pass journal machine learn research page introduction variational method graphical model machine learn variational algorithm approximate problem variational expectation timeserie model d t editor chapter page cambridge university press probabilistic amplitude frequency shawetaylor p bartlett fcn weinberger editor advance neural information processing system page bayesian skill rating system advance neural information processing system page stochastic model estimation yuan gaussian process general likelihood uncertainty artificial intelligence method information geometry volume herbert robbin stochastic approximation method annal mathematical statistic page expectation propagation divergence measure message pass technical report cambridge sampler adaptively set path length hamiltonian machine learn research
maximumlikelihood continuity mapping malcom alternative hmms computer research application nm computer application nm abstract describe maximumlikelihood continuity mapping malcom alternative hide markov model hmms processing sequence datum speech hmms discrete hide space constrain fix architecture continuous hide continuity constrain smoothness requirement path space fit probabilistic framework speech recognition hmms represent realistic model speech production process evaluate extent capture speech production information generate continuous speech continuity map speaker use path predict measured speech datum median correlation obtain speech acoustic articulator measurement independent test set use train malcom predictor unsupervised model achieve correlation speaker articulator low obtain use analogous supervise method use articulatory measurement acoustic introduction hide markov model hmms generally consider state art strength hmm framework include rich mathematical foundation powerful training recognition algorithm large speech probabilistic framework incorporate statistical hmms know poor model speech production process speech production continuous temporally evolve process hmms treat speech production discrete system current state depend immediately precede state furthermore hmms design capture temporal information state transition probability maximumlikelihood continuity mapping malcom alternative hmms suggest transition probability replace constant value recognition result significantly deteriorate transition consider component speech conventional hmm framework poor capture transition information deficiency consider alternative hmm approach maintain strength improve weakness paper describe model maximumlikelihood continuity mapping malcom review general statistical framework speech recognition compare hmm formulation consider abstract hide state represent malcom demonstrate empirically path malcom hide space closely relate movement speech production articulator general framework consider unknown speech waveform convert module sequence acoustic vector space possible utterance task speech recognition return likely utterance observed acoustic sequence use baye rule correspond recognition px typically ignore constant w posterior estimate product prior probability word sequence probability observed acoustic generate word sequence prior estimate language model production probability px estimate acoustic model continuous speech recognition product term maximize paper restrict attention form model candidate correspond sequence model m m consider possible path hide space candidate utterance calculate px m dy path hidden space hide markov model hmms machine fix architecture path hidden space correspond series discrete state simplify integral sum avoid compute contribution possible path viterbi single path maximize frequently use loss recognition performance term correspond product emission probability acoustic state sequence typically estimate mixture second term correspond product state transition probability find second term contribute little recognition performance modeling power conventional hmm reside term train hmm system involve estimate emission e transition probability real speech datum standard computationally efficient iteratively estimate distribution maximumlikelihood continuity mapping malcom contrast hmms multidimensional malcom hidden space infinite number state path hmm constrain fix architecture malcom constrain notion continuity hidden path path smooth continuous carry energy cutoff frequency discrete path hmm smooth hide path attempt motion speech articulator continuity map cm know evaluate integral eq currently viterbi approximation approximate consider single path maximize likelihood acoustic model m result eq analogously term m correspond acoustic generation probability hidden path second term correspond probability hidden path utterance model paper focus term term produce conventional hmm common set n probability density function pdf define cm hide space model likelihood code vector quantization acoustic space pdf define lowdimensional space instead highdimensional acoustic space malcom require parameter estimate correspond hmm malcom turn develop algorithm estimate cm pdf corresponding path maximize likelihood time series acoustic c px extension method propose instead maximize use vowel datum single speaker start random smooth training algorithm generate cm iterate follow step maximize c reestimate smooth path maximize log likelihood function specify log likelihood function c dependence claim independence assumption claim depend small extent utterance expression continuity constraint describe natural claim depend path configuration time influence correspond acoustic conditional independence assumption note assume independent assume datum model assume conditioning independent currently develop model replace corresponding term conventional hmm formulation continuity mapping malcom alternative hmms example depend depend smoothness constraint depend assume independent utterance log transformation baye rule obtain l t model pyt probability density function pdf particular model depend assign use simple multidimensional gaussian pdf currently explore use multimodal mixture represent sound stop inverse map acoustic articulation unique need estimate obtain sum partition estimate calculate relative frequency acoustic code codebook pdf estimation step training use optimization reestimate mean pdf acoustic partition gradient respect mean pdf l t e covariance matrix pdf result paper use common symmetric covariance matrix pdf reestimate covariance matrix path optimization step optimization employ follow initial guess mean path configuration correspond observe acoustic e construct l consider eq n acoustic partition determine search direction optimization use example gradient perform line search direction press repeat step avoid potential degenerate solution pdf optimization step dimension cm orthogonalize furthermore scale continuity map meaningless topological arrangement matter n scale unit variance path optimization step path estimation step training use optimization reestimate gradient log likelihood function respect specific yt currently explore effect individual diagonal covariance matrix e optimization employ follow gradientbased algorithm initial guess path mean correspond observed acoustic sequence low pass filter construct consider eq t determine search direction optimization use example gradient press lowpass filter search direction use filter step perform line search filter direction press et repeat step convergence line search direction initial estimate contain energy cutoff frequency lowpass filter linear estimate contain energy cutoff frequency step implement desire smoothness constraint malcom path speech articulation evaluate claim path related articulator motion construct regression predictor measured articulator datum use training datum test quality predictor independent test set speech consist datum male female speaker datum obtain record technical use speaker articulatory measurement acoustic record sentence sentence second long acoustic record use microphone sample use bit resolution khz prior receive datum datum resample hz represent acoustic signal compact vector timeserie use sample msec frame new frame start msec overlap transform frame coefficient vector feature log feature consist window frame acoustic space use classical use code model vowel datum code model stop code combine articulatory datum consist coordinate tongue low lip figure illustrate approximate location datum originally sample hz resample match articulatory sample overlap acoustic frame sample articulatory datum lowpass filter remove measurement sentence use training set sentence evaluation separate cm generate speaker use training datum use cutoff frequency measure articulatory datum little energy hz dimensional continuity map use principal component capture variance correspond articulator datum acoustic representation scheme determined work model real articulator datum use maximumlikelihood continuity mapping malcom alternative hmms tongue middle t tongue tongue t low lop low lj t c figure approximate position speech articulation measurement term computationally complex approximate term second term constant training c calculate end iteration use term start decrease point start use term eq pdf path optimization step convergence criterion maximum movement mean path convergence criterion entire algorithm correlation path iteration pdf path optimization dimension usually iteration evaluate extent hide path capture information relate articulation use training set estimate nonlinear regression function output generate malcom corresponding measure articulator use ensemble hide unit multilayer train different training early stop partition training set result ensemble test set average linear regression produce result approximately bad report contrast unsupervised malcom method test supervised method articulatory datum available training evaluation involve pdf optimization step malcom path fix measurement result use path optimization step determine path test datum acoustic measure fraction supervise performance unsupervised malcom attain result conclusion result regression test set plot figure path median correlation actual articulator datum compare comparable supervised method use speech acoustic generate continuity map correlation real articulator measurement low corresponding supervise model use articulatory measurement acoustic malcom fit probabilistic framework hmms malcom hide path capture considerable information speech production process believe prove alternative hmm speech processing task current work emphasize develop word model complete malcom formulation test speech recognition system furthermore applicable task hmms apply d e l l l articulator dimension figure correlation estimate actual articulator trajectory independent test set average speaker bar performance supervised analogy horizontal line bar performance include text processing acknowledgment like thank mozer helpful comment manuscript share datum work perform energy reference cm neural network pattern recursive estimation maximization posteriori probability application connectionist speech tr improve hide markov model constrain maximumlikelihood approach speech recognition speech code technical report maximum likelihood continuity mapping technical report rubin p stochastic word constrain speech recognition synthesis submit society neural network statistical recognition continuous speech proceeding ieee dp cl neural network speech probabilistic method infer articulation speech co dept computer science preparation mtt meter system speech articulatory movement journal society numerical sj review continuous speech processing magazine
monotonicity hint computation neural email cs email hint piece information target function learn consider monotonicity hint state function learn monotonic input variable application hint demonstrate realworld problem credit card application task problem medical diagnosis measure monotonicity error candidate function define objective function monotonicity derive bayesian principle report experimental result use monotonicity hint lead statistically significant improvement performance problem introduction researcher pattern recognition statistic machine learning draw contrast linear model nonlinear model linear model strong assumption function model neural network assumption approximate smooth function hide unit extreme exist frequently neglect middle ground nonlinear model incorporate strong prior information obey powerful constraint monotonic model example occupy middle area model flexible linear model highly constrained application arise good reason believe target function monotonic input variable screen credit card instance expect probability default decrease monotonically monotonicity hint useful able constrain nonlinear model obey monotonicity general framework incorporate prior information learning establish know learn hint piece information target function available inputoutput example hint improve performance model reduce capacity sacrifice approximation ability invariance character recognition symmetry forecasting hint prove beneficial application paper describe practical application monotonicity hint method test noisy realworld problem classification task concern credit card application regression problem medical diagnosis derive bayesian principle appropriate objective function simultaneously enforce monotonicity fit datum section iii describe detail result experiment section analyze result discuss possible future work bayesian interpretation objective function let vector draw input distribution xj statement monotonically increase input variable x define xi mean decrease monotonicity define similarly wish define single scalar measure degree particular candidate function obey monotonicity set input variable natural measure use experiment section define follow way let input vector draw input distribution let index input variable randomly choose uniform distribution variable monotonicity hold define perturbation distribution draw distribution define s depend monotonically increase decrease variable monotonicity error input pair x measure violation monotonicity expectation respect random variable believe good possible approximation architecture use probably approximately monotonic belief quantify prior distribution candidate function architecture distribution represent priori probability density likelihood assign candidate function level monotonicity error probability function good possible approximation decrease exponentially increase monotonicity error positive constant indicate strong bias monotonic function addition obey prior information model fit datum classification problem network output represent probability class condition observation input vector possible class denote wish pick probable model datum equivalently choose maximize use m l cm regression problem interpret conditional mean observe output t observation assume reasoning classification case objective function maximize m t m bayesian prior lead familiar form objective function term reflect desire fit datum second term penalize deviation monotonicity hint experimental result database obtain machine learn repository maintain credit card task predict default case database contain feature describe class label indicate default ultimately occur meaning feature reason continuous feature use experiment report case history feature miss example omit leave use experiment class occur equal frequency split intuition suggest classification monotonic feature specific meaning continuous feature know assume represent quantity number year current job common sense dictate high low likely default equal monotonicity feature assert motivation medical diagnosis problem determine extent blood test sensitive disorder relate excessive specifically task predict number particular patient consume day result blood test patient history collect consist test result number number variable normalize variance normalization result easy interpret trivial performance obtain simply predict mean number patient irrespective blood test justification case base idea result test excessive mean high low experiment batchmode backpropagation simple adaptive learn rate scheme use method test performance linear perceptron observe benchmark purpose experiment use nonlinear method single hidden layer neural network hide unit direct inputoutput connection use credit datum hide unit direct inputoutput connection use liver task basic method test simply train network training datum optimize objective function possible technique try use validation set avoid overfitte training model perform maximize term objective function ie maximize loglikelihood datum minimize training error finally train network monotonicity constraint perform use approximation obtain follow credit datum liver datum previous iteration result increase likelihood learning rate increase likelihood decrease learning rate cut half procedure use order statistically significant comparison difference performance method datum randomly partition different way split train test credit datum training test liver datum result table average different partition early stop experiment training set far set credit datum liver datum use direct training second validation set credit datum liver datum classification error validation set monitor entire course training value network weight point low validation error choose final value process train network monotonicity hint divide stage meaning feature direction know priori direction determine train linear percept ron training datum iteration observe result weight positive weight imply increase monotonicity negative weight mean decrease monotonicity direction determine network train monotonicity hint credit problem approximation theoretical objective function maximize liver problem objective function approximate represent network particular pair input vector pair generate accord method describe section input distribution model joint gaussian covariance matrix estimate training datum input variable pair vector represent monotonicity variable generate yield total hint example pair credit problem pair liver problem choose optimization attempt choose somewhat arbitrarily simply high value greatly penalize ie monotonicity test error measure use pair vector variable train error calculate contrast monotonicity test error monitor twolayer network train inputoutput example figure test error monotonicity error training time credit datum network train training datum ie hint average different datum split monotonicity hint test error monotonicity error iteration number t iteration number figure violation monotonicity track overfitting occur train monotonicity error multiply factor figure easily visible figure indicate substantial correlation overfitting monotonicity error course train curve liver datum look similar omit space consideration linear net net train error test error hint test error table performance method credit problem performance method table early stop twolayer network overfit perform bad linear model early stop performance linear model twolayer network difference statistically significant similarity performance consistent thesis monotonic target function monotonic classifier think nonlinear generalization linear classifier twolayer network advantage able implement nonlinearity advantage cancel case overfitting result excessive unnecessary degree freedom monotonicity hint introduce unnecessary freedom eliminate network allow implement monotonic nonlinearitie accordingly modest clearly statistically significant improvement credit problem nearly result introduction linear net net train error test error hint test error table performance method liver problem monotonicity hint improvement translate substantial increase bank monotonicity hint significantly improve test error liver problem target variance explain conclusion paper monotonicity hint significantly improve performance neural network noisy realworld task note beneficial effect impose monotonicity necessarily imply target function entirely monotonic exist target function monotonicity hint result decrease model ability implement function penalty improve estimation model parameter decrease model complexity use monotonicity hint probably consider case target function think roughly monotonic training example limit number noisy future work include application monotonicity hint real world problem investigation technique enforce hint author thank song useful discussion reference learn hint neural network complexity hint dimension neural computation p simard lecun denker efficient pattern recognition use new transformation distance nip financial market application learn hint neural network market e
convexity latent social network inference computer abstract realworld scenario nearly impossible collect explicit social network datum case network infer underlie observation formulate problem infer latent social network base network diffusion disease propagation datum consider contagion propagate edge unobserved social network observe time node infected infect node infection time identify optimal network explain observed datum present maximum likelihood approach base programming l penalty term encourage sparsity experiment real synthetic datum reveal method recover underlie network structure parameter contagion model approach scale infer optimal network thousand node matter minute introduction social network analysis traditionally rely datum collect collect datum expensive traditional social network study typically involve limited number people usually emergence large scale social computing application massive social network datum available important setting network datum hard obtain network infer datum example population drug user man sex man hide collect social network population impossible network infer observational datum infer social network attempt past usually assume pairwise interaction datum available case problem network reduce decide include interaction pair node edge underlie network example infer network pairwise interaction email record simply reduce select right threshold edge v include network interact time dataset similarly infer network interaction protein cell usually reduce determine right threshold address problem infer structure unobserved social network setting consider diffusion process contagion product spread edge network observe infection time node infect ie observe edge contagion spread goal reconstruct underlie social network edge contagion diffuse think diffusion network process neighboring node switch state inactive active network activation propagate usually unknown unobserved commonly observe time particular node infect observe infect case information propagation discover new information write explicitly cite source observe time infect infect similarly disease spread observe people usually know infect marketing set observe people product adopt particular behavior explicitly know cause question assume network static time possible reconstruct unobserved social network diffusion place structure network develop convex programming base approach infer latent social network diffusion datum formulate generative probabilistic model fix network contagion spread network write likelihood observed diffusion datum network diffusion model parameter series step obtain convex program l penalty term encourage sparsity evaluate approach synthetic realworld email marketing dataset experiment reveal recover underlie network structure parameter propagation model approach scale infer optimal network node matter minute far relate work different line work connect research network structure learn estimate dependency structure direct graphical model probabilistic relational model formulation intractable reside heuristic solution recently graphical lasso method static sparse graph estimation extension time evolve graphical model propose lot success work similar sense infection time target node infection time node additionally work relate link prediction problem different sense line work assume network visible work closely related infer network cascade datum algorithm propose netinf assume weight edge network homogeneous connect node network neighbor probability assumption hold algorithm accurate computationally feasible remove assumption order address general problem furthermore approximation algorithm approach guarantee optimality easily handle network thousand node problem formulation propose method define problem infer latent social network base network diffusion datum observe identity infect node node know interval node infect source node infection unknown assume infected node previously infect previously infect connect latent social network try infer methodology handle wide class information diffusion model independent contagion model si model calculate maximum likelihood estimator mle latent network diffusion model equivalent problem efficiently solve problem formulation cascade model start introduce model diffusion process contagion spread network leave trace cascade assume population node let weight adjacency matrix network unobserved aim infer entry model conditional probability infection transmission aij p node infect node node infect temporal property type cascade especially disease spread govern transmission period transmission time model wt long infection transmit node recovery model model time long node infect recover node infect time infect node time separate infection time sample infection time node t t distribute wt similarly duration node infection sample general probability distribution strictly nonnegative support cascade c initiate randomly select node infect time let denote time infection node infected infect neighbor independently network probability govern specifically infected susceptible infect probability aij determine neighbor infect infection time newly infect neighbor sum interval time sample transmission time new infection sample independently node infect depend model different scenario happen model node susceptible infection time ri hand model node recover infect work mainly consider si model node remain infect ie recover ri important note approach handle model modification cascade c observe node infection time duration infection source node infection remain hide goal base observe set cascade infection time d infer weighted adjacency matrix aij model edge transmission probability maximum likelihood formulation let set observe cascade cascade let time infection node note node infect cascade let t denote set node infected state time cascade c know infection node result unknown previously infect node connect component likelihood function infection dependent previously infect node specifically likelihood function fix d p infect p cd likelihood function compose term consider cascade c infect time compute probability previously infect node infect node compute probability node infect note assume cascade infection conditionally independent case model node infect multiple time single cascade multiple observed value ic function include infection time product sum omit detail sake clarity maximum likelihood estimate solution subject constraint node infect diagonal strictly leave optimization problem variable scale large network problematic break problem independent subproblem variable observe incoming edge node infer independently incoming edge note restriction structure example general stochastic matrix column infer independently let node current node interest like infer incoming connection mle column ai model strength edge solution ai d subject constraint ai d cdic cdic lastly number variable far reduce observe infect cascade node mle exclude set variable dramatically reduce number variable practice true induce large cascade cause cascade sparse number node infect problem hessian function general find globally optimal mle difficult derive optimization problem equivalent mle problem guarantee convergence globally optimal solution allow use highly optimize programming method begin problem d subject change variable c problem c bi c ic cdic cdic subject c c c wj use shorthand notation note fix note constraint inequality instead equality constraint objective function strictly increase increase c inequality bind constraint solution ie equality hold reason use inequality turn constraint upper bind t furthermore change variable objective function problem satisfy requirement geometric program order geometric program apply change variable reciprocal objective function turn minimization problem finally logarithm objective function constraint leave follow optimization problem bi cdic cdic subject c c network sparsity general social network sparse sense average node connect constant number constant fraction node network encourage sparse mle solution l penalty term add original loglikelihood function objective function log sparsity parameter indicate include penalty function dramatically increase performance method apply process new augment objective function result function c concave problem nonconvex instead propose use pn penalty function penalty function promote sparse solution long geometric program objective function global convexity preserve c implementation use library solve likelihood optimization break network inference series subproblem correspond inference edge special concern need sparsity penalty function presence penalty function method extremely effective predict presence edge network effect distort estimate edge transmission probability correct inference problem solve l penalty result solution edge transmission probability set restrict remain problem relax sparsity parameter set preserve precision recall edge location prediction generate accurate edge transmission probability prediction implementation describe node network infer inside minute run matlab implementation find experiment section evaluate network inference method refer network inference range dataset network topology include generate network real social network simulate real diffusion datum experiment focus si model apply real datum use synthetic datum synthetic datum experiment begin construction network run direct network construct use erdosrenyi random graph network node edge case network construct unweighted graph edge assign uniformly random transmission probability aij transmission time model experiment assume model wt transmission time know experiment realistic model transmission t time e powerlaw weibull e argue weibull distribution describe propagation model notice model assumption structure example approach handle exponential powerlaw mode monotonically decrease t weibull distribution mode value generate cascade select random starting node infection infection propagate node new infection occur infected node transmit infection probability aij transmission occur time sample accord distribution cascade algorithm precision precision precision netinf recall pr curve netinf recall c pr curve recall pr curve num edge edge edge mse figure ac precision recall compare netinf si diffusion model run graph generate transmission time model use power law weibull network contain node weight edge sample uniform random distribution mle method pr curve generate vary sparsity parameter mean square error edge transmission probability algorithm dot green line indicate number edge true network form series correspond node infect problem easy generate cascade edge network transmit infection number cascade need depend underlie network overall generate order cascade node network performance assess performance consider accuracy edge prediction accuracy edge transmission probability edge prediction record precision recall algorithm simply vary value obtain network different number edge infer network compute precision number correctly infer edge divide total number infer edge recall number correctly infer edge divide total number edge unobserved network large value infer network high precision low recall low value precision poor recall high assess accuracy estimate edge transmission probability aij compute error mse mse union potential edge position node pair edge latent network edge position algorithm predict presence edge potential edge location edge present weight set comparison method compare approach netinf iterative base submodular function optimization netinf reconstruct likely structure cascade base reconstruction select likely edge network algorithm assume weight edge constant value nonzero value apply algorithm problem consider simply use netinf infer network structure estimate edge transmission probability aij simply count fraction time predict cascade propagate edge figure precisionrecall curve synthetic network transmission model result erdosrenyi random graph omit space restriction similar notice approach achieve break point point precision equal recall notable result especially careful generate cascade cascade mean evidence problem figure plot mean squared error estimate edge transmission number diffusion mse pr breakeven netinf number diffusion breakeven exp number diffusion c mse number diffusion d netinf runtime netinf pr breakeven point mse netinf noise signal ratio e pr network size f runtime network size figure precisionrecall breakeven point method function number observe cascade power law transmission distribution square error point function number observed cascade pr breakeven point perturbation size apply infection probability aij function number edge infer network line indicate point infer network contain number edge real network notice estimate edge weight error factor small error netinf algorithm course expect netinf assume network edge weight homogeneous case test robustness figure accuracy breakeven point edge mse function number observe diffusion effect noise infection time noise add cascade add independent normally distribution perturbation observe infection time noise signal ratio calculate average perturbation average infection transmission time plot robust perturbation accurately infer network noise signal ratio high experiment real datum real social network experiment realworld network consider small network scientist research network second experiment real email social network node edge base email communication small edge network simply randomly assign edge transmission probability email network number email send person person indicate connection strength let cascade network assume probability email contain fix person send person email probability infect parameter simply enforce minimum edge weight pair email set email network generate cascade use powerlaw transmission time model network use weibull distribution sample transmission time run network inference cascade figure result similarly synthetic network approach achieve break point dataset edge transmission probability estimation error ideal method capable perfect recovery underlie social network relatively small number contagion diffuse recall edge mse precision recall network estimation precision precision num edge edge weight error recall recommendation network figure precisionrecall curve network estimation error leave predict transmission probability function number edge predict middle row result email network row network right precisionrecall curve infer real recommendation network base real product real social network real cascade investigate large recommendation network consist people recommendation product people generate cascade follow node person v buy product p time recommend nod node buy product option recommend trace cascade small subset datum consider recommendation network user edge set recommendation different product set user edge transmission model unknown model powerlaw distribution parameter present result rightmost plot figure approach able recover underlie social network surprisingly accurately break point approach netinf score note approach second infer network ground truth edge transmission probability compare compute error edge weight estimation conclusion present general solution problem infer latent social network network diffusion datum formulate maximum likelihood problem solve equivalent convex problem guarantee optimality solution furthermore regularization use enforce sparse solution preserve convexity evaluate algorithm wide set synthetic realworld network different cascade propagation model find method general robust compete approach experiment reveal method recover underlie network structure parameter edge transmission model approach scale infer optimal network node matter minute possible future work include learn parameter underlie model diffusion time fruitful apply approach dataset spread news story break new marketing social website extend additional model diffusion infer model structure latent social network gain insight position role node play diffusion process assess range influence node network acknowledgement research support nsf grant iis grant b reference e xing recover network dependency social biological study t mathematical theory disease application emergence scaling random network science m m infer relevant social network communication page infer network structure use datum friedman t hastie r sparse inverse covariance estimation graphical lasso l friedman taskar learn probabilistic model link structure jmlr ghahramani learn dynamic bayesian network adaptive processing sequence datum structure page l protein interaction map science m krause infer network diffusion influence hill c marketing identify likely network statistical science r d et bayesian network approach predict interaction genomic datum science empirical analysis evolve social network science b dynamic marketing acm l dynamic news cycle page e view large network m pattern influence recommendation network page d link prediction problem social network page p graph variable selection lasso annal statistic page m c infer network mechanism protein interaction network m learn graphical model structure use path page l song m e xing timevarye dynamic bayesian network nip b taskar m p abbeel link prediction relational datum nip vert supervise graph inference nip m p ravikumar lafferty highdimensional graphical model selection use regularize logistic regression p different curve severe reveal similar impact control measure amer j social network analysis method application university press
expectation training multilayer neural network continuous discrete weight electrical engineering network mnn commonly train use gradient method probabilistic graphical model use variational baye method expectation propagation ep base approach use train deterministic mnn specifically approximate posterior weight datum use meanfield factorize distribution online set use online central limit theorem find analytical approximation baye update posterior result baye estimate weight output different origin result expectation similar form efficiency additional advantage training initial condition prior mnn architecture useful largescale problem parameter tuning major challenge weight restrict discrete value especially useful implement train mnn precision limited hardware chip improve speed energy efficiency order test numerically binary text classification task task ebp outperform standard optimal constant learn rate previously report state art interestingly mnn binary weight usually perform mnn continuous real weight average mnn output use infer posterior introduction recently multilayer network deep architecture achieve stateoftheart performance supervised learning task network massive require large computational resource dense fast efficient hardware implementation train mnn build weight restrict discrete value example binary weight chip perform operation second power efficiency performance enable integration massive mnn small electronic device traditionally mnn train minimize error function use backpropagation relate gradient descent method approach directly apply weight restrict binary value discretization weight usually ie single layer weight method suggest clear approach scalable efficient method develop train singlelayer neural network snn binary weight use approximate bayesian inference implicitly explicitly theory prior baye estimate weight find posterior datum store update posterior usually intractable circumvent problem previous work use factorize meanfield form posterior weight datum explain use special case widely applicable expectation propagation additional approximation fanin neuron large input approximately gaussian error function analytically obtain baye estimate weight output use factorize approximation posterior good knowledge unknown approach generalize mnn relevant practical application work derive generalization use similar approximation section end result expectation backpropagation section algorithm online training mnn weight value continuous ie real number discrete eg binary notably training learning rate insensitive magnitude input algorithm similar efficient update linear computational complexity number weight test section supervised learning task high dimensional task classify text semantic class low dimensional medical discrimination task use mnn weight layer ebp outperform standard previously report state art task interestingly good performance usually achieve use baye estimate output mnn binary weight estimate calculate analytically average output mnn weight sample infer posterior preliminary general notation boldface letter x denote column vector component xi boldface letter denote matrix component index component denote denote denote p x probability distribution discrete case density continuous case random variable p p x p var cov x x integration summation discrete case condition use indicator function ie hold kronecker function mean covariance matrix denote density n furthermore use cumulative distribution function model consider general connection adjacent layer fig analytical simplicity focus deterministic binary neuron framework straightforwardly extend type neuron deterministic stochastic mnn layer width layer l collection synaptic weight matrix connect layer sequentially output layer l input layer l hidden layer vl output layer layer sign vl sign activation function neuronal layer operate ie sign sign output network g sign sign sign ie single layer weight assume weight constrain set specific restriction weight denote wijl s wijl disconnect simplicity assume layer constant bias optionally include standard way add constant output layer task examine supervise classification learning figure mnn model task bayesian framework fix set sequentially label datum pair data point label binary set brevity suppress sample index clear context common supervised learn mnn assume relation represent mnn know architecture hypothesis class unknown weight reasonable assumption mnn approximate deterministic function sufficient number neuron specifically exist s eq goal estimate probable mnn estimate probable possibly unseen x theory section explain specific learn mnn describe section arise approximate meanfield bayesian inference use context describe section online bayesian learn mnn approach task bayesian framework assume prior distribution weight p aim find p wdn posterior probability configuration weight w datum posterior select probable weight configuration maximum posteriori map weight estimate minimize expect loss weight weight estimate implement single mnn provide estimate label possibly unseen datum point alternatively aim minimize expect loss output commonly mnn literature example aim reduce classification error use map output estimate g p minimize loss w output result estimator generally form mnn s approximate average output mnn w value sample posterior note average output mnn common method improve performance aim find posterior p wdn online setting sample arrive sequentially nth sample receive posterior update accord baye rule p wdn p p wdn note mnn deterministic likelihood data point follow simple form p mnn stochastic activation function smoothed version baye update eq simply sure p wdn configuration w yk word posterior equal prior restrict legal weight domain renormalize appropriately unfortunately update generally intractable large network mainly need store update exponential number value wdn approximation require approximation meanfield order reduce computational complexity instead store p wdn store factorize meanfield approximation p wdn p wdn p factor normalize notably easy find map estimate weight factorize approximation wijl p wijl factor p find use standard variational approach perform baye update p wdn instead p wdn project result posterior family distribution factorize eq minimize reverse kullbackleibler divergence similarly straightforward calculation optimal factor marginal posterior appendix available supplementary material perform marginalization baye update rearrange term obtain update marginal p wijl p dn p dn p dn p wijl p wkrm dn marginal likelihood directly update factor p wijl single step equation problematic contain generally intractable summation exponential number value require simplification simplicity replace p p slight abuse notation mind distribution approximate simplify marginal likelihood order able use update rule eq calculate marginal likelihood p dn use eq brevity suppress index dependence dn obtain p p wkrm p wijl recall p simply indicator function eq assumption p arise feedforward mnn input x output vl perform summation eq convenient way layer layer define m wkrm p wkrm r r p vl define identically p summation perform configuration wijl fix wijl set p wijl write recursively p p m vm p vm p p vl p vl m l l l vm vm obtain result eq p ywijl p ywijl computation generally intractable summation eq exponential number value need additional approximation approximation large fanin simplify summation eq assume neuronal fanin large mind specific index fix weight weight wijl treat independent random variable mean limit infinite neuronal fanin m km use central limit theorem normalize input neuronal layer distribute accord gaussian distribution m km m m km actually approximation common effective use approximation eq sign eq calculate appendix distribution sequentially layer m value v wijl effectively simplify summation use gaussian integral appendix b end forward pass able find p ywijl p ywijl polynomial number step appendix b instead direct calculation eq exponentially hard use p ywijl eq update distribution p wijl immediately baye estimate weight output note computational complexity forward pass significantly low m diagonal true exactly special case example true hidden neuron fanout layer network single output order reduce computational complexity case m diagonal approximate distribution factorize meanfield version recall optimal factor marginal distribution appendix find p ywijl easily appendix component m direct calculation p ywijl computationally repeat similar calculation time order improve exploit fact large approximate ln p ywijl use expansion wijl mean order order term expansion calculate use backward propagation derivative term p km similarly forward pass m backward pass l obtain p ywijl wijl update p wijl expectation backpropagation algorithm use result efficiently update posterior distribution p weight operation accord eq summarize result general expectation ebp appendix apply algorithm special case mnn binary real continuous weight similarly original review input desire output perform forward pass calculate mean output layer perform backward pass update p weight forward pass pass perform forward calculation probability recall mean posterior distribution p wkrm dn initialize mnn input xk calculate recursively follow quantity m km km km r wkrm m r m m respectively mean variance input layer m result mean output layer m backward pass pass perform baye update posterior eq use expansion recall eq initialize l calculate m ln p dn ln p c kl constant depend wijl output use posterior distribution optimal configuration immediately find map weight estimate wijl ln p wijl output mnn implement weight eq define deterministic ebp output additionally map output eq calculate directly yk ln p ln use ensemble average output possible mnn weight sample estimate posterior p define output probabilistic ebp output ebpp note case single output output simplifie sign numerical inaccuracy calculate use eq generate value large happen use instead asymptotic form limit numerical experiment use high dimensional text dataset assess performance supervised binary classification task dataset contain binary task dataset amazon newsgroup spam specification example m feature result algorithm describe table detail datum include datum extraction labeling find test performance ebp mnn layer architecture m bias weight examine special case mnn real weight mnn binary weight real bias recall motivation section efficiently implement hardware real bias negligible cost recall type mnn output deterministic ebpp probabilistic explain evaluate result compare report stateoftheart result test dataset traditional use train m mnn real weight case use cross entropy mean square error mse loss function dataset report result loss function achieve minimal error use simple parameter scan regularization parameter traditional rate parameter result optimal parameter ie achieve good result report table optimal parameter find edge scan field lastly demonstrate effect naive quantization report performance mnn weight bias clip use sign function train dataset repeatedly present epoch algorithm additional epoch reduce test error epoch example random order determine order test result calculate use fold crossvalidation similarly empirically ebp run time similar real weight twice slow binary weight additional implementation detail e code available author website minimal value achieve epoch summarize table dataset ebpp perform perform ebpp usually binary weight appendix e ranking remain true fanin small contrast assumption deep layer architecture use dataset example feature real real ebpp binary binary ebpp clip news news spam d spam d news group comp group amazon review table datum specification test error fold crossvalidation good result boldface discussion motivate recent success mnn develop expectation backpropagation section approximate bayesian inference synaptic weight mnn supervise classification task label training datum prior weight deterministic online algorithm use train deterministic mnn need tune learn parameter learning rate furthermore synaptic weight restrict set finite binary number infinite eg real number open possibility implement train mnn hardware device require limited parameter precision algorithm essentially analytic approximation intractable baye calculation posterior distribution weight arrival new data point simplify intractable baye update rule use approximation approximate posterior use product marginal mean field approximation second assume neuronal layer large fanin approximate gaussian approximation baye update calculate polynomial time size mnn order far improve computational complexity w step additional approximation use large fanin perform order expansion second optionally perform second mean field approximation distribution neuronal input finally obtain posterior use baye estimate probable weight output find analytically previous approach obtain baye estimate limited purpose achieve stateoftheart performance small mnn scale laplace approximation variational baye base method require realvalue weight tune learning rate parameter stochastic neuron smooth likelihood previous message pass special case base method derive contrast ebp allow parameter free scalable training type mnn deterministic stochastic discrete eg binary continuous weight appendix continuous weight ebp identical standard specific choice activation function loss learn rate difference input normalize standard deviation eq right depend weight input rescaling learning amplitude change neuronal input result invariance sign activation function note performance directly affect amplitude input recommend practice rescale preprocesse numerically evaluate algorithm binary classification task use mnn synaptic layer datum set ebp perform standard optimal constant learning rate achieve stateoftheart result comparison surprisingly usually perform use train binary mnn suggest reviewer relate type problem examine text classification task large sparse input space bag word feature word important real value frequency distribution binary weight threshold activation function work order good performance binary mnn average output infer approximate posterior weight ebpp output calculate average analytically hardware output realizable average output binary mnn sample weight p efficiently appendix numerical testing mainly focus highdimensional text classification task shallow architecture work domain vision speech deep architecture achieve stateoftheart performance deep mnn usually require considerable additional trick unsupervised pretraine weight share momentum integrate method ebp use train deep mnn promising direction future work important generalization algorithm straightforward use activation function sign particularly important layer linear activation function useful regression task joint activation function useful task acknowledgment author grateful r helpful discussion review manuscript research partially fund fund computational intelligence foundation approximation require neuron mnn fanout depart online framework consider require sample update ie activation function xi softmax argmax reference r efficient supervise learn network binary synapsis d c m bishop ensemble learning multilayer network advance neural information processing system page r g training neural net search ieee transaction neural network m network pattern m machine learn springer r learn message pass network discrete synapsis physical review letter k m adaptive regularization weight vector machine learn e contextdependent pretraine deep neural network audio speech language process practical variational inference neural network advance neural information process system page e hinton neural network simple minimize description length weight colt g e r v p nguyen deep neural network acoustic modeling speech recognition share view research group signal processing magazine ieee approximation capability multilayer feedforward network neural network r r stochastic array processor ieee sensor krizhevsky hinton imagenet classification deep convolutional network nip l bottou learning apply document recognition proceeding ieee l bottou r efficient editor neural network trick trade springer c practical bayesian framework backpropagation network neural computation e constructive training method feedforward neural network binary weight international system t p minka expectation propagation approximate bayesian inference nip page p e neural network adaptation hardware implementation handbook neural computation m neal bayesian learn neural network m opper expectation propagation factorize distribution performance result simple model neural computation d e training feed forward net binary weight modify system s solla optimal perceptron learn online bayesian approach online learn neural network cambridge university press cambridge srivastava simple way prevent neural network overfitte machine learn bayesian prediction use context bioinformatics
electrical computer engineering electrical computer engineering statistical electrical computer engineering new levy process prior propose collection covariatedependent measure model process available covariate handle efficiently covariate assume observe datum sample customer latent covariate learn feature dish customer select dish infinite buffet manner analogous beta process add constraint customer decide probabilistically consider dish base distance covariate space customer dish customer consider particular dish dish select probabilistically beta process beta process recover limit case efficient gibb sampler develop computation stateoftheart result present image processing music analysis task introduction feature learning important problem statistic machine learning characterize goal typically infer lowdimensional set feature representation highdimensional datum desirable perform analysis nonparametric manner number feature learn set powerful tool learning process datum sample serve customer potential feature serve dish recently demonstrate correspond marginalization betabernoulli process construction find significant utility factor analysis wish infer number factor need represent datum interest beta process develop originally levy process prior hazard measure recently extend use feature learn interest paper refer measure beta process example levy process example process normalize gamma process know dirichlet process key characteristic model datum sample assume meaning datum change model author contribute equally work important line research concern removal assumption allow incorporation covariate coordinate available datum example introduce dependent dirichlet process context feature learn phylogenetic remove assumption sample impose prior knowledge relationship tree structure form tree constitute result covariate available sample tree necessarily unique dependent model introduce recently hierarchical process use account covariate dependence use gp constitute challenge largescale problem recently dependent hierarchical beta process develop yield encouraging result disadvantage assign kernel datum sample scale number sample increase paper develop new levy process prior term kernel beta process yield number covariatedependent measure beta process special case model interpret covariate feature dish index generative process nth datum sample covariate select feature view twostep process nth customer data sample decide examine dish draw zni xi parameter infer define meaning covariate space kernel design satisfy xi kxi xi xi xi second step customer draw feature associate dish employ datum sample parameter xi infer model compute posterior distribution model parameter number kernel require represent measure define number feature employ buffet typically small relative data size significant computational saving relative complexity model tie number data sample small number feature ultimately employ addition introduce new levy process examine property demonstrate efficiently apply important datum analysis problem hierarchical construction fully conjugate admit convenient complicated sample method require method demonstrate utility model consider application stateoftheart performance demonstrate compare relevant method kernel review beta bernoulli process beta process b distribution positive random measure space positive function b base measure define beta process example levy process levy measure d c c d draw draw set point poisson process measure yield b unit point measure b discrete measure probability r r infinite sum consequence draw poisson p atom d additionally set nth draw process define zn bni bni set draw use define feature utilize represent nth data sample feature employ bni marginalize measure analytically yield conditional probability zn correspond indian buffet process covariatedependent levy process betabernoulli construction measure b employ generation zn imply n sample probability use respective feature assume n sample interest associate set covariate denote respectively xn wish impose sample n similar covariate probable employ similar subset feature covariate distinct probable feature sharing manifest generalize consider b stochastic process random function draw independently p dependent collection levy process measure specific covariate xi constitute general specification interesting special case example consider x monotone differentiable link function r model gaussian process relate kernelbase construction choose potentially use model break probit logistic kernel stickbreake process remainder paper propose special case design term kernel beta process kbp characteristic function beta process recall b beta process measure space characteristic function satisfy d d subset beta process particular class levy process d d define let x assume x x specific example radial basis function kx r let represent random variable draw probability measure h support random variable draw appropriate probability measure q support eg context radial basis function draw probability measure support r define new levy measure d d levy measure associate beta process define assume parameter xi draw measure follow measure constitute bx evaluate covariate finite set covariate define random t random variable draw q respectively set b evaluate covariate set yield p process levy expression covariatedependent sure characteristic function arbitrary set covariate satisfy dx d d proof provide supplemental material additionally notational convenience draw valid covariate denote b h q c define d d relationship betabernoulli process covariatedependent measure employ define covariatedependent feature usage generalize measure p note equivalent distribution express model yield twostep generalization generative process betabernoulli process discuss introduction condition high probability observe covariate covariate deem attractive intuitive generative process come result rigorous levy process construction property summarize property b subset draw covariate bx b b b d b d r kx result reduce original assume c c c r constant let kx x represent infinitedimensional vector fix kernel parameter bx assume condition meet practice truncate number term use expression clearly impose desire property high correlation proof property provide supplemental material application model construction develop covariatedependent factor model generalize consider covariate consider datum rm associated covariate n factor loading factor model play role dish buffet analogy model datum wn q g m gamma prior place represent pointwise product m represent m m identity matrix dirichlet process base measure m m kbp base measure mixture atom factor loading application consider important atom reuse different point covariate space allow repeat structure manifest function space time image music application respectively column define respectively b vector note b draw kbp draw evaluate b define implement kbp truncate sum t term draw correspond set c set large model infer subset significant amplitude estimate number factor need representation datum practice let multinomial distribution discrete finite set respectively location kernel parameter detail discuss specific example ith column denote di draw b b draw dirichlet process multiple way perform dp clustering apply scheme assume series draw b successive conditional distribution follow form d g l unique dictionary element share column d dj model inference indicator variable ci introduce ci l probability proportional ci equal probability control ci l di value m new load introduce draw prior m extension relatively straightforward include additional model example consider context example specifically application inappropriate assume gaussian model noise residual section consider follow noise model n m n mn m gamma prior place p m term mn account noise potentially large amplitude represent probability noise data sample type noise model consider compare inference model inference perform gibbs sampler limited space variable update equation distinct bpfa include assume truncation level number dictionary element number unique dictionary element value current gibb iteration application consider paper xi define base euclidean distance xi t xi update multinomial distribution define q h respectively set discretized value uniform prior detail discuss update l l l l n x l x l l bni m p l di update ci l previously use new dj dk pi realize normalize equation update update component xi xi calculate normalize constraint update introduce set auxiliary variable zni zni datum assume zni zni xi specific zni bni zni bni bni equation derive conditional distribution x zni zni beta result hyperparameter setting corresponding prior set gamma concentration parameter prior gamma experiment number dictionary element truncate number unique dictionary element value initialize initialize initialize update set uniform prior q remain variable initialize randomly parameter tuning optimization perform music analysis consider music piece describe day life pepper heart band acoustic signal sample khz divide contiguous frame dimensional coefficient extract frame figure typical goal music analysis infer music piece function time audio datum mfcc vector associate time index use covariate finite set temporal sample point covariate employ define library xi uniform distribution set iteration collect sample iteration figure b frequency number unique dictionary element use datum base collect sample figure c frequency number total dictionary element use model define sparse indicate importance element data n vector wn normalize feature value frequency calculate collect sample frequency calculate collect sample observation index number unique dictionary element number dictionary element datum b c figure feature use music analysis horizontal axis correspond time day life base gibbs collection sample frequency number unique dictionary element total number dictionary element gibbs sample use compute correlation matrix associate time point music finally matrix average collection sample yield correlation matrix relate music fair comparison method model propose use hmm compute correlation window time divide piece multiple consecutive window temporal window include consecutive feature vector compute average correlation coefficient feature pair window temporal window total temporal window note sequence figure dimension correlation matrix accordingly compute correlation matrix propose model present figure compare performance result base bpfa covariate employ result dynamic clustering model dynamic hmm employ dynamic hdp dhdp use hmm bpfa result correspond replace correlation matrix compute bpfa figure b c respectively result yield reasonably good segmentation music unable infer subtle difference music time example voice music cluster different bpfa capture localized information music probability dictionary usage temporal position manifest good music segmentation contrast model yield good music segmentation capture subtle difference music time voice note use dp allow repeat use dictionary element function time covariate important repetition structure piece music observe segmentation sequence index sequence index sequence index sequence index sequence index sequence index c figure inference relationship music function time compute correlation weight b base state usage hmm c result day life result c author paper bpfa c image interpolation denoise consider image interpolation denoise additional potential application example image divide n overlap patch patch stack vector length m constitute observation rm covariate represent patch coordinate space probability measure correspond uniform distribution center patch image recover base average collection sample pixel average overlap patch reside example gibbs sample run discard image interpolation observe fraction image pixel sample uniformly random model infer underlie dictionary d presence miss datum weight dictionary element require represent observed component use infer dictionary associate weight readily miss pixel value table present average psnr value recover pixel value function fraction pixel observe table mean pixel miss uniformly random comparison model base base propose generally perform particularly large fraction miss propose algorithm yield result comparable employ covariate propose kbp construction significant computational advantage require kernel center location covariate model kernel image patch scale large image table comparison interpolate image pixel miss uniformly use standard image row cell result respectively result pixel observe select uniformly ratio man p l barbara m ill example figure image corrupt noise sparse consider sparse noise exist particular pixel select uniformly random amplitude distribute uniformly pepper image pixel corrupt noise standard deviation house image pixel corrupt noise standard deviation compare different method image model bpfa model augment term noise bpfa original bpfa model model propose denoise result visual quantitative evaluation result comparable significant computational age discuss note covariate kbp yield mark improvement application relative bpfa figure denoise result column noisy image psnr pepper house second column result infer bpfa model pepper house dictionary element column reconstruction column fourth fifth column result bpfa psnr pepper house sixth seventh column result pepper house case dictionary order base frequency usage start topleft summary new levy process beta process develop problem bayesian feature learn example result present music analysis image denoise image interpolation addition present theoretical property model stateoftheart result realize learning task inference perform gibbs sampler analytic update equation concern computational cost problem example model require second gibb iteration require second result run pc cpu research report support onr reference d levy process stochastic press stickbreaking process biometrika t ferguson bayesian analysis nonparametric problem annal statistic l griffith infinite latent feature model indian buffet process nip l nonparametric baye estimator base beta process model life history annal statistic process d infinite sparse factor analysis infinite independent component analysis independent component analysis signal separation dependent nonparametric process proceeding section statistical science griffith m process nonparametric prior latent feature rasmussen process machine learning logistic stickbreaking process machine learn l dynamic nonparametric bayesian model analysis music rodriguez nonparametric bayesian model probit stickbreaking process sethuraman constructive definition dirichlet prior r m hierarchical beta process indian buffet process aistat p buffet process aistat m h nonparametric bayesian dictionary learn sparse image representation nip m l dependent hierarchical beta process image interpolation denoise aistat
lattice filter model visual pathway drive abstract early stage visual processing think decorrelate whiten incoming temporally vary signal motivate cascade structure lateral lgn primary visual cortex propose model function use lattice filter signal processing device stagewise decorrelation temporal signal lattice filter model predict neuronal response consistent physiological recording cat primate particular predict temporal receptive field different type resemble socalle lag nonlagge cell lgn connection weight lattice filter learn use hebbian rule stagewise sequential manner reminiscent sequence mammal addition lattice filter model visual processing insect lattice filter useful abstraction capture temporal aspect visual processing sensory face ongoing stimulus world transmit information possible rest brain task sensory modality vision dynamic range natural stimulus order greatly exceed dynamic range relay neuron order magnitude reason high fidelity transmission possible continuity object physical world lead correlation natural stimulus imply redundancy turn redundancy eliminate compression perform end visual system lead reduction dynamic range compression strategy appropriate redundant natural stimulus predictive code predictive code prediction incoming signal value compute past value delay circuit prediction subtract actual signal value prediction error transmit absence transmission noise compression original signal decode receive end invert encoder prediction accurate dynamic range error small natural stimulus minimize dynamic range use predictive code reduce optimize prediction experimental support view end visual system predictive encoder come measurement receptive field particular predictive coding suggest natural stimuli temporal receptive field spatial receptive field centersurround prediction bear experimental measurement retinal cell lateral fly second order visual neuron large cell addition experimentally measure receptive field vary signaltonoise ratio expect optimal prediction theory furthermore experimentally observe whitening transmit signal consistent remove correlated component incoming signal natural stimulus contain correlation time scale great millisecond experimentally measure receptive field lgn neuron equally long decorrelation long time scale require equally long delay extended receptive field produce biological neuron synapsis time constant typically millisecond field signal processing offer solution problem form device lattice filter decorrelate signal stage sequentially add long long delay motivate cascade structure visual system propose model decorrelation lattice filter naturally visual system complex lattice filter perform operation lattice filter model explain exist observation vertebrate invertebrate visual system testable prediction believe lattice filter provide convenient abstraction model temporal aspect visual process paper organize follow briefly summarize relevant result linear prediction theory second explain operation lattice filter discrete continuous time compare lattice filter prediction physiological measurement linear prediction theory nonlinear nature neuron synapse operation neural circuit vertebrate invertebrate describe linear system theory advantage system optimal circuit parameter obtain analytically result intuitively clear surprisingly field signal processing rely heavily linear prediction theory offer convenient framework summarize result linear prediction use explain operation filter consider scalar sequence yt time suppose yt time point depend information provide vector goal generate series linear prediction yt vector yt define prediction error et yt yt yt look value minimize mean squared error yt zt weight vector optimal prediction sequence sequence z prediction error sequence e z orthogonal component vector series advance ie offline set socalled normal equation solve example processing neuroscience application setting online relevant time step prediction yt use current value zt w furthermore prediction update base prediction yt observe yt online set algorithm stochastic gradient descent use time step update direction negative gradient yt zt lead follow weight update know mean square lm predict sequence sequence et zt learning rate value represent relative influence recent observation compare distant large learning rate fast system adapt recent observation past remember paper interested predict current value sequence past value restrict prediction order k t t problem special case online linear prediction framework gradient update lm find weight optimize linear prediction filter wk long temporal extent difficult implement neuron synapsis lattice filter way generate long receptive field circuit biological neuron use cascade architecture know lattice filter calculate optimal linear prediction temporal sequence transmit prediction error section explain operation discretetime lattice filter adapt continuoustime operation discretetime implementation stage lattice filter figure calculate error order optimal prediction ie use precede element sequence second stage use output stage calculate error second order optimal prediction ie use previous value stagewise error computation possible lattice filter calculate stage error optimal prediction past value forward wk error optimal prediction past value recent value backward error bkt k weight optimal prediction example stage filter calculate forward error ft optimal prediction u backward error optimal prediction figure assume coefficient optimal linear prediction know return learn follow stage lattice filter perform operation input figure stage receive backward bk error previous stage delay backward error time step compute forward error ftk t optimal linear prediction ftk addition stage compute bk t optimal linear prediction bk t figure lattice filter contain forward prediction error backward prediction error branch interact stage crosslink operation lattice filter characterize linear filter act input compute forward backward error consecutive order predictionerror filter blue bar figure delay backward error branch temporal extent filter grow stage stage section argue predictionerror filter correspond measurement temporal receptive field neuron detailed comparison physiological measurement use result predictionerror filter figure bar forward predictionerror filter large weight absolute value combine weight remain coefficient correspond filter similarly backward predictionerror filter bar great weight rest combine fact arise observation forward predictionerror filter minimum phase backward predictionerror filter maximum phase figure discretetime lattice filter perform stagewise computation forward backward prediction error stage optimal prediction compute delay input time step multiply u upper summation unit subtract predict actual value output prediction error similarly optimal prediction compute multiply input low summation unit subtract optimal prediction actual value output backward error following stage optimal prediction ftk compute delay time step multiply u upper summation unit subtract prediction actual ftk compute output prediction error ftk similarly optimal prediction bk t multiply u low summation unit subtract optimal prediction actual value output backward error bkt black connection weight red connection learnable negative weight view forward backward error calculation application socalled predictionerror filter blue input sequence note temporal extent filter long stage stage derive learning rule find optimal coefficient u v online set bk use predict ftk t t obtain error substitute ft ft update ftk t similarly update bkt ftk interestingly update weight product activity incoming node corresponding crosslink update know hebbian learning rule think use biological neuron finally simple proof offline set entire sequence know equation error optimal kth order linear prediction let step time delay operator induction statement bk kth order forward backward error optimal linear prediction equivalent form bk normal equation satisfy d true directly follow definition assume true true k easy form bk correct form regard orthogonality use induction assumption remain note error optimal linear prediction x desire case prove similarly continuoustime implementation remain model neuronal circuit operate continuous time lattice filter discretetime operation obtain continuoustime implementation lattice filter simply time step size predictionerror filter short adapt discretetime lattice filter operation step introduce use polynomial shift operator generate basis function figure input signal pass integrator constant define distinct time step delay stage replace allpass filter l constant preserve magnitude fouri component input shift phase frequency dependent manner allpass filter reduce single timestep delay optimality general filter prove similarly discretetime filter simply replace operator d l proof section figure continuoustime lattice filter use polynomial compare discretetime version contain integrator l replace delay allpass filter second obtain continuoustime formulation lattice filter replace t t define inverse time scale t limit t fix result l l discrete continuous time l l representative impulse response continuous filter figure note similarly discretetime case area peak phase great area second rebound phase forward branch opposite true temporal extent rebound great peak forward branch basic discretetime implementation backward branch section prediction confirm physiological recording experimental evidence lattice filter visual pathway section demonstrate physiological measurement visual pathway vertebrate invertebrate consistent prediction lattice filter model purpose model visual pathway identify summation unit lattice filter neuron propose neural activity represent forward backward error fly visual pathway neuronal activity represent continuously vary grade potential vertebrate visual system neuron start ganglion cell spike identify firing rate activity lattice filter mammalian visual mammal visual processing perform stage synapse cell turn synapse retinal ganglion cell send axon lgn synapse lgn relay neuron project primary visual cortex addition feedforward pathway stage local circuit involve usually inhibitory horizontal cell neuron class come type differ connectivity physiological response complexity circuit pose major challenge visual connection lgn cortex temporal time figure distribution geniculate cell simple cell respect timing response distribution parameter derive impulse response geniculate cortical neuron peak time b time c rebound index peak time time strong response phase impulse response time time second phase rebound index area impulse response crossing divide area crossing impulse response good signal noise include sd baseline figure measure temporal receptive field progressively long cat visual pathway leave cat lgn cell red long receptive field correspond cell blue adapt report population datum right extent temporal receptive field simple cell cat v great correspond lgn cell quantify peak b time right temporal receptive field cat lgn cell peak strong weak rebound adapt simple cell geniculate cell differ temporal parameter measure considerable overlap distribution fig overlap raise follow question connectivity depend geniculate cortical response match respect time instance simple cell fast subregion early time peak early crossing receive input geniculate cell fast center figure illustrate visual response geniculate cell simple cell connect strong positive peak observe correlogram time window emphasize short latency fast rise time case central subregion geniculate center precisely peak subregion timing visual response subregion similar onset msec peak msec worth note central subregion simple cell fast strong lateral subregion response central subregion match timing contrast timing lateral subregion resemble closely timing geniculate peak example figure considerable number pair produce response different timing example figure illustrate case fully overlap strong simplecell subregion sign slow timing lgn onset onset msec peak msec pair neuron flat indicate absence connection right examine role timing connectivity measure response time course cell pair meet criterion overlap simplecell subregion sign second overlap cortical subregion nearoptimal position relative overlap material method fig cell pair high probability connected precise match position sign connect distribution peak time time rebound index cell pair similar distribution entire sample fig fig legend select cell pair include directional predict material method connect connect simple cell geniculate cell small receptive field simplecell subregion width sign cell large receptive field include connect cell pair use analysis similar response time course high probability connect fig particular cell pair similar peak time time connect fig selective simple cell include time group example figure selective cell time group msec respectively similar result obtain restrict sample geniculate center overlap dominant subregion simple cell interestingly efficacy contribution connection depend little relative timing visual response fig right sample small lag cell considerable interest comment record potentially lag lgn cell center superimpose simplecell subregion rebound index rebound index pair use time comparison pair baseline correlogram spike pair geniculate receptive field point experimental observation relate temporal processing visual system consistent lattice filter model measurement temporal receptive field demonstrate progressively long consecutive stage lgn neuron long receptive field correspond presynaptic ganglion cell figure leave simple cell v long receptive field correspond presynaptic figure observation consistent progressively great temporal extent predictionerror filter blue plot figure second weight peak integrate area curve great rebound lgn relay cell simple cell figure neuron peak weight exceed rebound refer nonlagge know lag find cat monkey reason clear response step stimulus figure compare experimentally measure receptive field continuous lattice filter figure identify neuron forward branch lag neuron backward branch way characterize response sign different lag relative sustained measurement crosscorrelation lgn cell spike lag nonlagge neuron reveal difference transfer function difference underlie circuitry consistent filter figure complex forward path result different transfer function replace complex different measurement crosscorrelation lgn cell spike lag nonlagge neuron reveal difference transfer function difference underlie circuitry consistent backward branch circuit filter figure different forward branch result different transfer function particular combination different receptor receptor think responsible observed response lag cell investigation correspond circuitry use connectomic technology desirable fourth crosslink weight lattice filter learn use hebbian rule biologically plausible interestingly weight learn sequentially start stage need additional stage add learn property map naturally fact course mammalian development stagewise fashion start lgn imply peripheral structure need adapt figure comparison measure response cat lgn cell continuoustime lattice filter model experimentally measure temporal receptive field response lgn cell adapt typical example response continuoustime lattice filter model lattice filter coefficient u model nonlagge cell u u model lag cell model contribution response additional integrator add circuit hebbian rule biologically plausible figure apply inhibitory crosslink point circuit mean represent computation perform specific implementation term neuron linear computation perform circuit different arrangement component multiple implementation lattice filter example activity nonlagge cell represent forward error crosslink pathway lag pathway excitatory general classification cell lag nonlagge independent classification insect visual pathway insect cell type l l postsynaptic play important role visual processing physiological response l l indicate decorrelate visual signal subtract predictable fact receptive field neuron use example predictive coding neuroscience number synapsis l l physiological property similar insect pair seemingly redundant neuron previously suggest l l provide input pathway map pathway vertebrate hypothesis role l l visual processing similar branch lattice filter incorporate distinction effectively linear lattice filter model anticipate description future argue section forward predictionerror filter peak great weight rebound backward predictionerror filter opposite true difference imply response sign sustained response compare initial different branch image response l l different predict lattice filter model figure b interestingly activity l represent forward error l backward error suggest lattice filter crosslink excitatory summarize prediction lattice filter model consistent physiological measurement fly visual system help understand operation stimulus time forward error error figure response lattice filter fly left response order discretetime lattice filter step stimulus right response fly l cell step stimulus adapt predict experimentally measure response qualitatively shape follow sustained response sign forward error l opposite sign backward error l discussion motivate cascade structure visual pathway propose model operation lattice filter demonstrate prediction continuoustime lattice filter model consistent course neural development physiological measurement v cat monkey fly neuron lattice filter offer useful abstraction understand aspect temporal processing visual system vertebrate invertebrate previously propose lag nonlagge cell result spike neuron agree lgn perform temporal decorrelation explanation rely nonlinear processing cascade architecture fundamentally different model generate follow prediction obvious lgn receptive field long receptive field long linear model generate difference ratio lgn different lag nonlagge cell consistent filter circuit selforganize use hebbian rule explanation receptive field normative framework light redundancy reduction argument introduction note goal system compress incoming signal use number lattice filter stage compression kind prediction error forward backward need transmit channel absence noise redundant transmit forward backward error flexibility continue decorrelation far add stage perform relatively simple operation grateful da e m helpful discussion reference spike explore neural code mit match code circuit cell molecule signal general principle retinal design eye progress retinal eye research aspect visual perception psychological review h redundancy reduction revisit network comp neural system rm gray linear predictive coding internet publisher predictive code view inhibition proceeding royal biological science t sa m dynamic predictive code nature ef peripheral origin nervous activity visual system study excitation inhibition collection paper laboratory page adaptation stimulus contrast correlation natural visual stimulation neuron natural scene experimental test computational theory journal neuroscience statistic natural image network computation system ft sensory information synapse journal neuroscience koch computation information processing single neuron s optimum quantization feature parameter speech conference record international conference speech communication process page sd adaptive filter theory fundamental adaptive distribute hierarchical processing primate inhibitory circuit visual processing thalamus current opinion neurobiology synaptic limitation contrast coding proceeding royal biological science dc lie linear algebra application organization behavior theory sejnowski natural pattern activity current opinion neurobiology z h adaptive filter signal processing ieee transaction wm rule connectivity geniculate cell simple cell primary visual cortex neuroscience rd spatiotemporal receptive field organization cat journal neurophysiology dn class property classification cell neurophysiology temporal diversity lateral cat visual neuroscience temporal response property lag nonlagge cell neurophysiology lag cell dn class input generation property neurophysiology p e neurotransmitter receptor excitatory input cell lag cell neurophysiology jm fast thalamocortical processing dark light visual target journal neuroscience m sn volume determine neuronal placement brain current biology da tr define computational structure motion detector pathway vision nature
field bridge level abstraction abstract network vision system inference information level representational abstraction low level invariant intermediate scene segment high level relevant object description paper network realize field construct mrf functionally equivalent hough transform parameter network establish principle probabilistic basis visual network second mrf parameter network capable flexible traditional method particular welldefine probabilistic interpretation incorporate feedback offer rich representation decision capability introduction nature vision problem dictate neural network vision inference information level representational abstraction example local image evidence edge use determine occlude boundary object scene paper demonstrate network use vote bridge level abstraction realize main result mrf construct functionality formally equivalent parameter net base hough transform establish random field bridge level abstraction equivalence provide sound probabilistic foundation neural network vision particularly important fundamentally nature vision problem second parameter network construct offer flexible capable framework intermediate vision traditional parameter network threshold decision making particular mrf parameter net offer rich representational framework potential complex decision surface integral treatment feedback probabilistically justify decision training procedure implementation experiment demonstrate feature result establish basis construction integrate network vision system single welldefine representation control structure incorporate feedback background transform parameter net approach bridging level abstraction vision combine local highly variable evidence segment describe compactly parameter hough transform offer method obtain highlevel parameter parameter network implement hough transform parallel feedforward network central idea vote local lowlevel evidence cast vote network compatible higherlevel parameterize hypothesis find line edge local evidence direction magnitude image contrast combine extract parameter line useful scene segment widely use computer vision bridge level abstraction markov random field offer formal foundation network geman similar machine define prior joint probability distribution set discrete random variable possible value variable interpret possible local feature hypothesis variable associate node undirected graph network write assignment value variable field configuration denote assignment single variable denote fullyconnecte neighborhood c configuration field weight interested probability distribution p random field field locality property e s px roughly state site dependent state neighbor characterize term energy function cooper gibbs distribution t temperature constant interested prior distribution energy function u define l c set clique define neighborhood graph clique potential specify clique potential provide convenient way specify global joint prior probability distribution p encode prior domain knowledge plausible structure suppose instead interested distribution field observation observation constitute combination spatially distinct observation local site evidence observation site denote p likelihood assume likelihood local spatially distinct reasonable assume conditionally independent baye rule derive mrf definition evidence current problem leave probability distribution possible configuration algorithm use find solution normally configuration maximal probability equivalently minimal energy express equation problem minimize energy function especially local subject recently paper focus develop mrf representation minimum energy configuration define desirable goal method find minimum experiment use deterministic high confidence widely use computer vision application include image restoration segmentation depth reconstruction geman application involve representation single level abstraction novel aspect work hierarchical framework explicitly represent visual entity different level abstraction higherorder entity serve interpretation datum role constraint satisfaction high level construct equivalent parameter network define markov random field compute hough transform detect higherorder feature weight vote lowlevel image component thresholde sum mrf discrete variable field bridge level abstraction parameterized segment potential highlevel variable label set linear sum threshold exist lowlevel variable label set max max input node figure leave parameter input determine confidence lowlevel feature confidence weight sum thresholde right equivalent mrf circle variable possible label nonzero unary clique potential line neighborhood potential labelling binary clique higherorder feature possible value discrete variable voting element possible value field replicate space compute feature simultaneously construction follow idea clique potential network define configuration need consider configuration penalize high clique potential ie low probability configuration encode decision higherorder feature exist exist second point energy exist configuration independent observation energy exist configuration improve strength evidence consider parameter net hough transform represent single parameterized image segment eg line segment set lowlevel feature edge vote figure leave variable label neighborhood equivalent mrf define right figure potential depend hough parameter right figure single neighborhood graph way label unary potential evidence apply label likelihood local observation configuration energy equal weight sum feature input configuration constant energy equal hough clique potential restrict possible configuration network minimum energy configuration highlevel feature correct label cooper proof energy contribute clique potential define e wi simplifie evidence contribute energy form e log ei substitute total posterior energy energy configuration depend evidence derive feature clique nonzero potential unary e e e proof mixed configuration e change label add energy evidence associate remove energy potential clique potential remove change label reduce energy mixed configuration e change lowlevel label add energy contribute evidence evidence associate binary clique potential potential remove choose reduce energy ensure compatible label configuration possibly minimal posterior energy configuration represent existence higherorder feature preferred weighted sum evidence exceed threshold transform desirable find mode highlevel parameter space element surpass fix threshold find single mode easy add lateral connection label highlevel feature form winnertakeall network potential clique large possible variable correspond highlevel feature label parameter net essential parameter network set variable represent feature set variable represent feature appropriate random field bridge level abstraction figure noisy image datum figure mrf experiment white dot low image indicate decision horizontal vertical local edge present upper image horizontal vertical line find left net transform middle net use positive feedback line edge right net use negative feedback line edge weight connection section explore characteristic natural mrf parameter network base variable connection limit binary label set decision procedure experiment feedback hough transform parameter net instantiation inherently feedforward contrast incorporate feedback experiment network design find line edge horizontal vertical edge input represent low level horizontal vertical line span image high level input datum look figure probabilistic evidence lowlevel edge generate pixel datum use model formation edge vote compatible line figure decision left edge exist local evidence sufficient line exist vote receive topology input representation mrf add topdown feedback change binary clique potential existence line high level strongly compatible existence edge miss edge fill middle line strongly existence edge noisy edge substantially remove right segmentation brown find collinear edge cooper reason line exploit topdown feedback representation decision making parameter net represent confidence local hypothesis framework intrinsic advantage simultaneously represent independent belief hypothesis active vision system reason gathering interpret evidence extend include label know allow explicit reasoning condition local evidence support decision express higherorder constraint set pair exploitation appropriate clique example useful cooper potential mrf relate local conditional probability principle way obtain observation use estimate local joint probability convert clique potential define prior distribution field evidence integration scheme require addition network topology parameter definition decision process eg thresholding theory parameter acquisition process estimate maximum posterior probability mrf hand decision possibility embed choose variable label space possible decision interpretation problem input rich parameter network net node evidence available define ndimensional problem input space weight di space region define good interpretation configuration problem region parameter net region separate plane sum input matter energy depend evidence sum potential allow general decision surface nonlinear decision easy encode impossible linear hough transform conclusion paper parameter network construct bridge representational level abstraction network vision system furthermore demonstrate offer potential significantly powerful implementation parameter net topological architecture identical traditional hough network short method available construct intermediate vision solution possible build entire integrate vision system single formal framework markov random field system unify representational scheme constraint evidence semantic single control structure furthermore feedback feedforward flow random field bridge level abstraction information crucial complete vision system intrinsic course task remain build function vision system domain paper definition specific feature constraint constitute useful system provide essential tool implement formal framework important step build robust function system acknowledgement support research provide nsf grant grant institute write simulator use experiment reference artificial intelligence m visual recognition use concurrent layer parameter division editor field theory application brown m brown theory practice bayesian image label international vision parallel structure recognition uncertainty couple segmentation match proceeding conference computer vision iccv geman stochastic relaxation gibb distribution bayesian restoration image pami neural computation decision optimization problem biological cybernetic et cd mp optimization simulate anneal science probabilistic solution inverse problem technical report mit artificial intelligence laboratory pearl reasoning intelligent system probabilistic approach lowlevel vision technical report department computer science learn high confidence estimation technical report dept computer science
learn nonparametric model probabilistic imitation computer abstract learn imitation represent important mechanism rapid acquisition new behavior human robot critical requirement learn imitation ability handle uncertainty arise observation process imitator dynamic interaction environment paper present new probabilistic method infer imitative action account observation teacher imitator dynamic key contribution nonparametric learning method generalize system different dynamic rely known forward model dynamic approach learn nonparametric forward model exploration advance approximate inference graphical model learn forward model directly use plan imitate sequence provide experimental result system biomechanical model human arm humanoid robot demonstrate propose method use learn appropriate motor input model arm imitate desire movement second set result demonstrate dynamically stable fullbody imitation human teacher humanoid robot introduction fundamental mechanism learn human imitation young minute age find imitate facial act tongue old child perform complicated form imitation range learn manipulate novel object particular way imitation base inference goal demonstration review robotic researcher increasingly interested learn imitation learn learn demonstration attractive alternative manually programming robot approach uncertainty account uncertainty imitation arise source include internal dynamic robot robot interaction environment observation teacher able handle uncertainty especially critical robotic imitation execute action high uncertainty imitation lead potentially consequence paper propose new technique imitation explicitly handle uncertainty use probabilistic model action sensory consequence rely parametric model system dynamic traditional method approach learn nonparametric model imitator internal dynamic constrained exploration period learn model use infer appropriate action imitation use probabilistic inference dynamic bayesian network dbn teacher observation evidence demonstrate approach use system biomechanical model human arm s s s c c figure graphical model system imitation learn dynamic bayesian network infer sequence imitative action sequence observation teacher model allow probabilistic constraint variable ct imitator state nonparametric model learning construct model empirical datum b link biomechanical model human arm use experiment learn reach movement imitation humanoid robot use experiment fullbody dynamic imitation humanoid robot set result illustrate propose method use learn appropriate motor command produce imitative movement model human arm second set result demonstrate dynamically stable fullbody imitation human teacher humanoid robot result suggest probabilistic approach imitation base nonparametric model learning provide powerful flexible platform acquire new behavior complex robotic system imitation inference constrain exploration section present approach select set action base observation agent state demonstration set probabilistic constraint present algorithm framework graphical model fig denote sequence continuous action variable use convention agent start initial state result execute action visit set continuous state note initial action trivially include imitation learn framework agent observe sequence continuous variable provide partial information state teacher demonstration conditional probability density encode likely observation teacher agree agent state perform motion task mark key difference partially observable markov decision process pomdp framework observation demonstrator generally different currently assume learner observe state probabilistic constraint state variable include graphical model set variable ct correspond constraint model encode likelihood satisfy constraint constraint variable use framework represent goal reach desire goal state way point ct choice constraint model domain dependent utilize central ct variance parameter constraint set use domain knowledge learn use feedback environment set evidence e c desire action maximize likelihood evidence space limitation rule thorough discussion achieve tractable inference focus compute marginal posterior action maximum posteriori map sequence principle algorithm compute marginal posterior distribution action variable use find convenient use pearl belief originally restrict tree structure graphical model discrete variable advance broaden applicability general graph continuous variable undirected graph structure derive belief propagation direct case note difference largely semantic convenience bayesian network represent random field generally factor graph approach similar nonparametric belief propagation key difference highlight section result perform belief propagation set marginal belief distribution p belief distribution product set message represent information come neighbor parent child node respectively belief compute message pass edge graphical model distribution single variable parent variable pass x distribution ui child variable pass x distribution discrete space case message easily represent discrete distribution case arbitrary continuous density message representation challenge propose nonparametric modelfree approach learn system dynamic follow want allow approximately represent multimodal nongaussian distribution arise inference approach adopt use mixture represent arbitrary message belief distribution convenience treat observe hide variable graph identically allow send message x message represent use dirac delta distribution observe value consider product message m child denote m message parent variable incorporate integrate conditional probability possible value parent time probability combination value evaluate corresponding message parent ui message update accord follow equation ij ij main operation eq integration multiplication mixture evaluation integral discuss introduce gaussian mixture regression product set mixture simply mixture complexity term number component output mixture grow exponentially number input mixture approximation need inference tractable action sequence length use multiscale sample method obtain set representative particle product assume compute exact product density set input mixture apply simple heuristic fix number mixture component find highly effective heuristic base empirical sparseness product mixture component prior probability example message come previous state m component message action component base gaussian model p component conditional product m p component experimentally component weight order magnitude maximal weight simply select component sparsity surprising p model component represent localize datum component tend overlap belief state propagate currently fix adaptive mechanism far speed inference likelihood figure nonparametric model selection value model selection criterion rise initial model l component peak component fall b series plot current parameter model blue ellipsis layer set regression test point green minimum span tree red line neighboring component projection dimensional datum project principal component training datum briefly describe algorithm action inference constrain exploration input action set evidence e instance dynamic bayesian network m pf pc compose prior initial state prior action forward model imitation observation model probabilistic constraint model respectively inference proceed invert evidence observation constraint variable yield message initialization prior perform forward plan pass compute forward state message similarly backward planning sweep produce message algorithm combine information forward backward message eq compute belief distribution action t belief distribution use mode select maximum marginal belief action find describe algorithm iteratively explore state action space satisfy constraint place system build inference base action selection describe input initial model m set evidence e number iteration perform iteration infer sequence maximum marginal action execute execution yield sequence state use update learn forward model section use new ideally accurate forward model able obtain imitation teacher newly infer action final model sequence action return n constrain exploration trial simplicity currently assume state action prior distribution observation constraint model evidence experiment specify model realworld domain study focus algorithm present learn forward model domain extremely complex derive analytically section describe result apply human arm model humanoid robot domain respectively nonparametric model learn section investigate algorithm learn nonparametric forward model mixture regression motivation select allow closed form evaluation integral find eq allow efficient inference need resort base approximation inference detailed algorithm refer technical report available common gaussian mixture model gmm form basis gaussian assume random variable x form concatenation random able x variable normally distribute ij regression derive apply result theorem xi use denote mean jth variable kth component mixture model likewise denote covariance variable kth component instead fix weight mean component weight function dependent condition variable x kj p likewise mean kth condition component xi function kj belief propagation require evaluation integral convolve conditional distribution variable gmm distribution variable fortunately rearrange term density reduce product gmm marginalize wrt xi integral operator turn problem learn model datum learning methodology wish adopt nonparametric want select number component priori rule common strategy use know expectation maximization learn model joint density px bayesian strategy exist select number component point joint density modeling approach rarely yield good model regression loss function adopt similar algorithm iterative pairwise replace simultaneously perform model fitting selection model parameter assume set state action history observe trial history ai si ai sit learn forward model construct joint variable space s s s denote result state execute action state timeinvariant dataset represent matrix model learning selection construct fully nonparametric representation training set l isotropic mixture component center data point xk parametrization exact prediction point training set generalize extremely poorly proceed merge component similar determine symmetric similarity metric mixture component follow use distance metric perform efficient compute minimum span tree mixture component iteratively merge close pair minimum span tree continue single gaussian component leave merge mixture require computation new local mixture parameter fit datum cover method moment approach merge component later run expectation maximization finetune select model find perform local maximum likelihood estimation mle model selection effective find accurate model order effectively perform mle merge randomly partition training datum set basis vector compute minimum span tree datum point experiment use random fifth datum basis vector goal modify algorithm find model describe regression point define regression likelihood current model parameter l l model size maximize criterion return use inference procedure demonstrate learning forward model biomechanical arm model section find model selection criterion effective generalize basis regression set result imitate reach movement constrain exploration trial hand position joint velocity b demonstration torque input random figure learn imitate reach motion row motion teacher hand black space target position imitator explore space body second plot red datum model learn constrain exploration perform find imitative reach motion iteration velocity joint imitation learning process trial number imitator thin line closely match demonstrator color line meet final velocity constraint c teacher original muscle torque follow torque torque compute constrain exploration set experiment learn reach movement imitation complex nonlinear model human arm arm simulator use biomechanical arm model consist degree freedom denote represent elbow joint arm control torque input denote degree freedom dynamic arm describe follow differential equation b m m force matrix vector force matrix force joint process learn perform reach motion imitation compute teacher simulate arm motion use modelbase base start target position hand execute sequence compute torque input specified initial state obtain state history demonstrator simulate balanced duration constrain exploration period prior trial figure robot dynamic imitation row consist frame datum observation second row result perform kinematic imitation simulator fourth row final imitation result obtain method constrain exploration simulator robot b duration execute imitation balance total t trial number random exploration trial red infer imitative trial blue note balanced duration rapidly rise th infer sequence able perform imitation fall natural partial human demonstrator human learner provide inference noisy measurement kinematic state torque probabilistic constraint dictate final joint velocity close dynamic humanoid imitation apply algorithm nonparametric action selection model learning constrain exploration problem fullbody dynamic imitation humanoid robot experiment consist humanoid demonstrator perform motion stand space limitation briefly describe experiment detail demonstrator kinematic obtain use available optical motion capture system base inverse kinematic ik model human restrict degree freedom represent humanoid motion use kinematic configuration problematic curse dimensionality fortunately respect wide class motion walk number degree freedom highly redundant simplicity use linear principal component analysis pca investigate use nonlinear embed technique use pca able represent observed kinematic compact space form dimension goal experiment perform dynamic imitation ie consider dynamic balance involve imitate human demonstrator dynamic balance consider use model robot sensor provide measurement angular rotation gt foot pressure point foot millisecond compute differential feature pressure sensor extract horizontal axis form dimensional representation dynamic state robot concatenate dimensional kinematic state dimensional dynamic state form dimensional state robot action simply point embed kinematic space bootstrap forward model robot kinematic dynamic perform random exploration body trajectory collect sufficient datum trial learn initial forward model subsequently place probabilistic constraint dynamic configuration robot use tight central distribution angular velocity pressure differential use constraint dynamic perform constrain exploration obtain stable motion imitate human motion result obtain imitate difficult balance motion fig conclusion result demonstrate probabilistic inference learn technique use successfully acquire new behavior complex robotic system humanoid robot particular nonparametric model forward dynamic learn constrain exploration use infer action imitate teacher simultaneously imitator dynamic account exist large body previous work robotic imitation learning example rely produce imitative behavior use nonlinear dynamical system focus biologically algorithm eg field reinforcement learning technique inverse reinforcement learn apprenticeship learn propose learn controller complex system base observe expert learn reward function role type expert human demonstrator distinguish case teacher directly control artificial system imitation learning paradigm observe teacher control body far kinematic similarity human humanoid robot dynamic property robot human different finally fact approach base inference graphical model confer major advantage continue leverage algorithmic advance rapidly develop area inference graphical model approach promise generalization graphical model complex system semimarkov dynamic hierarchical system reference p abbeel exploration apprenticeship learn reinforcement learning proceeding international conference machine learn c s schaal robot learn demonstration page m learn human arm movement imitation evaluation connectionist architecture robotic autonomous system m mixture anal mach robot controller use learn imitation d b r r p rao dynamic imitation humanoid robot nonparametric probabilistic inference proceeding robotic science system mit press t e b t freeman s willsky efficient multiscale sampling product thrun b scholkopf editor advance neural information processing system mit press cambridge trajectory formation imitation nonlinear dynamical system international conference intelligent robot system page r factor graph sumproduct transaction information theory e iterative regulator design nonlinear biological movement system proceeding int control automation robotic volume page element developmental theory imitation page s algorithm inverse reinforcement learning machine learn page pearl probabilistic reasoning intelligent system network plausible inference s schaal computational approach motor learning imitation d scott kernel mixture e b t freeman s willsky nonparametric belief propagation cvpr page sing mixture regression classification phd weiss correctness local probability propagation graphical model loop neural computation m learn extract task knowledge visual observation human performance ieee transaction robotic automation vol pp
balance suspense surprise timely decision information acquisition m electrical electrical abstract develop bayesian model decisionmake time pressure information acquisition model decisionmaker decide observe costly information sample underlie continuoustime stochastic process time series convey information potential adverse event terminate decisionmake process attempt predict occurrence adverse event decisionmaker follow policy determine acquire information time series continuation stop acquire information final prediction stop optimal policy rendezvous structure ie structure new information sample gather time series optimal date acquire sample computable optimal interval information sample balance tradeoff decision surprise ie drift posterior belief observe new information suspense ie probability adverse event occur time interval information sample characterize continuation stop region decisionmaker statespace depend decisionmaker belief context current realization time series introduction problem timely risk assessment decisionmake base sequentially observe time application medicine cognitive science signal process common setting arise domain decisionmaker provide sequential observation time series need decide adverse event financial clinical patient place future decisionmaker recognition adverse event need timely delay decision effective patient intensive care unit lead context cognitive science decisionmake task know force choice task structural solution optimal bayesian decisionmake policy derive inspire classical work sequential probability ratio test paper present bayesian decisionmake model decisionmaker adaptively decide gather costly information underlying time series order accumulate evidence adverse event decisionmaker operate time pressure occurrence adverse event terminate decisionmake process abstract model inspire practical task construct temporal pattern gather sensory information perceptual decisionmake lab conference neural information processing system nip test patient order predict clinical timely manner design cancer screen program early tumor detection characterize structure optimal policy prescribe decisionmaker acquire new information stop acquire information issue final prediction decisionmaker posterior belief process base policy prescribe reflect decisionmaker tendency occurrence adverse event future observe survival time series long time period information acquisition policy rendezvous structure optimal date acquire information sample compute current sample optimal schedule gather information time balance information gain surprise obtain acquire new sample probability survival underlie stochastic process suspense finally characterize continuation stop region previous model depend time context decisionmaker related work mathematical model analysis perceptual base sequential hypothesis testing develop model use tool sequential analysis develop optimal decisionmake policy task compute model decisionmaker sensory evidence use diffusion process model assume infinite time horizon decisionmake policy exogenous supply sensory information assumption infinite time horizon relax decisionmake assume perform pressure stochastic deadline deadline consider draw know distribution independent hypothesis realize sensory evidence assumption exogenous information supply maintain practical setting deadline naturally dependent realize sensory information patient event correlate physiological information induce complex dynamic decisionmake process decisionmake model introduce assume exogenous information supply infinite time horizon notion suspense surprise bayesian decisionmake recently introduce economic literature reference model use measure surprise originally introduce context sensory neuroscience order model explicit preference decisionmaker information goal design information policy model model impose suspense andor surprise behavioral preference decisionmaker emerge virtue rational decision timely decision information gathering time decisionmaker access timeserie model continuoustime stochastic process value r define time domain t r underlie filter probability space ft p process naturally adapt tr ft abstract information convey time series realization time t decisionmaker extract information guide action assume stationary markov process stationary transition r s t r realization latent random variable decisionmaker p p distributional property path determine realization decide p generate realization equal adverse event occur surely finite random time distribution dependent realization path insight result hold general dependency structure assumption simplify maintain tractability interpretability result adverse event information t x x x s survival t stop time partition path time figure exemplary stop sample path t exemplary partition decisionmaker goal sequentially observe infer adverse event happen inference declare latent decisionmaker adverse event occur ie access order model occurrence adverse event define time process assume follow stop time finite surely infinite surely ie p p stop time accessible markovian dependency history p p s t p map r p stochastic deadline model decision deadline model occurrence adverse event contextdependent depend time ie independent use notation t xt denote stop process decisionmaker access paper measure p assign probability measure path t t respectively assume p information decisionmaker observe set costly sample t continuous path sample observe decisionmaker capture partition specific time interval define t t t partition t interval t total number sample partition pt decisionmaker observe value t time instance sequence observation p process ti dirac measure space partition interval t denote denote probability measure p respectively path generate partition pt p decisionmaker observe t partition pt information time t convey stop event observable decision p denote algebra generate stop event information decisionmaker time t express ft decisionmake policy need measurable figure depict brownian path sample path wiener process satisfy assumption model exemplary partition pt time interval decisionmaker observe sample sequentially reason realization latent variable base sample process survival ie t decisionmaker information reside x x generate sample analysis hold stop time totally absolute continuity respect p mean sample path t fully reveal realization figure stop event simulate totally jump poisson process p algebra generate process survival policy risk decisionmaker goal come timely decision reflect prediction actual realization process t potentially stop unknown time decisionmaker follow policy continuoustime mapping observation gather time instance t type action sensing action t t decisionmaker decide observe new sample running process t time action t decisionmaker decide stop gather sample t declare final decision estimate t decisionmaker continue observe x t estimate policy t tr ft measurable mapping rule map information ft action t t t time instance t assume single observation decisionmaker draw t entail fix cost process t tr point process optimal policy denote space policy policy generate follow random quantity function path t probability space ft p stop time t time decisionmaker declare estimate t decision estimate random partition realization point process t comprise finite set strictly increase time decisionmaker decide sample path t loss function associate realization policy represent overall cost incur follow policy specific path t loss function t z delay type error deadline miss type error information c cost type error failure anticipate adverse event co cost type error predict adverse event occur cost delay declare estimate cost incur adverse event occur estimate declare cost miss deadline cost observation sample cost information risk policy define expect loss e expectation path x t section characterize structure optimal policy inf structure optimal policy decisionmaker posterior belief time t define t p important statistic design sequential policy start characterization investigate belief process t tr posterior belief process recall decisionmaker distill information type observation realization time series information pt survival note cost observe local continuous path infinite optimal policy t tr point process number observed sample policy partition policy p policy suspense phase risk bear posterior belief process t surprise phase risk assessment information gain t t stop time time figure exemplary belief path different policy process time t information following theorem study evolution decisionmaker belief integrate piece information information belief posterior belief trajectory t tr associate policy create partition pt pt t path p dp t t p p p respect p p derivative measure p follow elementary predictable process p p p survival probability likelihood ratio t pt p p t jump time t path t tr exactly index belief path left limit jump time index partition pt jump path t predictable ie know ahead time know magnitude jump precede mean decisionmaker obtain active information probe time series observe new sample information pt induce jump belief time stop event offer decisionmaker passive information observation process survival information source information manifest term likelihood ratio survival probability expression figure plot belief path policy p p ie policy observe subset sample observe plot predictable belief path policy observe sample jump active information cope fast truthful belief time jump belief process exhibit nonincrease predictable path feed new piece information policy belief drift away prior p wrong belief t distill information process survival favor hypothesis discussion motivate introduction follow key quantity information gain surprise t drift decisionmaker belief t respect belief time t information available time t ft proof provide supplementary material impose condition p fix partition pt exist posterior survival function suspense probability process generate time t information observe time p t function nonincrease function information gain surprise decisionmaker experience response new information sample express term change belief ie jump t survival probability suspense assessment risk adverse event place t time interval subsection optimal policy balance quantity schedule time sense t conclude analysis process t note lack information sample create belief belief path policy figure formally express behavior follow corollary policy posterior belief process t respect t t t r classical bayesian learning model belief martingale belief process model decrease time reason model time convey information decision deadline surely occur finite time path model occurrence adverse event hypothesis observe survival process informative contribute evolution belief acquire information sample process survival use decomposition write t t t t martingale capture information gain acquire sample predictable process capture information extract process survival optimal policy optimal policy minimize expect risk define generate tuple random process t response path t ft p way shape belief process t maximize maintain control cost following introduce notion rendezvous policy optimal policy definition rendezvous policy policy rendezvous policy random partition construct sequence sensing action point process predictable jump consecutive jump time t t t t t pt t ft measurable rendezvous policy policy construct sensing schedule t time t decisionmaker acquire information actually computable use information available time previous time instance information gather decisionmaker decide date gather information directly sense new information sample structure natural consequence information structure belief path jump predictable convey information ie decisionmaker respond predictable belief path sense stop decision decision right predictable path start lead save delay cost denote space rendezvous policy r following theorem establish rendezvous structure optimal rendezvous optimal policy rendezvous policy r direct implication time variable view state variable problem solve discretetime decisionmaker effectively jump time instance discrete manner alter definition action t indicator variable indicate sense time series rendezvous action real value specify time decisionmaker sense new sample t decisionmaker gather new sample transformation restrict policy design problem space rendezvous policy r know contain optimal policy establish result following theorem characterize optimal policy term random process t use condition theorem optimal policy optimal policy sequence action t tr result random process t pt follow property continuation stop t tr markov sufficient statistic distribution process t recent sample partition p ie t pt max policy recommend continuation ie t long belief t t time contextdependent continuation set follow property ct ct t x rendezvous decision t t p rendezvous t set follow t decrease decision issue base t o stop time belief threshold follow co c t t establish structure optimal policy prescribed action decisionmaker statespace order generate random tuple t optimally need track realization process t tr time instance optimal policy map current belief current time recently observe realization time series action tuple t t ie decision stop declare estimate sense new sample tr represent state decisionmaker decisionmaker process action partially influence state belief process ie decision acquire sample affect distributional property posterior belief remain state variable decisionmaker control note previous model exception policy model contextdependent state t policy recommend different action belief time different context t capture decisionmaker learn capture future ie belief t history large mean potential p context adverse event likely happen near future decisionmaker stop decision declare estimate manifest t time context continuation set dependence continuation set monotonically decrease time deadline pressure monotonically decrease dependence deadline time sample path policy continue sample t sample path t t stop declare figure policy context dependence optimal policy depict figure exemplary trajectory decisionmaker state action recommend policy time belief different context ie stop action recommend large correspond low survival probability belief time continuation action recommend low safe observe process survival probability high optimal decisionmake setting clinical critical care environment combination patient length stay ie t clinical risk score ie t determine decision current physiological test measurement patient admit intensive care unit second optimal policy decide stop gather information issue decision impose threshold posterior belief base weight estimate issue estimate threshold respective risk policy favor continuation issue rendezvous action ie time instance information gather rendezvous balance surprise suspense decisionmaker prefer maximize surprise order draw maximum costly sample acquire capture term expect information gain maximize surprise increase suspense ie probability process control survival function maximum entail survival risk high optimal policy select rendezvous t optimize combination survival risk survival capture cost survival function value information capture cost co c expect information gain conclusion develop model decisionmake information acquisition time pressure decisionmaker need issue decision adverse event potentially place optimal policy rendezvous structure optimal policy set date gather new sample current information sample observe optimal policy select time information sample balance information gain surprise survival probability suspense characterize optimal continuation stop condition depend context belief model help understand nature optimal decisionmake setting timely risk assessment information gathering essential acknowledgment work support onr nsf grant number reference f p optimal temporal risk assessment frontier neuroscience banerjee t v quick change detection sequential analysis e patient risk c timeserie classification task advance neural information processing system pp p framework prediction disease trajectory exploit multiresolution structure advance neural information processing system pp b b m r p delay study group impact transfer critically ill patient department intensive care unit critical care medicine pp t optimization schedule presence tumor inform journal compute pp et cancer screen programme current policy international pp sequential analysis rao r p bayesian framework model confidence perceptual decision advance neural information processing system pp s maximization sequential identification stochastic deadline pp baye planning human learning decisionmake bandit setting advance neural information processing system pp p strategic decisionmake advance neural information processing system pp r pouget optimal decisionmake timevarye evidence reliability advance neural information processing system pp j dayan p cohen dynamic attentional selection conflict rational bayesian account experimental psychology human perception performance p sequential hypothesis testing stochastic deadline advance neural information processing system pp r m n pouget cost accumulate evidence perceptual decision journal neuroscience m srivastava cohen theory decision making dynamic context advance neural information processing system pp e suspense surprise journal political pp p bayesian surprise attract human attention advance neural information processing system r brown e cohen d physics optimal decision formal analysis model performance task psychological review pp optimal stopping problem n optimal stopping rule vol springer science business medium e stochastic continuoustime model vol springer science business medium bertsekas p s e stochastic optimal control discrete time case vol academic press
parallel problem server interactive tool large scale machine learn husband room road abstract imagine wish classify datum consist thousand example reside dimensional space apply standard machine learn algorithm describe parallel problem server ppserver allow user network computer work large datum set matlab work motivate desire bring benefit scientific compute algorithm computational power machine learn researcher demonstrate usefulness system number task example perform independent component analysis large text consist thousand document minimal change original sejnowski apply technique datum previously reach lead interesting analysis datum datum set extremely large standard machine learn community text example wish process collection consist thousand document easily different word naturally like apply machine learn technique problem size datum difficult paper describe parallel problem server matlab ppserver linear server execute distribute memory algorithm large datum set user manipulate large datum set system bring efficiency power parallel computation researcher use network machine maintain benefit interactive environment demonstrate usefulness ppserver number task example perform independent component analysis large text corpus consist thousand document minimal change original sejnowski apply technique dataset previously p husband library interface routine machine local variable ml l m m m figure use ppserver completely transparent ppserver variable tie ppserver maintain handle datum use matlab object system function use ppserver variable invoke ppserver command implicitly reach discover interesting analysis datum algorithm parallel problem server parallel problem server ppserver foundation work ppserver realization novel model computation large matrix compatible platform support message pass interface mpi standard communication portable way write parallel code ppserver implement function create remove distribute matrix load store disk use portable perform elementary matrix operation matrix twodimensional single double precision array create function provide transfer matrix section client ppserver support dense sparse matrix ppserver communicate client use simple client request action issue command appropriate argument server execute command client command complete ppserver directly library package ppserver implement robust protocol communicate package client package load remove package execute command package package direct access information ppserver matrix package represent define set visible function support datum allow user hide subset function package load define function finally package support common parallel eg apply function element matrix easy add common functionality ppserver command implement package include basic matrix operation public library realize package use appropriate function package include sun optimize version large scale machine learning use parallel problem server function e h e j figure matlab code produce hilbert matrix influence p create ppserver object instead object directly use ppserver client communication interface possible application use ppserver functionality implement client interface collection object script language allow transparent integration matlab end parallel problem choice influence factor standard scientific computing enjoy wide use industry machine learn community example algorithm write script freely available scientific compute community algorithm prototype matlab optimize language interaction ppserver transparent possible user principle typical matlab user explicit ppserver current matlab program rewrite advantage space permit complete discussion refer reader husband briefly discuss use script modification accomplish simple p notation use object orient feature create ppserver object automatically p special object introduce matlab act integer user type p p obtain matrix distribute parallel reader guess use p indicate distribute row column user b matrix handle special distribute type exist ppserver reference variable eg command recognize ppserver figure code matlab build function produce hilbert matrix ij influence p parallel array result line create ppserver vector place handle note behavior interfere semantic loop n assign value column number n line produce ppserver matrix matlab indexing function result correct execution line p husband transpose operator execute line ppserver line e generate ppserver elementary matrix operation ppserver matrix line parallel problem server p test extensively variety platform currently run supercomputer cluster symmetric cluster test client include common computational performance vary depend platform clear system provide distinct computational advantage communication overhead experiment roughly millisecond ppserver command negligible compare computational space advantage afford transparent access linear application section demonstrate efficacy ppserver learn problem particular explore use ppserver task text retrieval find subset collection document relevant user information request standard approach base vector space model document vector dimension count occurrence different word collection document matrix d column document vector similarity document inner product query document relevance document query q q typical small collection contain vector dimensional space large collection contain vector reside thousand dimension clearly standard machine learn technique exhibit unpredictable behavior circumstance simply scale approach try construct set linear operator extract underlie topic structure document document query project new usually small space compare use inner product large matrix support enable use matrix decomposition technique extract linear operator easily explore different viola discuss standard algorithm demonstrate ppserver allow perform interesting analysis large dataset semantic indexing latent semantic indexing lsi construct small document matrix use singular value decomposition u u contain eigenvector cooccurrence matrix diagonal element refer singular value contain square root correspond eigenvalue eigenvector large eigenvalue capture axis large variation datum lsi project document kdimensional subspace span column denote document query similarly project score lsi obtain simple matlab able use execute matlab code parallel large scale machine learning use parallel problem server figure singular value collection document term singular value half collection computation collection minute use processor sparse read sparse matrix compute ksvd compute score return combine relevance judgement obtain precisionrecall curve display plot addition evaluate performance technique explore characteristic datum example implementation lsi large collection use subset document computational reason lead question affect figure singular value large collection random half collection shape curve remarkably similar half suggest derive projection matrix half collection evaluation technique easily perform use system experiment nearly identical retrieval performance independent component document independent component analysis sejnowski recover linear projection datum lsi find principal component find axis statistically independent big success probably application blind source separation party problem problem observe output number microphone microphone assume record linear mixture number unknown source task recover original source natural embedding text retrieval framework word observed microphone signal underlie topic source signal rise figure typical distribution word project axis find word value close histogram word large positive result collection contain press release document distinct word p husband figure distribution word large magnitude axis negative value group word term group word directly relate different individual word group example occur time context document concern policy general happen act discriminate word observe appear find set word s select related document h set word t element select element intuitively select document general subject area t remove specific subset document leave small set highly relate document suggest straightforward algorithm achieve goal directly local clustering approach similar unsupervised version query analysis similar collection reveal interesting behavior large dataset example know attempt find matrix rank conflict notion collection actually reside small subspace find experiment axis highly produce distribution conjecture axis result distribution split arbitrarily axis purpose axis uninformative provide automatic technique apply large dataset purpose comparison figure illustrate performance algorithm include clustering technique article p enable portable interactive use parallel problem server powerful mechanism access optimize algorithm far client communication protocol possible implement transparent integration sufficiently powerful client tool researcher use matlab way prototype algorithm work small problem possible operate visualize large datum set demonstrate claim use ppserver system apply ml technique large dataset allow analysis datum use implement version diverse gradient descent collection contain document word large scale machine learning use parallel problem server lsi recall figure comparison different algorithm reference sejnowski t approach blind source separation blind deconvolution neural computation s choi d e user guide viola p mimic finding optima estimate probability density advance neural information processing r indexing latent semantic analysis journal society information science b r editor information retrieval datum structure e use portable parallel programming messagepasse interface husband p c tool interactive proceeding conference parallel scientific compute viola p sparse high dimensional datum effective retrieval advance neural information processing system new method weight query proceeding th framework learning advance neural information processing system portable implementation distribute memory parallel computer preliminary proceeding iterative method management tool update indexing scheme scientific computation ppserver parallel problem server web page m m e apply multiple cause mixture model text categorization proceeding th learn routing query query proceeding conference research development
sublinear time orthogonal tensor decomposition dept computer science dept electrical computer engineering recent work fast know orthogonal tensor decomposition provable guarantee base compute sketch input tensor require read entire input number case achieve theoretical guarantee sublinear time ie read input tensor instead use sketch estimate inner product tensor algorithm use importance sample achieve sublinear time need know norm tensor slice p number important case symmetric tensor t estimate norm sublinear time p important case p small value k estimate norm tensor sublinear time possible general norm slightly sublinear time possible main strength work empirical number case algorithm order magnitude fast exist method accuracy introduction tensor powerful tool deal multimodal datum system use attribute lead recommendation occur example look user activity time time day attribute base prediction discussion similar low rank matrix approximation seek tensor decomposition store tensor apply quickly popular decomposition method canonical decomposition decomposition tensor decompose sum rank component refer reader application include datum mining computational neuroscience statistical learning latent variable model mention natural question emergence large datum set decomposition perform quickly number work topic related recent work significantly speed decomposition orthogonal tensor decomposition use randomized technique linear sketching work focus orthogonal tensor decomposition idea create sketch input tensor perform implicit tensor decomposition approximate inner product exist decomposition method exist method power method involve compute inner product vector rank matrix vector slice tensor inner product work visit support research laboratory contract conference neural information processing system nip approximate fast instead compute inner product sketch vector significantly low dimension replace sketching sample approximate inner product discuss sample scheme compare work contribution number important case achieve theoretical guarantee work apply later sublinear time read input tensor previous work need walk input create sketch instead perform importance sampling base current iterate read entry tensor help learn norm tensor slice use version sample importance sample source speedup work come approximate inner product iteration robust tensor power method estimate vi ndimensional vector work compute sketch approximate hsu svi instead u sample coordinate proportional ui know sample estimate vi ui unbiased variance guarantee similar use sketch constant significantly small sketch need read entire tensor perform sample symmetric tensor focus orthogonal tensor decomposition symmetric tensor explain extension asymmetric case symmetric tensor arise application example represent symmetric tensor field stress anisotropic example diffusion mri use symmetric tensor describe diffusion brain body spectral method symmetric tensor exactly come latent dirichlet allocation problem symmetrize tensor use simple matrix operation sublinear orthogonal tensor symmetric matrix underlie p vi input tensor t t e vi set orthonormal vector goal reconstruct result naturally generalize approximation vector approximation tensor different length different dimension simplicity focus order p robust tensor power method generate random initial vector u perform t update step ti matrix refer slice typically converge eigenvector small number iteration choose small number l random initial vector boost confidence successive eigenvector find analysis immediately extend high order tensor use sample estimate u achieve guarantee typical setting parameter constant k eigenvalue assumption need sample slice iteration result n time observe additionally know square norm sample total sample expectation application norm know cheap compute single pass assumption obtain norm sublinear time symmetric tensor tjjj ejjj note noise read approximation slice norm tjjj approximation factor depend eigenvalue noise obtain nontrivial guarantee robust tensor power method assume sup kwk ui wk particular imply ejjj assumption come random initial vector v noise bind problem ejjj choose subject ejjj vi small unit vector constant completely mask noise ejjj lot information slice norm suppose tjjj ejjj use ejjj imply tjjj notice read tjjj slice afford sample sublinear run time remain slice sample slice estimate contribution ti slice sublinear number sample previous illustrate idea need read tjjj entry decide sample slice analysis complicated sign cancellation tjjj vj ejjj vj detect large fix read entry sublinear time additional assumption formal analysis importantly instead symmetric tensor consider order symmetric tensor p sign cancellation case restriction estimate slice norm need slice norm estimate hold eigenvector eigenvalue estimate sufficiently optimization algorithm base careful implementation generate sorted list random number random permutation find empirically fast iteration previous sketching algorithm addition read entire input tensor preprocessing step asymmetric tensor tensor tensor form ui impossible achieve sublinear time general hard distinguish t ei random t necessary sufficient assumption entry ui arbitrarily small constant case slice norm sample slice achieve sublinear time apply assumption symmetric tensor empirical result main strength work empirical result iteration approximate ti total b time independently median increase confidence notation b correspond number independent sketch use median work empirically theoretical issue discuss let b total number sample iteration correspond size notation find empirically set b small achieve error guarantee explanation variance bind obtain importance sampling factor small pth order tensor factor p small idea small set b b achieve roughly square residual norm error synthetic datum set dimension find good rank approximation algorithm need set parameter set run time second preprocessing time algorithm running time second second preprocesse time refer reader table section total time fast demonstrate algorithm realworld application use real dataset dataset sparse consider spectral algorithm latent dirichlet allocation use tensor decomposition core computational step significant speedup achieve tensor occur application refer reader table section example wiki dataset tensor dimension run time fast sketchingbase method previous sample algorithm previous samplingbase scheme achieve guarantee use uniform sampling work tensor element nonuniform sample require entry tensor pass notation let denote let denote outer product u u let t p order tensor dimension let denote inner product tensor b pn pn ai bi ip tensor r k pn pn ip ai ip random variable let ex denote expectation variance quantity exist main result explain detail main result section state importance sample tensor application second explain quickly produce list tuple accord certain distribution need algorithm combine second fast way approximating tensor contraction use subroutine iteration robust tensor power method provide main theoretical result estimate slice norm need main importance sample approximate inner product simple application importance sample tensor regard inner product dimensional vector importance sampling apply suggest sample accord importance eg sample wk probability number sample large approximate true tensor contraction wk small variance final rescaling suppose random variable ui vj wk pi probability pi qj rk kwk iid sample denote let l similarly importance sampling slice ti ie face suppose random variable rk probability rk kwk sample let wi generate importance sample linear time need efficient way sample index vector base importance view problem follow imagine divide bin different length correspond probability select bin number index probability vector generate m random number uniformly random number belong random number sample index vector know algorithm solve problem m time alternative r t algorithm combine efficiently generate m sort random number om time generate random permutation m om time use notation p p algorithm create distribution rn respectively note apply previous algorithm require z time form distribution om sample implicitly time fast approximate tensor contraction propose fast way approximately compute contraction ti sublinear number sample t compute tensor contraction use entry t subroutine approximate tensor subroutine approximate contraction function pprox p d pn l g r t bbi si bbi si d b ti median median bbi function b b b pe p d l g r t pe sd l sd pi qj r return return exact answer time sublinear time explicitly compute tensor represent implicitly sample follow error bound pprox let bbi number sample pprox ti let total number sample t u define number u u ti u b vector t u bbi follow bound hold kf onk addition fix rn t ui kf b eq obtain observe random variable t ui independent pn ui b b bi remark coordinatewise median b estimate use boost success probability appear gap argument unclear achieve coordinatewise median fix instead pay factor proportional number iteration sample complexity expectation bound quantity apply markov bind union bind iteration suffice main theorem concern sublinear time obtain high probability bound run multiple time independently coordinatewise median output eigenvector empirically work median iteration line replace theorem rest analysis unchanged sketchingbase robust tensor power method line sketchingbase approximate tensor contraction replace importance sampling procedure t pprox ti use theorem main theorem concern correctness robust tensor use recent improvement state general guarantee algorithm iteration noise guarantee theorem remove early eigenvalue assumption theorem suppose t t e vi orthonormal basis vector r k let max min large function use shorthand resp indicate resp absolute constant c bbi b bbi t t u pprox b bbi u u b b arg u t t u pprox b bbi k u u b return sketch sample prescanne sample prescanne tensor dimension sketching importance sampling preprocesse time second function b si know ti si run time second main algorithm sketch sample prescanne sample prescanne tensor dimension preprocessing time figure run time grow dimension k output robust tensor power method exist small value absolute constant c c c c e satisfie ut t l furthermore c min l k k probability exist permutation bi c combine previous importance sample analysis obtain theorem main assume notation suppose pn bj bj number sample sample power iteration recover bi p vb b k nk t k slice bbi bkt min output guarantee hold constant probability total time space onk require bbi need scan entire tensor compute algorithm sublinear follow mild assumption algorithm sublinear sample uniformly bbi compute bound slice norm e satisfie time condition practical tensor slice equal norm case occur ti slice apply asymmetric tensor analysis extend certain case remove bounded slice norm assumption idea sublinear number sample tensor obtain upper bound slice norm analysis robust tensor power method extend p replace contraction ti x ti x outline section p sign cancellation theorem order constant sufficiently small constant p tensor t e rn e satisfy run time outline section p small k sign consideration low rank constant symmetric tensor t e rn e satisfy run time experiment experiment setup dataset implementation share code base sketchingbase robust tensor power propose run experiment cpu gb memory mode run version algorithm version scan tensor accurately measure frobenius norm sample slice proportion frobenius norm pprox ti version prescanne assume norm slice bound use sample slice b total number sample algorithm analogous sketch length b synthetic dataset generate orthonormal basis vi compute synthetic tensor t vi normalize t add symmetric gaussian noise tensor e l control ratio synthetic tensor eigenvalue generate different decay inverse decay inverse square decay linear decay k set generate tensor high rank eigenvalue add noise scalability generate tensor different dimension reallife dataset latent dirichlet allocation lda powerful generative statistical model topic model spectral method propose solve model critical step spectral decompose symmetric k tensor orthogonal eigenvector number model topic follow step build tensor dataset run algorithm directly work tensor real application experiment use dataset previous work wiki additional reallife dataset refer reader repository code result result consider run time square residual norm evaluate performance pk algorithm tensor t let ui wi kf denote squared residual norm obtain robust power method reduce experiment time look eigenvalue eigenvector algorithm capable find number list time preprocesse time table depend tensor dimension n sketching base method depend time short require pass sequential access tensor efficient sublinear time verification theoretical result suggest total number sample algorithm time large experiment observe achieve similar accuracy indicate practice synthetic dataset run algorithm large number synthetic tensor different dimension different table result tensor dimension nonzero eigenvalue decay reach roughly residual norm run time algorithm time fast sketchingbase robust tensor method thank fact usually need relatively small b good residual hidden constant factor run time sample small sketch scale large tensor sublinear nature figure sketchingbase method b b large require sketch observe reasonable recovery algorithm choose b residual norm method need time dimension advantage minimal preprocessing step figure b preprocessing time grow prepare sketch dimension increase application eigenvector need preprocesse time large overhead reallife dataset small tensor dimension algorithm speedup method time fast reallife dataset achieve square residual norm table report result dataset different setting b b like synthetic dataset empirically observe constant importance sampling small b use sketch error guarantee b b sketch base robust power method square residual norm run time preprocesse time importance sample base robust power method prescanne square residual norm run time preprocesse time importance sampling base robust power method prescanne square residual norm run time preprocesse time table synthetic tensor decomposition use robust tensor power method use order normalize dense tensor dimension noise add run sketchingbase samplingbase method find eigenvalue eigenvector set t vary b sketch base robust power method wiki e square residual norm run time preprocesse time b e e e e e e e e e e importance sampling base robust power method prescanne dataset wiki e square residual norm run time preprocesse time e e e e e importance sampling base robust power method prescanne dataset wiki e square residual norm run time preprocesse time e e e e e table tensor decomposition wiki dataset tensor generate spectral dimension symmetric normalize fix t vary b reference kakade m tensor decomposition learn latent variable model jmlr p s m kakade spectral algorithm latent dirichlet allocation nip page b generate sorted list random number acm transaction mathematical software new sampling technique tensor m blei m latent dirichlet allocation jmlr efficient sampling method discrete distribution international automata language programming page springer choi distribute factorization tensor nip page e hazan sublinear optimization machine learn acm r foundation procedure model condition explanatory multimodal factor analysis m u p fast detection overlap community online tensor method e e scale tensor time algorithm discovery page d e art computer programming vol algorithm read page tensor decomposition application m p pass lp sample application soda page r fast scalable polynomial kernel explicit feature map page h p fast alternate ls algorithm high order factorization ieee transaction signal processing e mach fast randomized tensor decomposition page m smola spectral method hierarchical dirichlet process efficient method generate discrete random variable general distribution acm transaction mathematical software scalable inference latent dirichlet allocation page personal communication online tensor decomposition ab fast guarantee tensor decomposition sketching nip page
performance synthetic network classification c krishnamurthy department electrical abstract study evaluate performance competitive learning network identify commercial aircraft measurement performance neural network classifier compare nearestneighbor classifier result indicate problem network classifier relatively insensitive change network topology noise level training datum problem traditional algorithm outperform simple neural classifier neural network potential improved performance introduction design system identify object base measurement radar signal traditionally predicate decisiontheoretic method pattern recognition true method characterize welldefine sense optimality depend availability accurate model statistical property radar measurement synthetic neural network attractive alternative problem learn perform classification label training datum require knowledge statistical model primary objective investigation establish feasibility use synthetic neural network identification radar object characterize neural network decisiontheoretic methodology design radar object identification system present study focus performance evaluation system operate receive radar signal commercial aircraft particular present result competitive learning synthetic network model compare result nearestneighbor maximumlikelihood classification algorithm paper performance classification algorithm evaluate mean krishnamurthy computer simulation study result compare number condition concern radar environment receiver model sensitivity network classifier respect number layer number hide unit investigate case result obtain use synthetic network classifier compare obtain use optimal maximumlikelihood classifier nearestneighbor classifier problem description radar system model system measure radar band frequency choose correspond region aircraft frequency wavelength approximately equal length object specific frequency employ study database maintain laboratory compact radar range optimal feature available measurement band performance result present system model inphase quadrature measurement capability coherent system system model signal magnitude measurement capability non coherent system coherent system observation vector x x x x x represent inphase quadrature component noisy measurement unknown target element correspond complex scatter coefficient magnitude square root measured cross section unit square meter m complex phase measure signal frequency system observation vector ai consist component magnitude noisy measurement correspond square root measure simulation experiment assume receive signal result superposition noise vector model sample additive white gaussian process coherent measurement case coherent radar system kth frequency component vector wi s inphase quadrature component signal inphase quadrature component sample additive white process frequency component model variable variance performance synthetic neural network classification total additive noise contribution frequency gaussian mean operation network classifier present observation vector dimension consist inphase quadrature component frequency measurement typically neural net train use sample observation vector commercial aircraft discuss desire output vector form dij desire aircraft example output vector di second aircraft appear second position structure neural net use represent nl nh input neuron hide layer neuron h hide layer output neuron experiment test perceptron net vary architecture figure little change performance net effect signaltonoise ratio datum observe training phase performance perceptron investigate result present figure network little change performance training datum snr reach repeat basic experiment use winnertakeall network figure performance network effect change network architecture net train noisy datum fig performance decrease snr training datum increase overall performance close performance final experiment compare performance perceptron net classifier near neighbor classifier result figure experiment training datum superimpose noise result superior require knowledge noise distribution average net perform perceptron near neighbor perform neural network model krishnamurthy e e x t r o figure performance perceptron different number hide unit c l figure performance perceptron hide layer synthetic neural network classification r r t l noise free o figure performance perceptron different snr training datum r e figure performance vary hide unit krishnamurthy l e noise free figure performance network different snr training datum perceptron near neighbor figure comparison classifier coherent datum case performance synthetic neural network classification measurement case radar system model observation vector frequency component s s inphase quadrature component inphase quadrature component additive white gaussian noise underlie noise process additive gaussian distribution observation component non coherent system model case non coherent measurement neural network classifier present observation vector component magnitude noisy measurement frequency coherent case neural net typically train sample aircraft use form discuss structure neural net experiment nl nh training testing procedure coherent case follow figure comparison performance neural net classifier maximum likelihood near neighbor classifier outperform classifier nearestneighbor classifier second performance neural network classifier perform roughly conclusion experiment lead conclude neural network good candidate radar classification application neural network learn method test similar performance relatively insensitive change network architecture network topology noise level training datum method use implement neural network classifier relatively simple level performance neural classifier impressive ongoing research concentrate improve neural classifier performance introduce sophisticated learn algorithm propose investigate method improve performance perceptron example increase training time krishnamurthy figure comparison classifier non coherent datum case reference b automatic target recognition state art survey ieee transaction electronic system vol r r introduction compute neural net vol s c krishnamurthy p e new competitive learning algorithm vector quantization use neural network network submit d feature selection reliable radar target identification proceeding ieee national conference selforganization associative memory
transportability multiple environment limited experiment completeness result computer computer paper address problem mztransportability transfer causal knowledge collect heterogeneous domain target domain passive observation limited experimental datum collect paper establish necessary sufficient condition decide feasibility mztransportability causal effect target domain estimable information available far prove previously establish algorithm compute transport formula fact complete failure imply nonexistence transport formula finally paper docalculus complete mztransportability class motivation issue generalize causal knowledge central scientific inference experiment conduct conclusion obtain laboratory set specific population domain study transport apply environment differ aspect laboratory target environment arbitrary drastically different study environment causal relation learn scientific progress come fact scientific continue provide useful information world suggest certain environment share common characteristic owe commonality causal claim valid experiment perform remarkably condition type extrapolation formally recently problem extensively discuss statistic economic health science external validity discussion limit form heuristic experimental researcher formal treatment problem attempt answer practical challenge generalize causal knowledge multiple heterogeneous domain experimental datum pose paper lack sound mathematical setting preclude main goal machine learning large computer science automate process class problem causal transportability formally consider general instance transportability know date problem transport experimental knowledge heterogeneous setting certain specific target introduce formal language encode difference commonality domain accompany necessary sufficient condition transportability empirical finding feasible domain source target condition extend complete characterization transportability domain unrestricted experimental datum subsequently assumption relax consider setting limited experiment available source domain far multiple source domain unrestricted experimental information available multiple heterogeneous source limited distinct experiment mztransportability specifically mztransportability problem concern transfer causal knowledge heterogeneous collection source domain n target domain domain experiment set variable zi perform causal knowledge gather potentially different passive observation collect constraint problem infer causal relationship r use knowledge obtain problem study generalize onedimensional version transportability limited scope multiple dimensional unlimited scope previously study interestingly certain effect individually transportable target domain experiment available source combine different piece source enable estimation conversely possible effect estimable multiple experiment individual domain experiment scatter domain discuss goal paper formally understand condition causal effect target domain estimable available datum sufficient condition mztransportability treatment fall short provide guarantee condition necessary augment replace general paper establish follow result necessary sufficient condition decide causal effect target domain estimable statistical information available causal information transfer experiment domain proof algorithm propose fact complete compute formula strategy devise combine empirical evidence synthesize target relation improve proof docalculus complete mztransportability class background transportability section consider transportability instance discuss relationship mztransportability set consider fig node represent factor produce difference source target population conduct randomized trial estimate causal effect treatment outcome age group z denote p ydox wish generalize result population find distribution x la different p z particular average age significantly high estimate causal effect denote p ydox selection diagram overlap diagram example fig convey assumption difference population factor determine age distribution z effect p invariant population factor represent special set variable selection variable s simply depict assumption overall causal effect derive follow r p z p z z line constitute transport formula r combine experimental result obtain observational aspect population z obtain causal claim traditionally machine learn literature concern discrepancy domain context exclusively predictive classification task oppose learn causal measure interestingly recent work learning leverage knowledge invariance underlie structure domain literature general modality learn use use structural interpretation causal diagram describe appendix z z z z z z c z d e figure selection diagram illustrate transportability p ydox domain trivially solve simple recalibration small diagram relation transportable cd selection diagram illustrate estimate r individual transportability z z experiment available z b r transportable selection diagram illustrate opposite phenomenon transportability multiple domain feasible z z domain selection variable s depict square node p trivial example transport formula simple recalibration effect account new age distribution general involved mixture experimental observational finding necessary obtain unbiased estimate target relation r certain case way synthesize transport formula instance fig b depict small example transportability feasible randomized goal characterize case real world application happen limited experimental information gather source environment question arise limited set experiment able estimate desire effect target domain illustrate subtle issue mztransportability entail consider fig concern transport experimental result source b infer effect p ydox diagram represent treatment level represent variable represent intermediate variable represent outcome heart failure assume experimental study randomize conduct domain domain b simple analysis r transport source experiment available variable combine experiment source allow determine effect target follow transport formula p p b z x p transport formula mixture experimental result b p b z x result experiment z p constitute consistent target relation far consider fig ef illustrate opposite phenomenon case experiment z available domain b r transportable z available domain r transportable equal p yx independently value z intriguing result entail fundamental issue answer paper docalculus complete relative problem find transport formula exist second assume exist sequence application docalculus achieve reduction require mztransportability find sequence computational intractable efficient way need obtain formula graphical condition mztransportability basic framework analysis rest structural causal model define pp model structural causal framework ch action modification functional relationship action causal model m produce new model p ui v set observable variable u set variable obtain replace new function output constant value follow convention denote variable letter realize value small letter similarly set variable denote bold letter set realize value bold small letter use typical terminology correspond p denote respectively set observable descendant parent node set graph denote induce subgraph g contain node arrow node finally stand edge subgraph arrow incoming arrow remove key analysis transportability notion identifiability express requirement causal effect computable combination data p assumption embody causal model induce diagram associate particular domain ie set population environment representation extend transportability capture property domain simultaneously possible assume structural equation share set argument functional form equation vary arbitrarily definition selection diagram let m pair structural causal model relative domain share diagram m induce selection diagram construct follow edge edge d contain extra edge exist discrepancy fi fi p p m m word locate mechanism structural discrepancy domain place armed concept identifiability selection diagram mztransportability causal effect define follow definition mztransportability let d dn collection selection diagram relative source domain target domain respectively z variable experiment conduct domain pair observational interventional distribution z zi p analogous manner observational interventional distribution r mztransportable d uniquely computable izi model induce d definition appear formalization statement r need uniquely computable information set naturally component multiple observational interventional distribution requirement computability izi source syntactic image docalculus capture follow sufficient condition let d dn collection selection diagram relative source domain target domain respectively si represent collection selection diagram di let izi respectively pair observational interventional distribution source target effect r p mztransportable d expression use rule docalculus expression apply subset izi si variable apply subset difficult fig fig ef sequence application rule docalculus reach reduction require yield transport formula section obvious sequence exist experiment available z b exist clear imply inability transport turn specific example sequence target relation r transportable mean exist model equally compatible datum ie generate dataset model entail different answer effect r violate uniqueness requirement demonstrate fact formally existence structural discuss reference assumption structural change domain relax structural assumption regard discrepancy domain hold eg transportability assume structural knowledge domain know order production respective causal diagram absence knowledge algorithm use infer diagram datum usually current state scientific knowledge problem encode form selection diagram constraint observed distribution way answer entail independently detail function probability exogenous x z u c b figure ab selection diagram possible transport p ydox experiment z cd example diagram path need extend satisfy definition model m m follow equality inequality distribution hold pm z pm z pm pm z pm pm pm x pm z pm value z value let assume variable u v binary let u u u common cause respectively let u u u random exclusive z respectively u u u extra random exclusive let sa index model follow way tuple represent domain b respectively define model follow u u u u sa z u u sa m m z u u u s b u u u represent exclusive function model agree respect p u define p ui difficult evaluate model note constraint eq satisfied include result follow goal demonstrate converse collect different example previous try sense pattern case generalize complete characterization mztransportability syntactic subtask mztransportability determine certain effect identifiable source domain interventional datum available fundamental result develop identifiability relevant mztransportability consider component ccomponent define stand cluster variable connect edge separable observable system key result causal graph subgraph induce unique ccomponent decomposition lemma decomposition instrumental series condition ordinary identification inability recursively decompose certain graph later use prove completeness definition ccomponent let causal diagram subset form span tree vertex ccomponent component subsequently propose extension ccomponent cforest essentially enforce ccomponent span forest close relation sophisticated argument evaluate model proof appendix definition let causal diagram maximal root set ccomponent observable node child consider fig c note exist nod root exist nod z root case trivial cforest pair cforest z z ie root set treatment variable structure hedge identifiability infeasible hedge exist clearly existence hedge effect interest mztransportable example hedge capture immediate way structure need characterize mztransportability ie graph hedge hedge edge subgraph target quantity mztransportable base observation propose follow definition lead boundary class mztransportable relation definition shedge let dn collection selection diagram relative source domain target domain respectively si represent collection selection diagram let causal diagram let izi s collection pair observational interventional distribution izi z zi p analogous manner observational interventional distribution set experimental variable consider pair cforest hedge induce collection pair cforest relative experiment hedge follow condition hold domain exist variable point induce diagram set collection pair cforest induce diagram relative furthermore shedge exist direct path r r r pass appendix definition shedge appear involve articulation computability requirement def implicitly syntactic goal explicit graphical fashion specifically certain factor need computation effect p ydox domain enforce separable induce root set component qi belong experiment available domain sufficient solve instance assume want compute q p ydox fig d q decompose factor q pz q case factor q hold true experiment available guarantee computability factor similar analysis apply q ie shedge q computable available datum def ask explicit existence path node root set r r simple example help illustrate requirement consider fig goal compute p ydox extra experimental information exist hedge q induce z note ccomponent induce graph lead counterexample computability p ydox use subgraph possible construct counterexample marginal effect p ydox fact p z computable p z quantity p identifiable structural model compatible subgraph generate value marginalization p happen root set r augment fig prefer add requirement explicitly definition involved scenario procedure trmz p s input x value assignment p local distribution relative domain s index active experiment weight scheme selection diagram selection node s relative follow set distribution globally define zi p output px term p p return return trmz p s set return trmz q c c return trmz p s c d p cd return p p d c c c c set return c c s c si zi trmz zi p zi zi return ei c figure modify version identification algorithm capable recognize mztransportability prefer omit sake presentation add direct path z pass construct follow counterexample q u u u u u u b u b m m u u p ui p p b immediate model produce desire property refer appendix formal proof statement definition shedge state connection hedge shedge directly proof find theorem hedge experimental datum available exist shedge domain consider experimental datum available result state shedge construct hedge imply operate shedge converse hold z finally concentrate general case shedge experimental datum multiple domain state sequel let d dn collection selection diagram relative source domain target domain respectively izi define appropriately shedge effect px relative experiment r mztransportable d powerful result state existence shedge preclude mztransportability proof statement somewhat involved supplementary material detail let consider selection diagram da relative domain b fig goal q p ydox experiment z b case exist shedge relative experiment note x exist selection variable s point domain condition satisfied trivial graph variable solve inspection somewhat involved efficiently evaluate condition definition structure motivate search procedure recognize shedge couple previous theorem complete algorithm mztransportability exist extensive literature concerned problem computability causal relation combination assumption datum section build work treat problem graphical mean concentrate particularly algorithm construct fig follow result input collection selection diagram correspond experimental datum corresponding domain return transport formula able produce main idea algorithm leverage ccomponent factorization recursively decompose target relation piece line try solve separately standard evaluation fail target domain line trmz try use experimental information available target source domain line concrete view work run example pp systematic fashion basically implement condition trmz sound theoretical guarantee failure find transport formula imply nonexistence complete lack transportability guarantee precisely state sequel assume trmz fail transport effect px exit failure execute line exist graph pair d c return fail condition contain edge subgraph cforest span shedge proof let subgraph local trmz fail root set d possible remove direct arrow d preserve r root result construction c close descendent direct arrow remove cforest construction fact recursive clearly subset original input failure evaluate false line difficult point respective experiment able break local hedge line remain stretch generate shedge construction apply supplementary material finally state completeness algorithm graphical condition completeness complete corollary shedge characterization px mztransportable d shedge d x furthermore docalculus complete establish mztransportability mean failure exhaustive application rule imply nonexistence mapping available datum target relation ie formula independently method use obtain mapping corollary docalculus characterization rule docalculus standard probability manipulation complete establish mztransportability causal effect conclusion paper provide complete characterization form graphical condition decide mztransportability far procedure introduce compute transport formula complete mean set transportable instance identify algorithm broaden assumption finally docalculus complete class problem mean find proof strategy language suffice solve problem nonparametric characterization establish paper rise new set research question analysis aim achieve unbiased transport asymptotic condition additional consideration need account deal finite sample specifically sample size vary significantly study statistical power consideration need invoke bias consideration furthermore transport formula exist approximation technique resort example replace requirement nonparametric analysis assumption linearity monotonicity certain relationship domain nonparametric characterization provide paper serve approximation scheme reference d experimental design publish identification prediction decision cambridge l hedge statistical method academic press experimental design generalize c causal inference method principle research analytical method social cambridge pearl e transportability causal statistical relation formal approach roth editor proceeding national conference artificial intelligence page e causal effect completeness result b editor proceeding national conference artificial intelligence page e pearl general algorithm decide transportability experimental result e limited experiment m m littman editor proceeding national conference artificial intelligence page park causal transportability experiment subset variable p smyth editor proceeding conference uncertainty artificial intelligence uai page press e causal effect formal approach p ravikumar editor proceeding international conference artificial intelligence statistic aistat page jmlr transportability causal effect multiple environment m m littman editor proceeding national conference artificial intelligence page park e transportability multiple environment limited experiment burge bottou m weinberger editor advance neural information processing system page iii d domain adaptation statistical classifier journal artificial research storkey training test set different learn transfer m nd editor dataset shift machine learn page mit cambridge scholkopf d learn j langford editor proceeding th international conference machine learning icml page b domain adaptation target conditional shift proceeding th international conference machine learn icml jmlr volume pearl causality model reasoning p r prediction search mit study causal reasoning learn computer pearl general identification condition causal effect proceeding national conference artificial intelligence page pearl identification joint interventional distribution recursive model proceeding national conference artificial intelligence page
fast variational largescale internet diagnosis way abstract web server internet need maintain high reliability cause failure web transaction use approximate bayesian inference problem web service diagnosis problem far large previously attempt require inference possible fault observation far inference perform second inference speed combine meanfield variational approximation use stochastic gradient descent optimize variational cost function use fast inference time series anomalous request real web service inference fast analyze network log billion entry matter hour introduction internet content provider depend correct internet communicate user provide service content provider lose network connectivity user critical quickly resolve problem failure lie system challenge content provider little direct internet cause user request failure request fail problem content provider system fault network user content provider include server fail request service attack bug user software compound diagnosis problem fault use probabilistic inference perform diagnosis use second challenge scale involve popular internet content provider receive billion http request week number potential cause failure numerous autonomous system user receive internet connectivity potential cause failure paper approximate bayesian inference scale handle high rate observation accurately estimate underlie failure rate large number potential cause failure scale bayesian inference problem simplify approximation introduce bipartite graphical model use overlap noisyor model interaction fault observation second use mean loss connectivity user translate directly lose revenue content provider cause problem network component field variational inference map diagnosis problem optimization problem far approximate integral variational method fourth speed optimization problem use stochastic gradient descent paper structure follow section discuss relate work paper describe graphical model section approximate inference model section include stochastic gradient descent section present result synthetic real datum section draw conclusion previous work original application bayesian diagnosis medicine original diagnosis network bipartite graphical model use noisyor model disease exact inference network intractable exponential number positive different approximation sample algorithm propose propose sample propose use variational approximation input network thousand possible disease consider challenge recently researcher apply bayesian technique diagnosis computer network work tend avoid inference large network speed constraint contrast attack inference problem directly graphical model diagnosis beta figure graphical model diagnosis internet fault initial graphical model diagnosis figure start observe large number binary random variable correspond single http request failure http request model noisyor set binary variable model underlie factor cause request fail p ri dij probability observation failure single underlie fault dij present matrix typically sparse small number possible cause failure request parameter model probability failure know cause set probability expert noisyor model causal structure network connection metadata associate http request example single request fail figure graphical model integrate instantaneous fault bipartite network beta distribution hide variable server fail cause lose connectivity content provider user agent compatible service underlying cause model independently request possible fault system variable dij depend underlie continuous fault rate variable d p dij j ij j dij probability fault manifest time model fj independent beta distribution bj beta function fanout fault rate different fault rate connect observation common connect order identify hidden fault goal model posterior distribution p track time existence dij random variable v dij distribution instantaneous problem want estimate d interesting fortunately exactly integrate variable connect observation noisyor integrate dij graphical model figure model completely analogous mode instead noisyor combine binary random variable combine rate variable p view generalization noisyor continuous variable approximation inference tractable order scale inference hide variable observation choose simple robust approximate mean factorize distribution field approximate posterior p v infer fault rate choose approximate p product beta distribution b meanfield variational inference maximize lower bind evidence model z p log p d max q q integral break term crossentropy approximate posterior prior expect loglikelihood observation e log q max d log p p integral negative sum beta distribution closed form bj dkl log bj function expect log likelihood noisyor integrate product beta distribution analytic form employ replace expectation log likelihood log likelihood expectation second term sum set log likelihood observation log ri failure log ri log success internet diagnosis case reasonable expect posterior distribution concentrate mean large datum available prove accuracy bound mf base number parent observation final cost function minimization routine dkl j variational inference stochastic gradient descent order apply unconstrained optimization algorithm minimize need transform variable positive valid parameterize gradient computation dkl similar gradient bj note gradient computation computationally expensive sum observation internet diagnosis decompose observation stream block size block determine quickly underlie rate fault change want sample rate typically use block observation computation gradient expensive far repeat inference thousand block datum prefer fast optimization procedure highly accurate investigate use stochastic gradient descent optimize variational cost function stochastic gradient descent approximate gradient variational require noisyor parameter prior observation vi initialize bj initialize number epoch fault j dkl dkl bj bj bj end observation parent fault observation vi j end parent fault observation vi j bj bj bj end end end single term gradient state optimization update use single term enable system converge quickly approximate answer detail stochastic gradient descent estimate sum equation single term add noise estimate example sign single gradient term depend sign order reduce noise estimate use momentum exponentially smooth gradient firstorder filter apply state variable momentum modification typically use large step size momentum term order react quickly change fault rate smooth noise stochastic gradient descent use purely online method datum point set number epoch alternatively high accuracy allow sweep datum multiple time possible approach consider test approach solve approximate problem propose variational inference method bipartite noisyor network variational parameter introduce observation network typically far observation possible fault previous approach force solve large optimization problem parameter instead solve optimization dimension equal number fault originally optimize variational cost function trustregion algorithm optimization turn far bad stochastic descent find c implementation describe nocedal speed exact optimization order magnitude report lbfgs performance speed descent experiment metropolishasting sample posterior use walk bj find time long update slow speed single update depend fanout fault internet diagnosis network fanout high single fault affect observation metropolishasting far slow variational try loopy belief propagation expectation propagation beta distribution conjugate noisy message pass close form finally try idea learn predict posterior observation sample generative model learn reverse mapping internet diagnosis know structure graphical model block datum ahead time structure depend metadata request log learning time predictive model result test approximation optimization method use internet diagnosis synthetic real datum synthetic datum know hide state test accuracy approximate inference difficult large graphical model true posterior distribution intractable probe reliability model synthetic datum set start generate fault rate prior fault draw randomly generate connection fault observation probability connection strength draw randomly generate observation noisyor model observation predict approximate posterior number observation large number fault expect posterior distribution tightly cluster rate generate observation difference true rate mean approximate posterior reflect inaccuracy estimation error rate estimate figure error estimate rate true underlie rate black dot red dot stochastic gradient descent epoch result run figure figure error estimate small useful understand network error slight systematic bias stochastic gradient descent compare improvement speed table worth loss accuracy need inference fast possible scale billion sample run time ghz code sgd sgd epoch table accuracy speed synthetic datum set real datum web server log test algorithm real datum major web service observation consist success failure single http request select possible fault occur frequently dataset include web server receive request autonomous system originate request user agent robot generate request analyze http log collect month stochastic gradient descent algorithm paper present analysis short hour window contain high rate failure order demonstrate algorithm help understand cause failure base observation break time series observation block observation infer hide rate block initial state optimizer set state optimizer convergence previous block stochastic momentum variable carry forward block block pm pm pm pm pm pm pm pm figure infer fault rate autonomous system function time fault high rate result tracking experiment figure figure use stochastic gradient descent beta prior figure fault probability high time interval correspond city cause failure roughly time common service attack originate city speed analysis fast real time data set sample require cpu second sgd pass datum block require second allow log contain billion entry matter hour conclusion paper present variational inference problem scale internet observation web server diagnosis determine web server need internet break web server compatible user agent order scale inference diagnosis problem approximation use meanfield variational inference approximate posterior distribution expect log likelihood inside variational cost function approximate approximation finally use stochastic gradient descent perform variational optimization currently use variational stochastic gradient descent analyze log contain billion request aware application variational inference scale future publication include conclusion analysis implication web service internet large reference m e failure diagnosis use decision tree computing page d tractable inference algorithm multiple disease page t probabilistic inference database artificial intelligence research m introduction variational method graphical model machine learn stochastic approximation recursive algorithm application p minka expectation propagation approximate bayesian inference page q recognition network approximate inference network page p m belief propagation inference empirical study page m algorithm twolayer network proc nip page nocedal s pearl probabilistic reasoning intelligent system network plausible inference m accuracy efficiency tradeoff probabilistic diagnosis page m empirical analysis simulation large medical belief network computer biomedical research m b e m p probabilistic diagnosis use reformulation knowledge base method information medicine lm algorithm momentum update circuit system page m endtoend service failure diagnosis use belief network network operation management symposium page
sequential transfer multiarmed bandit finite set model school computer team sequel computer science cmu abstract learn prior task transfer experience improve future performance critical build learning agent result supervised reinforcement learning transfer significantly improve learning performance literature transfer focus batch learn task paper study problem sequential transfer online learn notably multiarmed bandit framework objective minimize total regret sequence task transfer knowledge prior task introduce novel bandit base approach estimate possible task derive regret bound introduction learn prior task transfer experience improve future performance key aspect intelligence critical build learning agent recently multitask transfer learning receive attention supervised reinforcement learning set empirical theoretical encouraging result recent survey work focus scenario task batch learn problem training set directly provide learner hand online learn set lugosi learner present sample sequential fashion rarely consider example rl e discussion relate setting multiarmed bandit mab robbin simple powerful framework online learning partial feedback problem encompass large number application clinical trial web advertisement adaptive routing paper step understanding provide formal bound transfer stochastic mab focus sequential transfer scenario online learner act series task draw stationary distribution finite set mab learning problem task standard mab problem fix number step prior learn model parameter bandit problem know learner know distribution probability bandit problem assume learner provide identity task learning act efficiently setting crucial define mechanism transfer knowledge task fact learner encounter bandit problem learning efficient algorithm able leverage knowledge obtain previous task present problem address problem transfer estimate possible model prior task current model accurately estimate extension able efficiently exploit prior knowledge reduce regret task main contribution paper introduce tucb algorithm transfer model estimate task use knowledge achieve performance ucb prove new algorithm guarantee perform ucb early episode avoid negative transfer effect approach performance ideal case model know advance estimate model rely new variant method moment robust tensor power extend multitask bandit setting prove rtp provide consistent estimate mean arm model long pull time task prove sample complexity bound finally report preliminary result synthetic datum confirm theoretical finding extended version paper contain proof additional comment available preliminary consider stochastic mab problem define set arm k arm characterize distribution sample reward observe arm independent identically distribute focus setting exist set model contain possible bandit problem denote mean arm good arm good value model respectively define arm gap arm model model gap arm model define assume arm reward bound consider sequential transfer set episode learner interact task draw distribution step objective minimize episode measure difference reward obtain pull achieve x j number pull arm step episode introduce tensor notation let x rk random realization reward arm model realization conditional model ex ith component rk realization define second moment matrix m m ex moment tensor m m ex l realization conditionally independent ex ex p allow rewrite second moment p m m p v pth let rd order member tensor product euclidean space rk m define map follow set matrix p way array representation r use different norm euclidean norm matrix aij bandit finite model consider transfer problem require set model number step simple variation allow effectively exploit knowledge build t obtain significant reduction regret select arg mucb algorithm fig pull arm observe sample update input set model include current step t unknown model compute subset t contain figure mucb model mean current model obtain average current estimate mean notice estimate model involve solve latent variable model estimation problem stateoftheart tit pull uncertainty explicit definition term notice arm satisfy compatibility condition discard model model t mucb select model large optimal value pull correspond optimal arm choice coherent face uncertainty principle use algorithm mucb pull optimal arm correspond optimistic model compatible current estimate mucb incur regret bad ucb significantly small denote set arm optimal model set set model arm optimal set optimistic model model correspond optimal arm follow bound expect regret similar bound hold high probability lemma proof use standard tool bandit literature available theorem mucb run set m model q tit tit number pull arm beginning step t expect regret log mn mini set arm optimal optimistic model set optimistic model optimal arm remark comparison ucb ucb incur regret log mini mucb display major improvement regret write log mucb result suggest mucb tend discard model optimistic actual model discard result model t optimal arm pull end significantly reduce set arm actually pull mucb previous bind depend number arm k furthermore arm minimum guarantee large arm gap gap mini far improve performance mucb wrt ucb online transfer unknown model consider case set model unknown regret multiple task draw eq introduce tucb transfer estimate accuracy improve episode use approach bandit outline structure online transfer bandit use bandit umucb uncertain objective minimize regret episode rtp robust tensor power method episode compute estimate arm mean model bandit extension mucb algorithm compute set model mean compatible current estimate case exact model available model estimate uncertainty mean provide input umucb account definition require number arm k number model m constant initialize estimate model sample run run m end require set model num step pull arm time build compute compute arg pull arm arg maxi observe sample tit update end return sample r figure tucb figure umucb algorithm require sample r number model m arm episode m use reward sample estimate second moment m compute r r m large eigenvalue eigenvector m compute whiten mapping d tensor m plug compute b b vb return figure robust tensor active set compute compute upperconfidence bind value arm model return good arm optimistic model uncertainty model estimate model optimal arm upperconfidence bind mean arm use upperconfidence bind directly derive sample observe far guarantee consistent sample generate actual model umucb terminate update estimate model mean bj rk use sample obtain arm beginning task umucb pull arm time need sample arm accurately estimate nd rd precisely rtp use reward sample generate episode estimate nd rd moment m l l l m l l l l l vector l l l rk obtain divide sample observe episode l batch average l average m consistent l l independent estimate m second moment m m rtp rely fact model mean recover spectral decomposition symmetric tensor m whiten matrix m m definition mapping mild assumption later assumption model mean obtain v pair b t estimate eigenvector vb eigenvalue c vb compute estimate m m m tensor m b obtain inverse transformation bj detailed description pseudo inverse notice l l l empirical mean arm end episode c ie whiten matrix m c matrix r m unique choose d r diagonal matrix consist b correspond eigenvector column large eigenvalue m sample complexity robust tensor power method umucb require input uncertainty model estimate need sample complexity bound accuracy compute rtp performance m wrt true moment directly affect error estimate m prove number task grow error rapidly decrease rate result provide upperbound error need build confidence interval umucb follow definition assumption require result definition let m m set m large eigenvalue matrix m define min min m max max m max max define minimum gap distinct eigenvalue m min l assumption mean vector linear independent state main result form high probability bind estimation error reward vector model p theorem pick let c c max min min max universal constant assumption exist constant permutation q m logk bj k m logk min min min remark computation c illustrate fig umucb estimate bj accuracy bind report provide upper confidence bind error estimate contain term computable general practice c consider parameter parameter usually introduce definition term regret analysis umucb analyze regret umucb set model provide input episode model define set arm ie potentially optimal arm arm actual model set optimistic arm result set optimistic model case uncertainty model estimate mucb model discard end long episode optimistic model set model discard define finally want apply previous definition set model instead single model j proof follow result available report number pull corresponding regret bind corollary episode umucb run j eq parameter arm pull time j log log min j set model optimistic arm b ie set arm propose model discard ie set arm propose model discard previous corollary state arm optimal optimistic model optimistic arm pull umucb focus arm arm help remove model active set ie potentially pull ucb remain arm optimal model discard ie simply pull accord ucb strategy similar mucb umucb pull arm optimistic active set change long optimistic evidence actual sample derive perepisode regret umucb theorem umucb run step set model estimate rtp episode actual model j expect regret wrt random realization episode conditional log log min j j remark negative transfer transfer knowledge introduce bias learning process beneficial nonetheless case transfer result bias wrong solution bad learning performance phenomenon refer negative transfer interesting aspect previous umucb guarantee perform bad ucb imply tucb suffer negative transfer set contain highly uncertain model bias umucb pull suboptimal arm remark improvement mucb exploit knowledge focus restrict set arm pull ucb umucb improvement clear model know estimate online episode similar mucb main source potential improvement wrt ucb illustrate regret bind umucb focus arm potentially small set furthermore number pull arm small ucb estimate model gap b big eventually umucb reach performance improvement ucb mucb big fact set optimistic model reduce use mucb optimistic model optimal arm ie set optimistic arm correspond match condition mucb instance model order arm need episode optimistic model optimal arm independently actual identity model condition restrictive practice umucb start improve ucb early illustrate numerical simulation regret analysis tucb previous result derive bound cumulative regret theorem tucb run episode step task draw fix distribution set model cumulative regret x j log log min wrt randomization task realization arm episode complexity reward m m m m m m m m m m m m m m m m m m m m m m m m m m m m m m number task m m m m m model figure set model figure complexity task regret ucb tucb perepisode regret number step number task figure regret ucb tucb episode length figure perepisode regret tucb result immediately follow linear dependency number episode dependency price pay know identity current task task reveal beginning task bandit algorithm simply cluster sample come task incur small cumulative regret logarithmic dependency episode step ie nonetheless discuss previous section cumulative regret tucb bad ucb number task increase approach performance mucb fully exploit prior knowledge numerical simulation section report preliminary result tucb synthetic datum objective illustrate support previous theoretical finding define set problem arm mean report fig sect actual value model different color square correspond optimal arm optimal model set model choose challenge illustrate interesting case useful understand model differ optimal arm difficult distinguish arm optimal model potentially select mucb model share exactly mean value imply model discard pull suggest mucb pull arm case model challenge ucb small minimum gap arm actually optimal model report performance ucb assumption know immediately discard arm optimal perform ucb remain arm model distribution uniform m discuss transfer result compare ucb ucb mucb illustrate advantage prior knowledge wrt ucb fig report perepisode regret notice satisfie assumption small singular value estimation model difficult min algorithm episode different length performance tucb discuss later result average model run algorithm use confidence bind performance mucb significantly ucb ucb mucb efficient use prior knowledge furthermore fig horizontal line correspond value regret bound n dependent term constant different model average wrt algorithm actual value different model supplementary material value improvement observe practice accurately predicate upperbound derive analyze performance tucb fig perepisode regret change episode transfer problem task length tucb use eq c discuss ucb mucb define boundary performance tucb fact begin tucb select arm accord ucb strategy prior information model available hand task observe tucb able transfer knowledge acquire episode build increasingly accurate estimate model approach behavior mucb confirm fig complexity tucb change episode case regret complexity tucb reach performance mucb fact model relatively small gap number episode accurate estimate model reach performance mucb large remark final objective achieve small global regret eq fig report cumulative regret average total number task different value n graph tucb outperform ucb tend approach performance mucb increase value n conclusion open question paper introduce transfer problem multiarmed bandit framework task draw finite set bandit problem introduce able leverage prior knowledge set bandit problem reduce regret wrt ucb set model unknown define variant rtp consistently estimate mean model sample collect episode knowledge transfer umucb perform bad ucb tend approach performance mucb algorithm derive regret bound preliminary numerical simulation good knowledge work study problem transfer multiarmed bandit open series interesting direction include explicit model identification improve transfer regret optimality tucb episode tucb transfer knowledge acquire previous task achieve small perepisode regret use umucb strategy guarantee perepisode regret tucb bad ucb optimal strategy term cumulative regret episode fact large preferable run model identification algorithm instead umucb early episode improve quality estimate algorithm incur large regret early task linear approach performance mucb later episode fast tucb tradeoff identification model transfer knowledge suggest different algorithm tucb possible unknown size problem size model set m know learner need estimate problem address estimate rank matrix m equal m note relax assumption need positive assumption use estimate model size oppose m m depend mean model acknowledgment research support national award like thank kakade valuable discussion like acknowledge support high education research regional european seventh framework programme grant agreement project p instance ucb compute reference agarwal m schapire r e contextual bandit learn predictable reward proceeding th international conference artificial intelligence statistic aistat d p hsu d kakade spectral algorithm latent dirichlet allocation proceeding advance neural information processing system nip page d kakade s m tensor spectral approach learn mixed membership community model machine d kakade m m b tensor decomposition learn latent variable model kakade s m method moment mixture model hide markov model proceeding th annual conference learn colt volume page auer fischer p finitetime analysis multiarmed bandit problem machine learn e sequential transfer multiarmed bandit finite set model c linear algorithm online multitask classification machine learn lugosi g prediction learning dekel long p m singer online multitask learn proceeding th annual conference learn theory colt page mouline e upperconfidence bind policy switch problem proceeding international conference algorithmic learning theory page r generalize rank test use singular value decomposition journal algorithm multiarmed bandit information proceeding advance neural information processing system nip transfer reinforcement learn framework survey m m editor reinforcement learn state art springer lugosi online multitask learn hard constraint proceeding nd annual conference learn theory direct exploration reinforcement learn transfer knowledge proceeding tenth european workshop reinforcement learn survey transfer learn ieee transaction knowledge datum engineering robbin h aspect sequential design experiment online learning multiple task relationship proceeding th international conference artificial intelligence statistic aistat matrix perturbation e transfer reinforcement learning domain perturbation bound connection singular value decomposition bit numerical mathematic
linear learn landscape follow extend result learn example layer feedforward network linear unit particular examine happen layer large connectivity layer local investigate property autoassociative notation additional motivation reference find usual linear network linear function compute layer reduce proper multiplication matrix point view adopt assume architecture network depend external constraint purpose understand happen learning phase strategy adopt synaptic weight modify example application work linsker emergence feature detect unit linear network consider layer network n input unit output unit p hide unit p let set center inputoutput training pattern problem find matrix weight b minimize error function e b let x denote usual covariance matrix main result description landscape e saddle point absence precisely follow fact true fact fix x p matrix function coefficient b attain minimum b satisfy equation addition invertible rank p strictly unique minimum reach fact fix p matrix function coefficient attain minimum satisfy equation addition invertible rank p strictly unique minimum reach fact assume invertible matrix define critical point e ie point e aij e global map form p denote matrix orthogonal projection subspace span column satisfie linear learning landscape algorithm rank p b define critical point e equivalently satisfy notice matrix slope matrix ordinary square regression fact assume rank distinct eigenvalue t ip set let t p denote matrix form eigenvector associate eigenvalue ai p rank matrix define critical point e exist set invertible p p matrix c critical point critical point rank p product ordinary square regression matrix follow orthogonal projection subspace span p eigenvector map w associate index set p unique local global minimum e remain set correspond saddle point additional critical point define matrix b rank saddle point term orthogonal projection subspace span q eigenvector p deep network consider case deep network layer input unit m layer n output unit m hide layer error function l worth notice fact fix m m matrix ai convex remain matrix connection weight let p p denote unit small layer network hide layer p unit word network bottleneck size let index correspond layer set mi b let vary restriction impose b rank p conversely matrix b rank p decompose way product form result local error function deep network yield minima corresponding collapse layer network induce vice versa local minima global minimal map unique index set p notice course large number way decompose product form m saddle point error function b necessarily generate saddle point correspond expression correspond gradient different linear learning landscape force connection local connectivity assume error function form layer network value entry prescribe particular include case local connectivity describe relation form aij output unit input unit connect clearly error function convex constraint form define hyperplane space possible intersection constraint convex set minimize e constraint optimization problem local minima notice case network constrain layer matrix b set constraint form bk b set admissible matrix form general convex unreasonable conjecture local minima arise question need investigate great detail algorithmic aspect nice feature error landscape describe far absence local existence equivalence unique global minimum understand term principal component analysis square regression landscape characterize large number point constitute problem simple gradient descent learning phase proof low e value correspond saddle point difficult reduction possible number direction context learning assert relevant issue practical implementation require simulation experiment remain problem large size number saddle point encounter stage descent process use type terrain result adjust e learning rate turn simple algorithm autoassociative case layer network e case presence teacher avoid set try achieve compression input datum hide layer technique relate principal component analysis describe yt easy equation matrix identity optimum matrix b transpose heuristically suggest possible fast algorithm iteration gradient descent step apply connection matrix update symmetric fashion use avoid backpropagate error layer similar idea formally describe random obviously similar algorithm obtain set bk actually alternate gradient step iteration respect iteration respect b simple calculation rewrite ak wk linear learning landscape algorithm natural examine behavior algorithm eigenvector assume eigenvector wk eigenvalue l k easy eigenvector eigenvalue algorithm converge optimal converge look iterate function detail describe main point fix point iff x notice point correspond value xd x variable readily position curve relative position depend value assume respect obviously k algorithm converge learning rate large e interval algorithm converge exhibit complex oscillatory behavior k conclusion algorithm test learning rate choose exceed large eigenvalue propagation encounter problem proximity saddle point nonprincipal eigenvector learn rapidly incorporate projection direction late stage simulation require examine effect noisy gradient compute presentation training example multiple starting point variable learn rate momentum term work support nsf grant dms contract p reference neural network principal component analysis learn example local vol neural model principal component submit publication p w image compression propagation demonstration programming advance cognitive science vol e linsker r selforganization perceptual network computer r feature discovery learn ic report model ocular dominance column analytical dept dept physiology abstract previously develop simple mathematical model formation ocular dominance column visual cortex model provide common framework variety activitydependent biological study analytic computational result reveal following input specific eye locally correlate fire arbor monocular cell robustly form organize interaction column broad correlation eye anticorrelation eye create purely monocular cortex positive correlation arbor radius yield perfectly monocular cortex feature model understand analytically decomposition eigenfunction linear stability analysis allow prediction width column feature measurable biological parameter introduction develop visual system mammalian specie initially uniform overlap layer visual cortex input represent eye subsequently input patch stripe largely exclusively input serve single eye know ocular dominance patch ocular dominance patch small scale compare map visual world initially continuous map map represent eye patch layer cortex layer response dominate eye corresponding layer patch know ocular dominance column stryker discovery system ocular dominance basic feature development wiesel series study wiesel wiesel recent brief review stryker segregation patch depend local correlation neural activity great neighboring cell eye eye force eye fire prevent segregation force fire normally cause complete segregation normal segregation depend activity cortical cell normally eye close young critical period developmental plasticity monocular active open eye come exclusively drive cortical cell input influence closed eye largely confine small cortex cortical cell inhibit fire opposite case active eye dominant suggest correlation pre postsynaptic activation critical synaptic develop analyze simple mathematical model formation ocular dominance patch mammalian visual cortex briefly review stryker model provide common framework variety activitydependent biological model include synapsis activitydependent release factor study equation similar develop linsker study development orientation selectivity visual cortex extend analysis extensive simulation achieve complete understanding model result appear appear detail stryker miller stryker miller equation consider input carry information eye single cortical let t t respectively left eye right eye synaptic weight cortical coordinate time t consideration simple activitydependent model synaptic plasticity synapsis activitydependent release modification factor lead equation time development l variable value l r ax connectivity function number synapsis assume identity mapping eye coordinate cortical coordinate p measure correlation firing input eye input separate distance p ix spread influence cortex tell synapsis fire separate distance influence model ocular dominance column formation growth influence incorporate lateral synaptic case synapsis linear activation identity matrix matrix synaptic weight incorporate effect diffusion modification factor model involve factor constant constraint conserve limit total synaptic strength support single cell nonlinearitie synaptic weight positive maximum add subtract equation yield model equation difference t t t t t p t t e lr err e assume statistical equality eye simulation development equation study simulation use grid input layer represent eye single cortical layer input cell connect square arbor cortical cell center corresponding grid point square grid point initial synaptic weight randomly assign uniform distribution synapsis allow decrease increase weight synaptic strength cortical cell conserve subtract iteration active synapse average change synaptic strength cortical cell periodic boundary condition grid use typical time development cortical pattern ocular dominance figure simulation correlation eye decrease distance grid point symmetric gaussian grid point correlation cortical interaction function function excitatory near neighbor weakly inhibitory exp individual cortical cell receptive field refine size monocular exclusively single eye individual input arbor refine size confine alternate cortical stripe dependence result correlation function figure wide range correlation eye addition anticorrelation increase cortex sameeye anticorrelation decrease significant arbor radius grid point square arbor tend monocular organization low right simulation indicate sameeye correlation near neighbor sufficient periodic organization ocular dominance correlation arbor radius essentially fully monocular cortex stryker t t t t t r l figure time development cortical ocular dominance result iteration pixel represent ocular dominance e single cortical cell pixel repeat column row cortical grid reveal pattern periodic boundary condition simulation time development vary cortical interaction arbor function complete agreement analytical result present model reproduce experimental effect monocular include presence critical developmental period effect eigenfunction consider initial condition equation linearize complex nonlinear biological reality solution equation solution stable small perturbation equality eye restore perturbation unstable pattern ocular dominance grow unstable pattern initially grow fast inherently linear question depend behavior equation sd small nonlinear term negligible solve problem find characteristic independently grow mode equation eigenfunction operator right equation mode grow exponentially growth rate eigenvalue eigenvalue positive real corresponding mode grow sd solution unstable perturbation model ocular dominance column formation sameeye correlation figure cortical ocular dominance iteration choice correlation function left simulation figure row correspond correlation parameter grid point respectively middle column effect add weak broadly range anticorrelation eye gaussian parameter time large amplitude sameeye correlation right column effect instead add anticorrelation sameeye correlation function analytical characterization eigenfunction change variable equation cortex input cortex receptive field r r equation convolution cortical variable result assume result grid similar eigenfunction form sex t characteristic receptive field represent variation eigenfunction r vary cortical location fix m pair real number specify dimensional cortical oscillation additional index enumerate rfs m eigenfunction write characteristic arbor represent variation eigenfunction r input location fix function complex construct real eigenfunction similar property miller stryker monocular real eigenfunction illustrate figure stryker characteristic receptive field figure set identical rotation eigenfunction function use figure monocular receptive field synaptic difference sd different cortical location oscillation correspond arbor illustrate mode rfs dominate eye dominance wavelength cortex monocular mode rf change sign oscillation monocular field eye yield ocular dominance column fast grow mode linear regime dominate final pattern receptive field monocular wavelength determine width final column characterize eigenfunction analytically limit case general conclusion follow fast grow mode receptive field largely determine correlation function cd peak fourier transform cd correspond wavelength large arbor diameter mode monocular correspond wavelength small arbor diameter mode binocular select monocular mode broad sharply peak fouri spectrum increase dominance growth rate monocular mode mode limit model ocular dominance column formation constant distance monocular mode grow mode decay mode monocular peak fouri transform cortical interaction function select wavelength cortical oscillation select wavelength ocular dominance organization limit correlation broad respect arbor calculate growth rate monocular mode function oscillation m proportional e m fouri transform limit close contribute sum peak occur m maximize m exception result constraint conserve limit change total synaptic strength arbor input cell monocular mode wavelength long arbor diameter suppress growth rate individual input gain lose strength correlation function lead monocular cell purely excitatory cortical interaction function lead single eye constraint conserve synaptic strength input wavelength instead arbor diameter large wavelength growth rate suppress ocular dominance segregation occur purely excitatory cortical interaction function robust phenomenon analytically constraint strength afferent arbor implement subtract average change strength arbor iteration synapse arbor transform previous expression growth rate e m computation eigenfunction eigenfunction compute grid result function wavelength compare analytical expression absence constraint afferent result parameter use figure figure level indicate mode define normalize scale describe stryker analytical expression growth rate peak coincide case peak m accurately predict growth rate monocular mode far limit case expression derive broad correlation anticorrelation enhance mode growth rate monocular mode sameeye anticorrelation opposite effect sameeye anticorrelation short range compare fast grow mode binocular result obtain calculation presence constraint afferent predict excitatory cortical interaction function spectrum change constraint select mode wavelength equal arbor diameter wavelength wide cortex cortical interaction function use simulation constraint suppress growth monocular mode alter basic stryker sameeye correlation figure growth rate vertical axis function inverse axis set function use figure compute grid code maximum mode wavelength growth rate fully monocular white fully binocular black black curve indicate prediction relative growth rate monocular mode broad correlation describe text structure peak spectrum connection model model ocular dominance segregation emerge limit case model correlation constant bit arbor diameter model assume effective interaction synapsis depend eye origin distance model biological effective interaction limit case allow consideration general correlation function allow development individual arbor receptive field relationship overall ocular dominance equation similar equation study linsker volume important difference result model synapsis constrain remain positive biological synapsis model ocular dominance column exclusively positive exclusively negative particular projection visual input visual cortex purely excitatory model system excitatory inhibitory input population certainly statistically distinct activity single population strength positive negative s d hand biological variable start positive negative allow linear analysis result remain accurate presence nonlinearitie crucial biology second analyze effect synaptic interaction impact mode introduce phase variation oscillation cortex second typically enhance growth rate monocular mode relative mode sign vary receptive field acknowledgement support nsf fellowship grant system development foundation simulation perform supercomputer plasticity ocular dominance column monkey r linsker r basic network principle neural architecture r selforganization perceptual network ieee computer model neural development appear neuroscience connectionist theory ed stryker model formation ocular dominance column solve linear stability analysis soc stryker ocular dominance column development analysis simulation submit publication stryker development ocular dominance column mechanism model appear connectionist modeling brain function develop interface hanson r optimality principle unsupervised learn volume model formation ocular dominance stripe soc wiesel comparison effect eye closure cortical unit response
learn continuous hide variable model binary datum technology center neural direct generative model binary datum use small number hide continuous unit investigate clip nonlinearity distinguish model conventional principal component analysis relationship correlation underlie continuous gaussian variable binary output variable utilize learn appropriate weight network advantage approach illustrate translationally invariant binary distribution handwritten digit image introduction principal component analysis widely use statistical technique represent datum large number variable base assumption data embed high dimensional vector space variability datum capture low manifold particular manifold describe linear hyperplane characteristic direction eigenvector correlation matrix large eigenvalue success closely relate technique factor analysis mixture clearly indicate real world datum exhibit low dimensional manifold structure assume model linear manifold structure appropriate datum binary value variable binary value commonly occur datum computer bit stream image output feature detector spike train datum boltzmann machine neural network model incorporate hide binary spin variable principle able model binary datum arbitrary spin correlation unfortunately sompolinsky figure generative model ndimensional binary datum use small number p continuous hide variable computational time need train boltzmann machine render impractical application proceeding present model use small number continuous hide variable hidden binary variable capture variability binary value visible datum generative model differ conventional incorporate clip nonlinearity result spin configuration entropy relate number hide variable use result state connect small number spin flip learning particularly simple relate scalar transformation correlation matrix generative model figure schematic diagram generative process model assume datum generate small number p continuous hide variable hidden variable assume draw independently normal distribution unit variance pyi continuous hide variable combine use feedforward weight ij binary output unit calculate use sign p binary datum commonly obtain thresholding reasonable proper generative model incorporate clip nonlinearity generative process similar sigmoidal belief network continuous hide unit temperature nonlinearity alter relationship correlation binary variable weight matrix w describe realvalued exactly analogous visible variable conventional lie linear hyperplane determine span matrix correlation matrix learn continuous hide variable model binary datum r figure binary spin configuration variable p si vector space continuous hide construction correlation matrix rank p small number component consider binary output variable correlation calculate probability distribution integral equation analytically yield surprisingly simple result c ss sin correlation binary variable css relate correlation correspond gaussian nonlinear function normalization denominator argument reflect fact sign function unchanged scale change correlation matrix css generate correlation matrix easily relate equation qualitatively different property general correlation matrix css long low rank structure illustrate translationally invariant example section spectrum css contain eigenvalue nonzero eigenvalue typically use dimensionality reduction real variable model use compress binary output si output correlation c ss long display low rank structure generating c appropriate measure datum compression entropy binary output state consider possible binary state generate process equation e define p dimensional hyperplane state space hide variable dash line figure hyperplane partition si sompolinsky css eigenvalue rank figure translationally invariant binary spin distribution unit representative sample distribution illustrate left eigenvalue spectrum plot right region spin variable hyperplane state space hyperplane unique total number spin configuration si determine number cell bound divide hyperplane p dimension number cell approximately n p wellknown result perceptron lead order large entropy binary state generate process p log entropy spin configuration generate model directly proportional number hide variable p topology binary spin configuration si relate manifold structure continuous variable generate spin state represent cell p dimensional vector space hide variable p neighboring relate single small number spin flip state space binary spin configuration discrete continuous manifold structure underlie variable model manifest binary output configuration low entropy connect small hamming distance translationally invariant example principle weight learn apply maximum likelihood generative model result learning involve analytically intractable multidimensional integral alternatively approximation base mean field theory importance sampling use learn appropriate parameter equation suggest simple learning rule approximate computationally efficient binary correlation matrix css compute datum empirical css map appropriate gaussian correlation matrix use nonlinear transformation result gaussian correlation matrix variance individual xi fix unity weight calculate use conventional correlation matrix eigenvector large eigenvalue use form column learn continuous hide variable model binary data yield good low rank approximation scale variable result correlation matrix slightly different eigenvalue rank utility transformation illustrate follow simple example consider distribution binary spin figure half spin choose positive location positive arbitrary periodic boundary condition distribution translationally invariant correlation depend relative distance spin eigenvector fouri mode eigenvalue correspond overlap triangle wave eigenvalue spectrum css plot figure sort rank particular case correlation matrix positive eigenvalue corresponding range value consider matrix eigenvalue figure contrast different eigenvalue css spectrum gaussian positive eigenvalue rest exactly equal corresponding eigenvector cosine function generative process understand linear combination yield function phase function clip yield positive original binary distribution comparison eigenvalue eigenvalue spectrum obvious low rank structure generative process case original binary distribution construct use p hide variable clear eigenvalue css appropriate number illustrate utility determine principal component calculate correlation work directly observable binary correlation matrix css handwritten digit example model apply complex datum set large set black white image handwritten digit database pixel mean pixel correlation directly compute image generative model need slightly modify account nonzero mean binary output accomplish add fix bias clip bias si xi relate mean binary output si allow bias directly compute observed mean binary variable unfortunately nonzero bias relationship binary correlation css long simple expression find equation instead correlation relate follow integral equation empirical pixel correlation css handwritten digit integral equation numerically solve pair index yield appropriate sompolinsky css eigenvalue figure eigenvalue spectrum handwritten image inset p significant eigenvector arrange row right figure nonlinear different instance handwritten use eigenvector correlation matrix result eigenvalue spectra figure eigenvalue exhibit characteristic drop spectrum binary correlation css corresponding eigenvector large positive eigenvalue depict inset figure represent common image distortion rotation stretch appear qualitatively similar find standard generative model weight w correspond eigenvector figure use fit handwritten utility nonlinear generative model illustrate right figure image figure different example handwritten datum set generative model use example hidden value original image determine different example intermediate image construct linearly interpolate vector space hide unit clip nonlinearity induce nonlinear mapping output binary unit flip particular order determine generative model contrast use conventional pca result simple linear interpolation image intermediate image look original binary distribution correlation matrix happen contain small negative eigenvalue binary correlation matrix css positive definite transformation equation guarantee result matrix positive definite presence negative eigenvalue indicate shortcoming generative model datum particular clip gaussian model unable capture correlation induce global learn continuous hide variable model binary data constraint datum simple illustration shortcoming generative model consider binary distribution define probability density state distribution define constraint sum binary variable exactly n impossible find gaussian distribution visible binary variable match negative correlation induce sum example illustrate value use generative model learn correlation matrix underlie gaussian variable use correlation output directly clip nonlinearity convenient relationship hidden variable output variable particularly easy understand learning differ nonlinear model autoencoder inverse mapping function explicitly learn instead correlation matrix directly transform observable variable underlie gaussian variable correlation matrix determine appropriate feedforward weight result extremely efficient training procedure directly analogous pca continuous variable acknowledge support laboratory technology science foundation thank seung helpful discussion reference principal component analysis variable model factor analysis m model manifold image handwritten digit ieee transaction neural network sompolinsky m nonlinear statistic train preparation sejnowski learn algorithm cognitive science cover statistical property system inequality application pattern comput tip probabilistic highdimensional binary datum advance neural information processing system factor analysis variable lecun backpropagation apply handwritten code recognition neural computation sm nonlinear image interpolation use manifold learning advance neural information processing system principal curve statistical demer nonlinear dimensionality reduction advance neural information processing system
comprehensive linear speedup analysis asynchronous parallel optimization zerothorder firstorder computer science electrical computer engineering computer asynchronous parallel optimization receive substantial success extensive attention recently core theoretical question speedup benefit asynchronous bring paper provide comprehensive generic analysis study property broad range asynchronous parallel stochastic algorithm zeroth order order method result recover improve exist analysis special case provide insight understand asynchronous parallel behavior suggest novel asynchronous parallel zeroth order method time experiment provide novel application propose asynchronous parallel zeroth order method parameter tuning model problem introduction asynchronous parallel optimization receive substantial success extensive attention recently example use solve machine learning problem deep learning matrix completion svm linear system pca linear programming main advantage parallel optimization avoid synchronization cost minimize system overhead maximize efficiency computation worker core theoretical question speedup benefit asynchronous bring time save employ computation resource precisely people interested run time rt worker run time use single worker run time use t worker asynchronous parallelism worker rt measure roughly computational total computational complexity use single worker total computational complexity use t worker paper mainly interested condition ensure linear speedup property specifically upper bind t ensure exist study special case asynchronous stochastic gradient descent asgd asynchronous reveal factor simplicity assume communication cost dominant paper conference neural information processing system nip table asynchronous parallel algorithm c model stand inconsistent consistent read model respectively explain later base short problem type upper bind smooth strongly ok min composite k b smooth smooth constrain composite ax b ax b constrain zeroth order sgd scd sgd scd affect upper bind t example agarwal duchi upper bind depend variance stochastic gradient upper bind depend data sparsity dimension problem asgd find upper bind depend problem dimension diagonal dominance hessian matrix objective lack comprehensive generic analysis piece factor jointly affect paper provide comprehensive generic analysis study property broad range asynchronous parallel stochastic algorithm zeroth order order method avoid unnecessary complication cover practical problem algorithm consider follow stochastic optimization problem e random variable r r smooth necessarily convex function objective function cover large scope machine learn problem include deep learn component function paper common specification index set training sample loss function respect training sample index highlight main contribution paper follow provide generic analysis convergence cover exist algorithm include asgd implementation parameter server asgd implementation multicore system special case generic analysis recover improve exist result special case generic analysis suggest novel asynchronous provide analysis convergence rate speedup property good knowledge asynchronous parallel zeroth order experiment include novel application propose aszd method model blend parameter tune big datum optimization relate work review firstorder asynchronous parallel stochastic algorithm table summarize exist linear result asynchronous parallel optimization algorithm relate paper block table result paper prove convergence asynchronous reduce stochastic gradient svrg method speedup sparse set provide general perspective start point analyze asynchronous stochastic algorithm include h asynchronous scd asynchronous sparse fundamental difference work lie apply different analysis result directly apply special case addition line research study asynchronous admm type method scope paper encourage reader refer recent literature example end section review zerothorder stochastic method use denote dimension problem denote iteration number variance stochastic prove convergence rate apply optimization base prove convergence rate nk rate sgd smooth problem lower bind zerothorder method inaccurate evaluation duchi prove rate zeroth order sgd objective different assumption compare paper agarwal prove regret objective comprehensive review asynchronous algorithm refer long version paper notation ei rn denote natural unit basis vector e mean expectation respect random variable denote expectation respect random variable rn gradient respect let subset rn projection index set set component use rn denote short denote optimal objective value generic asynchronous stochastic illustrate assume centralized network require multiple child nod step length central node maintain ensure variable server implement computer randomly select component function index share memory set coordinate index implement multicore machine xk base algorithm child nod end run independently concurrently read central node result read mathematically define later calculate locally use modify central node need synchronize child node child nod stay consequently efficiency maximize word note asynchronous parallel mechanism variable central node update exactly follow child node return computation result x change child nod new analysis require fundamental question condition linear speedup guarantee word condition equivalently provide comprehensive analysis consider generic algorithm zeroth order hybrid scd sgd iteratively sample component function index coordinate s y constant update x approximation block coordinate gradient ei ei definition approximation parameter coordinate predefine practice use function value zeroth order information estimate easy close s close s particular algorithm theoretical analysis follow easily extend minibatch version apply asynchronous parallelism propose generic asynchronous stochastic algorithm algorithm essentially characterize value update predefined learning rate total number iteration note iteration number count central node update matter child node increase counter mention key difference asynchronous algorithm protocol equal xk asynchronous parallelism different way model value k consistent read k early exist state central node happen read write central node child node atomic operation instance implementation parameter server read k complicated atomic read guarantee happen example implementation multicore system mean child read central node child node perform modification time different coordinate read child node different age word k state reader want learn detail consistent read inconsistent read refer cover case note k represent follow generic form jjk subset index early iteration t upper bind expression consider note practical value t usually proportional number involved node worker total number worker upper bind treat follow discussion notation t abuse simplicity theoretical analysis main result paper let global assumption commonly use analysis stochastic bounded variance stochastic gradient gradient gradient objective component function gradient assumption define constant lmax let positive integer bound n define ls minimal constant satisfy follow inequality ei s z max z define n minimum constant satisfy maxi define lmax lmax ls l independence random variable independent bounded age let t global bind delay k define follow global quantity short notation ly main result follow theorem underlie assumption read write number omit point behavior guarantee modern architecture note lipschitz assumption component function eliminate come order method ie follow theorem theorem generic convergence rate choose constant ly suppose age t bound t y follow convergence rate ef roughly speak term rhs relate scd second term relate stochastic descent term result look complicated elegant capable capture important subtle structure subsequent discussion recover improve exist result prove convergence new algorithm use result interpretable use notation avoid explicitly write constant factor include follow corollary asynchronous stochastic coordinate apply theorem study asynchronous scd contain single randomly sample coordinate equivalently essential rule xk let theorem onk follow convergence rate hold convergence rate onk consistent exist analysis scd smooth optimization requirement ensure linear improve t t analyze strongly convex objective prove linear speedup small restrictive asynchronous stochastic gradient descent asgd asgd widely use solve deep learn nlp important machine learn problem typical implementation asgd type implement computer cluster parameter parameter server serve central node ensure atomic read write vector x lead follow update rule set xk note single iteration define modify vector type implement single computer multiple core case central node correspond share memory multiple core thread access simultaneously model read write guarantee purpose analysis update single coordinate account iteration turn follow update rule set xk reader refer detail illustration implementation corollary asgd let equivalently theorem k follow convergence rate hold note convergence rate tight consistent version sgd compare linear speedup property indicate result ensure rate agarwal duchi need t bound min inferior result need t bound achieve rate inferior result requirement consistent good knowledge good result far corollary asgd let equivalently theorem o follow convergence rate hold ef nk additional factor compare arise different way count iteration additional factor appear compare result require t bound requirement allow large value t especially small dominate kn compare assume objective function strongly convex sort compare apple meaningful believe strong convexity affect linear speedup property imply linear guarantee t assumption sparsity stochastic bound comparison require assumption sparsity gradient dependence improvement exist analysis analysis provide interesting insight essentially suggest large problem dimension beneficial linear example agarwal duchi suggest large stochastic variance imply number sample large beneficial linear speedup analysis effect n improve linear speedup jointly asynchronous descent end section apply theorem generate novel asynchronous descent set block size equivalently k good knowledge asynchronous algorithm corollary aszd set s constant suppose satisfy k min satisfie t kn follow convergence rate ef nk firstly note convergence rate consistent rate gradient method evaluate result perspective consider t lead serial result imply dependence compare obtain convergence rate reader notice way estimate stochastic gradient different use method estimate single coordinate gradient sample component function estimate gradient sample component function estimation accurate aggressive convergence rate actually improve small constant require requirement restrictive important insight requirement suggest dependence variance variance large allow large value insight meet common sense large variance mean stochastic gradient largely deviate true gradient allow choose large obtain exact estimation stochastic gradient affect convergence rate practical view point tend choose large value recall zerothorder method use function difference different point ei estimate differential practical system eg concrete control system usually exist system noise query function value point close word small obtain function difference dominate noise reflect function differential second consider case t lead asynchronous descent good knowledge algorithm upper bind t essentially indicate requirement linear speedup property linear property small linear fundamental understanding asynchronous stochastic algorithm improve linear speedup jointly experiment extensively validate recent paper conduct experiment validate propose aszd section apply estimate parameter synthetic black box system second apply aszd model combination music recommendation competition parameter optimization black box use deep neural network simulate black box system optimization variable weight associate neural network choose layer node network weight parameter totally weight randomly generate output vector construct apply network input vector gaussian noise use network generate sample synthetic sample use optimize weight black box know structure weight neural network black box optimize parameter black box apply propose aszd method experiment conduct machine architecture core run number core choose base good performance run core achieve precision objective value table rt aszd different thread synthetic datum rt report table observe iteration speedup linear run time slightly bad iteration speedup draw figure supplement curve objective value number iteration run time respectively asynchronous parallel model combination competition kddcup team challenge predict user rating music music datum set evaluation criterion root mean square error rmse test datum set rmse t uit r t user rating track test datum set rating true rating user item predict rating win team create model use different machine learn include matrix knn restrict boltzmann machine blend model use neural network bin linear regression validation datum set rating create model ensemble achieve rmse able obtain predict rating individual model kddcup test datum set kddcup team matrix row correspond test datum set sample column element indicate predict rating ith music test datum sample experiment try linearly blend model use information test datum set variable optimize vector rn coefficient predict rating model ensure linear overfit far split x randomly equal validation set denote rnn model true test set define objective function rmse output validation set ax r r correspond true rating validation set ax predict rating blend assume entry r directly compute gradient experiment treat information value model blend coefficient similar submit model kddcup obtain rmse test set know actual value test set apply aszd algorithm minimize information table compare rmse test datum set kddcup winner team rd result implement algorithm use core e machine run number iteration different number thread measure run time rt figure supplement similar experiment algorithm linear speedup completeness figure supplement square root objective function value rmse number iteration run time second algorithm run thread achieve rmse test set result comparable kddcup winner table goal performance algorithm assume submit solution unlimited time real iteration converge fast reasonable small rmse figure conclusion paper provide generic linear speedup analysis zerothorder firstorder asynchronous parallel algorithm generic analysis recover improve exist result special case asgd parameter implementation asgd multicore implementation generic analysis suggest novel aszd algorithm guarantee convergence rate speedup property good knowledge asynchronous parallel zeroth order experiment include novel application propose aszd method model parameter tune big data optimization acknowledgement project support nsf grant especially thank provide code datum music competition reference agarwal duchi distribute delay stochastic optimization nip agarwal p m kakade stochastic optimization bandit feedback nip page h revisit asynchronous linear solver provable convergence rate randomization journal acm bengio r p neural probabilistic language model machine learn research c duchi r asynchronous stochastic optimization noise noise sgd care nip page linear ensemble individual model music rating prediction m m p large scale distribute deep network nip m music dataset kddcup kddcup page duchi l bartlett m wainwright randomized smoothing optimization p asynchronous principal component analysis r m asynchronous minibatch regularize optimization zerothorder method optimization d smith distribute asynchronous online learning natural language processing page m distribute asynchronous incremental algorithm admm base approach s dhillon parallel asynchronous stochastic dual coordinate descent icml page query complexity optimization nip m j long e distribute machine learn parameter server asynchronous parallel stochastic gradient optimization nip page coordinate descent parallelism convergence property r v s asynchronous parallel stochastic coordinate descent icml s asynchronous parallel randomized pan d m perturb analysis asynchronous optimization nemirovski robust stochastic approximation approach stochastic programming optimization random function foundation computational mathematic page s approach parallelize stochastic descent nip jin asynchronous gradient descent speed neural network training nip m algorithmic framework asynchronous parallel coordinate update l stochastic gradient descent distribute asynchronous matrix completion graph partition acm conference system s b smola variance reduction stochastic descent asynchronous variant nip page p m complexity randomized descent method minimize composite function mathematical programming c sa c c r c r unified analysis nip page approximate efficient lp solver round nip e p xing consistent server distribute dhillon stochastic asynchronous decentralize matrix completion r asynchronous distribute admm consensus optimization icml s lecun deep learning elastic averaging fast asynchronous parallel stochastic gradient descent approach convergence guarantee page
articulated pose estimation graphical model image dependent pairwise relation abstract present method estimate articulated human pose single static image base graphical model novel pairwise relation adaptive use local image measurement precisely specify graphical model human pose exploit fact local image measurement use detect joint predict spatial relationship image dependent pairwise relation spatial relationship represent mixture model use deep convolutional neural network learn conditional probability presence spatial relationship image patch model combine representational flexibility graphical model efficiency statistical power dcnns method significantly outperform state art method lsp flic dataset perform buffy dataset training introduction articulated pose estimation fundamental challenge computer vision progress area immediately apply important vision task human tracking action recognition video analysis work pose estimation base graphical model graph node represent body joint edge model pairwise relationship score function energy model contain unary term node capture local appearance cue pairwise term define edge capture local contextual relation recently deeppose model pose holistic manner capture context body deep convolutional base paper present graphical model image dependent pairwise relation illustrate figure reliably predict relative position neighbor presence observe local image patch model local image patch input unary pairwise term strong pairwise term datum independent relation typically loose helpful strict model highly variable pose approach require method extract information pairwise relation presence local image patch require method efficient share feature different relationship train output low arm upper arm elbow wrist figure motivation local image measurement eg image patch reliably predict relative position neighbor detect center panel local image patch center elbow reliably predict relative position wrist local image patch center wrist reliably predict relative position elbow leave right panel define different type pairwise spatial relationship ie mixture model pair neighboring left panel typical spatial relationship elbow neighbor wrist right panel typical spatial relationship wrist neighbor elbow estimate presence spatial relationship use unary pairwise term score function weight parameter different term model train use structure support vector machine summary model combine representational flexibility graphical model include ability represent spatial relationship datum drive power dcnns perform experiment standard pose estimation benchmark lsp dataset flic dataset method outperform state art method significant margin dataset evaluation buffy training dataset obtain strong result ability model generalize model graphical model variable represent human pose graphical model e node specify position joint edge e indicate spatially relate simplicity impose graph structure form tree position denote l specify location edge graph e specify discrete set spatial relationship index correspond mixture different spatial relationship figure denote set spatial relationship t tji e image write define score function follow sum unary pairwise term unary term unary term local evidence v lie location base local image patch form wi appearance term parameter specify section scalar weight parameter image dependent pairwise relational idpr term idpr term capture intuition neighboring e roughly predict relative spatial position use local information figure model relative position discretize type tij mixture different relationship correspond mean relative position small deformation model standard quadratic deformation term formally pairwise relational score edge e tji t lj wij tji x standard quadratic deformation feature image dependent pairwise relational idpr term parameter specify section wij weight parameter notation specify dot product boldface indicate vector score score f l function location pairwise relation type input image express sum unary pairwise term tij tji ije scalar weight constant bias term t model consist set parameter mean relative position r e different pairwise relation type parameter appearance term idpr term weight parameter wi wij section learning parameter image dependent term dcnns appearance term idpr term depend image patch word local image patch evidence presence relationship neighbor j ni ni e require learn distribution state variable tij condition image patch order specify distribution define state space precisely number pairwise spatial relationship vary different different number neighbor figure need consider possibility patch contain define c random variable denote present c present ie background define random variable determine spatial relation type c value neighbor wrist mini tij neighbor mini tij c define space represent pk s k size space s element space correspond type pairwise relationship background use learn conditional probability distribution pc suitable task efficient enable share feature section detail specify appearance term idpr term term pc marginalization c relationship model briefly discuss method relate standard model pictorial structure recover pictorial structure model allow relationship type ie case idpr term convey information model reduce standard unary image independent pairwise term slight difference use learn unary term instead use filter describe model mixture template template type type define relative position respect parent obtain restrict model predict relative position parent parent case associate informative idpr term merge appearance term define different type method use dcnns conditional random field model relate conditional random field datadependent prior datadependent prior unary term typically model separately paper efficiently model image dependent term ie unary term idpr term single exploit fact local image measurement reliable predict presence pairwise relationship neighbor inference detect optimal configuration person search configuration location l type t maximize score function t arg relational graph tree efficiently dynamic programming let set child graph leaf si maximum score subtree root locate maximum score subtree compute follow max use equation recursively compute overall good score model optimal configuration location type recover standard backward pass dynamic programming computation pairwise term quadratic function location lj max operation equation accelerate use generalized distance transform result approach efficient lk time image dependent term compute t number relation type total number location k total number model analysis assume pairwise spatial relationship number type ie tji t e computation image dependent term efficient compute location single apply slide fashion inherently efficient computation common region naturally share learn consider problem learn model parameter image label location datum available human pose dataset derive type label tij location annotation adopt supervise approach learn model model consist set parameter mean relative position r different pairwise relation type parameter image dependent term weight parameter learn separately algorithm r mean relative position type label label positive image let dij relative position neighbor cluster relative position training set tij cluster experiment pairwise relation cluster correspond set instance share similar spatial relationship neighbor define cluster pairwise relation type tij model use center cluster mean relative position associate type way mean relative position different pairwise relation type type training instance derive base cluster index use kmean experiment set tij cluster parameter image dependent term derive type label local image patch center annotate location label category label indicate present type label indicate relation type neighbor way set label patch kn positive image positive image provide patch set background patch sample negative image label patch background patch train multiclass classifier standard stochastic gradient descent use softmax loss consist convolutional layer maxpooling layer fullyconnected layer final dimension output define conditional probability distribution architecture network summarize figure weight parameter pose positive image label annotate location derive type label tn use learn weight parameter structure prediction problem simplify use loss training example dimension label correct dimension label wrong denote example late example score function equation linear weight parameter write optimization function min max tn sparse feature vector represent nth example concatenation image dependent term calculate deformation feature constant experiment section introduce dataset clarify evaluation metric describe experimental setup present comparative evaluation result diagnostic experiment dataset evaluation metric perform experiment publicly available human pose estimation benchmark sport pose lsp dataset contain training testing image sport activity annotate fullbody human pose frame label contain training testing image movie annotate human pose follow previous work use annotation benchmark train model use negative training image dataset image contain people use popular evaluation metric allow comparison previous work percentage correct pcp standard evaluation metric benchmark include dataset discuss alternative interpretation pcp lead severely different result paper use strict version state evaluate single estimation result test image body consider correct segment endpoint joint lie length groundtruth annotate endpoint test image contain annotate person refer version pcp strict flic dataset use strict pcp evaluation metric specify percentage detect joint pdj pdj measure performance use curve percentage correctly localize joint vary localization precision threshold localization precision threshold normalize scale define distance left right ground truth pose scale invariant multiple people flic image dense dropout dense dense dropout conv conv conv conv norm pool conv norm pool figure architecture dcnns size input patch pixel pixel flic dataset dcnns consist convolutional layer maxpooling layer fullyconnecte dense layer final s dimension output use dropout local response normalization norm overlap pool pool describe ground truth person annotate detection box evaluation return single estimation result ground truth person restrict localize window define provide implementation detail datum million parameter training image available order reduce overfitting augment training datum rotate positive training image image flip double training image increase number training example body different spatial relationship neighbor elbow diagonal left panel figure hold random positive image validation set training weight parameter train hold set reduce overfitting training datum note train use local patch background patch instead image naturally increase number training example factor k number number dimension final output increase linearly number number parameter slightly increase fullyconnected layer parameter share different benefit large training example parameter experiment increase add point annotate result flic dataset cover person reduce distance neighboring good model graph structure define fullbody graph structure graph structure flic dataset respectively graph connect annotate point form tree skeleton figure setting use number type pair neighbor simplicity set dataset ie tij tji e result spatial relation type neighbor wrist spatial relation type neighbor eg elbow figure patch size set pixel dataset pixel flic dataset flic image high resolution use similar architecture dataset differ layer different input patch size figure visualize architecture use use implementation experiment benchmark result strict pcp result lsp dataset table flic dataset table pdj result flic dataset figure method outperform state art method significant margin dataset detailed analysis figure estimation example lsp flic dataset method head deeppose table comparison strict pcp result lsp dataset method improve significant margin outperform good previously publish result average note deeppose use annotation train extra image mean table comparison strict pcp result dataset method significantly outperform wrist percentage detect joint pdj percentage detect joint pdj elbow deeppose normalize precision threshold deeppose normalize precision threshold figure comparison pdj curve elbow wrist flic dataset legend pdj number threshold diagnostic experiment perform diagnostic experiment generalization ability model understand influence term model generalization directly apply train model flic dataset official test set buffy dataset ie training buffy dataset contain human pose buffy test set include subset people detect new detection window compare result previously publish work subset previous work evaluate official evaluation buffy use strict implementation refer version buffy report strict pcp table pdj curve figure criterion method significantly outperform state art good generalization ability method note deeppose method train flic dataset compare figure margin method deeppose significantly increase figure imply model generalize buffy mean elbow table pcp result buffy test subset pcp number buffy pcp state note method train wrist percentage detect joint pdj percentage detect joint pdj strict deeppose normalize precision threshold deeppose normalize precision threshold figure pdj curve buffy test subset legend pdj number threshold note method deeppose train flic dataset consider correctly localize average distance endpoint joint groundtruth length groundtruth annotate endpoint method head mean model table diagnostic term analysis strict pcp result lsp dataset unary term powerful good result train classifier method pairwise term dependent image term analysis section comparable performance stateoftheart add idpr term significantly boost final performance term analysis design experiment understand influence term model experiment use unary term localize independently second experiment replace idpr term image independent prior ie equation replace scalar prior term respectively retrain weight parameter new prior term case pairwise relational term depend image instead mixture deformation image independent bias refer experiment second short idpr term experiment dataset use identical appearance term fair comparison strict pcp result table term model significantly improve performance detail conclusion present graphical model human pose exploit fact local image measurement use detect joint predict spatial relationship image dependent pairwise relation spatial relationship represent mixture model type spatial relationship use dcnns learn conditional probability presence spatial relationship image patch model combine representational flexibility graphical model efficiency statistical power dcnns method outperform state art method flic dataset perform buffy dataset training figure result lsp flic dataset localization result graph use model row failure case typically large occlusion overlap people acknowledgement research support grant onr aro use research generously reference r r yuille detect detect represent object use holistic model body computer vision pattern cvpr ng l yuille adaptive occlusion state estimation human pose track histogram orient gradient human detection computer vision pattern cvpr m appearance share human pose estimation computer vision m m zisserman articulated human pose estimation unconstrained image international journal computer vision p p pictorial structure object recognition computer vision m zisserman search space reduction estimation computer vision pattern recognition cvpr m r representation matching pictorial structure ieee transaction computer open source convolutional architecture fast feature embed cluster pose nonlinear appearance model human pose estimation machine vision l use link feature learn nonparametric model conference computer vision eccv krizhevsky hinton imagenet classification deep convolutional network neural information processing system nip lafferty mccallum c pereira conditional random field probabilistic model labeling sequence datum international conference machine learning icml deep learning human pose estimation computer vision pattern recognition cvpr l m p condition pictorial structure computer vision pattern recognition cvpr l m p strong appearance expressive spatial model pose estimation international conference computer vision iccv d learn parse image articulated body neural information processing system nip blake interactive extraction use iterate graph cut acm transaction graphic taskar pose prior pictorial structure computer vision pattern cvpr b taskar multimodal decomposable model human pose estimation computer vision pattern recognition cvpr taskar cascade model articulated pose estimation conference computer vision eccv p m r fergus integrate recognition localization detection use convolutional network international conference learn representation c szegedy deeppose human pose estimation deep neural network computer vision pattern recognition cvpr joachim support vector machine learn interdependent structured output space international conference machine learning icml l yuille approach action recognition computer vision pattern recognition cvpr pose estimation flexible computer vision pattern recognition cvpr human detection flexible mixture ieee transaction pattern analysis machine intelligence
neuronal adaptation probabilistic inference perceptual bistability school abstract argue perceptual multistability reflect probabilistic inference perform brain sensory input ambiguous alternatively traditional explanation multistability refer lowlevel mechanism neuronal adaptation employ deep boltzmann machine dbm model cortical processing demonstrate different approach combine framework base recent development machine learning neuronal adaptation understand mechanism improve probabilistic samplingbase inference use ambiguous necker cube image analyze perceptual switching exhibit model examine influence spatial attention explore binocular rivalry model approach work early study demonstrate principle underlie relate cortical processing offer novel perspective neural implementation approximate probabilistic inference brain introduction bayesian account cortical processing brain implement probabilistic model learn reason cause underlie sensory input nature potential cortical model mean implementation particular interest context bistable perception percept switch time interpretation case ambiguous stimulus necker cube different image present eye binocular rivalry case ambiguous conflict sensory input result bimodal posterior image interpretation probabilistic model perceptual bistability reflect specific way brain explore represent posterior classic explanation explain bistability lowlevel mechanism probabilistic approach bistability fundamental aspect brain implement probabilistic inference recently suggest cortex employ approximate inference scheme estimate probability distribution set sample study psychophysical datum interpret light focus binocular rivalry point particular markov correlated sample draw time approximate distribution naturally account aspect perceptual bistability stochasticity fact perception point time reflect individual interpretation image distribution possibility provide concrete neural model early work consider deep boltzmann machine model cortical perception relate hierarchical inference generative model attention connection mcmc bistability establish natural explore model bistability gibb sample mcmc method perform inference importantly perspective gibb sample simply correspond standard way run dbm neural network stochastic fire unit know mcmc method general gibb sample particular problematic practice complex multimodal distribution sample stick individual mode chain mix recent machine learn work introduce heuristic algorithm rate fast persistent contrastive divergence aim improve sample performance machine model dynamically change model parameter connection strength closely relate work suggest potential connection dynamic synapsis brain neuronal adaptation mean change neuronal synaptic efficacy actually means enhance sample base aim demonstrate lowlevel probabilistic account bistable perception combine present biological interpretation term neuronal adaptation neuronal synaptic specifically use dbm train interpretation necker cube adaptation lead bistable switching internal representation model present actual ambiguous necker model role spatial attention bias perceptual switching finally explore approach apply binocular rivalry neuronal adaptation deep boltzmann machine section briefly introduce dbm motivate machine learn perspective explain relation biology dbm consist stochastic binary unit arrange hierarchically layer symmetric connection layer connection layer layer contain visible unit clamp datum image inference high layer contain hidden unit learn representation generate datum visible state layer denote connection weight wk bias bk probability unit switch determine input adjacent layer use sigmoid activation function bi m l run network switch unit manner implement gibb sample probability distribution determine energy function e p xkt xkt bk intuitively speak run model perform random walk energy landscape shape learn attract jump mode distribution correspond traverse neuronal synaptic unfortunately complex inference task mcmc method gibbs prone stick individual mode result incomplete exploration distribution work machine learn improve sample method recently introduce rate fast persistent contrastive divergence utilize sample restricted boltzmann machine rbms layer building block base use train briefly contribution weight training update require model run continuously independently datum explore probability distribution currently learn important model stick individual mode find introduce fast change component weight bias dynamically change energy landscape alleviate problem fast weight add actual weight analogue bias bf update accord px bf bf px visible clamp current datum item current sample freely run model parameter determine rate adaptation decay parameter limit weight change contribute fast weight second term parenthesis effect change weight bias state currently sample model likely follow eventually push model mode stick term parenthesis compute datum lead model draw state support current input computation term parenthesis equation require training datum turn general sample applicable outside training training data long simply replace term socalled rate pairwise statistic average training datum bf ex t bf ex sample condition datum rate compute training use sample find term sufficiently serve stabilize sample scheme yield improved performance gibbs sample let consider equation biological perspective interpret weight parameter synaptic strength bias overall level neuron equation suggest capability network explore state space improve dynamically adjust neuron parameter cf depend current state neuron connected second term parenthesis draw set value term rate statistic need neuron store average firing activity learn bias statistic synapsis remember average firing correlation connected neuron weight statistic particular activation pattern network sparse neuron time average term low neuron fire strongly preferred stimulus stimulus interpretation firing probability decrease synaptic efficacy drop allow network discover potential alternative interpretation stimulus case sparse activity equation implement form neuronal synaptic precede introduction sampling algorithm utilize mechanism apply bias biological model model neuronal firing help system robust noise corruption input lead total sensory underlying mechanism understand adaptation long term depend time scale involve experiment necker cube train dbm binary image cube location represent unambiguous interpretation necker cube test model actual ambiguous necker cube practice minibatche use apply dbm rbm section decode hidden state input training test set example time perceptual bistability figure example unambiguous training image leave ambiguous test image right b inference ambiguous image decode hidden state reveal perceptual switch result neuronal adaptation consecutive sample cycle figure use similar setup describe localized receptive field size increase low high hide layer sparsity encourage simply initialize bias negative value training aforementioned study interested infer hidden layer image present visible hide state compute reconstructed image hide layer end start state hidden layer interest activation ie fire probability subsequent low layer compute deterministically single topdown pass double weight compensate lack bottomup input reconstructed image obtain visible way reconstructed image determine state initial layer independently actual current state layer present necker cube image hidden state find converge sample cycle consist pass sample hidden layer unambiguous interpretation remain exhibit perceptual switching respective alternative interpretation employ model neuronal adaptation note utilize dbm rbm inference instead generate datum sample ie case visible clamp image rate statistic compute measure unit activity pairwise correlation train model run training datum neuronal internal representation decode hidden layer find switch time image interpretation model exhibit perceptual example switching internal representation display figure b observe perceptual state distinct high layer quantitative analysis compute square reconstruction error image decode layer regard image interpretation plot time figure internal representation evolve trial representation match image interpretation relatively stable manner sample cycle degradation short transition phase perceptual switch examine effect adaptation individual neuron pick unit layer high variance activity level neuronal parameter change image pixel hidden layer x unit layer training dbm note behavior network depend heavily specific training datum set use employ simple training method pretraine tuning dbm claim advanced method lead sample behavior especially simple toy datum use pcd instead find switching reconstruction noisy argument hand important general bad mix model problem alleviate method use setup exhibit problem useful point binocular rivalry section reconstruction error activation mean weight sample cycle match internal state interpretation sample cycle single unit property figure time course square reconstruction error decode hide state wrt image interpretation apart transition period percept point match close error interpretation high error activation ie fire probability mean synaptic strength arbitrary origin unit layer unit participate code interpretation dash line mark currently active interpretation recovery synaptic efficacy preferred interpretation respectively lead change activation precede perceptual switch trial indicate unit involve code image interpretation figure b plot time course activity level ie fire probability accord equation mean synaptic efficacy weight strength connection unit expect firing probability unit close interpretation close especially initial time period perceptual switch neuron firing rate synaptic activity deviate low average level synaptic efficacy change plot example instantiation preferred stimulus interpretation drop neuronal ultimately lead activity precede change overall network subsequently trigger perceptual switch trial use image necker cube different position unit constant low firing rate indicate involve represent image neuronal parameter find stable trial slight initial monotonic change allow neuron assume low baseline activity determine rate statistic unit find relatively stable high firing rate image trial code feature stimulus common image interpretation neuronal parameter equally adapt activity extent adaptation limit decay parameter equation adaptation set sufficiently strong allow exploration posterior representation unambiguous image feature similarly note internal representation model present unambiguous image training set stable adaptation setting parameter value quantify statistic perceptual switching measure length time model state stay interpretation test image result histogram percept duration time interval switch display figure separately interpretation shape gamma distribution qualitatively agreement experimental result human subject bias apparent model interpretation different different image bias observe human visible datum potentially induce statistical property environment datum set involve bias merely artifact produce basic training procedure use change weight bias equivalent attention attention attention attention norm count norm count attend percept duration percept duration percept duration interpretation leave right figure attention b figure histogram percept duration perceptual switch interpretation leave right respectively test image ignore peak small interval length stem fluctuation transition histogram fit distribution black curve omit right figure avoid clutter plot figure histogram spatial attention employ section interior corner necker cube b distribution shift remain unchanged depend attend corner salient image interpretation question role spatial attention statistic perception influence human subject necker cube direct gaze corner cube especially interior effect explain feature way salient interpretation explanation match simplified setup cube use training uniquely match interpretation lack interior corner follow model eye movement attention involve internal attentional affect perceptual switch present image remain unchanged spatial bias internal representation model employ hide layer implement use fact receptive field topographically organize sparsity dbm break symmetry unit possible suppress represent information suppress activity specific hide unit use gaussian shape center salient internal corner necker cube figure b apply hide unit additional negative bias activity far away effect attention percept duration test image display figure datum obtain attention comparison interpretation match corner attend find shift long percept duration figure left distribution interpretation relatively unchanged figure right average test image mean interval spend represent interpretation favor spatial attention increase change interpretation model spatial attention percept salient feature attend qualitatively line experimental datum term attention effect specific depend nature stimulus detail instruction experimental subject experiment binocular rivalry related study consider perceptual multistability light probabilistic inference focus binocular rivalry human present different image eye perception find switch image depend find experimental study examine attention interior corner necker cube simulate r l figure example image binocular rivalry experiment train image leave contain horizontal vertical bar left right image half identical correspond left right eye test image left right half draw independently come category example conflict category middle example test reconstruction error reconstruction error training r sample cycle percept eye image category sample cycle b percept eye image conflict category figure binocular rivalry display square reconstruction error layer representation compute input image input image come category vertical bar percept prominent result modest similar error image b input image conflict category percept alternate strongly image intermediate fuse state case necker cube change error find result individual bar appear disappear percept specific size content image perception switch completely image fuse vary degree time demonstrate simple experiment phenomenon binocular rivalry address framework end model architecture use number visible unit double unit separate left right eye train set visible simply receive image test left right half set independently draw training image simulate binocular rivalry experiment unit hide layer set monocular sense receptive field cover visible unit left right half high layer distinction datum set use image contain vertical horizontal bar figure switching observe adaptation generally perceptual state find bias image period fuse image extent transition phase figure interestingly fuse prominent depend nature conflict input image image category vertical horizontal line fuse occur figure image conflict category percept represent image happen primarily transition period figure b quantify compute reconstruction error decode hide state regard image absolute difference average trial measure internal state represent image individually version find measure time high conflict category result qualitatively line psychophysical experiment differ compatible image different patch source image relate work discussion study contribute emerge trend computational neuroscience consider approximate probabilistic inference brain complement recent paper examine perceptual multistability light argue interpret multistability inference base mcmc focus binocular rivalry importantly use random field highlevel description perceptual problem possible cause generate image topology match stimulus argue brain implement mcmc inference external variable statement wrt underlie neural mechanism contrast model mcmc perform internal neurally embody latent variable learn datum bistability result learn highdimensional hide representation directly problem formulation study model perceptual switching cube include influence image context explore future work similar start highlevel description problem design abstract inference process different prediction model model sample draw posterior mode represent interpretation accumulate time old sample exponentially discount separate decision process select sample determine interpretation reach model current percept simply determine current overall state network switch dynamic direct result state evolve time explain binocular rivalry predictive code framework identify switch exploration energy landscape suggest contribution stochasticity adaptation connection sample provide computational model work example model thing necker cube bistability provide biological detail consider role spatial attention study instance approach base switch neuronal adaptation functional role multistability relegate instead functional relevance adaptation role play learn similarly early work dayan utilize adaptation process deterministic probabilistic model binocular rivalry suggest sample provide stochasticity relation sampling adaptation address approach support recent result indicate noise neuronal adaptation necessary explain binocular rivalry note setup course simplification abstraction explicitly model depth perceive necker cube actually cube use training d cube actually contrary depth information available d image cube actually d cube collection line flat surface flat cube represent brain hierarchical architecture consist specialize area realize high level area code object eg area cortex represent d cube area primarily involve depth represent flat surface work early dbm different hide layer represent different partially conflict information cf figure finally note preliminary experiment depth information use real value visible perceptual switching occur conclusion provide biological interpretation seemingly distinct explanation perceptual multistability probabilistic inference neuronal adaptation merge framework approach account combine sample base inference adaptation concrete neural architecture utilize learn representation image study far demonstrate relevance cortical model believe far develop hybrid approach combine probabilistic model dynamical system classic connectionist network help identify neural substrate bayesian acknowledgment support thank reviewer comment reference change view perception trend cognitive dayan p hierarchical model binocular rivalry neural computation r p bayesian modeling cue interaction bistability perception journal optical society r p perceptual multistability predict search model bayesian decision vision predictive coding explain binocular rivalry review cognition e tenenbaum perceptual multistability advance neural information processing system blake r neural theory binocular rivalry psychological review cortical model d perception curve surface d image development attention bistability vision g m statistically optimal perception learn behavior neural representation trend cognitive science e d griffith t l tenenbaum optimal decision sample proceeding annual conference cognitive science society d p p storkey j induce deep boltzmann machine model advance neural information processing system d p p storkey hierarchical generative model recurrent attention visual cortex proceeding international conference artificial network bengio quickly generate representative sample process neural computation m dynamical weight learn proceeding th annual international conference machine acm salakhutdinov r deep boltzmann machine proceeding th international conference artificial intelligence statistic aistat vol pp use fast weight improve persistent contrastive divergence proceeding th annual international conference machine acm w m dynamic stochastic synapsis computational unit neural computation g neuron synaptic scaling excitatory synapsis cell perceptual dominance time distribution visual perception biological cybernetic m attention selectively bias bistable perception difference binocular rivalry ambiguous figure t c perception mechanism control perception m b s direct spatial attention object alter functional equivalence shape description experimental psychology human perception performance r attentional control compete percept ambiguous stimulus reveal analysis mean difference vision research m blake neural basis binocular rivalry trend cognitive science t r r distance feature space determine visual rivalry vision m blake r cause dominance binocular rivalry attention perception
incremental variational sparse regression robotic intelligent robotic intelligent abstract recent work scale gaussian process regression gpr large dataset primarily focus sparse gpr leverage small set basis function approximate gaussian process inference majority approach batch method operate entire training dataset preclude use dataset streaming large fit memory previous work consider incrementally solve variational algorithm fail update basis function perform propose novel incremental learn variational sparse base stochastic ascent probability density reproduce hilbert space new formulation allow update basis function online accordance manifold structure probability density fast convergence conduct experiment propose approach achieve empirical performance term prediction error recent stateoftheart incremental solution variational sparse gpr introduction gaussian process gps nonparametric statistical model widely use probabilistic reasoning function gaussian process regression use infer distribution latent function datum merit gpr find maximum posteriori estimate function provide profile remain uncertainty drawback nonparametric learning technique time space complexity scale training datum observation inference involve invert covariance matrix require operation gpr large infeasible regression solution trade accuracy computational complexity instead parameterize posterior use n observation idea approximate gp use statistic finite function value leverage induce lowrank structure reduce complexity om m memory sparse express term distribution xi m induce point general representation leverage information induce function define indirect measurement bounded linear operator integral compactly capture gp work general notion induce function trivially include choose denote parameter identity abuse notation reuse term induce point define induce function conference neural information processing system nip learn sparse gp representation regression summarize inference hyperparameter induce point statistic induce function approach learning treat parameter hyperparameter find solution maximize marginal likelihood alternative approach view induce point statistic induce function variational parameter class gps approximate true posterior solve problem variational inference robust overfitte method design batch set datum collect advance use training dataset extremely large datum stream encounter sequence want incrementally update approximate posterior latent function early work opper propose online version gpr greedily perform moment match true posterior sample instead posterior sample recently attempt modify variational batch incremental algorithm learn sparse method rely fact variational sparse gpr fix induce point hyperparameter equivalent inference conjugate family propose stochastic approximation variational sparse problem base stochastic natural gradient generalize approach case general gaussian process prior original variational algorithm find optimal point hyperparameter algorithm update statistic induce function paper propose incremental learning algorithm variational sparse denote ivsgpr leverage dual formulation variational sparse gpr reproduce perform stochastic mirror ascent space probability density update approximate posterior stochastic gradient ascent update hyperparameter ascent similar stochastic natural gradient ascent consider manifold structure probability function converge fast naive gradient approach iteration solve variational sparse gpr problem size minibatch result ivsgpr constant complexity iteration learn hyperparameter induce point associate statistic online background section provide brief summary gaussian process regression sparse process regression efficient inference proceed introduce incremental variational sparse gaussian process regression section process regression let family realvalue continuous function r gp distribution function finite set gaussian distribute x mx represent mean covariance f respectively shorthand write covariance kernel function parametrize set hyperparameter encode prior belief unknown function work simplicity assume kernel parameterize x positive definite kernel scaling factor s denote hyperparameter objective gpr infer posterior probability function data d learn function value treat latent variable observation xi model function corrupt let xi posterior probability distribution pf compactly summarize denote vector crosscovariance x r r denote empirical covariance matrix training set hyperparameter gp learn maximize loglikelihood observation max log sparse process regression straightforward approach approximate gp prior interest degenerate gp formally xi assume m denote denote probabilistic independence random variable original empirical covariance matrix replace covariance m crosscovariance let diagonal induce point treat hyperparameter find jointly maximize loglikelihood max approach sparse view special case problem deterministic training conditional approximation set pfx fully independent training conditional approximation include result sum match true covariance noise set diagonal term general maximum likelihood scheme find induce point adopt variation major drawback approach prior parametrization overfit high variational sparse alternatively formulate approximate posterior function gp parameterize induce point statistic induce function specifically propose use qfx pfx approximation pf approximate pfx m conditional probability gp pfx novelty qfx parametrization finite parameter gp infinitedimensional inference problem variational sparse gpr solve minimize kldivergence pfx practice minimization problem transform maximization lower bind pfx max qfx log qfx pfx max pfx qfx log s max treatment equality result exact maximization m s pfx likelihood note qfx function m s result variational parameter optimize function overfitting compare variational approach regularize learning penalty exhibit generalization performance subsequent work employ similar strategy adopt variational approach regression set scale basis function use expectation propagation solve approximate posterior factorization incremental variational sparse gaussian process regression leverage sparsity batch solution variational objective require operation access training datum optimization step mean learn large dataset infeasible recently attempt incrementally solve variational sparse gpr problem order learn model large dataset key idea rewrite explicitly sum individual observation pfx qfx log s log pyi m qfx identical problem stochastic variational objective function fix inference conjugate exponential family exploit idea incrementally update statistic m s stochastic natural gradient ascent tth iteration direction derive limit maximize subject natural gradient ascent consider manifold structure probability distribution derive kl divergence know fisher efficient optimal induce point update new observation difficult design natural statistic m s online pfx depend gradient ascent learn induce point observation evaluate divergence respect pfx qfx iteration infeasible propose novel approach incremental variational sparse work reformulate rkh dual form avoid issue posterior approximation pfx qfx refer observation result perform stochastic approximation monitor divergence posterior approximate change iteration specifically use stochastic mirror ascent space probability density rkh recently prove efficient stochastic gradient ascent iteration solve subproblem fractional bayesian formulate standard variational sparse gpr size minibatch om nm m operation nm size minibatch dual representation gaussian process rkh h hilbert space function satisfy reproduce property kx general infinitedimensional uniformly approximate continuous function compact set simplify notation write h l h infinitedimensional process dual representation h exist h positive semidefinite linear operator h x h kx x mean function realization h define kernel t covariance function equivalently represent linear operator shorthand abuse notation write note assume sample h following loss generality assume prior consider regression mx subspace parametrization approximate posterior gp posterior approximation pfx qfx write equivalently subspace m parametrization use h fix experiment potentially update stochastic gp infinitedimensional define density gaussian measure notation use indicate gaussian measure define equivalently h define rm rm m pm define satisfy m m suppose qfx ai m s imply relationship x covariance relate induce function define sparse structure result m r posterior gp find note scaling factor associate evaluation induce function addition distinguish hyperparameter length scale control measurement basis parameter induce point precisely subspace parametrization correspond gp family completely determine statistic m induce point gps lie nonlinear submanifold space gps degenerate gp special case allow ignore sparse process regression rkh reformulate variational inference problem rkh follow previous section sparse gp structure posterior approximate qfx correspond dual qfx relate follow representation pfx determinant change measure equality allow rewrite term simply equivalently space limitation proof equivalence find appendix benefit formulation sample form pyi m approximate posterior summarize variational parameter s refer sample pfx qfx kldivergence iteration use online learn incremental learning consider structure variable induce bregman divergence optimization apply solve variational problem convex space probability density ignore dependency simplicity iteration solve t qt max lq yt subgradient l respect q step size observation solve numerical error yt converge t assume set finite assumption suffice learn allow restrict span infinitedimensional h equivalence write term derivative normal gaussian measure denote gaussian measure rkh representation subproblem actually equivalent sparse variational gp regression general prior definition lq derive qt q pf log q log equation equivalent prior modify pf likelihood modify pyi isotropic subspace parametrization express basis function qt suppose mean t precision equivalent constant factor t t t express basis addition far write form solve standard sparse variational program modify m s appendix detail derive equation single observation use convergence rate reduce variance change factor t nm hyperparameter gp update variational parameter use gradient ascent gradient log relate work equivalent perform stochastic natural gradient ascent project distribution lowdimensional manifold specify subspace parametrization tth iteration define qt t gp view infinitedimensional gaussian distribution view result natural stochastic gradient ascent step size t order project qt subspace parametrization specify m basis function view perform stochastic natural gradient control ascent divergence projection perspective induce function fix subproblem ivsgpr degenerate algorithm recently research consider manifold structure induce kl divergence hoffman use trust region mitigate sensitivity stochastic variational inference choice hyperparameter initialization let t size trust region tth iteration solve objective t subproblem apply difference t decay step sequence ascent manually select similar formulation appear variational parameter linearize use particle density estimator approximate posterior set contrast follow adopt gp focus approximate posterior avoid difficulty estimate posterior x approximate posterior relate function value shed light convergence condition p point solve accuracy long t t p order t t solve linearize approximation step report satisfactory empirical result variational sparse solve nonlinear empirically optimization objective function increase significantly iteration base result argue online learning solve approximately perform small fix number line search experiment compare method ivsgpr vsgprsvi stateoftheart sparse base stochastic variational inference datum sample training dataset update model consider gp prior generate automatic relevance determination kx d sd length scale dimension d dimensionality input induce function modify multiscale kernel d d d d d lengthscale parameter definition include kernel special d case recover identify x d d sd d cross covariance compute follow experiment set number induce function model initialize hyperparameter induce point hyperparameter select optimal batch variational sparse train subset training dataset size induce point initialize random sample minibatch choose learning rate t t stochastic ascent update posterior approximation learning rate stochastic gradient ascent update hyperparameter set t evaluate model term normalize mean square error nmse heldout test set iteration perform experiment realworld robotic dataset dataset sarco variation ivsgpr sarco dataset implement vsgprsvi use stochastic variational inference update fix hyperparameter induce point solution batch variational sparse training datum vsgprsvi reflect perfect scenario perform stochastic approximation select learning rate consider optimal goal want approach experimental result sarco summarize table general scheme perform good observe perform small fix number iteration ivsgpr ivsgpr result performance close vsgprsvi possible explanation change objective function gradientbased algorithm dominant iteration find induce point hyperparameter finite numerical resolution batch optimization example figure change test error iteration learn joint sarco dataset method convergence rate improve large minibatch addition figure b observe require number step need solve decay number iteration small number line search require iteration table b table c experimental result large dataset experiment mix offline online partition original split training testing dataset order create online streaming scenario compare vsgprsvi dataset compute induce point hyperparameter batch infeasible stand model closely follow find difference vsgprsvi great large benchmark auxiliary experimental result illustrate convergence experiment summarize table b c include appendix training datum testing datum attribute sarco training datum testing datum attribute offline data online datum attribute number subscript denote number function allow nonlinear solve subproblem denote solve relative function change sarco sarco sarco sarco sarco sarco svi sarco table testing error nmse iteration nm denote joint test error b function figure online learn result sarco joint nmse evaluate hold test set dash line solid line denote result nm respectively b number function use solve maximum impose conclusion propose stochastic approximation variational sparse ivsgpr reformulate variational inference update statistic induce function induce point unify stochastic mirror ascent probability density consider manifold structure experiment ivsgpr performance direct variational inference solve variational sparse gps execute fix number operation minibatch suitable application training data sensory datum robotic future work interested apply ivsgpr extension process dynamical system modeling reference process general likelihood convolve process regression advance neural information processing system page efficient gaussian process variational induce kernel international conference artificial intelligence statistic page amari natural gradient work efficiently learn neural dhillon clustering divergence machine learn research manfre opper sparse online gaussian process neural computation song scalable bayesian inference gaussian process sparse use induce feature advance neural information processing system page process big datum unifying framework anytime sparse process regression model stochastic variational inference big datum proc icml page inference machine learn research transform machine learning dimensional analysis quantum probability related topic pascal kullbackleibler proximal variational inference advance neural information processing system page incremental local gaussian regression neural information processing system page learn research nemirovski robust stochastic approximation approach stochastic programming optimization rasmussen unifying view sparse regression journal machine learn research information geometry descent information theory ieee transaction process machine learn seeger forward selection speed sparse regression artificial intelligence statistic number variational generalized gp model proceeding international conference machine learn icml page process use advance neural information processing system page local global sparse approximation international conference artificial intelligence statistic page hoffman trustregion method stochastic variational inference application streaming datum preprint variational learning induce variable sparse process international conference artificial intelligence statistic page proceeding th international conference machine learn page acm
abstract gate new notation represent mixture model independence factor factor graph provide natural representation messagepasse algorithm expectation propagation message pass mixture model capture factor graph entire mixture represent factor message equation structure gate capture structure allow independence messagepasse equation model readily visualize different variational approximation mixture model understand different way draw gate model present general equation expectation propagation variational message pass presence introduction graphical model bayesian network factor graph widely use represent fix dependency relationship random variable graphical model commonly use data structure inference algorithm allow independency variable exploit lead significant efficiency gain widely use notation represent contextspecific dependency dependency present absent condition state variable graph notation necessary represent communicate contextspecific dependency able exploit contextspecific independence achieve efficient accurate number notation propose represent contextspecific dependency include case factor diagram bayesian network label graph widely adopt raise question property notation need achieve widespread use believe need simple understand use flexible represent contextspecific independency real world problem data structure allow exist inference algorithm exploit contextspecific independency efficiency accuracy gain conjunction exist representation factor paper introduce gate graphical notation represent dependency believe achieve section describe gate use represent contextspecific independency number example model section motivate use gate inference section expand gate use standard inference algorithm expectation propagation variational message pass vmp gibb sample section placement gate tradeoff cost accuracy inference section discuss use gate implement inference algorithm b p m m p true c false c c m p p m p m m p pn d n c c figure gate example dash rectangle indicate gate contain gaussian factor selector variable c b gate different key value use construct mixture c multiple gate share selector variable draw selector variable connect gate mixture construct use gate plate clarity factor correspond variable prior omit gate gate enclose factor graph switch depend state latent selector variable gate selector variable particular value key value gate allow contextspecific independency explicit graphical model dependency represent factor gate present context selector variable key value mathematically gate represent raise contain factor power gate fi x c selector variable diagram gate denote dash box label value key selector variable connect box boundary label omit boolean key true example paper refer factor gate notation use direct bayesian network undirected simple example gate figure example represent term m p true gate gaussian distribution mean m precision p gate uniformly distribute connect use gate different key value multiple component mixture represent figure b mixture gaussian represent use gate different key value true false true distribution m p distribution m p selector variable differ key value draw figure gate rectangle selector variable connect gate notice example integer selector variable use key value integer large homogeneous mixture gate use conjunction plate figure d mixture n represent place factor variable plate replicate gate nest imply conjunction condition avoid ambiguity gate partially overlap gate contain selector variable edge label b e e genetic variant p pixel intensity trait m figure example model use gate line process neighboring pixel intensity independent edge exist b testing dependence genetic variant gn observe quantitative trait selector variable c encode dependency represent structure gate present absent gate contain variable factor variable behaviour gate default value false depend variable type mathematically variable gate represent dirac delta gate default value figure b example variable contain gate example describe follow section example model gate figure line process use gate clear assumption neighbor image pixel xi dependency intensity value edge factor hide contextspecific independence gate use test independence case selector variable connect gate example figure b model use functional genomic aim detect association genetic variant gn quantitative trait height weight intelligence datum set individual binary selector variable c switch linear model genetic variant contribution trait individual gate default value trait explain background model infer posterior distribution allow association genetic variation trait detect gate arise messagepasse mixture model factor graph notation arise naturally describe message pass algorithm sumproduct similarly gate notation arise naturally consider behavior message pass algorithm mixture model example consider mixture model figure b precision p p constant use key instead true false joint distribution px m m gaussian distribution apply approximation model obtain follow fixedpoint system pc log log log mk update interpret messagepasse combine blur raise power example update interpret message message update interpret blur message m blur message m occur message send factor random exponent factor exponent exponent act affect message pass use graphical notation gate hold factor switch gate blur operation happen message leave gate message pass gate gate unchanged graphical property hold true algorithm example model blur message mean linear combination function follow gate equivalent pick factor possible rewrite model factor exponent gate necessarily change approximation blur effect cause exponent operate direction blur effect cause intermediate factor example suppose try write model use factor h c c introduce latent variable h model px m m h pm h m h m h pick factor correctly blur message m m pick factor blur message reach factor incorrect approach pick m m reach factor model m m m pm m m case message blur message m m blur correct message m m blur reach incorrect variable gate consider example natural consider variable gate model px m m pm use structured variational approximation condition c fixedpoint equation k pc k k log log log x k log k log k log k log notice message mk blur message blur think sit gate message gate c interpret evidence submodel contain inference gate previous section explain gate notation arise perform message pass example mixture model section describe gate notation generally incorporate variational message pass expectation propagation gibb sample allow algorithm support contextspecific independence reference table message need apply standard vmp use fully factorize approximation notice use different message deterministic factor factor form derive child variable different vmp message use deterministic derive variable algorithm marginal distribution obtain derive child variable approximate model evidence obtain product contribution variable factor table contribution algorithm exception deterministic factor derive variable contribute vmp perform inference model gate useful employ normalise form model form variable gate link factor gate variable gate link factor gate requirement achieve split variable copy inside copy gate connect equality factor gate factor gate connect selector gate key value instead addition gate balance ensure variable link alg type variable factor factor variable stochastic ai det parent log alg evidence variable p q si vmp p xi log log det child m evidence factor p p q sa m log p q table message evidence computation table message variable xi factor notation refer neighbor factor neighbor par parent factor derive variable child variable deterministic factor operator return distribution sufficient statistic match p table evidence contribution variable factor algorithm factor gate selector variable c variable link factor gate key value selector variable c achieve connect variable uniform factor gate missing value c balance gate gate block set gate activate different value condition variable detail variational message pass gate vmp augment run gate model normalise form change message gate introduce message gate selector variable message send node gate message gate unchanged standard variational distribution variable gate implicitly condition gate selector end section follow individual gate denote selector variable c key derivation message gate modify follow message factor fa gate g selector variable usual vmp message raise power following case variable xi child number deterministic factor gate block selector variable c variable treat derive message average individual vmp message message kg xi gg usual vmp message unique parent factor projection exponential family message gate g selector variable product evidence message contain nod kg kg sa si vmp evidence message factor variable respectively table set contain factor include gate treat single factor contain gate deterministic variable factor send evidence message deterministic factor fa parent variable xi instead send factor send sa log child variable xi gate different evidence message log message parent message parent allow nest gate define evidence message gate expectation propagation gate support gate model normalise form small modification messagepasse rule message node gate unchanged recall follow gate balance gate gate block follow individual gate denote selector variable c key kg derivation message gate follow message selector variable gate gate block product message variable exclude message gate message variable neighboring factor gate block product message variable exclude message factor let set variable outside connect factor gate compute intermediate quantity define xi usual message xi unique neighboring factor term use cancel denominator sa definition table quantity message gate specify combine message factor gate block selector variable variable weighted average message send factor kg note gate message gate block g selector variable c kg p gg finally evidence contribution gate block p gg gibb sample gate gibb sample easily extend gate contain factor gate contain variable require compute evidence submodel gibb sample provide note gibb sampling support deterministic factor graph normalise constraint algorithm start set variable initial value send value neighboring factor variable xi turn query neighboring factor conditional distribution xi factor gate currently replace uniform distribution gate g conditional distribution proportional key value product factor multiply distribution neighboring factor variable conditional distribution sample new value variable conditional distribution enlarge gate increase approximation accuracy gate induce structured approximation node gate trade inference accuracy cost gate gate block node variable factor gate block equivalently place gate increase accuracy separate set message maintain case increase cost example suggest structured approximation mixture model instead approximation modification view gate figure enlarge gate block include multiplication factor remove increase accuracy come additional cost use gate message need m dirichlet gamma m m discrete b m gamma m m discrete dirichlet gamma m gamma figure mixture model use gate model b structured approximation suggest interpret enlarge gate discussion conclusion gate prove useful implement library inference graphical model use gate library allow mixture arbitrary submodel mixture factor gate use compute evidence model place entire model gate binary selector variable log evidence b log p b true log p b false similarly gate use model comparison place model different gate gate block marginal selector posterior distribution model graphical model provide visual way represent probabilistic model use data structure perform inference model gate similarly effective graphical modelling notation construct inference reference factor graph algorithm proc communication control compute c friedman m independence network th conference uncertainty artificial intelligence page mcallester m collin pereira diagram structured probabilistic modeling uncertainty artificial intelligence b d sontag l approximate inference infinite bayesian network th workshop artificial intelligence statistic e label graph notation graphical model extend report technical report uci ic l operation learn graphical model geman geman stochastic relaxation gibb distribution bayesian restoration image ieee tran pattern anal machine e s mapping factor underlie quantitative trait use map genetic variational approximation mean field theory junction tree algorithm page c m bishop variational message pass jmlr t p minka expectation propagation approximate bayesian inference page t minka gate graphical notation mixture model technical m m bishop robust bayesian mixture modelling c m robust bayesian cluster neural network
correlated nonparametric topic model computer abstract topic model learn statistical model variation document collection design extract meaningful semantic structure desirable trait include ability incorporate annotation metadata associate document discovery correlated pattern topic usage parametric assumption manual specification number topic propose correlated nonparametric topic dcnt model model simultaneously capture property model metadata flexible gaussian regression arbitrary input feature correlation scalable covariance representation nonparametric selection unbounded series potential topic stickbreake construction validate semantic structure predictive performance dcnt use corpus nip document annotate metadata introduction problem explore huge collection discrete datum sequence text document development increasingly sophisticated statistical model probabilistic topic model represent document mixture topic distribution discrete vocabulary dirichlet allocation hierarchical bayesian topic model remain influential widely use suffer key limitation jointly address propose model assumption dirichlet prior implicitly neglect correlation topic usage diverse true semantic topic exhibit strong positive negative correlation neglect dependency distort infer topic structure correlate topic model use prior express correlation latent gaussian distribution usage softmax multinomial logistic transformation require global normalization turn fix finite number topic second assumption document represent solely bag word text datum accompany rich set metadata author publication date relevant keyword topic consistent metadata relevant dirichlet multinomial regression model condition lda dirichlet parameter linear regression allow topic frequency retain limitation dirichlet recently model incorporate correlation topic level topic covariance document level appropriate gp function model remain parametric treatment number topic computational scaling large dataset challenge learn number document exactly sample dirichlet distribution draw vector independent gamma variable normalize sum normalization induce slight negative correlation assumption priori choice number topic direct nonparametric extension hierarchical dirichlet process hdp hdp allow unbounded set topic latent stochastic process impose dirichlet distribution finite subset topic alternatively nonparametric baye allocation model capture correlation unbounded topic collection infer direct recently discrete infinite logistic normal model topic correlation use gaussian gp rescale hdp construction base process representation dp goal similar propose different model base stickbreake representation dp choice lead arguably simple learn algorithm facilitate modeling document paper develop correlated nonparametric topic dcnt model capture correlation correlation induce metadata unbounded set potential topic describe global softmax transformation replace stickbreake transformation input determine linear regression covariance representation choice lead wellpose nonparametric model allow tractable mcmc learning validate model use toy dataset corpus nip document annotate author year publication correlated nonparametric topic model dcnt hierarchical bayesian nonparametric overview model structure fig focus key innovation document metadata consider collection document let denote feature vector capture associate document d d matrix assume unbounded sequence topic let r denote associate significance weight feature rf vector weight place topic weight vector mean feature response diagonal precision matrix hierarchical bayesian fashion parameter prior bf appropriate value hyperparameter bf discuss later score topic sample realvalue score map topic frequency subsequent section topic correlation topic order sequence topic define sequence weight sample variable vkd follow vkd k let denote low triangular matrix contain value slightly abuse notation compactly write transformation l l infinite diagonal precision matrix critically note distribution vkd depend entry infinite tail score subsequent topic marginalize covariance equal l classical factor analysis model encode representation output covariance matrix integration input metadata close connection semiparametric latent factor model replace kernelbase gp covariance representation featurebased regression matrix let denote column vector index row vector index figure direct graphical representation dcnt model d document contain word unbounded set topic word distribution k topic assignment word depend topic frequency d correlated dependence metadata d produce variable implement mapping simplify mcmc method similar low triangular representation factorize covariance matrix conventional bayesian factor analysis model place symmetric gaussian prior prior grow linearly produce artifact standard factor analysis dcnt unbounded instead propose alternative prior variance entry row reduce factor shrinkage carefully choose remain constant constrain diagonal matrix recover simplified correlate nonparametric topic scnt model capture topic correlation model precision parameter assign conjugate prior logistic mapping stickbreake topic frequency stick break representation widely use application nonparametric bayesian model lead convenient algorithm let probability choose topic document d dcnt construct probability follow vkd vkd vkd classic logistic function satisfy transformation socalled logistic stickbreaking process model motivate different application employ different prior distribution distribution topic assignment indicator word document d draw accord finally wdn word distribution topic sample dirichlet prior symmetric hyperparameter inference use markov method approximately sample posterior distribution dcnt parameter choice conditionally conjugate prior lead close form gibb sample update logistic stickbreaking transformation close form resample v intractable instead use independence sampler sampler base finite truncation dcnt model prove useful stickbreake prior let maximum number topic experiment demonstrate number topic utilize learn model k possibly loose upper bind number notational convenience let matrix regression coefficient u d truncated model low triangular matrix matrix satisfy t d similarly topic set eq probability final topic set valid distribution ensure vkd gibb update topic assignment correlation parameter hyperparameter precision parameter control variability feature weight associate topic regression model gamma prior conjugate bf bf similarly precision parameter v gamma prior posterior d l d entry regression matrix rescaled gaussian prior gamma prior precision parameter follow gamma aa condition feature regression weight mean weight hierarchical prior feature pf sample linear function relate topic condition document column conditionally independent t t similarly score document conditionally independent l t d l la t la resample note row conditionally independent posterior entry document d ak row depend u d t u scnt model related simple update supplemental material sample algorithm lda analytically marginalize word topic let denote number instance word assign topic exclude token document d mk number total token assign topic vocabulary unique word type posterior distribution topic indicator d recall topic probability determine equation independence sampler update topic activation posterior distribution close analytical form logistic nonlinearity underlie stickbreaking construction instead employ metropolishasting sampler proposal draw ik prior combine likelihood word token proposal accept probability proposal cancel prior distribution acceptance ratio final probability depend ratio likelihood function easily evaluate count number word assign topic experimental result toy bar dataset follow related validation lda model run experiment toy corpus image design validate feature dcnt consist image document contain vocabulary word type arrange x grid document visualize display pixel intensity proportional number correspond word figure training document contain word token topic define correspond possible horizontal vertical pixel bar consider toy dataset random number topic choose document corresponding subset bar pick uniformly random second induce topic correlation generate document contain combination horizontal topic vertical topic bar dataset associated input feature simply set use toy dataset compare lda model version dcnt set number topic true value similar previous toy experiment set parameter dirichlet prior topic distribution topic smooth parameter dcnt model set gamma prior hyperparameter b correspond mean variance initialize sampler set precision parameter prior mean sample variable prior compare variant model correlated scnt constrain diagonal dcnt dcnt k final case explore stickbreaking prior successfully infer number topic toy dataset correlated topic result run sample algorithm iteration illustrate figure relatively clean datum model limit figure dataset correlated toy bar example document image leave left right true count word generate topic recover count dcnt note true topic order identifiable infer topic covariance matrix correspond model note lda assume topic slight negative correlation dcnt infer positive correlation potential dcnt topic infer high probability low variance topic recover correct topic topic dcnt recover true topic redundant copy bar typical behavior sample run length extend run usually merge redundant bar development rapidly mix method interesting area future research determine topic correlation correspond set learn model parameter use detail supplemental material matrix easy visualize use topic label good alignment ground truth topic assignment note significant block positive correlation recover dcnt reflect true correlation use create toy datum nip nip use consist publication previous nip conference include metadata year publication author section category compare variant model model ignore metadata model indicator feature year publication model indicator feature year publication presence highly author publication model feature year publication additional author publication case feature matrix binary model truncate use topic sampler initialize conditioning metadata learn dcnt model provide prediction topic frequency change particular metadata associate document figure predict topic frequency change time conditioning author sejnowski word relevant topic illustrate conditioning particular author change predict document content example visualization associate frequency topic associate probabilistic model gradually increase year topic associate neural network decrease condition large mass topic focus model develop group finally condition sejnowski dramatically increase probability topic relate neuroscience correlation topic dcnt model capture correlation topic fig visualize use diagram size grid proportional magnitude correlation figure dcnt predict topic frequency year document author b feature d sejnowski feature stickbreake distribution frequency topic average year note middle row illustrate word distribution topic highlight red dot respective column large word probable figure hinton diagram correlation pair topic size square indicate magnitude dependence red blue square indicate positive negative correlation respectively right word strongly correlate topic pair visualization paper interactive download page coefficient topic result display figure model train metadata model learn strong positive correlation function learn topic strong semantic similarity identical positive correlation model discover topic visual course paper nip study brain visual cortex strong negative correlation find network model topic reflect separation paper study neural network probabilistic model predictive likelihood order quantitatively measure generalization power dcnt model test variant version toy bar correlate uncorrelate compare model nip explore realistic datum available test datum toy dataset consist document generate process training datum perplexity toy datum perplexity score nip lda hdp figure perplexity score low compute estimator topic model leave test performance toy dataset uncorrelated bar correlated bar right test performance nip metadata feature year feature year author feature publication year additional author feature publication nip split training test subset contain respectively year total test document calculate predictive likelihood estimate use estimator detail supplemental material previous comparison estimator find far accurate alternative harmonic mean estimator note correctly implement estimator dcnt model possibility rejection metropolishasting proposal predictive negative loglikelihood estimate normalize word count determine perplexity score test model include scnt lda hdp concentration parameter toy bar datum set number topic model hdp learn nip set model hdp learn toy dataset model perform similarly scnt apparently ability capture distribution topic occurrence pattern nip datum model substantially accurate hdp include encode year publication possibly author provide slight additional improvement accuracy interestingly large set author feature include accuracy slightly bad appear overfitte issue author publication training example scnt model provide improved predictive likelihood recent study human interpretability topic model score necessarily correlate meaningful semantic structure way interactive visualization illustrate provide dcnt capture useful property real discussion correlated nonparametric topic model allow incorporation arbitrary feature associate document capture correlation exist dataset latent topic learn unbounded set topic model use set efficient mcmc technique learning inference support set tool allow user visualize infer semantic structure acknowledgment research support contract number support nsf fellowship view conclusion contain author interpret necessarily represent official policy express imply government reference gaussian process topic model m blei d correlated topic model science m blei m latent dirichlet allocation learn m read leave human interpret topic model nip ferguson bayesian analysis nonparametric problem stat rubin bayesian datum analysis chapman t l griffith m find scientific topic h gibb sample method stickbreake prior mar d blei mccallum nonparametric baye allocation m model assessment factor analysis mccallum topic model condition arbitrary feature regression murray salakhutdinov evaluate probability highdimensional latent variable model nip page blei discrete infinite logistic normal distribution model aistat logistic stickbreaking process jmlr rodriguez nonparametric bayesian model probit stickbreaking process j bayesian analysis sethuraman constructive definition dirichlet prior stat sin teh m m blei hierarchical dirichlet process journal teh m seeger latent factor model aistat h m wallach murray r salakhutdinov d evaluation method topic model icml
principal difference analysis interpretable characterization difference abstract introduce principal difference analysis pda analyze difference highdimensional distribution method operate find projection maximize wasserstein divergence result univariate population rely device require assumption form underlie distribution nature difference sparse variant method introduce identify feature responsible difference provide algorithm original minimax formulation semidefinite relaxation addition derive convergence result illustrate approach apply identify difference cell population manifest single cell broad framework extend specific choice wasserstein divergence introduction understand difference population common task biomedical datum analysis textual analysis example biomedical analysis set variable feature gene profile different condition eg cell type variant result population compare hope analysis answer population differ variable relationship contribute difference case interest comparison challenge primarily reason number variable profile large population represent highdimensional set sample information lack nature possible difference exploratory analysis focus comparison high dimensional population iid set sample px pq goal answer follow question underlie multivariate random variable p rd q px minimal subset feature t marginal distribution differ complement version q additionally pose ask feature contribute overall difference probability distribution respect scale variable measure twosample analysis focus characterize limited difference mean shift general difference mean feature remain interest include statistic undesirable restrict analysis specific parametric difference especially exploratory analysis nature underlie distribution unknown univariate case number nonparametric test equality distribution available accompany concentration result popular example divergence refer probability metric include divergence kullbackleibler distance metric unfortunately simplicity vanishe dimensionality d grow complex design address difficulty appear highdimensional setting work propose principal difference analysis pda framework circumvent curse dimensionality explicit reduction univariate case statistical divergence measure difference univariate probability distribution seek find projection maximize t q subject constraint avoid reduction justify device ensure px exist direction univariate linearly project distribution differ assume d positive definite divergence mean nonzero distinct univariate distribution projection vector produce capture arbitrary type difference highdimensional px furthermore approach straightforwardly modify address q introduce sparsity penalty examine feature nonzero weight result optimal projection result comparison marginal distribution sparsity level refer approach sparse difference analysis relate work problem characterize difference population include feature selection receive great deal study limit discussion method family method close approach multivariate twoclass datum widely adopt method include sparse linear discriminant analysis lda logistic interpretable method seek specific difference average difference operate stringent assumption model positivedefinite divergence aim find feature characterize priori difference general multivariate distribution similar general approach procedure data project normal separate hyperplane find use linear svm distance weight discrimination method follow univariate twosample test project datum projection choose contrast approach choice projection method optimize test statistic note restrict divergence measure technique method sparse linear support vector machine view special case divergence case measure margin project univariate distribution suitable find project population fail general difference possibly multimodal project population general framework principal difference analysis divergence measure univariate random variable find projection p solve p pnq t pb k t p r u feasible set sparsity constraint p pnq denote observed random variable follow empirical distribution t instead impose hard cardinality constraint instead penalize add penalty term natural relaxation shrinkage use lda sparse pca sparsity setting explicitly restrict comparison marginal distribution feature nonzero coefficient evaluate null hypothesis px sparse variant marginal use permutation testing p pnq x d practice shrinkage parameter explicit cardinality constraint choose crossvalidation maximize divergence heldout sample divergence d play key role analysis define term density function divergence use univariate kernel density estimation approximate project additional tuning bandwidth hyperparameter choose objective shrinkage smooth function amenable project gradient method variant contrast define project direction wasserstein distance focus paper objective discrete jump empirical specifically address combinatorial problem imply wasserstein distance divergence assess general difference distribution equation typically optimization end develop semidefinite relaxation use wasserstein distance pda use wasserstein distance remainder paper focus squared wasserstein distance distance define min pxy px minimization joint distribution marginal intuitively interpret work require transform distribution provide natural dissimilarity measure population integrate fraction individual different magnitude difference component analysis base wasserstein distance limit divergence successfully use application univariate case analytically express l distance quantile function efficiently compute empirical project wasserstein distance sort x y sample projection direction obtain quantile estimate use wasserstein distance empirical objective equation sample population pq m p t t max wm pb k m pm pb k m pm m set n m nonnegative match matrix fix row sum column sum m detail omit fix inner minimization matching matrix set solution simply large eigenvector similarly sparse variant m problem solvable sparse pca actual problem complex respect propose twostep procedure similar tighten relax framework use attain rate sparse pca solve convex relaxation problem subsequently run steepest ascent method initialize global optimum relaxation greedily improve current solution respect original nonconvex problem relaxation tight finally emphasize computationally resemble sparse actually special case set connection explicit consider twoclass problem pair sample q follow multivariate gaussian distribution large principal component difference fact equivalent direction maximize project wasserstein difference distribution delta distribution semidefinite relaxation problem express term symmetric matrix b max min tr b m pm subject correspondence come write b note solution unit norm ie impose sparsity constraint pda relax simply drop objective supremum linear function b result semidefinite problem concave convex set write max min tr m pm convex set positive semidefinite matrix trace p rdd denote global optimum relaxation q good projection simply dominant eigenvector relaxation tight truncate b treat dominant eigenvector approximate solution original problem obtain relaxation sparse version follow closely b imply k obtain equivalent cardinality constrain problem incorporate nonconvex constraint relaxation squared constraint k select optimal lagrange multiplier constraint obtain equivalent penalize reformulation parameterize sparse semidefinite relaxation follow concave problem max min tr b m pm relaxation bear strong resemblance relaxation sparse pca inner maximization matching prevent direct application general programming solver let denote matching minimize tr standard project subgradient ascent apply solve tth iterate subgradient ptq q approach require solve optimal transport problem large m matrix iteration instead turn dual form assume m m m ij ij m m simply maximization p br u p p rm long require match matrix rowcolumn constraint dual variable u v solve closed form fix b sort describe simple subgradient approach work relax solve semidefinite relaxation return large eigenvector solution desire projection direction input ddimensional datum pq n m parameter control regularization stepsize use b update stepsize use update dual variable t maximum number iteration improvement cost terminate initialize pq d d d b pq pq b pq p br pq m number iteration improvement objective function t rn n p rm dd p t ptq ptq ptq ui vj m m m end ptq b ptq projection b ptq output rd define large eigenvector base corresponding eigenvalue matrix b q attain good objective value iteration projection algorithm project matrix positive semidefinite cone matrix br feasible set relaxation step apply proximal operator sparsity input b p rdd parameter control regularization actual stepsize use arg min p r quadratic program qt r s p t r p output b u relax box project subgradient method compute step scaling large sample alternatively employ incremental direction step replace draw random pair subgradient step projection feasible set br quadratic program involve current solution eigenvalue sparsity encourage proximal map correspond penalty overall form iteration match update convergence analysis relax incremental variant guarantee approach optimal solution dual solve provide employ sufficiently large t small stepsize practice fast accurate convergence attain step ensure balanced update constrain b b use learn rate initially set large unconstrained dual variable multiple subgradient step dual variable update tighten relaxation unreasonable expect semidefinite relaxation tight far refine projection obtain relax use starting point original optimization introduce sparsity constrain procedure apply project gradient ascent original nonconvex objective pm t wm force lie t p rd sparsity level k fix base relaxed solution initialize pq p rd tighten procedure iterate step gradient direction follow straightforward projection unit set accomplish greedily truncate entry large k let m p denote matching matrix choose response fail differentiable r m p unique occur sample identical projection r situation increasingly likely m interestingly smooth overall assume distribution admit density function m p q m p q lie small neighborhood admit welldefine gradient p q practice find approach local optimum stepsize note projection efficiently calculate gradient matrix m p q p q sort ptq ptq ptq gradient directly expression nonzero determine appropriately match empirical quantile represent sorted index univariate distance simply l distance quantile function additional computation save employ insertion sort run nearly linear time sort point point sort ptq direction sorting direction ptq likely similar small stepsize tighten procedure efficient respective runtime log q iteration require combine step good performance projection find tighten algorithm heavily depend starting point pq find close local optimum figure important good solution produce relax additionally note firstorder method relax tighten algorithm number scheme momentum technique adaptive learn rate variant nesterovs method property semidefinite relaxation conclude algorithmic discussion highlight basic condition pda relaxation tight assume m imply maximize nearly rank equivalently r b r supplementary information s intuition tighten procedure initialize r produce global maximum pda objective exist direction project wasserstein distance nearly large overall wasserstein distance rd occur example large small distribution need px q q px q q underlie covariance structure arg pb q nearly rank example primary difference covariance shift marginal variance feature ie v diagonal matrix theoretical result section characterize statistical property empirical pnq t pnq q note algorithm succeed tion arg max pb find global maximum severely problem denote square wasserstein distance univariate distribution c represent universal constant change line line proof relegate supplementary information follow simplify assumption m admit continuous density function compactly support nonzero density euclidean ball r theory generalize aa obtain similar complex statement careful treatment distribution tail region flat suppose exist direction p pnq p b dp t t probability great theorem basic concentration result projection use empirical application method relate distributional difference ambient ddimensional space estimate divergence univariate linear representation choose turn theorem finally theorem provide guarantee case exhibit large difference certain feature subset know cardinality p pnq pnq q theorem identically distribute rd x probability great r c measure difference random variable p rd define follow metric distribution rd parameterize xd aq aq addition aa assume following theorem subgaussian tail mean satisfie q note mean difference trivially capture linear projection difference interest follow theorem suppose u d p q gp q q d q p q define density projection direction p pnq pnq q c probability great define c suppose exist feature subset t q h marginal distribution c c identical arg max p pnq t pnq satisfie experiment j pb p p c probability great c r figure illustrate cost function dimensional distribution detail supplementary information s example point convergence p tighten method random initialization green significantly inferior solution produce relax red important use relax tighten synthetic dataset use nip feature selection challenge consist point m feature scatter vertex hypercube interaction feature consider order distinguish class feature noisy linear combination original feature focus challenge extract feature useful classifier direct attention interpretable model figure b demonstrate red sparse principal component black sparse logistic lasso blue able identify relevant feature different setting respective regularization parameter determine cardinality vector return method red indicate result automatically select crossvalidation procedure information underlie feature importance indicate report result challenge sample test cardinality b p value relevant feature datum dimension figure example pda feature selection method c power test multidimensional problem dimensional difference restrictive assumption logistic regression linear discriminant analysis satisfied complex dataset result poor performance pca successfully utilize numerous challenge participant find sparse pca perform par logistic regression lda lasso fairly efficiently pick relevant feature identify rest severe similarly bayesian svm automatic relevance determination select relevant feature application goal characterize set difference select subset feature maintain predictive accuracy suited alternative objective setting return relevant feature false positive choose automatically crossvalidation projection return contain nonzero element correspond relevant feature figure c depict average pvalue produce overall wasserstein distance rd maximum mean discrepancy green blue control problem underlie difference vary degree sparsity indicate overall number feature include relevant supplementary information s detail evaluate significance method statistic permutation test test guarantee exactly control type error compare respective power determine set figure demonstrate clear leverage underlie sparsity maintain high power increase overall dimensionality feature differ match power method consider space select single direction base control datum experiment demonstrate pda retain great power similar method recent advance allow complete thousand individual cell goal fine molecular characterization cell population expression measure currently standard apply expression measurement gene profile single cell cortex cell sample brain mouse result p identify previously gene respect informative result standard differential expression method supplementary information s detail finally apply normalize datum marginal order explicitly restrict search gene relationship gene expression different cortex cell analysis reveal gene know heavily involved signal important process form functional interaction gene supplementary information s detail type important change detect standard differential expression analysis consider gene isolation require explicitly identify feature conclusion paper introduce overall principal difference methodology demonstrate numerous practical benefit approach focus algorithm wasserstein distance different divergence suit certain application theoretical investigation framework interest particularly highdimensional set rich theory derive compressed sensing sparse leverage idea restrict isometry covariance natural question analogous property px theoretically guarantee strong empirical performance observe highdimensional application finally extension method present employ multiple projection succession adapt approach comparison multiple population acknowledgement research support grant reference m m powerful twosample test high dimension use random projection nip b sparse discriminant analysis weak convergence empirical process springer gibb choose bound probability metric international statistical review c high dimensional hypothesis test computational graphical statistic exact distributionfree test compare multivariate distribution base adjacency journal royal statistical society series m testing equal distribution high dimension kernel twosample test machine learn research h theorem distribution function journal mathematical society sharp form theoretical probability m maximum covariance estimator multivariate analysis regression shrinkage selection royal statistical society series feature selection support machine icml direct formulation sparse use review aa wainwright highdimensional analysis semidefinite relaxation sparse principal component annal statistic good p permutation test practical guide resample method testing duchi hazan e singer adaptive subgradient method online learning optimization machine learn research sj optimization algorithm machine learning nip r m nonnegative matrix factorization distance metric image analysis ieee transaction pattern analysis machine intelligence e p distance mallow distance insight statistic iccv tighten relax sparse pca polynomial time nip network optimization continuous discrete model scientific bertsekas dual coordinate step method linear network flow problem mathematical programming incremental gradient subgradient proximal method survey optimization machine learning mit press pp m fast iterative algorithm linear inverse problem image science feature extraction foundation application tibshirani sparse principal component analysis graphical statistic d et detail distribution study variability current opinion cell type reveal science
deterministic singlepass algorithm abstract develop deterministic singlepass algorithm latent dirichlet allocation lda order process receive document time discard excess text stream algorithm need store old statistic datum propose algorithm fast batch comparable batch algorithm term perplexity experiment introduction huge quantity text datum news article post arrive continuous stream online learning attract great deal attention useful method handle grow quantity streaming datum process datum time batch algorithm feasible setting need datum time paper focus online learning latent dirichlet allocation lda blei widely use probabilistic model text datum run short memory usage online learning develop banerjee exist study base sample method incremental gibbs sampler particle filter sampling method inappropriate streaming datum sample method represent figure overview use lot sample basically need inference time sample algorithm need resample step sampling method apply old datum store old datum old sample affect good property online algorithm particle filter need run m parallel processing parallel algorithm need memory algorithm useful large quantity datum especially case large vocabulary example need store number word observe topic number topic t vocabulary size v m require memory size om t propose deterministic online algorithm incremental algorithm singlepass algorithm incremental algorithm incremental variant reverse incremental algorithm update parameter replace old sufficient statistic new datum singlepass algorithm base incremental algorithm need store old statistic datum singlepass algorithm propose sequential update method dirichlet parameter asuncion wallach indicate importance estimate parameter dirichlet distribution distribution topic distribution document deal grow vocabulary size real life total vocabulary size unknown ie increase document observe summary fig relationship variational batch inference cvblda collapse variational inference incremental algorithm singlepass section briefly explain inference algorithm section describe propose algorithm online learning section present experimental result overview latent dirichlet allocation section overview lda document represent random mixture latent topic topic characterize distribution word define notation describe formulation number topic m number document v vocabulary size number word document denote word document denote latent topic word m multinomial distribution dirichlet distribution denote t dimensional probability vector parameter multinomial distribution represent topic distribution document t multinomial parameter v dimensional probability tv specify probability generate word topic t dimensional parameter vector dirichlet distribution assume follow generative process t topic t draw t tv m document draw nj word document draw topic m draw word tv likelihood document wj pwj nj j variational baye inference lda vb inference introduce factorize variational posterior t q variational parameter specify probability topic word topic t parameter dirichlet distribution t respectively loglikelihood document lower bounded introduce z t t z parameter update nj tv tv v indicator function estimate fix point iteration minka asuncion introduce gamma b ie t gt b t t tell njt tell tell new t nj old old iteration l m update iteration l m update update njt replace old end update end update update end t new update replace old end end update end b parameter gamma distribution vb inference scheme collapse variational baye inference lda teh propose cvblda inspire collapse gibb sample find convergence cvblda experimentally fast outperform term perplexity cvblda introduce variational posterior marginalize prior inference optimize follow lower bind m log pwj z derivation update equation slightly complicated involve approximation compute intractable summation use secondorder expansion approximation asuncion usefulness approximation use information update use information njt njt denote subtract provide cvb inference scheme deterministic online algorithm purpose study process text datum news article post arrive continuous stream use propose learning algorithm apply timeserie text stream situation want process text time discard repeat iteration word document update parameter arrive document discard document iteration need store statistic discard document derive incremental algorithm extend incremental algorithm incremental learn provide framework incremental learning general estimate sufficient statistic si datum compute sufficient statistic si datum update parameter use incremental learn datum estimate compute update parameter easy extend exist batch algorithm incremental learning sufficient statistic parameter update construct simply summarize datum statistic incremental process datum subtract old add new incremental algorithm need store old statistic datum batch algorithm update parameter sweep datum incremental algorithm update parameter datum time result parameter update batch algorithm incremental algorithm converge fast batch algorithm incremental learning motivation devise incremental algorithm compare cvblda statistic njt update word update cvblda update schedule similar incremental algorithm incremental property reason cvblda converge fast cvblda optimize find optima let consider incremental algorithm start optimize lowerbound different form use reverse minka follow pwj t t t t equation derive inequality follow log log log log lower bind loglikelihood log t t maximum respect t tv update eq note use maximum posteriori use avoid negative value lower bound introduce cvblda equation incrementally update topic distribution document word cvblda need eq marginalize equation fix point update interpret coordinate ascent update entire document compare algorithm look hybrid variant batch update incremental update consider incremental update analogous cvblda update word note setup independent identically distribute data point document word incrementally estimate document statistic number word generate topic t document incremental algorithm lda algorithm incrementally optimize lower bind incremental algorithm iteration l m update end new replace v m iteration l update end update update end update update j bj end n eq end update end singlepass lda singlepass inspire bayesian formulation include sequential update posterior distribution contribution data point separate pxn denote parameter indicate use posterior observed datum prior use parameter learn observed datum prior parameter datum example m interpret tv eq represent tv m m prior parameter m th document singlepass sequentially set prior arrive document use sequential setting prior parameter present singlepass algorithm update parameter arrive document prior parameter tv iteration tv tv tv t explain set prior parameter use statistic document document follow finally discard document j tv tv update repeat document need store statistic word document word document cvb irem algorithm dirichlet parameter use batch update ie update use entire document iteration need algorithm process streaming text parameter update eq construct simply summarize sufficient statistic datum prior derive singlepass update dirichlet parameter use follow interpretation consider expectation t b document d prior b ie et b t m ajt b b m bj ajt tell njt tell tell bj old old regard ajt bj statistic document indicate parameter actually update t b eq update simple ajt bj prior parameter update t observe document ajt t t ajt bj bj bj bj bj et t b bj use prior jth document analysis section analyze propose update parameter previous section eventually update parameter document j ajt bj t d bj d tv njt d vocabulary size total observe singlepass sequentially set prior arrive document select prior dimension dirichlet distribution correspond observe vocabulary fact property useful problem vocabulary size grow text stream update indicate interpolate parameter estimate old new datum update look stepwise algorithm stepsize interpolate sufficient statistic update interpolate parameter update set stepsize parameter equivalent set hyperparameter prior need newly introduce stepsize parameter tv update appearance rate word topic t document add old parameter weight gradually decrease document observe relation hold influence new datum decrease number document observation increase theorem important role analyze convergence parameter update use convergence convergence analysis theorem exist satisfy sj satisfie note eq proof support material experiment carry experiment document modeling term perplexity compare inference set text datum associate number document m vocabulary size second m ordering document timeserie comparison metric document modeling test set perplexity randomly split datum set training set test set word document test set stop word eliminate dataset perform experiment inference pf vb cvb irem srem denote particle filter use et set t pf number particle denote p number word resample denote r effective sample size threshold control number set cvb cvb collapse variational inference lda use secondorder information respectively represent incremental reverse cvb estimate dirichlet parameter topic distribution dataset ie batch framework estimate irem dataset cvb clarify property compare l denote number iteration document srem indicate singlepass variant irem l denote number iteration document srem iteration document figure demonstrate result experiment test set perplexity low value indicate performance run experiment time different random initialization average srem calculate test set perplexity sweep set converge slow irem irem outperform cvb convergence rate outperform algorithm case low number topic convergence rate cvb depend number topic srem outperform irem term perplexity performance srem close result recommend srem large number document document stream srem need store old statistic document algorithm addition convergence srem depend length document number document process document individually control number iteration correspond length arrive document finally discuss run time run time srem time short vb cvb irem average time hour hour hour hour conclusion develop deterministic onlinelearne algorithm latent dirichlet allocation lda propose algorithm apply excess text datum continuous stream process receive document time discard propose algorithm fast batch algorithm comparable batch algorithm term perplexity experiment exclude error bar standard deviation small hide plot sreml testset perplexity testset perplexity sreml number topic b e vb e cvb e cvb e irem e e testset perplexity testset perplexity number topic e e vb e cvb e cvb srem srem e e pf pf number number d e e e e testset perplexity testset perplexity e vb e cvb e cvb e irem srem srem e pf pf number number testset perplexity testset perplexity e sreml sreml sreml sreml number topic number topic h figure result experiment leave line indicate result right line indicate result b compare test set perplexity respect number topic e compare test set perplexity respect number iteration topic t t respectively relationship test set perplexity number iteration document ie l reference online lda adaptive topic model mining text stream application topic detection track conference datum mining asuncion m p smyth teh smoothing inference topic model proceeding international conference uncertainty artificial topic model text stream study batch online unsupervised learn international conference datum mining p bertsekas programming scientific m blei m dirichlet allocation journal machine learn owe martingale fast bayesian online model griffith online inference topic latent dirichlet allocation proceeding twelfth international conference artificial intelligence stochastic approximation method annal mathematical statistic page estimate dirichlet distribution url use low bound approximate integral r neal view justify incremental sparse variant m learn graphical model online algorithm normalize max collapse variational bayesian latent dirichlet allocation advance neural information processing system wallach lda prior matter bengio editor advance neural information processing system page efficient method topic model streaming document collection proceeding th acm international conference knowledge discovery data mining page acm
grade grammaticality prediction artificial abstract introduce novel method construct language model avoid problem associate recurrent neural network method create prediction fractal machine briefly describe experiment present demonstrate suitability language model distinguish reliably minimal pair behavior consistent hypothesis wellformedness grade absolute discussion potential offer insight language acquisition processing follow introduction cognitive linguistic development recent year important related trend firstly widespread interest statistical grade nature language traditional notion wellformedness present accurate picture utterance represent secondly analysis state space trajectory artificial neural network ann provide new insight type process account ability learn device acquire represent language appeal traditional linguistic concept remarkable advance come connectionist common use recurrent network simple recurrent network especially study language eg recurrent neural network suffer particular problem suit language task vast majority work field employ small network dataset usually artificial interesting linguistic issue tackle real progress evaluate potential state trajectory grade grammaticality underlie process responsible linguistic phenomenon limit experimental task remain small certain obstacle network train network tend grade grammaticality prediction fractal machine long train time size input set network increase realtime recurrent learning backpropagation time potentially model temporal dependency training time long difficult potential interference lack stability problem include rapid loss information past event distance present increase dependence learn state trajectory training datum initial weight vector analysis difficult type learn device suffer problem standard model require allocation memory large value impractical markov model train large datum set important related concern cognitive linguistic find method allow language model scale similar spirit recurrent neural network encounter problem scale b use method new insight grade grammaticality state trajectory arise large datum set accordingly present new method generate state trajectory avoid problem previously study financial prediction task method create fractal map training datum state machine build result model know prediction fractal machine useful property state trajectory fractal representation fast computationally efficient generate accurate infer large vocabulary training set catastrophic interference lack stability problem way representation build demonstrate topic future work training time significantly recurrent network experiment describe small model minute build large hour comparison ann long day train little loss information course input sequence allow computer scalability advantage train large corpus text enable assessment potential new insight arise use method truly language task prediction fractal machine brief description method create interested reader space constraint preclude detailed key idea predictive model transformation sequence alphabet point hypercube d dimensionality hypercube large symbol identify unique vertex h particular assignment symbol vertex arbitrary transformation crucial property symbol sequence share suffix context map close specifically long common suffix share sequence small euclidean distance point representation transformation use study correspond iterative function system p consist affine map h ti e ld j sequence l l symbol alphabet construct point representation center ld hypercube h note common iterative function system literature refer symbol map depend context construct point representation subsequence appear training sequence slide window l training sequence position transform sequence length appear window point set point obtain slide training sequence partition class kmean vector quantization euclidean space class represent particular codebook vector number code book vector require choose experimentally quantization class group point lie close sequence point representation class potentially share long suffix quantization class treat prediction context corresponding predictive symbol probability compute slide window training sequence count quantization class sequence map class follow particular symbol mode new sequence l symbol transformation perform close quantization center find corresponding predictive probability use predict symbol experimental comparison pfms recurrent network performance compare recurrent network prediction task grammatical tag sentence start character use model train concatenate sequence tag remainder use create test datum follow large training corpus context case possible correct continuation simply count correctly predict symbol assess performance fail count correct response target extent model distinguish grammatical ungrammatical utterance additionally measure generate minimal pair compare negative log likelihood nll symbol respect model likelihood compute slide test sequence window position determine probability symbol appear immediately processing progress probability multiply negative natural logarithm divide number symbol significant difference nll grade grammaticality prediction fractal machine hard achieve member minimal pair grammatical random sequence good measure model validity minimal pair generate manipulation tend long ungrammatical small removal grammatical necessarily remove large information manipulation perform switch position symbol sentence test set symbol switch distance apart sentence long result sentence ungrammatical surface instantiation change little possible sentence ungrammatical goal retain task distinguish grammatical ungrammatical sequence difficult possible test datum consist pair test set tag ungrammatical meaningless test set contain code list time use measure baseline performance storder network train epoch use network consist input output layer unit correspond tag hide layer unit context layer unit connect hide layer second hide layer use increase flexibility map hidden representation recurrent portion tag activation output layer logistic sigmoid activation function use learning rate momentum set training sequence present rate tag pfms derive cluster fractal representation training datum time number codebook vector experiment perform use pfms neural network case experience choose appropriate number codebook vector initially lack type datum result follow average neural network pfms derive number codebook vector network correctly predict tag grammatical ungrammatical test set respectively pfms match performance codebook vector respectively exceed high number vector respectively vector network generate mean nll symbol grammatical ungrammatical test set respectively difference meaningless test set difference nll grammatical meaningless datum match difference nll codebook vector difference nll meaningless datum codebook vector difference nll grammatical meaningless datum difference nll grammatical ungrammatical grammatical meaningless datum set large increase number codebook vector difference performance grammatical ungrammatical test set highly significant case p model distinguish grammatical conclusion support fact mean nll meaningless test set high minimal pair set discussion pfms exceed performance network large number codebook vector possible network hide node term ease use certainly superior great advantage representation create section hypothesis testing rapid straightforward speed train possible statistically significant observation large number clustering run introduction grade wellformedness speak new hypothesis nature language use minimal pair design distinction grammatical ungrammatical utterance appear leave issue reality result use likelihood measure imply brown corpus consist representative different discourse type fiction government document traditional notion grammaticality lead treat ungrammatical sentence minimal pair test set equally ungrammatical nll experiment tell different story grammatical version consistently low associate high probability ungrammatical version difference small meaningless datum grammatical ungrammatical datum support concept grade grammaticality nll meaningless datum sort benchmark measure degree note pfms appear associate meaningless datum significantly high nll network difference nll grammatical ungrammatical datum great power discrimination grade wellformedness recurrent network use research need validity vary grammatical ungrammatical test set sentence sentence word word discourse style discourse style increase dramatically manipulate portion ungrammatical sentence encounter word grammatical sentence exhibit similar effect subsequence utterance occur rarely training set high associated nll way ungrammatical likely happen large corpus grammatical structure rare consistent recent finding human sentence processing wellformedness link expectation measure score interesting remarkable variation discourse style mean nll discourse style test set low grammatical ungrammatical version guarantee grammatical version test set low nll ungrammatical version grammatical ungrammatical nll interleave observe figure nll discourse style lie middle range interestingly nll grammatical version discourse style order accord lie range clear predictor discourse style style class formal eg grade grammaticality prediction fractal grammatical version discourse type text grammatical text fiction grammatical fiction science fiction grammatical science fiction ungrammatical t q t o codebook vector figure nll minimal pair test set contain different discourse style suggest grade wellformedness base learned government document test set low nll press test set cluster fiction test set language use clustering high end similarly learn government test set low nll intuition usage lie close prototypical training set test set fiction expect contribute prototype suggest usage vary significantly fiction test set conclusion work use language modeling early stage result date lot offer large project plan examine hypothesis grade grammaticality wellformedness apply syntax language semantic integral use large corpus identification vertex feature atomic symbol identify possibility combine example means normal method create trajectory subject current study acknowledgment work support fund project adaptive information system modeling economic management science artificial support transport p reference p construct source fractal representation symbolic sequence linguistic prototype linguistic theory r r statistical cue language acquisition word segmentation proc cognitive science society conference m press emergence grammaticality connectionist network approach language proc th symposium cognition s aspect resolution artificial neural network implication grade state machine representation temporal contingency simple recurrent network advance neural information processing system m appear dynamical model sentence processing cognitive science connectionist perspective development l finding structure time cognitive science gile press natural language grammatical inference recurrent neural network ieee tran knowledge datum engineering j hertz r g introduction theory computation m cohen catastrophic interference connectionist network sequential learning problem psychology learn motivation vol academic k connectionist model human category learn r p lippman neural information process neural network natural intelligence mit bengio p simard p learn dependency gradient descent difficult ieee tran neural network m p dynamic discretetime computation application recurrent neural network machine extraction neural computation ron singer tishby power machine learn p c gile finite state machine recurrent neural network automata dynamical system approach e ed neural network pattern recognition academic m fractal academic m expect unexpected response violation language cognitive process
basis function network complexity function learn computer dept math abstract paper apply method complexity regularization derive estimation bound nonlinear function estimation use single hide layer radial basis function network approach differ previous complexity regularization neural network function learn scheme operate random covering number metric entropy consider broad family activation function function bounded variation constraint previously impose network parameter eliminate way network train mean complexity regularization involve empirical risk minimization bound expect risk sample size obtain large class loss function rate convergence optimal loss derive introduction artificial neural network find effective learn noisy example learning problem unknown target function infer set independent observation draw accord unknown probability distribution inputoutput space use datum set learner try determine function fit datum sense minimize empirical loss function target function class function realizable learner case class realizable function consist class artificial network problem extensively study different viewpoint recent year special class artificial neural network radial basis function rbf network receive considerable attention rbf network solution regularization problem function estimation certain standard smoothness functional use reference convergence net function estimation classification prove convergence rate approximation scheme comparable sigmoidal net recent paper study tradeoff approximation estimation error provide extensive review problem paper consider hide layer rbf network look problem choose size hidden layer function available training datum mean complexity regularization complexity regularization approach apply model selection result optimal choice sigmoidal network parameter approach differ barron use metric entropy instead supremum norm allow consider general class activation function function bounded variation restricted class activation function satisfy lipschitz condition example activation jump allow complexity regularization approach able choose network parameter freely discretization parameter require regression estimation square error loss considerably improve convergence rate result obtain section problem formulate result estimation error complexity regularize net present general loss function theorem sharpen version square loss approximation bound combine obtain estimation result section yield convergence rate function learn net problem formulation task predict value real random variable observation value accuracy predictor r measure expect risk lr lr lr nonnegative loss function assume exist predictor inf good predictor base datum copy goal expect risk small possible choose class candidate function paper set candidate function set network radial basis function activation unit family network k hide node weight parameter satisfy certain constraint particular radial basis function characterize kernel family network real number weight ci ck e r d ai nonnegative definite x d matrix denote transpose column vector complexity regularization principle learning problem introduce vapnik fully develop lugosi enable learn choose candidate class automatically pick radial basis function network estimate function minimize empirical error training datum complexity regularization penalize large candidate class bind small approximation error favor small balance estimation error let subset space real function set let p covering number p define minimal number close ball union cover word p integer exist n p satisfy n p fi s case family real function function p zn case use notation n emphasize dependence metric p let define family function e member k map lr r assume k finite sure uniform upper bind random covering number assume loss generality monotone decrease e finally assume uniformly surely bound constant le e complexity penalty kth class n training sample nonnegative number satisfy nonnegative constant ck satisfy e note nonincrease possible choose kn n result complexity penalty optimize upper bind estimation error proof define estimate let minimize empirical risk training sample penalize empirical risk define e estimate define minimize penalize empirical risk class follow theorem expect estimation error complexity regularization scheme theorem complexity regularization estimate satisfie kl assume loss generality log easy choice satisfie square error loss special case obtain upper bind estimate instead complexity penalty satisfy uniform upper bind random cover number assume let closure l lj denote distribution unique e square loss achieve ef follow bind difference assume set consider squared error loss suppose b e rd complexity regularization estimate complexity penalty satisfy min c proof result use idea barron uniform probability inequality recently obtain rbf network consider radial basis function rbf network hide layer network characterize kernel net node wk real number weight ci ai nonnegative definite x d matrix kth candidate class fa function estimation task define class network node satisfy weight condition b fix b t radial basis function network let let denote probability measure induce ofthe convex hull function ct constant function h x e c e nonnegative x d matrix closure let e arbitrary assume uniformly bound corollary p r denote norm approximation error deal use result optimal happen case obtain inf value p close great importance robust neural network bounded total variation t constant ai depend total variation dimension d constant b definition p following consequence theorem prove lp regression estimation theorem let bounded variation assume bound p error complexity regularize estimate satisfie j p regression estimation rate know optimal factor squared error loss obtain inf easy check class convex collection net define result square root assume bounded variation suppose furthermore bound random variable let lx complexity rbf square regression estimate satisfie inf inf inf e result co result sharpen extend theorem weak convergence rate obtain formulation square loss network regression estimation rate vary linearly dimension result valid large class scheme include network consider improve convergence rate result advantage allow kernel continuous convergence rate result hold case exist minimize risk member closure word exist k member precise characterization difficult base work describe large class function contain let t real bounded function variable e suppose sign measure finite total variation define e lp probability measure reasonably expect approximate function l case m d t investigate detailed description function space arise different choice basis function extend approach approximation combination translate gaussian function general prove follow let gx t define t t class probability measure p approximate arbitrarily closely member radial basis function network prove lemma need slightly adapt proof elementary way follow line probabilistic proof apply network consider paper let d t c c obtain contain function integral representation ai b constraint weight acknowledgement work support center grant grant reference r barron complexity regularization application artificial neural network nonparametric functional estimation page series academic publisher r barron approximation estimation bound artificial machine c m l e sontag rate approximation result motivate robust neural network learn proc sixth annual workshop rate convergence radial basis function neural network editor speech page chapman m poggio regularization theory neural network architecture neural computation t nonparametric estimation classification use radial basis function net empirical risk minimization ieee transaction neural network p l bartlett r c williamson efficient agnostic learning network bounded fanin publish ieee transaction information theory concept learning use complexity regularization ieee transaction information theory p relationship generalization error hypothesis complexity sample complexity radial basis function neural computation vapnik estimation base empirical
discriminatively train sparse code gradient technology center compute seattle wa abstract find contour natural image fundamental problem serve basis task image segmentation object recognition core contour detection technology set handdesigned gradient feature use approach include stateoftheart global operator work contour detection accuracy significantly improve compute sparse code gradient scg measure contrast use patch representation automatically learn sparse code use ksvd dictionary learning orthogonal matching pursuit compute sparse code orient local neighborhood apply multiscale pooling power transform classify linear svms extract rich representation pixel avoid collapse sparse code gradient effectively learn measure local contrast find contour improve fmeasure metric bsds benchmark contour learn approach easily adapt novel sensor datum rgbd camera sparse code gradient depth map surface normal lead promise contour detection use depth verify depth dataset introduction contour detection fundamental problem vision accurately find object boundary interior contour far reach implication vision task include segmentation recognition scene understand image segmentation increasingly rely contour analysis widely use system global contour segmentation extensive use shape matching object recognition accurately find contour natural image challenging problem extensively study availability dataset groundtruth contour variety approach propose evaluate summary learn classify contour group feature hierarchical region analysis approach thing common build set gradient feature measure local contrast orient disc use chisquare distance histogram color effort use generic image feature learn handdesigned gradient widely use decade support algorithm benchmark work demonstrate contour detection improve replace handdesigned gradient rich representation automatically learn datum use sparse code particularly orthogonal matching pursuit ksvd learn representation patch instead direct classification patch sparse code pixel pool multiscale orientation spirit svm image gray svm rgbd contour normal local sparse code pool orient gradient power transform linear svm code figure combine sparse coding orient gradient contour analysis color depth image sparse code automatically learn rich representation patch datum pool orient gradient efficiently capture local contrast lead accurate contour detection use handdesigned feature include global gradient classify linear svm output smoothed suppress orientation commonly produce final contour fig sparse code gradient scg effective capture local contour contrast exist feature change local feature smoothing globalization fix improve fmeasure bsds benchmark substantial step accuracy precisionrecall curve fig large improvement accuracy observe dataset include msrc pascal approach build unsupervised feature learning directly apply novel sensor datum rgbd image depth camera use depth dataset verify scg approach combine strength color depth contour detection outperform adaptation rgbd large margin relate work contour detection long history computer vision fundamental building block modern approach contour detection evaluate dataset natural image groundtruth work combine set gradient feature use color outperform edge detector version pb develop find beneficial build gradient approach study globalization aspect ie local classification enforce consistency continuity contour develop model learn junction type use circular embed enforce work compute gradient eigenvector affinity graph combine local cue addition gradient et learn boost tree generic feature gradient wavelet use sift feature use raw pixel setting closely relate work discriminative sparse model et use ksvd represent multiscale patch moderate success bsds major difference work use orient gradient compare directly classify patch measure contrast orient easy problem effectively learn sparse coding represent signal reconstruct use small set basis function wide use vision example face recognition similar deep network approach recent work try avoid feature engineering employ sparse coding image patch learn feature scratch texture analysis object recognition particular orthogonal matching pursuit greedy algorithm incrementally find sparse code ksvd efficient popular dictionary learn closely related work different problem recognition use match pursuit ksvd learn feature code hierarchy extend approach rgbd datum thank mass production active rgbd camera quickly adopt vision research application pose estimation use random forest learn huge datum use rgbd camera scan large environment d model rgbd datum study context object recognition scene label study contour segmentation problem depth datum need fast grow interest rgbd perception contour detection use sparse code gradient start examine processing pipeline global highly influential widely use system contour detection detection stage local contrast estimation multiple scale globalization local cue use spectral group core approach lie use local cue orient gradient originally develop set feature use relatively simple pixel representation histogram color similarity function chisquare distance manually choose compare recent advance use rich representation highlevel recognition set pixel representation aggregation pixel information local neighborhood improve large extent learn adapt input datum representation section use orthogonal matching pursuit ksvd efficient sparse coding dictionary learn algorithm readily apply lowlevel vision extract sparse code pixel sparse code approach view similar spirit use avoid manual choice directly apply rgbd datum learn dictionary number channel exhibit different characteristic chromaticity depth surface normal section pixellevel sparse code integrate multiscale pooling rich representation orient local neighborhood compute orient gradient high dimensional representation use double power transform code feature linear classification linear svm efficiently effectively train orientation classify contour yield local contrast estimate accurate handdesigned feature local sparse representation rgbd patch ksvd orthogonal matching pursuit ksvd popular dictionary learning generalize kmean learn dictionary codeword unsupervised datum set image patch ksvd jointly find dictionary d dm associate sparse code matrix xn minimize reconstruction error kxi denote xi column k count nonzero entry sparse code predefined sparsity level number nonzero entry optimization solve alternate manner dictionary d optimize sparse code matrix decouple subproblem solve orthogonal match pursuit greedy algorithm find sparse code code dictionary associate sparse coefficient update sequentially singular value decomposition purpose represent local patch dictionary d small size use x patch require lot sample patch learn matter minute dictionary d learn use orthogonal match pursuit compute sparse code pixel efficiently convolution batch version typical bsds image resolution sparse code extraction efficient second sparse representation rgbd datum advantage unsupervised dictionary learning readily apply novel sensor datum color depth frame rgbd camera learn ksvd dictionary channel color depth grayscale luminance chromaticity color lab space depth distance camera learn dictionary visualize fig dictionary interesting grayscale b chromaticity ab c depth surface normal figure ksvd dictionary learn different channel grayscale chromaticity image depth surface normal depth image cd use fix dictionary size patch channel visualize use constant luminance dimensional surface normal visualize surface look qualitatively distinctive example surface normal codeword tend smooth flat surface depth codeword smooth chromaticity codeword respect opponent color pair channel code separately code multiscale neighborhood measure contrast multiscale pool orient decade research contour detection related topic number fundamental observation repeatedly contrast key differentiate contour orientation important respect continuity multiscale useful wish principle instead seek adopt principle case high dimensional representation sparse code pixel present sparse code extract small patch aggregate pixel information use orient use illustration fig orientation process separately orientation pixel p scale s define rectangle b size p rotate orientation use average pooling nonzero entry ie hybrid average max pool generate representation jth entry sparse indicator function nonzero rotate image sparse coding use integral image fast computation cost independent size orient n b scale s compute difference gradient vector d nsb elementwise absolute value operation divide nsb norm nsb positive number magnitude sparse code vary wide range local variation illumination occlusion step appearance feature robust variation increase discriminative power commonly contour detection object recognition value hard set find value instance stage train classifier scale convert scalar value contrast resemble chisquare distance function instead find avoid separately scale combine multiscale feature joint representation allow interaction codeword scale final representation contrast pixel p concatenation sparse code pool scale s use dna nb nsb nb nsb addition difference include union term nsb capture appearance disc union half disc normalize nsb double power transform linear classifier concatenate feature nonnegative provide multiscale contrast information classify p contour location particular orientation high dimensional experiment need pixel orientation prefer use linear svms efficient testing train directly learn linear function work instead apply double power transformation feature suitable linear svms dp dp empirically find double power transform work transform single power transform classification provide intuition power transform help classification renormalize distribution feature gaussian form plausible intuition double power transform optimal exponent different feature dimension power transform allow classifier pick linear combination different dimension stage supervised training local contrast global contour change local contrast estimation step fix step include filter smooth response find peak location orientation optionally apply globalization step compute spectral gradient local gradient linearly combine spectral gradient local sigmoid transform step need convert svm output dp compute spectral gradient experiment use evaluation framework extensively compare publicly available global system widely use state art contour detection result report run contour detection evaluation code default parameter accuracy verify publish result include number criterion include precisionrecall pr curve contour match fig fmeasure compute pr table fix contour threshold od threshold average precision benchmark dataset main dataset use bsds benchmark extension original bsds benchmark commonly use contour evaluation include natural image roughly resolution include train validation testing conduct color grayscale experiment convert bsds image grayscale retain groundtruth addition use msrc pascal segmentation dataset work msrc image resolution randomly choose half training half test pascal dataset include image training validation set roughly resolution randomly choose half training half test rgbd contour detection use depth dataset include pair color depth frame resolution groundtruth semantic region choose image training testing scene labeling setup image low quality bsds resize frame experiment train sparse code gradient sparse code ksvd orthogonal matching pursuit train sparse code gradient classifier linear svm orientation sample location positive datum sample contour location estimate orientation location use groundtruth negative datum location orientation random subtract mean patch datum channel bsds typically work focus contour detection address derive segmentation contour horizontal edge edge vertical edge edge pool disc size pixel average precision average precision average precision single scale scale dictionary size gray color sparsity level figure analysis sparse code gradient use average precision classification sample boundary effect accumulate small b accuracy increase dictionary size orientation channel effect sparsity level exhibit different behavior grayscale chromaticity global precision local gray table fmeasure evaluation compare color image local contour detection global detection ie combine spectral gradient analysis recall figure precisionrecall curve scg grayscale color image substantial step current state art reach accuracy green dot datum point use spatial scale size dictionary size scale feature length datum channel rgbd datum dimension bsds train use training image modify dense matrix feature dense pool look empirically analyze number setting sparse code gradient particular want understand choice local sparse coding affect contour classification effect pool dictionary size sparsity level number report intermediate result mean average precision orient gradient classifier degree sample location grayscale note validation reference average precision task pool single good scale filter consistent setting accumulate scale use scale small current level accuracy continue increase saturate suggest use large scale dictionary size minor impact small observable benefit use dictionary large particularly diagonal orientation sparsity level intriguing issue fig c grayscale normalize near neighbor hand color need large k possibly nonlinear space combine grayscale color want vary orientation horizontal vertical edge require small k diagonal edge use final fmeasure bsds empirically evaluate double power transform single power transform transform transform average precision single power transform good choice exponent average precision double power transform exponent od scg rgbd table fmeasure evaluation compare scg approach additional image dataset contour pascal rgbd od table fmeasure evaluation rgbd contour detection use compare use color image depth figure example bsds dataset image middle output output work scg operator learn preserve fine detail face time achieve high precision contour eg contour double width sake compute improve average precision translate large improvement contour detection accuracy image result table fig precisionrecall sparse code gradient bsds benchmark conduct set experiment use color grayscale image globalization component use exactly setup use sparse code gradient lead significant improvement accuracy case local version scg operator ie use local contrast globalization version local scg spectral gradient compute local scg reach fmeasure large step forward precisionrecall curve fig bsds fmeasure observe pick detail high recall rate maintain high precision entire range example fig scale range similar scheme allow flexibility learn balance scale separately code word help detect detail supplemental material contain comparison example table result additional dataset msrc pascal observe large improvement accuracy somewhat different nature scene dataset improvement msrc large partly image small contour small scale computational cost use integral image local scg second compute core cpu bsds image slow comparable highly optimize c implementation second figure example rgbd contour detection panel input image input depth contour contour color good pick detail wall depth useful color uniform corner room row illumination poor rgbd contour detection use second version depth dataset high quality groundtruth version median filtering apply remove double contour boundary adjacent region pixel rgbd baseline use simple adaptation depth value meter use directly grayscale image gradient computation use linear combination soft color depth gradient weight set validation table list precisionrecall evaluation scg rgbd contour detection scg setting scale dictionary size bsds outperform case particular contour design approach learn lowlevel representation depth datum fully automatically require manual achieve large boost combine color depth demonstrate color depth channel contain complementary information critical rgbd contour detection qualitatively easy rgbd combine strength color depth promising direction contour segmentation task scene analysis general fig example rgbd contour scg operator case color depth fail extract contour meaningful scene succeed discussion work successfully learn code local representation extract contour natural image approach combine prove concept orient gradient powerful representation automatically learn sparse code sparse code gradient scg perform significantly handdesigned feature use decade push contour detection close accuracy illustrate bsds benchmark compare handdesigned feature pb maintain high dimensional representation pool orient neighborhood collapse compute chisquare distance scale pass rich set information learn contour classification double power transform effectively code feature svms compare previous learning approach discriminative dictionary use multiscale pooling orient gradient lead high classification accuracy work open future possibility learn contour detection segmentation illustrate lot information locally wait extract learning approach sparse coding provide principle way rich representation automatically construct adapt particularly important novel sensor datum rgbd understanding increasingly need reference m m ksvd algorithm design overcomplete sparse representation ieee transaction signal process p detection hierarchical image segmentation l ren hierarchical matching pursuit image classification architecture fast advance neural information processing system l ren feature learn rgbd base object recognition international symposium experimental robotic p z supervise learning edge object boundary cvpr volume page m zisserman pascal visual object class challenge voc r library large linear classification machine learn research t l object detection contour segment network eccv page recognition use region cvpr page p m e ren fox rgbd mapping use depth camera dense d modeling environment international symposium experimental robotic teh fast learning algorithm deep belief net neural computation highly accurate boundary detection group cvpr page ren fox largescale hierarchical rgbd object page h r r ng convolutional deep belief network scalable unsupervised learning hierarchical representation icml page zisserman discriminative learn dictionary local image analysis cvpr page j m m discriminative sparse image model edge detection image interpretation eccv page learn detect natural image boundary use texture advance neural information processing system p orthogonal match pursuit recursive function approximation application wavelet decomposition conference signal system computer page improve image classification eccv page m zisserman m edge detection segmentation computer vision graphic image processing page multiscale improve boundary detection natural image eccv page l rgbd scene labeling feature algorithm computer vision pattern cvpr ieee conference page ieee ren c cue integration labeling advance information processing system r m m efficient implementation ksvd use batch orthogonal match pursuit technical report m sharp m r blake realtime human pose recognition single depth image cvpr volume page joint appearance shape context model multiclass object recognition segmentation eccv fergus scene segmentation use structured light sensor workshop representation recognition face recognition spatial pyramid matching use sparse code image classification cvpr page lafferty learn image representation pixel level hierarchical sparse code cvpr page q cycle contour grouping iccv
hierarchical penalization abstract hierarchical penalization generic framework incorporate prior information fitting statistical model variable organize hierarchical structure convex functional perform soft selection group level shrink variable group favor solution lead term final combination framework originally derive prior knowledge account useful linear regression parameter use model influence feature regression learn multiple keyword optimization constrain optimization supervise learn regression method sparsity feature selection introduction regression want explain predict response variable set explanatory variable r r purpose use model function able characterize observe residual error supervise learning consist estimate available training dataset xi achieve predictive perspective predict accurate response future observation correlation exist set explanatory variable response variable interpretation model t linear case function consist estimate apply predictive perspective produce estimate observation perspective interpret degree relevance variable ordinary square ols minimize sum residual square error explanatory variable numerous correlate variability estimate tend increase lead reduce prediction accuracy interpretation model coefficient shrinkage major approach regularization procedure linear regression model overcome drawback describe add constraint norm estimate accord choose norm coefficient associate variable little predictive information shrink remove variable irrelevant late case refer variable selection particular ridge regression shrink coefficient regard norm lasso absolute shrinkage selection operator lar angle regression shrink remove coefficient use norm x l q q sx q r l sx x l l q q x r x q figure leave original structure variable right equivalent tree structure consider formalization scaling problem application explanatory variable share similar characteristic gather group factor organize hierarchically instance explanatory variable product gene factor identify prior information available hierarchy gene necessary find method retain meaningful factor instead individual variable grouplasso consider hierarchical penalization method tree height define hierarchy perform variable selection encourage sparseness predefine factor technique sense hierarchy extend level sparseness integrate group paper propose derive adaptive penalization formulation highlight factor interest balance constraint element level hierarchy perform soft selection factor level shrink variable group favor solution lead term section introduce framework hierarchical penalization associate present section section framework apply linear regression conclude general survey future work hierarchical penalization formalization introduce hierarchical penalization consider problem variable organize tree structure height example display figure node label set child leave denote cardinality display figure branch stem root node label branch reach leaf label consider problem minimize differentiable loss function l subject constraint subset define tree hierarchy read l subject d d lagrangian parameter control shrinkage define continuation second term expression penalize accord tree structure scaling factor constraint shrink coefficient group level inside group follow problem joint shrinkage encourage sparsity group level important property prove optimization problem tractable equivalence optimization problem exhibit exact nature constraint apply coefficient proposition provide l problem proof problem minimize convex criterion convex set convex l positive criterion provide z convex compute xz x yz hessian positive semidefinite criterion constraint define set equality constraint b define linear subspace dimension set intersection set convex set constraint define convex admissible set problem convex proposition problem equivalent j jjk sketch lagrangian problem l k jjk d optimality condition l algebra optimality condition express p p plug condition criterion yield claim result jjk sparseness proposition penalization influence group variable variable group note thank squared term expression far simplify jjk l onetoone mapping expression interpret lagrangian formulation constrained optimization problem set define display shape admissible set figure compare ridge regression favor sparsity lasso encourage sparsity variable account group structure grouplasso invariant rotation variable hierarchical penalization combine feature lasso grouplasso regression lasso grouplasso hierarchical penalization figure admissible set penalty horizontal axis plane group vertical axis second group look curvature set meet axis good intuition regression suppress variable grouplasso group variable variable hierarchical penalization intuition correct hierarchical penalization boundary admissible set differentiable curvature high solution lead term variable encourage hint provide figure detail optimality condition minimize firstorder optimality condition p l dk p equation respectively variable belong group estimate irrelevant penalize high strength limit number group influence solution group nonzero relevance variable enter set active variable provide influence fitting criterion penalization strength increase rapidly smooth step function small value limit number large magnitude overall hierarchical penalization expect provide solution active group lead variable group solve problem use active set algorithm base approach propose lasso algorithm iterate phase optimization problem solve suboptimal set active variable nonzero variable define j current active set variable vector coefficient associate gk subset coefficient associate iteration solve problem min l l alternate step b describe second set active variable incrementally update detailed step compute candidate update admissible vector goal solve min l h current estimate solution difficulty solve stem derivative absolute value difficulty circumvent replace enable use powerful continuous optimizer base conjugate gradient method accord size problem obtain new admissible vector let step m let min let set index m large step direction m variable m b set h sign sign compute new direction step new solution sign sign remove step b iterate step b c test optimality appropriate optimality condition hold inactive variable p d jjk p jjk l solution step d select variable enter active set l select variable maximize group variable update active set initial vector sign new component sign d step algorithm initialize variable select process describe step experiment illustrate dataset hierarchical penalization useful exploratory analysis prediction algorithm apply multiple learning database problem consist predict age physical measurement dataset compose attribute concern sex encode sex sex variable female variable define group second group compose attribute concern size parameter length diameter height group compose weight parameter weight randomly select example training include tuning fold cross validation leave test mean square test error par coefficient estimate training set report table weight parameter main estimation age sex essential size weight table coefficient obtain dataset column represent value dk p jjk database problem consist predict median price house different survey region survey region represent information measurement prototype available prototype compose variable derive prototype include variable relate variable final dataset compose variable split group randomly select observation training leave testing divide training observation distinct dataset dataset parameter select fold cross validation mean squared error compute testing set report table mean squared test error obtain hierarchical penalization grouplasso lasso estimate mean table mean square test error obtain different method dataset hierarchical penalization perform lasso dataset perform grouplasso dataset obtain equal result dataset low overall mean error achieve grouplasso multiple multiple draw interest classification support svms start work problem consist learn convex description dataset available combination kernel optimization hierarchical penalization suited purpose kernel predictor illustrate effect smooth regression smoothing study nonparametric statistic consider model response variable estimate sum kernel scale factor bandwidth h residual error purpose combine bandwidth general criterion read nk kj penalize model apply dataset problem enable display contribution bandwidth solution use bandwidth range figure display result obtain different penalization parameter estimate function obtain combination select bandwidth contribution bandwidth model display setting penalization parameter correspond slight overfitting good fit slight coefficient bandwidth h null display expect penalization parameter increase smooth number contribute bandwidth decrease observe effective contribution bandwidth limit kernel term expansion conclusion work hierarchical penalization generic framework enable process hierarchically structure variable usual statistical model structure provide model constraint subgroup variable define level hierarchy fit model bias statistical explanation simple respect structure solution promote small number group variable lead component paper detail general framework hierarchical penalization tree structure height discuss specific property term convexity propose efficient active set algorithm incrementally build optimal solution problem finally illustrate approach use group feature discrete variable exist encode binary variable result group variable finally algorithm use learn multiple kernel regression perform quantitative empirical evaluation application regression classification clustering comparison regularization scheme grouplasso plan extend formalization hierarchy arbitrary height property currently study able tackle new application genomic available gene hierarchical structure faithfully approximate tree reference r tibshirani regression shrinkage selection royal statistical society series b t hastie tibshirani angle regression annal statistic m yuan model selection estimation regression group royal statistical society series combine h figure hierarchical penalization apply kernel smoothing datum combine point represent datum solid line function estimate response isolate bandwidth point represent partial residual solid line represent contribution bandwidth model adaptive scale feature selection svms advance neural information processing system volume mit press m b lasso dual journal computational graphical statistic blake uci repository machine learn database url datum evaluate learn valid experiment m statistical framework genomic data fusion bioinformatic apply nonparametric regression volume economic society
optimal classification strategy run computer computer abstract consider classification scc game learner adversary game target distribution completely know learner learner goal construct classifier capable guarantee tolerance error minimize false negative error identify hard soft optimal classification strategy different type game demonstrate soft classification provide significant advantage optimal strategy bound provide low bound standard scc motivate new approach solve scc introduction classification scc learner observe training set example sample target class goal create classifier distinguish target class class unknown learner training problem essence great application fault novelty detection scc receive research attention machine learning pattern recognition community example survey paper cite altogether paper extensive body work scc encompass mainly empirical study heuristic approach suffer lack theoretical contribution principle empirical comparative study propose solution extent exist literature basic question remain let p underlie distribution target class define space p target distribution let tolerance parameter learner observe training set sample p construct classifier capable distinguish target class view scc problem game learner adversary adversary select distribution q new element draw p q switch parameter unknown learner goal learner minimize false negative error guarantee false positive error main consideration previous scc study statistical guarantee prescribed false positive rate finite sample p question lead solution idea lowdensity rejection propose approach typically generative discriminative generative solution range density estimation partial density estimation quantile estimation level set estimation local density estimation discriminative method attempt generate decision boundary appropriately enclose high density region training set paper abstract away statistical estimation component problem model set learner large sample target class fact assume learner know target distribution p precisely assumption render entire body scc literature turn significant decisiontheoretic component scc problem remain far case result obtain immediately apply scc instance low bound fundamental question arise setting optimal strategy learner particular popular lowdensity rejection strategy optimal scc paper adopt strategy literature find formal justification partially good news lowdensity rejection worstcase optimal learner confine hard decision strategy general worstcase optimal learner strategy soft learner play randomized strategy result significant gain identify monotonicity property optimal scc strategy use establish optimality lowdensity rejection hard case equivalence lowdensity rejection constrain twoclass classification problem class uniform distribution equivalence motivate new approach solve scc problem turn attention power adversary issue literature crucial impact scc solution application example consider detection application necessary assume attack distribution worstcase characteristic important quantify precisely adversary know simple observation setting unlimited adversary know parameter game include learner strategy completely learner use hard strategy use soft strategy learner achieve average biased coin false negative rate analyze case limited adversary select sufficiently distant q satisfy know parameter main contribution complete analysis game include identification optimal strategy learner adversary achievable false negative rate optimal learner strategy good achievable rate obtain solution linear program specify term problem parameter result immediately applicable low bound standard scc problem use inspire new type algorithm standard scc close form expression achievable rate provide numerical example demonstrate compare optimal hard soft performance problem formulation classification scc problem define game learner adversary learner receive training sample example target distribution p define space basis training sample learner select rejection function r r r probability learner reject basis knowledge p andor r adversary select select attack distribution q define new example draw p q switching probability unknown learner rejection rate learner use rejection function r respect distribution d r d ed r notational convenience r eg r r corresponding accordingly eg main quantity interest false positive rate type error p false negative rate type error start game learner receive tolerance parameter maximally allow false positive rate rejection function r valid false positive rate p valid rejection function strategy optimal guarantee small false negative rate valid strategy consider model learner know target distribution p exactly focus decisiontheoretic component scc clearly model approximate setting learner large training set result obtain immediately apply case low bound scc instance scc game sum game learner set r p r valid rejection function learner strategy space let strategy space adversary consist allowable distribution q select adversary concern optimal learner strategy game variant distinguish knowledge learner strategy p andor limitation q distinguish special type game hard setting learner deterministically reject accept new event r rejection function term hard general game define soft function soft set far know hard setting consider scc literature far soft setting rejection function learner reduce type error reject ie increase r optimal r p p follow switching parameter selection optimal strategy specifically error optimal strategy p q q minimize minimize type error q assume paper finite support size p p pn q probability mass function additionally probability distribution refer distribution fix support set note assumption leave infinite game learner pure strategy space r p infinite characterize monotone rejection function section characterize structure optimal learner strategy intuitively plausible learner assign high rejection value high probability event p expect reasonable rejection function r monotonically decrease probability value ie pk rk monotonicity key justification large body scc work base low density rejection strategy surprisingly optimal monotone strategy guarantee follow example example optimality hard setting p valid hard rejection function r r let clearly q q optimal break monotonicity generally example hold q q q soft setting let p note r p r q q clearly maximize minimize optimal rejection function clearly break monotonicity example hold q q c fix p adversary strategy space let r p set optimal valid rejection function r r p minqq p minqq q note r case consider simple observation r r exist r r ri probability r follow property ensure r include monotone optimal hard strategy mean search space learner confine monotone strategy set distribution satisfy property later consider limited strategic space property hold game describe extensive form game tree learner select rejection function follow chance determine source p q test example probability case q select adversary choose randomly use test example game choice q depend knowledge p r certain strategy space q necessary consider infimum minimum case necessary replace q definition theorem closure q property define paper weak purpose proof complicate way currently define sufficient reasonable q definition property let p distribution set distribution q property wrt p q pk qj exist q qi monotone hard decision learner restrict satisfie property wrt p r rj rk proof let assume rejection function exist r let r r let p exist pk rk r monotone define r r value r r k r ri note p p p let q q q exist property particular k qj result q q k q exist q q q q q q consequently q r long pair need rejection level fix label r repeat procedure change r r event minimal probability procedure repeat time final r r satisfie rj rk theorem provide formal justification lowdensity rejection strategy ldr popular scc literature specifically assume p p pn correspond valid p low density rejection function place discussion soft decision facilitate property b theorem follow definition property b let p distribution set distribution q property b wrt q p q pk exist q qi technical proof follow theorem omit lack space appear supplementary monotone soft decision q satisfy property b wrt p r rj rk rj rk lowdensity rejection twoclass classification section focus hard setting lowdensity rejection strategy ldr define section optimal optimal hard performance obtain solve constrain twoclass classification problem class uniform distribution result consider family satisfy follow property definition property c let p distribution set distribution q property c wrt p q pk exist q qj qj state proof follow lemma proof find let r valid lowdensity rejection function ldr let r monotone valid rejection function minqq minqq q q satisfy property example violation property c illustrate violate property c result violation let valid ldr rejection function r r let q q q q q q q q ldr r exist monotone r minqq minqq q consider weak notion monotonicity hard strategy valid optimal q satisfie property exist monotone optimal rejection function follow corollary establish optimality corollary valid ldr optimal q satisfie property property c strategy worstcase optimal learner confine hard rejection function space satisfie property property c optimal ldr solution equivalent optimal solution follow constrain bayesian twoclass decision problem let class c distribution p x second class c uniform distribution let c class prior loss function cost decide ci instead goal construct classifier c c minimize total bayesian risk constraint term problem binary theorem optimal binary classifier bayesian binary problem induce optimal hard solution scc problem ldr q satisfie proof sketch let c optimal classifier bayesian binary problem induce hard rejection function r rx cx c set feasible classifier satisfy constraint p clearly induce r p let c cx note constraint equivalent p baye risk classify ri total baye risk p p c r c r x minimize c difficult r monotonically decrease increase respectively follow m c m p p c c constraint maintain decrease difficult r r x follow p p transfer c reduce property immediately imply induce valid ldr theorem motivate different approach scc sample uniform distribution attempt approximate optimal baye solution constrained binary problem justify certain heuristic find literature adversary game strategy bound unrestricted adversary game analyze adversary completely unrestricted mean q set distribution game leave little opportunity learner rejection function r define rmin mini ri imin ri rmin pn distribution d d di ri di rmin rmin particular q rmin choose imin r adversary achieve q rmin rejection rate achieve q imin r soft set q maximize rejection function r pi r pi equivalent flip biased coin event p achievable type error hard setting clearly rmin achievable type error precisely absolutely achieve simple analysis scc game adversary powerful order consider scc problem consider reasonable restriction adversary lead useful game type restriction limit knowledge r p andor type directly limit strategic choice available adversary section focus type constrain adversary seek constraint q helpful recall essence scc problem try distinguish probability distribution unknown natural constraint lower bind distance distribution follow similar result hypothesis testing like consider game adversary select q constant kldivergence unfortunately constraint pi case adversary optimally play strategy unrestricted game meet constraint fortunately effectively constrain adversary note usual learner reject probability null event p adversary choose distribution q probability event henceforth assume p p p qi define q q note possess property b wrt p theorem exist monotone r r hard soft setting corollary valid ldr q concentrate single event meet adversary play strategy unrestricted game learner select r game nontrivial require log maxi similarly optimal r exist imin r rmin distribution q completely achieve rmin unrestricted game r r maximize rmin assume optimal r begin analysis game identify useful characteristic optimal adversary strategy theorem effective support optimal q size base property provide linear program compute optimal rejection function follow lemma state technical proof minimize q meet constraint pk iii pk qj log log log pk pk qj pk theorem optimal adversarial strategy q effective support size proof sketch assume optimal q effective support size event j event effective support qi q global minimizer q subject constraint qi lagrangian problem lq hard use v point ri solve partial derivative q q q lq p log p assume q p p r q p q p strictly concave lq strictly concave lagrangian function unique global maximum smooth function q q root q partial derivative infinite number root convex pair decrease transfer probability log log qj log domain q internal point exist distribution q meet equality criterion lagrangian unique global lq maximum lq q turn attention learner selection r note sufficient learner consider monotone rejection function function rj rk learner partition kp event subset correspond probability level set event level set probability ps subset define variable r r rk represent rejection rate assign level set ri group level set probability l m optimal q adversary select effective support size effective support size event q level set l violate belong single level set m m sm index m feasible solution q sm rm hand q effective support size hard event level set l level set h combination result violation iii single solution log log probability q assign event respectively distribution q choice optimal distribution q m k possibly different rejection rate rate linear combination variable ri introduce additional variable z represent rejection rate theorem optimal soft rejection function lowerbound type error z obtain solve following linear program r rk z z subject ri r rk m numerical example compare performance hard soft rejection strategy constrain game value different family target distribution p support family arbitrary probability mass function n event discretized gaussian bin generate random distribution p family p solve optimal hard soft strategy compute correspond worstcase optimal type error result figure result present wide variety problem parameter qualitatively evident soft hard strategy small clearly soft approach significantly low error hard approach sufficiently large let r solution linear program derivation linear program dependent assumption event imin r discussion precede assumption discuss optimal strategy r optimal hard prove case r solution linear program optimal necessary generate p ensure distant q exist distribution family arbitrarily random distribution generate sample point p uniformly point draw u normalize sum p second family b gaussian center discretize evenly space bin range discretized random select choose uniformly range min max min set minimum ensure probability limited precision max set cumulative probability possible max arbitrarily set min hard soft arbitrary bad case type error bad case type ii error soft hard figure type error n distribution generate value error bar depict standard error remark introduce approach scc problem approach analysis allow prove condition lowdensity rejection optimal monotone rejection function guarantee exist analysis introduce soft decision strategy allow significantly performance observe learner face unlimited adversary consider restrict adversary provide analysis interesting family constrain game work open new future research believe result useful inspire new algorithm scc problem example equivalence lowdensity rejection binary problem section obviously motivate new approach clearly randomized strategy carry finite sample case approach extend develop way interesting setting consider adversary partial knowledge problem parameter learner strategy adversary know p subspace additionally desirable extend analysis infinite continuous event space finally nice determine explicit expression low bind obtain linear program reference m learn distribution paradigm learn teacher page cm bishop novelty detection neural network validation proceeding vision image signal process identify local outlier page survey outlier detection methodology artificial intelligence review detection nip page l srivastava comparative study scheme network detection m s novelty detection review statistical approach signal processing m s novelty detection review neural network base approach signal process classification framework machine learn research m uniform object generation optimize oneclass machine learn research classification mapping convergence machine learn
aggregate constraint additive factorial hmms application energy disaggregation abstract blind source separation problem difficult inherently entire goal identify meaningful source introduce way incorporate domain knowledge problem constraint sac encourage total signal unknown source close specified value base observation total signal vary widely unknown source good idea total value expect incorporate sac additive factorial hide markov model afhmm formulate energy disaggregation problem mixture signal assume observe quadratic program approximate inference employ recover source signal realworld energy disaggregation datum set use dramatically improve original afhmm significantly improve recent stateoftheart approach introduction learn task require separate time series linear combination large number source signal general problem blind source separation arise application domain include audio processing biology modelling electricity usage problem difficult inherently source dimension original time series problem especially main goal interest people interpret result source signal example consider application energy disaggregation application goal help people understand appliance use energy time appliance use importance place electricity monitor appliance household expensive instead researcher propose perform total household electricity usage effective deal issue identifiability confidence electricity usage look water address problem need incorporate domain knowledge regard sort source hope find recently number general framework propose incorporate prior constraint generalpurpose probabilistic model include posterior regularization generalize expectation criterion learn approach leave open question type domain knowledge include paper consider precisely research issue identify class constraint prior knowledge general wide variety domain perform efficient computation paper observe application total signal vary widely different unknown source good idea total value expect introduce signal aggregate constraint sac encourage aggregate value sum source signal close specify value example energy disaggregation problem know advance use day unlikely use incorporate constraint additive factorial hide afhmm commonly use model raise difficult inference issue constraint function entire state sequence chain afhmm decompose accord markov structure model instead solve relaxed problem transform optimization problem quadratic program computationally efficient datum electricity disaggregation domain section use significantly improve performance result decrease normalize disaggregation error compare original afhmm significant improvement performance compare recent stateoftheart approach disaggregation problem summarize contribution paper introduce signal aggregate constraint blind source separation problem section convex quadratic program relaxed afhmm section evaluation section use sac realworld problem energy disaggregation relate work problem energy disaggregation nonintrusive load monitoring introduce subject research interest review energy disaggregation find approach propose improve basic afhmm constrain state hmms additive factorial approximate maximum posteriori afamap introduce constraint chain change state time point approach propose nonhomogeneous hmms combine constraint change chain time alternately semimarkov model represent duration distribution hide state approach constrain hide state apply disaggregation problem employ kind additional information improve afhmm approach applicable constrain constraint introduce hmms work probabilistic database consider aggregate constraint work consider model simple graphical structure independent discrete variable problem set suppose observe time series sensor reading example energy measure hour electricity meter denote yt yt r assume signal aggregate component signal example energy individual appliance use household suppose component component signal represent xi r observation signal represent summation component signal follow t assume mean variance t disaggregation problem recover unknown time observed datum essentially problem mixture signal observe discuss early unique solution model identifiability problem component signal model model paper assume component signal xi model hidden markov chain common work simplicity markov chain assume finite set state chain denote number state chain idea sac fairly general easily incorporate model hide source additive factorial hmm baseline model afhmm afhmm natural model generation aggregated signal component signal xi assume hide markov chain state time afhmm variant afamap model parameter denote unknown parameter ik initial probability chain p zi transition probability p parameter estimate use approximation method structured variational approximation paper focus infer sequence time hide state hidden markov chain assume know interested maximum posteriori map inference posterior distribution follow form p p zi t gaussian distribution alternative way represent posterior use binary vector sit sit sit t represent discrete variable posterior distribution follow form log p sy log t sit log p t sit t t yt sit p transition probability matrix exact inference tractable number chain state increase map value find use viterbi optimize jointly chain sit sequence hold chain constant stick local optima instead paper solve convex quadratic program relaxed version map problem section solution guarantee optimal identifiability problem effort provide tractable solution problem constrain state hide markov chain section introduce signal aggregate constraint help address problem additive factorial hmm signal aggregate constraint add signal aggregate constraint afhmm yield new model afhmmsac afhmmsac assume aggregate value component signal entire sequence expect certain value know advance word sac assume constraint value obtain expert knowledge experiment example energy disaggregation domain extensive research estimate average national consumption different appliance incorporate constraint afhmm use formulation result follow optimization problem map log p sy maximize subject ti t assume known tuning parameter similar role use ridge regression lasso instead solve optimization problem directly equivalently solve penalize objective function maximize log p sy s ti sit complexity parameter onetoone correspondence tuning parameter bayesian point view constraint term view logarithm prior distribution state s objective view log posterior distribution s viterbi algorithm applicable directly time t state sit depend state time step regularization term inherently follow section transform optimization problem convex quadratic program efficiently solve note constraint equation constraint chain time period t describe series constraint define constraint constraint chain b s ij denote time period constraint j tij reasonable particularly household energy datum represent fact appliance commonly use unlikely use straightforward extension complicate algorithm simplicity use single constraint chain rest paper convex quadratic programming afhmmsac section derive convex quadratic program relaxed problem problem convex constraint relax log p convex add additional set variable obtain convex problem similar define new variable matrix h hit order present problem define follow notation t column vector size t element denote t size t kronecker product t denote et vector element denote et log size t represent t t ti t size p t yt p denote sit t size t sit size denote column row vector matrix respectively objective function equation equivalently represent ls sit hit hit sit si sit t t sit si sit t c constant aim optimize problem maximize subject t sit l sit hit problem problem equation note matrix positive semidefinite problem integer quadratic program hard solve instead solve relaxed problem hit problem solve problem use package specify solve program note relaxed problem afhmm obtain set concern computational complexity afhmmsac polynomial time number time step time total number state hmms practice implementation afhmm afamap afhmmsac scale similarly section relation posterior regularization section objective function derive posterior regularization framework posterior regularization framework guide model approach desire behavior constrain space model posterior distribution define model posterior distribution afhmm desire o distribution p interested define constrain space s s y p ensure valid distribution require optimize t sit minimize sy e p subject s denote kldivergence accord optimal solution p unique s exactly distribution distribution p p equation result section afhmmsac evaluate apply disaggregation problem toy datum set energy datum compare afhmm afamap performance toy datum section afhmmsac apply toy datum set evaluate robustness method chain generate state value initial transition probability randomly generate suppose generate chain xi t aggregated datum generate equation t t follow gaussian distribution mean variance afhmmsac apply datum component signal note simply set experiment include energy datum hyperparameter tune use cross validation denote estimate signal disaggregation performance evaluate normalize disaggregation error p energy datum particularly interested recover total energy use appliance objective disaggregation estimate total energy consume appliance period time measure employ follow aggregate error t t sae order assess sac regularizer affect result value use afhmmsac algorithm figure nde result euclidean distance input vector true signal aggregate vector increase nde increase affect performance afhmmsac normalize disaggregation signal aggregate error error distance figure normalize disaggregation error signal aggregate error compute afhmmsac use input vector euclidean distance input true signal x energy disaggregation section afhmm afamap afhmmsac apply electrical energy disaggregation problem use household electricity survey s datum s recent study food monitor total household study monitor household entire year remain monitor month year period select representative different individual appliance overall electricity consumption monitor household carefully select representative overall population datum record minute depend household frequency datum present challenge disaggregation technique typically study rely high data rate eg datum datum measure main reading use compare model model parameter define afhmm afamap afhmmsac appliance estimate use day datum household simply assume state appliance assume state require computational cost estimate use kmean cluster appliance signal training datum energy datum main reading experiment generate aggregate datum add appliance signal main read measure household household study day usage use test datum household model parameter table normalize disaggregation error nde signal aggregate error sae compute time obtain afhmm afamap afhmmsac energy datum house main value day national total consumption average consumption appliance training day ttc true total consumption appliance day household test datum m afhmmsac afhmmsac ttc t second estimate use day datum training datum future work straightforward incorporate sac unsupervised disaggregation approach use prior information national survey estimate afhmm afamap afhmmsac apply aggregated signal recover component appliance afhmmsac kind total consumption vector use vector national total consumption average consumption appliance training day household datum set second comparison true total consumption ttc appliance day household obviously ttc optimal value regularizer afhmmsac oracle result indicate large possible benefit include kind sac table nde method apply day datum household afhmmsac outperform afhmm term nde afamap outperform afhmm term sae perform similar term nde afhmmsac use ttc perform good method difference constraint able obtain ttc reality look mean value table conclude afhmmsac use improve stateoftheart afamap term respectively verify compute pair mean nde obtain afhmmsac afamap different significance level demonstrate computational efficiency computing time table indicate afhmm afamap afhmmsac consume similar time inference energy datum main reading study house main appliance measure experiment apply model directly measure main signal scenario difficult previous section main power include demand appliance include training datum realistic summary house table training datum use estimate model parameter number appliance correspond number hmms model main measure test day model recover consumption appliance compute use training datum afhmmsac nde compute house method result figure house compute pair nde sae compute afamap mean error different significance level indicate house afhmmsac improve afamap overall result test day table afhmmsac improve afhmm afamap term compute time afhmmsac similar afhmm afamap note look table method require time datum main main algorithm time converge realistic datum result indicate value signal aggregate constraint problem table summary house main t rain day e day table normalize disaggregation error signal aggregate error sae compute time obtain afhmm afamap afhmmsac use main input value compute test day house national total consumption average consumption appliance training day ttc true total consumption appliance day household test datum m afhmmsac afhmmsac ttc normalize disaggregation error signal aggregate error t second error error house figure mean plot nde sae compute afhmm afamap afhmmsac use main input house conclusion paper propose additive factorial hmm signal aggregate constraint regularizer derive prior distribution chain state objective function derive framework posterior regularization focus find map configuration posterior distribution constraint dynamic programming directly applicable pose optimization problem program solve relaxed problem simulate datum afhmmsac robust error specification constraint value real world datum disaggregation problem afhmmsac perform simple afhmm previously publish research acknowledgment work support engineering physical grant number reference combinatorial dynamic gene expression bioinformatic bach m speech separation spectral learning approach neural information processing system page p editor handbook blind source separation independent component analysis application academic taskar posterior regularization structured latent variable model machine learn research hide markov model machine learn m grant graph implementation nonsmooth program editor recent advance learning lecture note control information science page springerverlag limit m grant programming version nonintrusive appliance load monitoring proceeding ieee hastie r tibshirani friedman editor element statistical learning willsky bayesian nonparametric hide semimarkov model machine learn research m m disaggregation low frequency power measurement proceeding conference datum mining page approximate inference additive factorial hmms application energy disaggregation proceeding international conference artificial intelligence statistic aistat volume page p learn measurement exponential family annual international conference machine learn page g mccallum generalize expectation criterion semisupervise learning random field proceeding association computational linguistic page unsupervised training method nonintrusive appliance load monitoring meter nonintrusive load monitoring use prior model general appliance type proceeding conference artificial intelligence page s t roweis microphone source separation advance neural information processing page memory markov chain decompose complex stochastic process mixture simple machine learn statistical inference hide markov model use constraint eprint query uncertain datum aggregate constraint proceeding acm conference management datum page m interleave factorial nonhomogeneous hidden markov model energy disaggregation neural information processing system workshop machine learn m nonintrusive appliance load monitoring review electronic p household electricity survey nonintrusive load monitoring approach energy sense survey sensor
m statistic industrial system computational college compute abstract detect emergence abrupt changepoint classic problem statistic machine learn kernelbase nonparametric statistic propose task assumption distribution traditional parametric approach exist kernel statistic provide computationally efficient way characterize behavior statistic characterization crucial set detection threshold control significance level offline case average run length online case paper propose related computationally efficient m statistic kernelbase changepoint detection background datum large novel theoretical result paper characterization tail probability statistic use new technique base changeofmeasure characterization provide accurate detection threshold offline online case computationally efficient manner need resort expensive simulation bootstrappe method perform synthetic real world introduction detect emergence abrupt changepoint classic problem statistic machine learn sequence sample domain interested detect possible changepoint sample p iid p socalled background distribution changepoint sample q iid q postchange distribution time horizon t fix number t offline problem t fix new sample sequential online problem goal detect existence changepoint offline setting detect emergence changepoint soon possible occur online setting restrict attention detect changepoint arise monitor problem example seismic event detection like detect onset event precisely understand quickly possible streaming datum ideally detection algorithm robust distributional assumption wish detect kind seismic event different background typically large background datum seismic event rare want algorithm exploit datum computationally efficient classical approach changepoint detection usually parametric meaning rely strong assumption distribution nonparametric kernel approach distribution free robust provide consistent result large class data distribution possibly powerful setting clear distributional assumption particular kernel base statistic propose machine learn literature typically work real datum assumption exist kernel statistic provide computationally efficient way characterize tail probability value statistic characterization tail probability crucial set correct detection threshold offline online case furthermore efficiency important consideration typically background datum large case freedom sample background datum statistical design gain computational efficiency hand changepoint detection problem relate statistical twosample test problem usually difficult changepoint detection need search unknown changepoint location instance offline case correspond maximum series statistic correspond changepoint location similar idea use offline case online case characterize average run length test statistic hit threshold necessarily result maximum statistic time statistic usually highly correlated analyze tail probability test statistic changepoint detection typically require sophisticated probabilistic tool paper design relate m statistic changepoint detection base discrepancy twosample test nice unbiased minimum variance u statistic estimator mmdu directly apply cost compute base sample n datum point changepoint detection case translate complexity quadratically grow number background observation detection time horizon adopt strategy inspire recently develop statistic design statistic changepoint detection high level method sample block background datum size compute mmdu reference block postchange block average result different simple twosample test case order provide accurate changepoint detection threshold background block need design novel structure way offline setting update recursively online set present new m statistic contribution include derive accurate approximation significance level offline case average run length online case statistic enable determine threshold efficiently recur simulation repeat bootstrappe obtain closedform variance estimator allow form m statistic easily develop novel structure way design background block offline setting rule update online setting lead desire correlation structure statistic enable accurate approximation tail probability approximate asymptotic tail probability adopt highly sophisticated technique base changeofmeasure recently develop series paper siegmund numerical accuracy approximation validate numerical example demonstrate good performance method use real speech human activity datum find twosample testing scenario beneficial increase block size b distribution statistic null alternative separate long case online changepoint detection large block size cause large detection delay finally point future direction relax correct skewness kernelbase statistic background related work briefly review kernelbase method maximum mean discrepancy reproduce kernel hilbert space function r inner product h element kx satisfie reproduce property consequently kx mean view evaluation function point inner product assume set observation domain draw distribution p draw distribution q maximum mean discrepancy define unbiased estimate obtain use statistic mmdu hxi kernel statistic define hxi intuitively empirical test statistic mmdu expect small close p q large p q far apart complexity evaluate form socalled gram matrix datum p u statistic degenerate distribute infinite sum chisquare variable improve computational efficiency obtain threshold testing recently propose alternative statistic bt key idea approach partition n sample p q nonoverlapping block constant size b compute pair pn block average n block result mmdu constant computational complexity significant reduction compare mmdu furthermore average mmdu independent block asymptotically normal leverage central theorem property allow simple threshold derive twosample test resort expensive approach later statistic inspire changepoint detection setting require significant new derivation obtain test threshold care maximum compute different point time changepoint detection case consist sum highly correlate statistic form common test block datum changepoint detection problem test datum reference datum use central limit theorem martingale version adopt aforementioned changeofmeasure approach relate work nonparametric changepoint detection approach propose literature offline set design kernelbase test statistic base socalled run maximum partition strategy test presence changepoint study relate problem anomalous sequence sequence detect construct test statistic use online set present compare datum reference window datum current window use empirical distance measure kernelbase detect abrupt change compare set descriptor extract online signal time instant immediate past set immediate future set base soft margin support vector machine svm build dissimilarity measure asymptotically equivalent ratio gaussian case feature space set estimate density intermediate step use estimation detect changepoint model use parameter update online stochastic gradient work lack theoretical analysis behavior statistic average run length m statistic offline online changepoint detection sequence observation denote sequence background reference datum assume large reference data available goal detect existence changepoint changepoint sample iid distribution p changepoint sample iid different distribution q location changepoint occur unknown formulate problem hypothesis test null hypothesis state changepoint alternative hypothesis exist changepoint time construct kernelbase m statistic use maximum mean discrepancy measure difference distribution reference test datum denote block datum potentially contain changepoint refer postchange block test block offline setting assume size bmax want search location changepoint observation different distribution inspire idea bt sample reference block size bmax independently reference pool index n search location b b bmax changepoint construct contiguous datum point denote form statistic construct reference block contiguous datum point b block index xi illustrate fig compute block contain potential change point pool reference b bmax bmax bmax bmax bmax bmax bmax mmdu bmax bmax b b b block contain potential change point pool reference data sample time b t pool reference data sample time offline t b sequential figure illustration offline case datum split block size bmax index t consider block size b bmax online case assume large reference background datum follow null distribution b mmdu average block b b b b b b b b denote jth sample denote sample property mmdu null hypothesis let denote variance null expression follow section depend block size b number block n b increase varzb decrease illustrate figure appendix consider statistic maximize value b define offline m statistic detect changepoint m statistic exceed threshold b m zb varzb b offline changepoint detection zb vary bmax correspond search unknown changepoint location online setting suppose postchange block size b construct use slide window case potential changepoint declare end block form statistic n b sample replacement assume reference datum distribution p reference pool form reference block compute quadratic mmdu statistic reference block postchange block average new sample time t new sample reference block remove old sample postchange block reference pool reference block update accordingly end point reference block reference pool new point sample reference block fig b use slide window scheme describe similarly define online m statistic form standardized average mmdu postchange block slide window reference block b t fix reference block size b time postchange block size b time t online case characterize average run length test statistic hit threshold necessarily result maximum statistic time online changepoint detection procedure stop time detect changepoint normalize exceed predetermine threshold b p varzb online changepoint detection z mt note online case actually maximum standardized statistic time recursive way calculate online m statistic efficiently explain section appendix stop time t claim exist changepoint tradeoff choose block size b online set small block size incur small computational cost important online case enable small detection delay strong change point magnitude disadvantage small b low power correspond long detection delay changepoint magnitude weak example amplitude mean shift small example offline online m statistic demonstrate base synthetic datum segment real seismic signal propose offline m statistic detect existence changepoint accurately change occur online m statistic quickly hit threshold soon change happen normal laplace seismic signal normal signal laplace time time b offline null b time peak statistic statistic b offline time statistic statistic normal signal signal time online time seismic signal figure example offline online m statistic n b offline case changepoint bmax maximum obtain c online case changepoint detection delay use real seismic signal m statistic different threshold theoretical value mark red theoretical performance analysis obtain analytical expression variance varzb leverage correspondence mmdu statistic u statistic form statistic exploit known property u statistic derive covariance structure online offline standardized zb statistic crucial prove theorem variance zb null fix block size b number block null hypothesis cov hx iid null distribution p suggest easy way estimate variance varzb reference datum estimate need estimate time draw sample replacement reference datum use evaluate sample function value form similarly estimate cov y covariance structure standardized zb statistic null hypothesis v bmax offline case u cov zu v online case s b s b s offline set choice threshold b involve tradeoff standard performance metric significant level probability m statistic exceed threshold b null hypothesis ie changepoint ii power probability statistic exceed threshold alternative hypothesis online set analogous performance metric commonly use analyze changepoint detection procedure expect value stop time change average expect detection delay define expect stop time extreme case change occur immediately focus analyze arl method play key role set threshold derive accurate approximation quantity function threshold b prescribe arl solve corresponding b analytically let p e denote respectively probability measure expectation null offline case bmax c constant c significant level offline m statistic define b b b max b e b b o special function u u u probability density function cumulative distribution function standard normal distribution respectively proof use changeofmeasure argument base likelihood ratio identity likelihood ratio identity relate computing tail probability null compute sum expectation alternative distribution index particular parameter value illustrate assume probability density function pdf null u function index set introduce family alternative distribution u log log moment generate function parameter assign arbitrary value easily verify pdf use family alternative calculate probability event original distribution calculate sum expectation e p e e e indicator function event true expectation use pdf u ratio freedom choose different value basic idea changeofmeasure setting treat zb varzb random field index b characterize need study tail probability maximum random field relate setting correspond correspond correspond threshold crossing event compute expectation alternative measure step choose parameter value b pdf associate parameter value b b b equivalent set mean alternative probability threshold b zb b allow use local central alternative measure boundary cross large probability second express random quantity involve expectation function socalled local field term b b b loglikelihood ratio asymptotically independent b b grow order b simplify calculation step analyze covariance structure random field following approximate use note term zu correlation construction share postchange block apply localization theorem obtain final result arl online case b c constant c average run length arl stop time t define b t p b b b b b b proof similar fact m p t m p max b tm need study tail probability maximum random field t fix block size similar changeofmeasure approach use covariance structure online case slightly different offline case tail probability turn form p t m m use similar t asymptotically exponentially distribute p t m m consequently e lead p theorem log arl hand typically order use identity careful analysis carry future work kullbackleibler divergence null alternative distribution order constant desire arl typically order error estimate threshold translate linearly mean typically reasonably accurate b cause little performance loss similarly theorem sl b similar argument offline case numerical example test performance m statistic use simulation real world datum highlight main result detail find appendix c follow example use bandwidth use median trick bandwidth estimate use accuracy estimate fig appendix empirical distribution zb case generate random instance compute datum follow n r represent null distribution plot sample mean match empirical distribution note approximation work block size decrease skewness statistic correct discussion section accuracy theoretical result estimate threshold offline case compare threshold obtain numerical simulation bootstrappe use approximation theorem sl value choose maximum block size bmax appendix fig demonstrate threshold obtain simulation threshold b correspond quantile empirical distribution offline m statistic range b value fig b compare empirical sl value simulation predict theory accurate small desirable usually care small s obtain threshold table approximation work determine threshold s threshold obtain theory match obtain simulation null distribution n r bootstrappe real datum scenario bootstrap threshold speech signal dataset case null distribution p unknown sample speech signal generate bootstrap sample estimate threshold fig appendix bs obtain theoretical approximation little performance degradation discuss improve section table comparison threshold offline case determine simulation bootstrappe theory respectively sl value bmax b bmax b bmax b online case compare threshold obtain simulation use instance arl respectively predicate theory threshold consistently accurate null distribution fig note fig precision improve b increase null distribution consider include exponential distribution mean erdosrenyi random graph node probability form random edge laplace distribution expect detection delay online setting compare assumption detect changepoint signal dimensional transition happen theory theory b b c figure online case range arl value obtain simulation null distribution nonzero mean postchange mean elementwise equal constant mean shift setting fig demonstrate tradeoff choose block size block size small statistical power m statistic weak large hand block size large statistical power good large way update test block optimal block size case fig b optimal block size decrease mean shift increase expect test performance m statistic use real datum dataset include realworld speech dataset speech resource provide human activity sense challenge datum compare m statistic stateoftheart algorithm relative limitation suitable highdimensional datum estimate density ratio highdimensional setting illpose achieve reasonable performance algorithm adjust bandwidth regularization parameter time step computationally expensive m statistic method use area curve large performance metric m statistic competitive performance compare baseline algorithm real data testing report main result detail find appendix d speech datum goal detect onset speech background noise background noise real acoustic signal background noise highway overall auc m statistic baseline algorithm human activity detection datum aim detection onset transition activity datum consist human activity information collect portable overall auc m statistic baseline algorithm discussion able improve precision tail probability approximation theorem account skewness argument need choose parameter value b b b b currently use gaussian assumption b improve precision able estimate skewness particular include skewness log moment generate function approximation zb estimate parameter set derivative b solve quadratic equation b change lead exponent term b b b b similar improvement acknowledgment research support nsf iis nsf career iis ls available available reference m davy online kernel change detection tran harchaoui highdimensional changepoint detection sparse alternative m scholkopf twosample test journal machine learning research harchaoui e mouline kernelbase method testing ieee magazine page z harchaoui f bach e mouline kernel changepoint analysis adv information processing system nip change data stream proc changepoint detection timeserie datum direct estimation neural network decrease power distance base nonparametric hypothesis test high dimension conference artificial intelligence z e automatic direct p seismic phase fault head wave int scholkopf alexander learning support regularization optimization mit r ustatistic approximation theorem mathematical statistic son siegmund sequential analysis test confidence interval springer d siegmund e use generalize likelihood ratio statistic sequential detection changepoint ann statist d siegmund detect emergence signal noisy image stat interface changepoint detection annal statistic extreme random field theory application m kernel twosample test neural info proc nip poor nonparametric detection anomalous datum mean embed arxiv
genetic climbing old dept psychology dept computer abstract analyze simple hillclimbe previously outperform genetic algorithm ga simple royal road function analyze idealize genetic significantly fast lower bind ga speed identify feature iga rise speedup discuss feature incorporate real ga introduction goal understand class problem genetic suited particular outperform search algorithm study empirically compare gas search optimization method simple hillclimbe simulate anneal linear nonlinear integer programming technique traditional optimization technique comparison typically compare version ga second algorithm single problem set problem use performance criterion appropriate comparison typically identify feature lead performance algorithm hard distill general principle isolated result paper look depth simple method idealize form ga order identify general principle ga outperform c figure royal road function previous work develop class fitness landscape function forr forr mitchell design simple class contain feature relevant performance purpose develop landscape carry systematic comparison search method simple royal road function r l figure rl consist list partially specify bit string schema denote card o schema coefficient ci order schema number define non bit bit string instance schema e match define position fitness bit string x define follow e example instance exactly order rl likewise building block hypothesis state ga work instance short schema building block confer high fitness form instance large schema confer high fitness hypothesis initially expect structure rl layout royal road ga follow optimal string expect simple hillclimbe scheme perform poorly large number bit position optimize simultaneously order instance schema instance higherorder intermediate schema expectation mitchell experiment simple ga use selection scale crossover point mutation optimize rl slowly hitchhike instance higherorder schema discover high fitness allow schema spread quickly population position string hitchhike schema define position slow discovery schema position especially close highly fit schema define position hitchhike general bottleneck ga observe similar effect genetic hill table mean median number function evaluation find optimum string run ga hillclimbe algorithm r standard error parenthesis variation original ga outperform simple hillclimbe prove wrong mitchell compare gas performance variation rl different method steepest ascent hillclimbe hillclimbe method mutation hillclimbe rmhc rmhc string choose random fitness evaluate string mutate randomly choose single new fitness evaluate mutation lead equal high fitness new string replace old string procedure iterate optimum find maximum number function evaluation perform repeat experiment r result similar r forr mitchell table compare mean median number function evaluation find optimum string mean median absolute run time ga application evolve architecture time perform function evaluation dominate time require execute algorithm reason consider algorithm exclude function evaluation negligible time result find optimum ri average function evaluation find optimum maximum function evaluation find optimum rl average function factor fast ga difference landscape originally design royal road ga need rigorous answer question pose early condition ga outperform search algorithm hill climbing analysis rmhc ga begin answer question analyze rmhc respect r suppose fitness function consist n adjacent block expect time number function evaluation find optimum string ask simple question expect time ek find single block analysis yield ek slightly large converge slowly personal communication example suppose want discover string block time discover block ek find time discover second block long function evaluation waste testing mutation block proportion mutation proportion mutation occur position block expect time ek find second block k similarly total expect time actual value bit large expect time block depend bad time block expression approximately ek r r constant value expression run rmhc rl function time average number function evaluation optimum agree reasonably expect value ga reason expect ga perform rl theoretically ga fast implicit parallelism string population instance different schema population large initially choose random large number different number string sample parallel result quick search short schema confer high fitness second reproduction ga conserve instance high crossover rate quickly combine instance schema different string create instance long confer high fitness previous experiment mitchell simple depart principle behavior major hitchhike limit implicit parallelism fix certain schema region sub optimally ga work exactly describe quickly find optimal string answer question consider idealize genetic algorithm explicitly feature describe iga know ahead time desire schema function evaluation determination string contain iga time step single string choose random uniform probability bit string evaluate determine instance desire time string find subsequent discovery instance schema new string cross string string contain desire schema discover far procedure practice require know priori schema relevant general algorithm ga rmhc genetic hill climbing directly measure fitness string know ahead time schema contribute high fitness idea ga implicitly iga able explicitly idea elaborate suppose desire consist block expect time number function evaluation save string contain desire solution suggest g personal communication personal communication detailed solution main idea note probability find single desire block random string p k probability find time probability block find time probability n block find exactly time ptn expect time ptn sum expand simplify work approximation small p obtain follow approximation lip n nl major point iga expect time order rmhc expect time order factor slow kind analysis help predict outperform hill climbing iga fast rmhc primary reason iga perfectly implement implicit parallelism new string completely independent previous new sample independently schema region contrast rmhc space string mutation original string new sample bit previous sample new string new sample schema region iga spend time rmhc construct new sample count function evaluation ignore construction time function evaluation know exactly desire way lower bind number function evaluation need problem independent sampling allow speedup iga way allow possibility desirable schema appear simultaneously sample mean waste sample rmhc comparison rmhc iga significantly fast rl similar landscape hillclimbe level h h s figure royal road function r method work mutate single bit small number bit obtain new sample hitchhike effect describe early result loss independent sample real ga goal real ga possible approximate iga course iga work explicitly know desire schema real ga information estimate desire schema implicit sampling procedure possible real ga approximate number feature iga independent sample population size large selection process slow mutation rate sufficient sure single fix single value large majority string population desire schema selection strong preserve desire schema discover slow equivalently relative fitness nonoverlapping desirable schema small prevent significant highly fit crowd desire schema string instantaneous crossover crossover rate time crossover occur combine desire schema small respect discovery time desire rmhc string length function large n speedup factor significant mechanism mutually compatible mutation work schema carefully balance discussion balance achieve result experiment step explore balance design r variant previous function forr mitchell base feature describe r desire schema fig combination r r order schema separate bit position contribute mitchell length r string instance desire schema receive fitness time new level string find instance schema small increment add fitness string level instance schema fitness u string level fitness experiment set u genetic climbing table r mean function evaluation run attain level ga rmhc ga run number function evaluation sample evaluation value actually upper bind interval length standard error parenthesis percentage run reach level head run run successfully reach level include function evaluation calculation level purpose help maintain independent sample schema position prevent schema position independence sample help use large population slow selection scheme function preliminary experiment r hitchhike ga reduce significantly population able maintain instance schema run study r illustrate figure r identical r far r define bit string double size problem preliminary run r use population size mutation rate mutation flip bit crossover number crossover point pair parent select poisson distribution table mean number evaluation reach level reach level maximum function evaluation time reach level comparable algorithm ga fast reach level far ga discover level approximately twice rmhc necessary balance maintenance independent sample desire schema preliminary result suggest r job maintain balance early royal road function work balance great detail topic future work conclusion present analysis algorithm rmhc iga use analysis identify general principle genetic outperform hill climbing present preliminary experimental result compare ga rmhc modify royal road landscape analysis result step achieve original design simple class fitness landscape distinguish ga search method characterize general feature fitness landscape suitable ga modify royal road landscape r rl mean realistic example problem apply ga mean idealize problem certain feature relevant gas explicit gas performance study detail claim order understand ga work general useful understand work useful simple carefully design landscape work report step direction acknowledgment thank r suggest rmhc share careful analysis g analysis iga thank e l r helpful comment discussion regard work gratefully acknowledge support adaptive computation program grant b national science foundation grant reference l d representational bias test design l b ed proceeding fourth international conference genetic analysis behavior class genetic adaptive system forr m relative fitness hypothesis foundation genetic adaptation natural artificial system cambridge innovation complex adaptive system mathematical sketch work l genetic algorithm fast simulated comparison mathematical computer model r insert improve genetic algorithm success rate cue biology r l b ed proceeding fourth international conference genetic algorithm m forr royal road genetic algorithm fitness landscape ga performance p ed proceeding european conference artificial life
datum clustering markovian information bottleneck method tishby school computer science engineering center neural university email abstract introduce new nonparametric principle distance base clustering method method combine pairwise base approach method provide meaningful interpretation result cluster idea base turn distance matrix markov process examine decay relaxation process cluster emerge quasistable structure relaxation extract use information bottleneck method cluster capture information initial point relaxation effective way method cluster datum geometric bias assumption underlie distribution introduction datum clustering fundamental pattern recognition problem numerous algorithm application problem goal find reasonable partition data point class cluster mean reasonable depend application representation datum assumption origin data point thing important class clustering method case data matrix pairwise distance similarity measure distance come empirical measurement complex process direct access precise definition distance function case distance form metric datum necessarily come sample meaningful distribution issue generalization sample sample fluctuation define algorithm use pairwise distance explicit use distance measure employ statistical mechanic analogy graph theoretical property point group base global criterion connected component small cut minimum alignment energy algorithm computationally inefficient case difficult interpret result work support science foundation human frontier science project support grant cluster e hard determine common property point cluster look reasonable second class clustering method represent generalize vector quantization fit model gaussian distribution point cluster average know distortion datum point corresponding representative minimize type algorithm rely theoretical framework rate distortion theory provide interpretation result cluster type algorithm computationally efficient require calculation distance distortion datum model pair datum point hand require knowledge distortion function specific assumption underlie structure model datum paper present new information theoretic combination pairwise clustering meaningful intuitive interpretation result cluster addition algorithm provide clear objective figure merit cluster assumption origin structure datum point pairwise distance markovian relaxation step algorithm turn pairwise distance matrix markov process follow simple intuition assign state markov chain data point transition probability function pairwise distance datum consider direct graph point node pairwise distance need symmetric form metric arc graph distance normally consider additive ie length trajectory graph sum probability hand multiplicative independent event want probability random trajectory graph naturally relate length transition probability point exponential distance denote pairwise distance point xi transition probability markov chain point time point time pij choose ex length scale factor equal mean pairwise distance near neighbor point detail rescaling important final result similar distance probabilistic interpretation perform clustering work proper normalization row require turn matrix stochastic transition matrix transition matrix imagine random walk start point graph specifically probability distribution position random walk start t time step jth row iteration transition matrix denote transition matrix pt pt tth power transition probability matrix probability random walk start time time henceforth number datum point point index run implicitly state assume pairwise distance finite obtain way ergodic markov process single stationary distribution denote distribution transition matrix t limit distribution ri dynamic markov process initial state distribution relax final stationary distribution information initial point random walk completely lose rate information loss example data rate loss cluster accuracy o figure left example datum consist point d middle plot rate information loss relaxation notice prior information circle ellipsis rate information loss slow random walk stabilize sub structure datum propose cluster right plot rate information loss cancer datum accuracy obtain cluster different relaxation time original class relaxation mutual information natural way quantify information loss relaxation process mutual information initial point variable point random walk time mutual information random variable symmetric functional joint distribution l r p markov relaxation mutual information prior probability state p probability xi time dkl divergence define log information theoretic measure p similarity distribution row relax r divergence t clear information initial point decay monotonically exponentially asymptotically rate decay convey information structure datum point consider simple example datum point figure rate information loss initial point random walk positive slow specific time relaxation relaxation location indicate formation quasistable structure graph relaxation transition probability matrix approximately projection matrix satisfy invariant subgraph correspond cluster approximate stationary transition correspond slow information loss identify derivative information loss time way phenomenon observe row pt conditional distribution row follow partial relaxation correspond point similar conditional distribution rest graph time t point belong structure graph directly observe matrix relaxation figure quasistable structure graph t t t t o figure relaxation process directly matrix different time example datum figure dark color correspond high probability density row point order ellipsis easy clear emergence block conditional distribution row matrix relaxation process large t complete relaxation row equal stationary distribution process good correlation result cluster original ellipsis high accuracy value obtain intermediate time underlie structure emerge relaxation process precisely desirable cluster remain question correct way group initial point cluster capture information position graph word replace initial point initial cluster enable prediction location graph time similar accuracy answer question naturally provide recently introduce information bottleneck method cluster preserve information problem selforganization member set base similarity conditional distribution member set introduce term distributional clustering question recently b e specific case fundamental problem feature variable relevant prediction relevance variable general problem natural information theoretic formulation find compressed representation variable denote mutual information high possible constraint mutual information surprisingly variational principle yield exact formal solution conditional distribution px constrain information optimization problem e information bottleneck method original approach solution result equation use base analogy deterministic approach clustering topdown hierarchical algorithm start single cluster undergo cascade cluster split determine stochastically phase transition soft fuzzy tree cluster propose alternative approach base greedy bottomup merge information bottleneck simple work approach situation algorithm apply example information bottleneck method random variable objective information bottleneck method extract compact representation variable denote minimal loss mutual information relevance variable specifically want find possibly stochastic map px maximize mutual information relevance variable constraint lossy coding length x word want find efficient representation variable x prediction close possible direct prediction introduce positive lagrange multiplier enforce mutual information constraint problem maximization lagrangian ix respect lx subject markov condition y normalization minimization yield directly follow equation map px dkl normalization function familiar divergence variational principle equation solve iteration prove converge finite value lagrange multiplier natural interpretation inverse temperature suggest deterministic explore hierarchy solution variational principle eq determine shape process change mutual information vary optimal curve analogous rate distortion function information theory follow strictly concave curve information bottleneck algorithm provide information theoretic mechanism identify quasistable structure graph form meaningful cluster clustering application variable relaxation process discussion vary temperature t information bottleneck algorithm explore structure datum resolution low t resolution high point appear cluster high t point group cluster process resemble appearance structure relaxation important difference mechanism bottleneck cluster form blur conditional distribution correspond data point point cluster distribution sufficiently similar process sensitive global topology graph represent datum understand look example figure consider oppose point ellipsis cluster distribution overlap example unfortunately happen ellipsis completely direct application bottleneck original transition matrix bind fail case relaxation process hand distribution merge markovian dynamic graph specific example oppose point similar reach state similar probability follow partial relaxation process preserve fine structure underlie graph enable fine partitioning datum necessary combine process stage relax markov process quasistable point term rate information loss point natural underlying structure emerge reflect partially relax transition matrix second stage use information bottleneck identify information preserve cluster example apply method standard clustering problem obtain good result famous datum easily obtain misclassifie point interesting application obtain know gene expression datum cancer datum set provide datum set consist tissue sample come tumor rest normal patient gene expression level gene result matrix study datum calculate pearson correlation kp expression row measure distance simple transformation define figure right panel present rate information loss datum obtain cluster original tissue class cluster time slow information loss clearly t iteration information bottleneck apply relaxation time discover original tissue class misclassifie tissue figure comparison sophisticated supervise technique apply data misclassifie point good result tissue reference pattern gene expression reveal cluster analysis tumor normal tissue probe array friedman m tissue classification gene expression profile biology appear m m e datum clustering use model neural computation m cover element information theory m p p brown cluster analysis display wide expression pattern m randomize algorithm pairwise clustering proceeding nip fisher use multiple measurement taxonomic problem annual t pairwise datum cluster ieee transaction pami pereira tishby l distributional clustering english word annual meeting association page rise deterministic cluster compression classification regression related optimization problem ieee tishby information bottleneck proceeding nip tishby pereira information bottleneck method proceeding th annual conference communication control compute
hippocampal contribution control way advanced study biological engineering computational neuroscience unit abstract recent experimental study focus different neural structure different type instrumental behavior recent theoretical work provide normative account control system output different controller integrate controller identify associate forward model prefrontal cortex second associate computationally simple habitual method argue normative additional far marginalize control system associate episodic memory involve medial temporal cortex analyze depth class simple environment episodic control useful range case characterize complexity noise particularly early stage learn long set interpret datum transfer control light hypothesis introduction use episodic memory possibility experience play critical role enable act appropriately world act basis single seemingly normative use accumulate statistic multiple event task build statistical model normally semantic memory main form memory issue kind frequently discuss multiple memory system consider normative viewpoint memory directly use answer initial question computational challenge use semantic memory forward model sequential decision task action goal reach forward backward search tree action state ie modelbase reinforcement learn domain impose demand work memory store partial evaluation possible expand tree reasonable time think result error evaluation form computational noise uncertainty use semantic memory control expect subject substantial error main task paper explore understand circumstance episodic control seemingly efficient use experience expect accurate evident neurally argument episodic control exactly parallel recently habitual cache control modelfree reinforcement learning method computationally trivial accurate time use learn function function cache result expensive difficult search modelfree method learn form know inefficient use experience optimal employ control modelbased control sufficient experience inaccuracy learning noise induce use exact tradeoff depend prior statistic possible task general modelfree control modelbase control substantial experience episodic control modelbase control limited experience class environment controller significantly domain optimal use semantic control analysis purely computational psychological neural implication association argue transition modelbase modelfree control explain psychological observation transition course learn goaldirecte control consider modelbase habitual control modelfree turn associate apparent functional segregation implement modelbase control input implement modelfree control exactly uncertainty associate type control calculate clear know cortex involve psychological construct episodic control obvious neural realization likely medial temporal cortical region work controller clear suggestion uncertainty represent neurally evidence transfer control hippocampal structure course learn suggest happen unfortunately study possibility additional step prefrontal cortex fully test paper explore nature episodic control section describe treestructure markov decision problem use illustrate argument section provide detailed approximate analysis uncertainty learning environment finally section use analytical method simulation study model tradeoff paradigm analysis seek analyse computational statistical tradeoff arise choose action maximize reward sequential decision problem tradeoff originate uncertainty associate learning inference characterize task markov decision process mdps transition reward structure initially unknown subject draw parameterize prior know key question different possible control strategy perform prior measure experience simplify exploration use form parallel sampling model order focus ability controller exploit knowledge extract environment performance naturally measure use average reward collect trial average average draw mdp stochasticity associate exploratory action analyse controller modelbased controller computational noise provide theoretical upper limit performance realistic modelbase b performance action nonterminal state simulation analytical approximation transition probability learning reward probability terminal state figure example treestructure mdp depth branch factor b available action nonterminal state horizontal stack bar box leave middle column transition probability different action nonterminal color code state lead match color correspond arrow transition probability distribution iid accord dirichlet distribution parameter gaussian right column reward distribution terminal state unit variance mean draw normal distribution mean r standard deviation r parameter late figure note b validate analytical approximation numerical simulation controller computational noise regard model semantic control episodic controller concentrate simple subset mdps treestructure mdps illustrate figure define formally support material expect qualitative characteristic finding apply general mdps use represent firstorder analytically tractable approximation general problem present mdp decision point time ie decision tree action lead state potentially reward possible action state available key difference general mdp state revisit time episode impossible approach neglect correlation value future state formally correct limit infinitely mdps approximation modelbase controller paradigm task modelbased controller use datum exploratory trial work posterior distribution unknown transition reward structure report good action state know actually intractable understand tradeoff different controller need analyze expect return average random quantity technical contribution work set analytically approximation average present supplementary material base knowledge parameter govern generation function exploratory experience proceed stage consider modelbased controller case experience sample parameter know exactly provide approximate upper bind expect performance controller second approximate impact incomplete exploration corrupt controller noise magnitude determine parameter problem finally approximate additionally effect limited computational resource add assume induce bias extra variance step calculate asymptotic performance infinitely datum collect limit transition probability reward distribution treat know quantity critical analysis independence symmetry property regular imply need analyze single tree nonterminal state successor state result tree recursion case asymptotic analysis recursive formulation turn allow closedform solution mean variance approximate gaussian distribution characterize average value tree start r r d r r r mean variance normal distribution mean reward distribution terminal state draw constant depend parameter calculation depend characterize order statistic multivariate gaussian distribution term covariance matrix equation actually interesting result indicate extent controller advantage variability r boost expect return root node function depth tree second step observe expect benefit episodic control apparent limit exploratory experience analytical progress force significant assumption effect model assume controller access true value action noisy version noise come fact compute value different action base estimate transition probability reward distribution estimate inherently stochastic base stochastic experience able form result noise action value effect scale true value action state factor add extra noise unable find closedform solution effect performance controller recursive analytical formulation involve possible support material figure b learn curve modelbased controller compute use analytical prediction blue line use exhaustive numerical simulation red line average performance sample learning process time inaccuracy entail approximation parameter simulation point use analyse performance optimal modelbase controller dark blue solid curve figure label performance modelbased control function number exploration sample equivalent dark blue curve figure b comparison dash line asymptotic expect value slight decrease approximate value arise approximation slightly loose noise able simulation analysis highly accurate compare extensive actual sample final step model effect computational complexity modelbase controller performance arise severe demand place work memory necessitate ignore decision tree approximation treat effect approximation force controller access noisy version action value incomplete exploration model noise combination true action value parameter add excess variability note term characterize effect learn determine number sample set hand capture assume effect inference computational complexity asymptotic value curve figure value demonstrate effect noise performance b relative performance performance asymptotic learn time learn time figure learn curve modelbased controller different level computational noise increase approximation use compute curve accurate limit slight decrease performance perfect controller noise end learn dash line asymptotic approximation accurate limit demonstrate inaccuracy approximation b performance noisy controller normalize perfect controller environment experience brown line correspond difficult environment great depth note learn time measure number time stateaction pair sample decrease performance complex environment increase sparsity experience far separately consider effect computational noise uncertainty limited experience reality factor affect modelbase controller plot figure b interaction factor figure b datum figure scale performance noisefree controller experience computational noise asymptotic performance bad simply average reward learn effectively slow adverse effect computational noise depend difference value possible action value appear widely different computational noise preserve order truly good likely choose action value appear roughly little noise easily change ordering controller choose suboptimal little experience small apparent difference value boost effect noise experience controller increasingly learn distinction different action look early work suggest modelbase control superior limit exploratory sample optimal statistical inference low datum limit factor performance influence computational noise experience constitute little noise count course relative complexity environment episodic control modelbased control computational noise limited exploration effective alternative scope formal analysis particularly important nonstationary environment effect change bind effective number exploratory sample habitual controller bad limit learn main account transfer goaldirecte habitual control suggest habitual controller step expect episodic control useful intuitively subject experience complex environment time find sequence action work reasonably provide exploitation exploration obvious subject b learning time performance modelbase perfect modelbase episodic performance performance modelbase perfect modelbase modelbase perfect modelbase episodic learn time learning time figure episodic modelbase control solid red line performance noisy modelbase control blue line episodic control dash red line case perfect modelbase control constitute good performance possibly achieve factor environment increase b b b b c repeat exactly action try build use complex model act particular sequence event past exactly instance episodic control specifically employ extremely simple model episodic memory assume time subject experience reward consider large large expect priori store specific sequence stateaction pair lead reward try follow sequence state include multiple successful sequence available state yield maximal reward follow expect strategy useful low data limit control issue temporal credit assignment modelbase control exhaustive involve action selection course advantage ultimately use single sample adequate time controller expect approximate analytical method provide insight characteristic far able use simulation study episodic controller usual class compare blue episodic red modelbased noisy curve figure apparent episodic control outperform noisy modelbase control low datum limit dash curve performance idealize modelbase controller noisefree emphasize choice noise level great noise long dominance episodic control complicated environment small noise modelbased control brown line fig b issue learning curve figure point different way happen complexity environment increase increase branch factor level computational noise episodic control modelbase control increase volume exploratory sample expect true complexity environment increase increase depth tree instead figure point asymptotic performance episodic controller poor improve extra learn episodic strategy involve eliminate sample trajectory perform discussion episodic controller operate remember state single action lead good outcome far observe study nature benefit episodic control controller statistically inefficient solve markov decision problem compare normative strategy build statistical forward model transition outcome search optimal action episodic control computationally straightforward suffer excess uncertainty noise arise severe search complexity forward model imply forward model control circumstance explore characterize class regular treestructure markov decision problem use critical parameter depth tree fanout state number action state characteristic dirichlet statistic transition action use theoretical empirical method analyze statistical structure base forward model face limited datum control readily outperform episodic controller suffer computational inaccuracy particular limit high task complexity significant noise modelbased controller noise particularly effect course learn corrupt choice action value appear limited experience close actually result obviously partial particular constraint use regular treestructure severe intuition result consider conventional mdps model class experiment probe transfer far important consider model exploration general parallel sampler provide homogeneous sampling stateaction pair particular challenge exploration exploitation couple sample interdependent analysis parallel noisy controller cache controller opposite limit substantial experience environment cache controller computationally straightforward rely completely different structure learning psychological term episodic controller good thought goaldirecte outcome form episode recall unfortunately difficult distinguish goaldirecte control result forward model term episodic controller likely rely investigate system involve episodic memory medial temporal cortex importantly direct evidence transfer control hippocampal structure course learn evidence episodic habitual control simultaneously active unfortunately datum structure control competition transfer process test intermediate phase prefrontal mechanism instantiate forward model dominant prediction work associate experimental test paper extended answer question computational benefit episodic memory speak store particular sample semantic memory store probability distribution course answer instance subject cache obviously remember exactly particular store food search place likely semantic prior equally game theoretic interaction competitor typically stochastic seemingly excellent candidate control base semantic memory advantage opponent require exactly remember action deviate stationary statistic episodic memory useful tool potential result method associate reasoning density estimation create semantic memory episodic instance recall episode close cue weight measure distance form semantic memory arise process method computational phenomenon prime extremely difficult recall multiple closely relate sample episodic memory let statistically unbiased way sum provide normative justification perspective appropriate control episodic component multiple memory system press theoretical perspective rich understanding integration competition information reside decision system involve choice acknowledgement grateful helpful discussion funding foundation ml framework reference m face nature dayan p offline maintain memory model interaction oreilly complementary learning system insight success failure connectionist model learning memory rev nm multiple parallel memory system brain rat learn dayan p competition prefrontal system behavioral control mit watkin learn delay reward m dayan p uncertainty phase oscillatory hippocampal recall advance information processing system ed scholkopf b hoffman mit mg double dissociation nucleus lesion acquisition water maze task evidence multiple memory system interactive memory system human brain nature kearn m convergence rate indirect algorithm advance neural information processing system vol ed kearn gp moment order statistic multivariate normal distribution ann math shadmehr r dynamic memory consequence optimal adaptation change body rodriguez p memory system interact evidence human classification learn learn memory nature n jm social cognition natural r soc d reasoning communication acm simulation model human memory method comput
learning word embedding cca computer information science computer information science abstract recently substantial interest use large unlabeled datum learn word representation use feature supervised classifier nlp task current approach slow train model context word lack theoretical paper present new learning method low rank use fast spectral method estimate low dimensional word representation unlabeled datum representation feature use supervised learner lrmvl extremely fast guarantee convergence global optimum theoretically elegant achieve stateoftheart performance entity recognition ner chunk problem introduction related work past decade increase interest use unlabeled datum supplement label datum semisupervised learning setting overcome inherent datum sparsity improve generalization accuracy high dimensional domain nlp approach empirically successful achieve excellent accuracy variety task difficult adapt approach use conjunction exist supervise nlp system approach enforce particular choice model increasingly popular alternative learn representational embedding word large collection unlabeled datum typically use generative model use embedding augment feature set supervise learner embed method produce feature low dimensional space small vocabulary size traditional approach work original high dimensional vocabulary space dimension time broadly embed method fall category cluster base word representation clustering method hierarchical use group similar word base context dominant approach brown cluster recently hmms use induce multinomial distribution possible cluster dense representation representation dense low dimensional realvalue dimension representation capture latent information combination syntactic semantic word property induce use neural network embedding hierarchical embedding word cooccurrence matrix semantic semantic indexing unfortunately representation slow train sensitive scaling embedding especially base approach stick local optima train hmm learn single embed word type ie occurrence word bank embed irrespective context word suggest mean financial bank paper propose novel contextspecific word embed method low rank learn fast train guarantee converge optimal solution present embedding contextspecific context oblivious embedding use trivially model furthermore build recent advance spectral learning sequence model hmms strong theoretical particularly estimate low dimensional contextspecific word embedding preserve information datum datum generate linear face stick local optima case train fall category mention learn realvalue contextspecific word embedding perform canonical correlation analysis cca past future view low rank approximation datum general method work trigram cooccurrence matrix use long word sequence information estimate contextspecific embedding reason mention remainder paper organize follow section brief overview cca form core method section describe propose detail theory support performance section demonstrate effectiveness nlp task entity recognition chunk conclude brief summary section brief review canonical correlation analysis cca cca analog principal component analysis pca pair matrix pca compute direction maximum covariance element single matrix compute direction maximal correlation pair matrix depend observation scale invariance cca linear datum transformation allow proof dominant singular vector large singular value faithfully capture state information specifically set n pair observation vector l r rn case matrix left l right r context matrix like simultaneously find direction l r maximize correlation projection l l projection r r express l r l r l r e denote empirical expectation use notation denote cross auto covariance matrix l r lr respectively left right canonical correlate solution r follow equation crr crr r low rank learn lrmvl compute cca past future view datum large unlabeled find common latent structure ie hidden state associate token induce representation token use feature supervise classifier typically discriminative context word consist word right leave sit high dimensional space vocabulary size word context require indicator function dimension key project word space dimensional state space eigenvector computation space time small original space typical vocabulary contain word use state space order dimension fold reduction size calculation need core fast spectral method learn v k matrix map word vocabulary kdimensional state vector matrix eigenfeature dictionary describe method theorem provide intuition work formally present experiment section low approximation allow achieve stateoftheart performance nlp task method unlabeled token sequence wn want learn low dimensional state vector z observe token key find v k matrix algorithm map word vocabulary reduce rank kdimensional state later use induce context specific embedding token supervised learn context specific embedding supplement information token identity feature prefix suffix membership use feature section algorithm formally key step algorithm general term word left right target word left right context project dimension use cca reduce rank leave right context use result model estimate dimensional state vector hidden state token cca hide state token singular vector associate form new estimate eigenfeature dictionary view type cotraine state token similar token similar state occurrence word document use outer iteration lrmvl advantage different type similarity alternately estimate word state use cca smooth state word target token use average state associate occurrence word theoretical property lrmvl present theory particularly reduce rank allow significant data reduction preserve information datum estimate state good possible job capture label information infer linear model let l n hv matrix word left context token context length r correspond hv matrix right context matrix indicator function word use follow assumption point proof assumption l w r come rank ie rank observation matrix transition matrix domain example dimension hidden state vocabulary size observation matrix v rank condition similar use assumption view l w r assume exist hidden state dimension k row matrix s ri row l r respectively assumption follow assumption assumption l r w r expected correlation x assumption rank condition similar assumption l r w distinct singular value assumption proof little clean repeat singular value singular vector unique phrase result term subspace identical singular value need define cca function compute left right singular vector pair matrix definition cca compute cca matrix let matrix contain large singular vector sort large likewise define function want s use leave left singular vector right right singular vector note result singular vector use redundant estimate x hidden state relate hidden state exist definition define symbol mean sample size define following limit right singular vector l r assumption l r l r rr instead find cca context word cca left right context estimate dimensional state cca state word result supplementary material proof h denote matrix form stack copy right let h project word context kdimensional reduce rank l r follow address core desire dimensionality reduction specifically previous hold reduce theorem assumption exist unique matrix l r l r right h stack form supplementary material proof use worth note matrix correspond matrix sufficient compute probability sequence word generate hmm limited space provide statistically efficient estimate u use estimate sequence probability assumption asymptotically limit infinite datum benefit estimate state find cca left right context find estimate state word instead directly find cca left right context word distribution word word rare unique asymptotic limit case cca rare word context informative find cca left right context good state vector estimate unique word find cca context estimate state vector associated word use exponential smooth practice replace project left right context exponential smooth weight average previous token state previous token smooth different time scale dimension reduction factor context length h word divide number smooth use mixture short long context capture short long range dependency require nlp problem ner chunk exponential smooth linear preserve linearity method use exponential smooth compute describe learn large unlabeled datum input token sequence state space size k smooth rate initialize eigenfeature dictionary random value repeat set state token eigenfeature vector corresponding word wt smooth state estimate token pair view smoothing rate leave view right view r t row l r respectively concatenation smooth find left right canonical correlate eigenvector l r l rr r r ll l r r r ll l rr r project left right view space span k left right cca respectively rr l r matrix compose singular vector r large magnitude singular value estimate state word wt union left right estimate estimate eigenfeature word type w average state estimate word compute change previous iteration output kl iteration algorithm sufficient converge solution problem single solution issue local minima start random matrix slightly large true rank correlation matrix extremely high likelihood converge iteration small distance true principal component case assumption detail satisfied method converge equally rapidly true canonical variate mention early dimensionality reduction step replace left right context matrix set exponentially smoothed value reduce rank projection context word step find cca left right context step estimate state combine estimate left right context know estimate state step cca estimate state z matrix word w vector indicator function cca trivial form set average estimate cca model use generate context specific embedding token train development test set describe embedding far supplement baseline feature use supervised learner predict label lrmvl induce context specific embedding datum input model kl output algorithm token sequence project left right view l r smooth space span k leave right respectively word eigenfeature train form final embed matrix concatenate estimate state output embed matrix contextspecific representation token embedding augment baseline set feature mention section learn final classifier note context oblivious embedding ie embed word type use eigenfeature dictionary output algorithm experimental result section present experimental result entity recognition syntactic chunking task compare stateoftheart semisupervised approach alternate structure optimization semisupervise extension embedding brown clustering dataset experimental setup experiment use datum share task chunk experiment use share task datum standard training development testing set split dataset token respectively set entity recognition use set baseline feature use experiment detailed list feature current word type information prefix suffix word token window current word wi wi wi wi pattern window previous prediction conjunction embed feature window current word applicable follow use regularize average perceptron model set baseline feature task use text chunk representation fast greedy inference superior performance detail datum competition available augment set baseline feature standard practice experiment tune free parameter size lrmvl embed development scale embed feature norm token far multiply normalization constant choose cross validation use conjunction categorical feature linear classifier extra influence size embedding statespace good performance development set total size embedding good normalization constant omit validation plot space chunk chunk experiment use similar base set feature current word word token window current word d wi wi wi tag ti window current word word conjunction feature wi wi tag conjunction feature ti ti ti ti embed feature window current word applicable chunk datum development set randomly sample sentence training datum sentence development train chunking model training sentence evaluate score development sentence use crf supervised classifier tune size embed magnitude regularization penalty crf development set log log magnitude value feature regularization penalty good performance development set good size embedding statespace finally train crf entire original training datum ie sentence unlabeled datum induction embedding induce embedding use contain contain token sentence case leave ie remove sentence learning approach robust noisy datum news text correlate strongly text article induce embedding period day core hour ghz cpu entire datum perform iteration vocabulary size use variety smooth rate capture correlation short long context theoretically tune smooth parameter development set find mixture long short term dependency work practice far embedding concern ie brown cluster download detail induction parameter tuning find report good number worth note unsupervised training time fast embedding result result ner chunking table respectively perform significantly stateoftheart compete method chunk task embedding learn use linear model model thing scale normalization choose particular dataset fair comparison report result use unlabeled datum embedding train method fast cluster cluster fscore set test set table ner result note lrmvl co context oblivious embedding fscore harmonic mean precision recall current stateoftheart ner task test set use token cluster test set fscore table chunk result important note problem final accuracy depend performance robustly able correlate past future view able learn representation rare word result overall accuracy occur time relative reduction error brown respectively chunk corresponding number worth mention model context embedding improvement accuracy ner chunk problem case word location organization sport team bank use feature know list city person organization high increase fscore model context compare case feature capture information word model context help word chunk dataset case embedding context help significantly relative improvement accuracy context oblivious embedding summary conclusion paper present novel learning method large scale sequence learn problem arise nlp spectral method work low dimensional statespace computationally efficient use train use large unlabeled datum stick local optima train embedding learn use use feature supervised strong theoretical simple fast compete method achieve stateoftheart accuracy ner chunk problem acknowledgement author like thank alexander anonymous review provide valuable feedback like thank answer question regard paper reference framework learn predictive structure multiple task unlabeled machine learn research semisupervise sequential labeling segmentation use unlabeled datum brown p p r model natural language comput pereira tishby distributional clustering english word annual meeting distributional representation handle sparsity supervised computational linguistic collobert r unified architecture natural language process deep neural network multitask learn icml new graphical model statistical language modelling icml r use latent semantic analysis improve access textual information human factor computing system acm hsu d kakade spectral algorithm learn hide markov model colt hide markov model aistat song l sm hilbert space embedding hide markov model icml canonical correlation analysis cca journal educational psychology mitchell t combine label unlabeled datum cotraining colt n finding structure randomness probabilistic algorithm construct approximate matrix decomposition robust risk minimization base entity recognition system bengio word representation simple general method semisupervise learn linguistic roth design challenge entity recognition p semisupervise learn natural language master thesis phrase clustering discriminative learning proceeding joint conference th annual meeting th international natural language processing volume volume computational linguistic
efficient agnostic active learning abstract develop new active learning streaming setting satisfy important property provably work classifier classification problem include severe noise efficiently erm oracle aggressive previous approach satisfy create algorithm base newly define optimization problem analyze conduct experimental analysis efficient agnostic active learning algorithm evaluate strength weakness different setting introduction label budget good way learn classifier active learning approach question know yield exponential improvement supervised learning strong assumption weak assumption agnostic active learning particularly know work classifier representation label distribution data source learning decide unlabeled example sequence request label revisit decision possible active learning work classifier representation label distribution computationally tractable computational tractability critical concern know algorithm set require explicit classifier imply computational complexity compare typical supervised learning algorithm active learning base empirical risk minimization erm oracle overcome use passive classification algorithm oracle achieve computationally acceptable solution achieve generality robustness acceptable computation cost method label request nearly unlabeled example empirically good classifier disagree result poor label complexity short informationtheoretic limit general robust solution section design new algorithm active construct query probability function minimize probability query disagreement set point good classifier query require new algorithm maintain cover set empirically good classifier cover result solve optimization problem section specify property desirable overview exist literature include alternative setting additional assumption place datum source query probability function cover size provide practical computation label complexity demonstrate complexity analysis present section section prove effectively maintain set good classifier achieve good generalization error label complexity bind tight previous approach label complexity bind depend disagreement coefficient completely capture advantage algorithm end section provide example hard active learn problem substantially superior previous tractable approach result substantially theory agnostic active learning algorithm work practice previous work address question empirically important analysis reveal degree exist classification algorithm effectively provide erm oracle conduct extensive study section simulate interaction active learning streaming supervise dataset result wide array dataset agnostic active learning typically outperform passive learning magnitude improvement depend carefully active learn hyperparameter choose detail theory proof empirical evaluation long version paper preliminary let p distribution let set binary classifier assume finite simplicity let ex denote expectation respect px marginal p expected error classifier h errh error minimizer denote arg errh importance weight empirical error h p importance weight label example draw r errh disagreement region subset classifier h h h regret classifier h relative h regh errh analogous empirical regret regh errh s second classifier empirical regret omit empirical error minimizer active learner receive label example y p time label hide learner decide query goal produce classifier h low error errh query label possible iwal decision query label randomly learner pick probability p query label probability p unbiased error estimate produce use inverse probability weight specifically classifier unbiased estimator e errh base p p follow query e p e easy check errh label query produce importance weight label example p algorithm statistical guarantee new algorithm break example stream epoch admit epoch schedule long epoch length satisfy m m technical reason query label algorithm start m query probability function pm use sample datum point query epoch maintain object interest step good classifier sample collect far mix query predict label radius m base level concentration want empirical quantity satisfy set consist classifier empirical regret m pm determine probability query example disagreement region set good classifier example assumption finite relax use standard argument label query produce example weight purpose maintain correct count opportunity ensure correct normalization errh active c input constant c c c confidence error radius parameter m satisfy m m m initialize m c log m query label yi unlabeled example set h p m set s let arg min m c m m log m hh errh m compute solution pm problem op increment m m end unlabeled point dm coin bias pm xi add example pm outcome head add add example predict end end return region query label predict error estimate unbiased compute pm solve optimization problem op far discuss objective function op encourage small query probability order minimize label complexity constraint op bind variance regret estimate h key ensure good generalization later use bound rely random variable small variance specifically constraint measure variance empirical regret estimate measure example disagreement region dm importance weight form pm apply example region use predict label importance weight rhs constraint consist term term ensure feasibility problem p dm satisfy constraint second empirical regret term constraint easy satisfy bad crucial rule large label complexity case bad hypothesis disagree benefit easily regret force query probability disagreement region finally term order second hypothesis include capture allow level slack constraint exploit efficient implementation section addition control variance good concentration require random variable interest appropriately bound ensure constraint impose minimum query probability disagreement region disagreement region use predict label importance weight estimate bound bias region note optimization problem write respect marginal distribution data point px mean infinitely constraint section describe solve optimization problem efficiently use access unlabeled example draw require input parameter satisfy c c nm parameter control tightness variance constraint parameter c control threshold define set empirically good classifier c use minimum probability simply set optimization problem op compute p dm bm h p p p dm ex regh m m m m min m log m schedule arbitrary schedule subject m m natural extreme epoch m m double epoch m m main difference lie number time solve substantial computational consideration state assume double schedule query probability erm classifier olog generalization label complexity present guarantee generalization error label complexity assume solver op provide section provide bind generalization error define m p m p m c m errm c log m m errm h essentially m population counterpart quantity m use rely errm h true error restrict disagreement region m capture inherent problem modulate transition type error bound theorem pick e recall epoch m m probability regh h m regh m proof section use bound imply epoch m maintain predict label use algorithm identical h disagreement classifier observe example observation critical proof exploit fact use label predict instead observe label certain example introduce bias favor ensure drop optimal classifier bind classifier small regret classifier yield main generalization error bind classifier output additionally clarify definition set set good classifier classifier small population regret relative realizable setting error m m lead regret n unlabeled example present algorithm extreme errm constant regret interesting regime constant errm measure disagreement region decrease rapidly specifically appendix e expect regret classifier return achieve optimal rate noise condition provide label complexity guarantee term disagreement coefficient h px x theorem probability number p label query example m epoch errm prove appendix d term label complexity bind linear number unlabeled example small small errm n realizable set second term grow constant realizable problem consequently attain logarithmic label complexity realizable setting noisy setting label complexity improve obtain label complexity exponentially bad realizable problem relate algorithm label complexity scale bad dependence comparison use errm provide qualitatively superior analysis previous result depend errh capture fact noisy label disagreement region affect label complexity finally regret analysis appendix e label complexity achieve lower bind section example label complexity significantly small iwal cal virtue rarely query disagreement region example consider distribution classifier space follow structure example single good classifier predict differently remain classifier example classifier predict way case little advantage gain label provide evidence single classifier active query disagreement region probability close case probability case query probability imply time query efficient implementation computation erm operation perform efficiently efficient passive learner available remain test algorithm find solution op considerably challenge schedule help op solve olog time necessitate extremely efficient solver start issue follow dasgupta observe dm efficiently determine use single erm oracle specifically apply method use oracle find arg h argue dm regret h regh m solve op efficiently big challenge variable p point constraint bind constraint p lead infinitely variable constraint erm oracle computational primitive available eliminate bound constraint use barrier function notice objective ex p barrier p enforce lower bind modify objective dm ex ex p p parameter choose ensure p dm modify goal minimize nonnegative p subject solve problem dual large finite number optimization variable efficiently maximize dual use coordinate ascent access erm oracle let denote deal constraint unconstrained oracle coordinate ascent solve input accuracy parameter initialize loop arg bm h find h p m p return update end end loop bm h m lagrange multiplier constraint classifier minimize primal variable p yield solution dm q p q hh dm clearly p dm bound constraint op satisfied choose plug solution p lagrangian obtain dual problem maximize dual objective ex x dm h bm h c hh constant c equal dm approximately solve problem present algorithm parameter specify degree constraint approximate concave rescaling step solve use straightforward numerical line search main implementation challenge find violate constraint step fortunately step reduce single erm oracle note constraint violation write bm x dm h p p m m m m second term righthand expression simply scale risk classification error respect actual label term risk h predict sample label accord importance weight p dm note weight positive negative term depend access px sample approximate discuss shortly violate constraint find solve erm problem define label sample sample draw px label appropriate importance weight detail appendix primal constraint approximately satisfied stop follow guarantee convergence theorem run p satisfie simple bind constraint iteration output solution exactly variance constraint additive factor ex p x p p solution furthermore set m m constraint violation analysis number iteration number erm oracle proof appendix table summary performance ora iwal ora iwal passive solve op expectation sample far consider solve op define unlabeled data distribution px practice natural substitute px sample draw appendix solve sample variant op lead solution original op similar guarantee theorem experiment agnostic active learning efficient number erm oracle need store past example result large space complexity theorem suggest query probability function need classifier far increase storage demand aim scalable implementation consider online approximation section main difference instead batch erm oracle invoke online oracle instead repeatedly solve op scratch maintain set classifier nonzero dual variable cover represent query probability update cover new example manner similar coordinate solve conduct empirical comparison follow efficient agnostic active learning online approximation active section iwal iwal algorithm variant use tight threshold ora ora iwal ora iwal version iwal passive passive learning label subsample draw uniformly random iwal detail algorithm section highlevel difference algorithm explain context disagreement region query label optimize query probability disagreement region use predict label iwal iwal maintain nonzero minimum query probability ora iwal ora iwal query label respective disagreement region probability use predict label implement algorithm fast learning system base online convex optimization use logistic regression erm oracle perform experiment binary classification dataset vary size diverse feature characteristic detail dataset goal evaluate test error improvement label query achieve different algorithm simulate streaming set randomly dataset run active learning algorithm datum evaluate learn classifier remain repeat process time reduce variance random permutation active learning obtain test error rate classifier train double number label query start formally let q denote test error classifier return use hyperparameter set p jth permutation d immediately hit let q actual number label query small q reach end training datum hit label budget evaluate algorithm consider area curve test error log number label query q q log good active learning small value auc indicate test error decrease quickly number label query increase use logarithmic scale number label query focus performance label query active learning relevant detail hyperparameter appendix g relative improvement test error relative improvement test error passive baseline number label query passive baseline number label query good hyperparameter dataset good fix hyperparameter figure average relative improvement test error number label query measure performance algorithm follow aggregated metric d mean max median d j d denote auc passive use default hyperparameter set ie learning rate metric maximal gain achieve good hyperparameter set dataset second gain use single hyperparameter setting perform good average dataset result discussion table summary performance different algorithm use hyperparameter optimize basis row table achieve large improvement passive baseline iwal achieve improvement iwal improve slightly variant perform bad passive good learning rate dataset lead average improvement auc default learning rate use good fix hyperparameter set dataset row table active learning algorithm achieve improvement compare passive improvement good fix learning rate particular improvement suggest careful tuning hyperparameter critical important direction future work figure describe behavior different algorithm detail algorithm identify good fix hyperparameter setting j p d plot relative test error improvement use p average dataset label q q mean algorithm include passive perform similarly label query iwal perform good label budget large iwal ora good follow ora iwal ora iwal perform bad passive label budget figure plot result obtain use good hyperparameter set j max d expect algorithm perform benefit use good hyperparameter setting dataset appendix detailed result include test error rate obtain algorithm different label query budget individual dataset sum use good fix hyperparameter set iwal outperform use good hyperparameter setting tune dataset iwal perform equally algorithm reference long active passive learning linear separator distribution conference learn theory page agnostic active learning proceeding international conference machine learn page margin base active learning proceeding th annual conference learn theory page springerverlag dasgupta j langford importance weight active learning icml d langford t agnostic active learning constraint nip minimax bound active learning information theory ieee transaction r improve generalization active learning learn dasgupta coarse sample complexity bound active learning advance information processing system s dasgupta hsu general agnostic active learning nip s theoretical foundation active learning theory active learning foundation trend machine learn generalization sample replacement finite amer statist active learning schapire efficient agnostic active learning online importance weight aware update proceeding conference uncertainty artificial page koltchinskii rademacher complexity bound excess risk active learning mach learn tsybakov optimal aggregation classifier statistical learning ann statist agnostic active learning advance neural information processing system page
energy learning image segmentation abstract introduce new machine learning approach image segmentation use neural network model conditional energy segmentation image approach combinatorial energy learning image segmentation place particular emphasis model inherent combinatorial nature dense image segmentation problem propose efficient algorithm learn deep neural network model energy function local optimization energy space agglomeration extensively evaluate method publicly available microscopy dataset voxel ground truth datum test set find method improve reconstruction accuracy compare stateoftheart baseline method segmentation output convolutional neural network train predict boundary random forest classifier train generate convolutional neural network introduction map pursuit link computational model consistent observe function actual physical structure fundamental problem neuroscience primary interest map network structure neural circuit identify neuron location synaptic connection neuron field connectomic currently promising approach obtain map neural circuit structure volume microscopy fix block tissue technique use successfully decade map structure complete nervous system need manually cut image align trace neuronal process nm serial section small circuit require year spend image analysis time scale approach large circuit practical recent advance volume feasible imaging large circuit potentially contain thousand neuron sufficient resolution small neuronal process high image quality resolution achievable method enable datum treat true volume significantly reconstruction process run parallel axis potentially amenable automate image processing conference neural information processing system nip fullyconnecte layer network image s voxel position boundary classification global energy p shape descriptor initial oversegmentation agglomeration local energy figure illustration computation global energy single candidate segmentation s local energy es compute deep neural network sum shape descriptor type voxel position image analysis remain key challenge primary bottleneck segment volume fill entirely heavily neuronal process volume occupy individual neuron cell boundary provide strong visual cue case neuron extend path length place narrow single mistake path render connectivity information neuron largely inaccurate exist automate segmentation method sufficiently reduce human require recent reconstruction neuron mouse require hour human efficient method trace skeleton neuron recent reconstruction neuron column visual pathway require hour manual automate segmentation relate work algorithmic approach image segmentation formulate variation follow pipeline boundary detection step establish local hypothesis object boundary region formation step integrate boundary evidence local region region agglomeration step merge adjacent region base image object feature extensive integration machine learn pipeline begin yield promise segmentation result argue pipeline previously formulate fundamentally neglect potentially important aspect achieve accurate segmentation combinatorial nature reasoning dense image segmentation structure fundamental importance shape criterion segmentation quality contribution propose method attempt overcome deficiency particular propose model score segmentation quality use deep neural network integrate shape image information combinatorial energy learning image segmentation celis pursuit model paper specific contribution novel connectivity region data structure efficiently compute energy configuration object binary shape descriptor efficient representation d shape configuration neural network architecture intermediate unit output train convolutional network input deep fullyconnecte neural network architecture score segmentation image training procedure use pairwise object relation segmentation learn model experimental evaluation propose baseline automate reconstruction method massive knowledge scale reflect true size connectomic dataset require biological analysis billion conditional energy modeling segmentation image define global energy model predict cost complete segmentation correspond image cost analogous negative prior work recognize importance combinatorial reasoning previously propose global optimization method allow local decision interact limited way loglikelihood segmentation image actually treat probabilistically goal define model true segmentation correspond image find minimize cost energy reflect prior object configuration compatibility object configuration image fig define global energy sum local energy model define deep neural network different scale s compute fashion center position s x e local energy es depend local image context center position way vector representation compute deep convolutional neural network local configuration scale s way novel local binary shape descriptor define section find locally segmentation model use local search space agglomeration start initial segmentation use simple greedy policy step consider possible agglomeration action ie merge adjacent segment pick action result low compute energy single segmentation require compute shape descriptor evaluate energy model position volume small volume million voxel stage agglomeration thousand thousand potential agglomeration step result unique segmentation order choose good step know energy potential segmentation computational cost perform computation directly supplement prove collection theorem allow efficient implementation compute energy term incrementally represent d shape configuration local binary descriptor propose binary shape descriptor base subsample pairwise connectivity information specification k pair position offset b relative center box size bs corresponding binary shape descriptor particular segmentation bounding box define connect bi fig bit descriptor specify particular pair position segment determine constant time use suitable datum structure limit case use list pair position information lose hamming distance descriptor precisely equal rand index general sample subset pair possible sample uniformly random retain property expect hamming distance descriptor equal rand index find pick bit provide reasonable tradeoff fidelity representation size pair randomly sample initially naturally obtain consistent result learn model base descriptor use fix list position define descriptor training test time note descriptor serve general type sketch segmentation bounding box restrict position pair center position bounding box instead obtain sketch single segment contain center position refer descriptor case general case pairwise fig b use shape descriptor represent local subregion segmentation represent shape information large volume compute shape descriptor position slide window fashion fig brief descriptor similarly define binary descriptor base subset pair point patch bit base intensity difference connectivity pair r r r sequence computation shape descriptor r r r b shape descriptor compute multiple scale pairwise descriptor left center consider arbitrary pairwise connectivity shape descriptor right restrict position pair center point r r r c shape descriptor compute position volume figure illustration shape descriptor connected component bounding box descriptor compute distinct color pairwise connectivity relationship define descriptor indicate dash line connect pair white disconnected pair black connectivity determine base connected component underlie segmentation geometry line illustration d experiment shape descriptor compute fully d connectivity region define single shape descriptor represent segmentation shift position bounding box obtain descriptor correspond different local region large segmentation size bounding box determine scale local representation raise question connectivity define local region voxel connect long path descriptor like shape descriptor consistent local topology pair consider disconnected shape descriptor define respect connectivity large connectivity region necessarily contain descriptor bound box general significantly small segmentation conceptually descriptor slide possible position contain region slide necessarily result minor inconsistency context different position reduce computational memory cost obtain shape descriptor position simply space overlap rectangular connectivity region appropriate uniform size supplement connectivity region size determine degree locality connectivity information capture shape descriptor independent descriptor size affect computational cost describe supplement energy model learn s r v shape descriptor learn neural define local energy model e network model compute realvalued score shape descriptor r image feature vector simplify presentation define follow notation forward discrete derivative respect e base notation discrete derivative energy function e es s e denote result merge correspond e exist segmentation s greedy policy simply choose step action e minimize t s t denote current segmentation step prior work treat classification problem goal match sign t es t error corresponding change segmentation error respect ground truth segmentation measure use variation information local training procedure t term simply sum change energy position r descriptor type heuristic optimize parameter energy model e independently shape descriptor s seek minimize expectation s si e xi s e s rs xi s e index training example correspond particular position xi merge action ei apply segmentation denote binary classification loss function predict probability true label positive weight note action e improve score want low predict score descriptor e high predict score opposite apply test standard log loss log sign linear loss closely match si term contribute overall score descent sgd use perform optimization obtain training example use expert policy greedily optimize error s segmentation state t agglomeration step include initial state possible agglomeration action e position volume compute t rs s t e reflect state respectively t rs t e emit training example correspond descriptor pair obtain stream example es error rs s t rs stream example contain billion example highly correlated far require learn parameter reduce resource requirement use sample base es error obtain fix number weight sample replacement descriptor type s total weight true merge example error false merge example error order avoid learn degenerate model experiment test approach large publicly available electron microscopy portion dataset collect nm example weight false merge example occur balance model simply learn assign score increase number bit shape descriptor split error rand celis oracle merge error figure segmentation accuracy set leave frontier informationtheoretic splitmerge error use previously evaluate segmentation right comparison variation information low score hyperparameter optimize metric training set resolution use electron microscopy approach use segment large neuronal process cubic micron volume comprise voxel knowledge challenge dataset large publicly available electron microscopy corresponding ground truth segmentation experiment split dataset separate training testing portion axis training portion comprise testing portion comprise voxel boundary classification oversegmentation obtain image feature oversegmentation use input agglomeration train convolutional neural network predict base image context region center adjacent voxel z direction prior work optimize parameter network use stochastic gradient descent log loss train different network vary hyperparameter boundary training datum order increase extracellular space voxel component small voxel exclude supplementary information description network architecture use connection affinity apply obtain approximate oversegmentation use parameter voxel energy model architecture use type dimensional shape descriptor pairwise descriptor type bounding box descriptor type bound box respectively connectivity position bounding box descriptor sample uniformly use dimensional fullyconnected layer output lowlevel classification convolutional network image feature vector shape descriptor type r v concatenate shape use follow architecture local energy model e descriptor image feature vector obtain dimensional input vector use dimensional fullyconnected linear hide layer follow logistic output unit apply dropout p hidden layer effectively compute score raw image patch shape descriptor expensive convolutional image processing depend shape descriptor architecture allow benefit intermediate image feature vector position training energy model boundary classifier perform use asynchronous sgd use distribute architecture evaluation compare method stateoftheart agglomeration method train random forest classifier predict merge decision use image feature derive boundary probability obtain probability lowlevel convolutional network predict edge affinity adjacent voxel prediction compute minimum connection probability voxel connectivity neighborhood treat cell comparison evaluate procedure apply affinity graph output vary parameter choice measure accuracy deep boundary classification use agglomeration procedure finally evaluate accuracy publicly release automate segmentation refer basis process use obtain ground truth produce apply segmentation variant agglomeration prediction train test use initial oversegmentation training test region compare accuracy reconstruction compute measure segmentation consistency relative ground truth variation information rand score define classification score connectivity pair volume primary metric use prior work advantage segment linearly size quadratically agglomeration method ultimately limit quality initial oversegmentation compute accuracy oracle agglomeration policy greedily optimize error metric directly compute true agglomeration metric intractable serve approximate upper bind useful separate error agglomeration error initial oversegmentation result figure optimal tradeoff test set split merge error method obtain vary choice hyperparameter agglomeration threshold variation information rand score obtain training hyperparameter consistently outperform method significant margin metric large gap oracle result automate reconstruction indicate large room improvement evaluation single dataset single large dataset verify improvement celis broad general localize specific image volume evaluate accuracy independently voxel evenly space test region outperform good exist method metric median reduction variation information error rand error suggest celis improve accuracy volume span significant variation shape image characteristic support multichannel image feature potentially represent predict probability additional class use functionality train datum additional class discussion introduce framework model image segmentation use learn energy function specifically exploit combinatorial nature dense segmentation describe approach use model conditional energy segmentation image result model use guide agglomeration decision experiment challenge microscopy reconstruction problem celis improve reconstruction accuracy good exist method offer strictly tradeoff merge error wide margin compare exist method experimental result unique scale evaluation test region order magnitude large use evaluation prior work believe large scale evaluation critically important find evaluation small volume contain short fragment unreliable predict accuracy large volume merge error major challenge computationally expensive prior method nonetheless practical successfully run celis volume approach matter hour use thousand cpu core addition advance state art image segmentation work significant implication application area study connectomic reconstruction reflect stateoftheart technique sample preparation imaging reconstruction particular highly representative large dataset actively collect adult fly brain expect significant improvement automate reconstruction accuracy dataset directly translate corresponding decrease human effort require reconstruct volume tissue corresponding increase total size neural reasonably reconstruct future work specific area particularly fruitful endtoend training energy modeling pipeline include model compute image feature representation aggregation local energy position scale exist pipeline fully differentiable directly amenable endtoend training integration energy model discriminative training neural agglomeration policy policy depend distribution local energy change sum feature propose prior work use energy model fix error minimization procedure propose paper base greedy local search limit perform merge energy model capable evaluate arbitrary change segmentation evaluation candidate split base hierarchical initial segmentation heuristic criterion allow use potentially robust simulated anneal procedure capable split merge recent work integrate deep neural network conditional random field model similar celis approach combine deep learning structured prediction differ key way restriction model factor pairwise potential approach able use mean field pseudomarginal approximation perform efficient approximate inference energy model contrast sacrifice factorization rich combinatorial modeling provide propose d shape descriptor generally prior crf method focus prediction eg improve boundary semantic segmentation feedforward network correct high level contrast celis design correct fundamental inaccuracy feedforward convolutional network critical case ambiguity reflect great complexity structured model acknowledgment material base work support national science foundation grant reference b andre m segmentation volume datum neural tissue hierarchical classification pattern recognition page andre winfrie denk fre globally optimal segmentation connectomic computer page springer learn handdesigned feature representation winfrie denk circuit reconstruction volume electron microscopy technique current opinion neurobiology neuronal cell biology new technology pascal brief binary robust independent elementary feature european computer vision page springer semantic image segmentation deep convolutional net fully connect learn deep structured model icml schmidhuber deep neural network segment neuronal membrane electron microscopy image nip page le large scale distribute deep network f pereira burge l bottou weinberger editor advance neural information processing system page winfrie denk helmstaedter structural neurobiology miss link understanding neural computation nature review neuroscience winfrie denk scan electron microscopy reconstruct threedimensional tissue plo sample estimation arbitrary subset sum journal acm access andre fre matthew efficient automatic branch neuron datum computer vision pattern recognition cvpr ieee conference page ieee r automate collection serial section large volume reconstruction microscopy supplement helmstaedter winfrie denk structural imaging brain electron current opinion neurobiology helmstaedter winfrie denk reconstruction nature neuroscience helmstaedter sebastian winfrie denk connectomic reconstruction inner layer mouse nature helmstaedter winfrie denk sebastian seung learning hierarchy advance neural information processing system serial section scan electron microscopy adult brain tissue use journal neuroscience compare information base distance multivariate analysis machine learning hierarchical clustering segment d image plo objective criterion evaluation clustering method alexander fully connect deep structured network preprint arxiv fre interactive learning segmentation biomedical imaging ieee international symposium page ieee visual motion detection circuit suggest connectomic nature circuit variation different column visual system proceeding helmstaedter winfrie denk sebastian affinity learning image segmentation advance neural information processing system page mit press cambridge winfrie denk sebastian network learn generate affinity graph image segmentation neural comput segmentation fusion connectomic computer vision iccv ieee international conference page ieee white e structure nervous system transaction royal society random field recurrent neural network proceeding ieee international conference computer vision page design implementation efficient parallel algorithm affinity sebastian seung image segmentation single clustering
low rank neural receptive field center system electrical computer engineering receptive field sensory neuron describe neuron integrate sensory stimulus time space typical experiment spatiotemporal stimulus rfs highdimensional large number coefficient need specify integration profile time space estimate coefficient small datum pose variety challenge statistical computational problem address challenge develop bayesian reduce rank regression method rf estimation correspond model sum separable filter approach substantially reduce number parameter need specify example consider confer substantial benefit statistical power efficiency introduce novel prior lowrank rfs use restriction matrix normal prior manifold lowrank matrix use localize row column covariance obtain sparse smooth localize estimate spatial temporal rf component develop method inference result hierarchical model fully bayesian method use blockedgibb sample fast approximate method employ alternate ascent conditional marginal likelihood develop method poisson model lowrank estimate substantially outperform rank estimate use neural datum introduction neuron linear receptive field filter map highdimensional sensory stimulus onedimensional variable underlie neuron spike rate white noise experiment dimensionality determine number stimulus element spatiotemporal window influence neuron probability spike stimulus movie pixel frame coefficient number movie frame neuron temporal integration window typical neurophysiology experiment result rfs thousand parameter mean think vector high dimensional space high dimensional setting traditional rf estimator whitened average exhibit large error particularly correlated stimulus substantial literature focus method regularize estimate improve accuracy face limited experimental datum bayesian approach regularization involve specify prior distribution assign high probability rfs particular kind structure popular method involve prior impose sparsity smoothness localized structure rf coefficient develop novel regularization method exploit fact neural rfs model lowrank matrix tensor approach justify observation rfs describe sum small number separable filter substantially reduce number rf parameter rank p receptive field dimension require parameter single separable filter spatial coefficient temporal coefficient ie temporal unit vector commonly occur experimental setting parametrization yield considerable saving statistic literature problem estimate lowrank matrix regression coefficient know reduce rank regression problem receive considerable attention econometric literature bayesian formulation tend focus informative prior formulate novel prior reduce rank regression use restriction matrix normal distribution manifold lowrank matrix result gaussian prior rf coefficient equal ar gaussian prior lineargaussian response model posterior rf row column conditionally lead fast efficient samplingbase inference method use localized form row column covariance matrix normal prior hyperparameter govern smoothness locality rf component space time addition fully bayesian samplingbase inference develop fast approximate inference method use coordinate ascent conditional marginal likelihood temporal column spatial row hyperparameter apply method lineargaussian encoding model good performance neural paper organize follow describe lowrank model localized prior describe fully bayesian inference method use blockedgibb sample step introduce fast method approximate inference use conditional empirical bayesian hyperparameter estimate extend estimator poisson encode model finally application simulated real neural dataset hierarchical lowrank receptive field response model likelihood begin define probabilistic encoding model provide likelihood function inference let denote number spike occur response dt dx matrix stimulus denote number temporal spatial element respectively let denote neuron dt dx matrix receptive field consider linear gaussian denote vectorize stimulus vectorize respectively variance response noise bias term second consider denote nonlinearity example g include exponential soft function rise concave loglikelihood prior low rank receptive field represent rank p use factorization column matrix r p contain spatial filter p contain temporal filter column matrix define prior matrix use restriction matrix normal distribution cx ct prior write cx z involve integration space matrix know closedform expression prior control column covariance matrix ct row covariance matrix govern temporal spatial rf component respectively express factorize form eq rewrite prior cx tr formulation clear conditionally prior cx ct ct cx t cx denote kronecker product r kx define define parametric form control hyperparameter respectively form adopt automatic locality determination prior introduce prior covariance matrix encode tendency localize spatiotemporal frequency spatial covariance matrix cx hyperparameter s s scalar determine overall scale covariance vector specify center location rf support space spatial frequency respectively number spatial dimension d standard visual pixel stimuli positive definite matrix d determine size local region rf support space spatial frequency respectively temporal covariance matrix ct hyperparameter directly analogous determine localized rf structure time temporal finally place gaussian prior scalar bias term posterior inference use markov complete dataset d design matrix vector response goal infer joint posterior pk b develop efficient markov chain sampling method use blockedgibb sample blockedgibb sample possible closedform conditional prior likelihood yield closedform conditional marginal likelihood d kt t d respectively blockedgibb sample t conditional evidence simultaneously sample conditional posterior sample t b sample similarly sample conditional evidence use sample low dimensional space hyperparameter sample use closedform formula introduce shortly mean conditional posterior detail algorithm follow step sample kx draw sample t b kt t b d b kxi xi b kxi xi section fix likelihood extension describe divide sample b conditional posterior b d pt b kt t b wt vector t mx concatenation vector matrix generate project stimulus stack row mean ith row block diagonal matrix diagonal b ct use standard formula product gaussian obtain close form conditional evidence t b exp t t t mean covariance conditional posterior wt mx t use search low dimensional hyperparameter space conditional evidence eq target distribution uniform b sample b conditional posterior step ith sample t b draw sample kt b pxi kt t b b kt b divide sample conditional posterior kt t b d px t z px b t cx matrix generate project stimulus stack row mean ith row use standard formula product gaussian obtain close form conditional evidence kt b cx exp x x x b mean covariance conditional posterior b step uniform conditional evidence target distribution sample kx conditional posterior summary algorithm omit sample index notational fully inference use blockedgibb sample datum d condition sample variable iterate follow sample b t conditional evidence t b conditional posterior b sample conditional evidence eq conditional posterior convergence approximate algorithm fast posterior inference develop alternative approximate algorithm fast posterior inference instead integrate hyperparameter attempt find point estimate maximize conditional marginal likelihood resemble empirical bayesian inference hyperparameter set maximize marginal likelihood model evidence close form conditional evidence t b conditional evidence t b close form eq alternate maximize conditional evidence set t b find map estimate maximize conditional evidence set x find map estimate kx b b t t max b bkt approximate algorithm work conditional evidence tightly maximum note hyperparameter fix iterative update alternate coordinate ascent posterior b extension likelihood likelihood nongaussian blockedgibb sample tractable close form expression conditional evidence introduce fast approximate lowrank rf model likelihood basic step approximate gaussian approximation conditional posterior b laplace approximation approximate conditional evidence t b posterior mode detail follow conditional evidence t b proportional conditional posterior wt approximate gaussian distribution laplace approximation t t b d t conditional map estimate wt obtain numerically maximize conditional posterior use newton method log b t t covariance conditional posterior obtain second derivative log conditional posterior mode hessian negative logt t likelihood denote t ml true gibb fast space sample time fullrank sample gibb training datum figure simulate datum datum generate rank rf pixel parameter fullrank model rank model true rank leave estimate obtain approximate method blockedgibb sample use sample use sample respectively average mean square error rf estimate method average independent repetition log conditional evidence log eq posterior t simply mode log b t log log maximize set t b space limit omit derivation conditional posterior conditional evidence b appendix result simulation test performance blockedgibb sample fast approximate simulate rank temporal bin spatial pixel fig compare method maximum likelihood estimate fullrank lowrank estimate obtain blockedgibb sample approximate algorithm perform similarly achieve low mean squared error estimate linear b sample ml sample training datum figure simulate datum datum generate poisson rank fig nonlinearity estimate obtain approximate method linear gaussian model method model use sample respectively average mean square error rf estimate independent repetition estimate model perform test performance method simulate rf nonlinearity estimate use method linear gaussian model model fig lowrank rank gibb relative likelihood stimulus rank space gibb rank space rank simple cell time rank fast time simple cell relative likelihood stimulus rank figure comparison estimate simple cell use white noise bar stimuli relative likelihood test stimulus leave lowrank rf estimate different rank right relative likelihood ratio test likelihood rank estimate use minute training datum rank estimate obtain blockedgibb sample approximate method achieve high test likelihood estimate row achieve high test likelihood noise add lowrank rank increase estimate row relative likelihood rank similar plot simple cell estimate obtain blockedgibb sample approximate method achieve high test likelihood cell relative likelihood rank estimate perform fullrank estimate regardless model estimate model achieve low mse application neural datum apply method estimate rfs simple cell retinal ganglion cell detail datum collection describe perform fold crossvalidation use minute training minute test datum fig fig average test likelihood function rf rank linear gaussian model estimate obtain method lowrank pp ui singular value ui leave compute right singular vector respectively stimulus distribution nongaussian large bias spatial extent temporal extent relative likelihood stimulus gibb rank rd spatial extent temporal extent lowrank gibb relative likelihood stimulus figure comparison estimate retinal datum use binary white stimuli consist spatial pixel temporal bin rf coefficient relative likelihood test stimulus leave left singular vector middle right singular vector right estimate rf cell samplingbase estimate benefit rank representation use distinct spatial temporal component performance degrade rank relative likelihood rank b similar plot cell relative likelihood rank estimate perform good b ml fullrank minute training datum runtime time prediction error minute training datum figure rf estimate v simple cell datum rf estimate obtain ml leave lowrank blockedgibb sample linear approximate model right different training datum min consist temporal spatial dimension rf coefficient average prediction spike count error subset available datum lowrank estimate model achieve low prediction error method runtime method approximate algorithm fullrank inference method time long finally apply method estimate v simple cell different training datum minute compute prediction error estimate linear gaussian model fig estimate use min training datum compute test likelihood estimate set rf rank find rank estimate achieve high test likelihood term average prediction error estimate obtain fast approximate algorithm achieve low error runtime algorithm significantly low fullrank inference method conclusion describe new hierarchical model rfs introduce novel prior matrix base restrict matrix normal distribution feature preserve gaussian prior regression coefficient use localized form define row column covariance matrix matrix normal prior allow model learn smooth sparse structure rf spatial temporal component develop inference method exact base mcmc blockedgibb sample approximate base alternate evidence optimization apply model neural datum use poisson noise model find poisson model perform increase approximate inference overall find lowrank estimate achieve high prediction accuracy significantly low computation time compare fullrank estimate believe lowrank model especially useful highdimensional setting particularly case stimulus covariance matrix fit memory future work develop fully bayesian inference method lowrank rfs noise model allow quantify accuracy approximate method secondly examine method infer rf rank number separable component determine automatically datum acknowledgment thank c v datum e m retinal datum work support sloan research fellowship award nsf career award ii reference estimate spatiotemporal receptive field auditory visual neuron response natural stimulus network computation neural system smyth b organization simple cell primary visual cortex natural scene stimulation journal neuroscience m evidence optimization technique estimate function nip predict neuronal response natural vision network computation neural system m park e receptive field inference localize prior plo m structure receptive field area ai auditory neurophysiology schreiner gabor analysis auditory field composition journal neurophysiology e p simoncelli dimensionality reduction model generalization average covariance analysis journal vision m e p e j simoncelli spatiotemporal correlation visual complete neuronal population regression multivariate linear model multivariate analysis c regression theory application reduce rank regression distribution theory notational consideration bayesian application biometrika l maximum likelihood estimation cascade neural encoding model network computation neural system m park bayesian active learning localize prior fast receptive field characterization nip page c schwartz spatiotemporal element macaque receptive
cotraining expansion bridge theory computer science dept computer science dept computer science dept abstract cotraining method combine label unlabeled datum example think contain distinct set feature number practical success previous theoretical analysis need strong assumption datum unlikely satisfied paper propose weak expansion assumption underlie datum distribution prove sufficient iterative cotraining succeed appropriately strong feature set extent necessary expansion assumption fact motivate iterative nature original cotraining algorithm strong assumption independence label allow simple oneshot cotraining succeed heuristically analyze effect performance noise datum predict behavior qualitatively match synthetic experiment introduction machine learn case unlabeled data substantially cheap label datum result number method develop use unlabeled datum try improve performance eg cotraine method substantial success scenario example think contain distinct sufficient feature set specifically label example form x example label far assume existence function c c respective feature set c intuitively mean example contain view view contain sufficient information determine label example redundancy imply underlie structure unlabeled datum need consistent structure unlabeled datum informative particular idea iterative cotraining use small label sample train initial classifier h respective view iteratively bootstrap unlabeled example hi confident use confident label example learning view improve classifier example webpage classification webpage contain text point small label sample learn classifier h link word point page page probably positive example find unlabeled example property use label page learning use text page approach variant use variety learn problem include entity classification text classification natural language process large scale document classification visual detector cotraine effectively require distinct property underlie datum distribution order work principle exist low error classifier c view second view hand highly correlated need example confident vice versa cotraining algorithm actually unfortunately previous theoretical analysis need strong assumption second type order prove guarantee include conditional independence label use assumption weak rule dependence use primary contribution paper theoretical analysis substantially relax strength second assumption form expansion underlying distribution natural analog notion expansion conductance sense necessary condition cotraining succeed need fairly strong assumption learn algorithm produce confident wrong formally algorithm able learn positive datum heuristic analysis case hold key feature assume expansion data specifically motivate iterative nature cotraining algorithm previous assumption analyze imply strong form expansion oneshot version cotraining succeed section fact theoretical guarantee exactly type distribution easily satisfy weak condition allow oneshot learn work describe natural situation form additional property result algorithmic nature sufficiently strong efficient algorithm target function feature set use achieve efficient guarantee cotraining mention need strong assumption base learn algorithm use section begin formally define expansion assumption use connect standard notion expansion conductance prove statement expansion sufficient iterative cotraining succeed strong base learn algorithm view prove bound number iteration need converge section heuristically analyze effect feature set cotraine accuracy finally section present experiment synthetic graph datum qualitatively bear notation definition assumption assume example draw distribution d instance space correspond different view example let denote target function let denote positive negative region x respectively simplicity assume binary classification paper assume view sufficient correct classification c decompose function c c view d probability mass example c let think let xi xi xi let d d denote marginal distribution respectively order discuss iterative cotraining need able hypothesis confident confident example convenience identify confident confident positive mean think hypothesis subset xi mean confident xi positive mean opinion abstract away initialization phase cotraine label data use generate initial hypothesis assume initial set s s init init goal cotraining bootstrap set use unlabeled datum prove guarantee iterative cotraining assumption learn use view able learn positive datum distribution d expand define section assumption base learn algorithm view assume learn algorithm view able positive datum specifically distribution di access example algorithm able produce hypothesis xi error b probability error algorithm type naturally think predict positive confidence know fit framework example concept class learnable positive datum include conjunction rectangle instance case rectangle simple achieve guarantee output small rectangle enclose positive example want consider algorithm confident direction confident positive instead use notion reliable useful learn sloan class function learnable manner addition nice feature assumption need d expand especially natural positive class large consist document topic negative eg document topic note effectively assume algorithm correct confident relax heuristic analysis section expansion assumption underlie distribution s let boldface denote event example xi think s confident set view prs denote probability mass example confident view prs denote probability mass example confident section probability respect definition d expand s prs min prs prs d expand respect hypothesis class h h hold s s h denote xi set xi h definition notice expansion sense necessary iterative cotraining succeed s confident set expand example hypothesis help section definition fact sufficient weak definition requirement helpful consider slightly strong kind expansion expansion definition s prs prs s prs prs expansion require pair expand strictly necessary pair expand pair rare unlikely encounter confident set cotraining process ok d hold index reverse finally d property immediately obvious expansion fact imply definition converse necessarily true introduce notion reason useful intuition si confident set set small train classifier learn positive datum conditional distribution si induce error distribution definition imply confident set large probability clear useful cotraine initial stage secondly notion help clarify assumption restrictive consider previously specifically independence label independence label imply s prs prs prs prs prs tiny mean expand factor fact expand nearly weak dependence weak dependence relaxation conditional independence require s prs prs restrictive notice prs prs imply definition weak dependence prs prs sufficiently small small expand nearly mean conditional independence algorithm positive datum train conditional distribution drive error conditional distribution perform cotraine iteration connection standard notion expansion definition expansion definition natural analog standard notion conductance high conductance stationary distribution set state probability probability mass transition exit time probability graph high random walk graph high conductance stationary distribution walk view equal probability edge equivalent partition graph piece s s number edge cross partition fraction number edge small half connect definition think wellknown example random degree bipartite graph high probability expand fact motivate synthetic datum experiment section example simple example satisfy expansion weak dependence example suppose rd rd target function view suppose random positive example d look pair uniformly distribute rectangle way specifically identical x random coordinate rectangle distribution satisfy weak dependence set disjoint axis hard verify d expand example imagine learning problem datum fall n different cluster positive class union cluster negative class union imagine likewise true look simplicity suppose cluster probability mass independence label positive cluster ci equally likely positive cluster cj suppose weak ci associate cj ie cj distribution clearly weak dependence property learning algorithm assume cluster label hypothesis space consist rule split cluster graph cluster associate graph distribution expand respect particular label example learning generalize entire cluster propagate node associate cluster cj main result present main result assume d expand respect hypothesis class h h initial confident set s s prs init target function write c c c h view algorithm learn positive datum iterative cotraining consider proceed round let si x si confident set view start round construct si feed example accord condition unlabeled example d current predictor confident feed positive run error confidence parameter simultaneously create si number round specify terminate output predictor label example positive sn sn negative begin state lemma useful analysis let t s t sj probability respect suppose pr t pr proof s t pr follow expansion property pr t pr imply t pr suppose let pr s t pr s proof s s t pr follow expansion property pr pr imply t t let final desire accuracy confidence parameter achieve error rate probability run cotraine n log init round time run accuracy confidence parameter set respectively proof sketch assume x confident set view step cotraine define probability respect interested bound technically easy bind instead pn probability obviously imply good guarantee round probability si si particular imply probability p pr pr init consider probability use lemma obtain probability si similarly apply lemma obtain qi probability pr assume case learning algorithm successful round note happen probability observation imply long pi init mean n init iteration cotraine situation point notice round drop factor total log init round predictor desire accuracy desire confidence heuristic analysis error propagation experiment far assume existence perfect classifier view example x x addition assume positive example input learning able generalize way error ie confident wrong section heuristic analysis case assumption relax synthetic experiment heuristic analysis error propagation confident set x ith iteration let define precision coverage recall covi si let define opposite coverage previously assume imagine fraction example view disagree positive negative region expand uniformly rate initially natural assume follow form increase cov min covi covi covi accuracy negative accuracy positive overall accuracy accuracy negative accuracy positive overall accuracy iteration iteration accuracy negative accuracy positive overall figure cotraine noise rate respectively solid line indicate overall accuracy dash increase curve accuracy positive covi red dash decrease curve accuracy negative correspond positive negative confident region expand way proof fraction new edge example label examine simple observation initially coverage low o step roughly cov cov cov expect coverage increase exponentially drop linearly coverage large begin saturate high time begin drop rapidly exponential increase cause covi particular calculation omit positive negative overall accuracy increase point drop qualitative behavior bear experiment experiment perform experiment synthetic datum line example noise add section specifically create bipartite graph nod represent positive cluster node represent negative cluster connect node left node right neighbor choose probability random node class probability random node opposite class begin initial confident set propagate confidence round cotraine monitor percentage positive class cover percent negative class cover overall accuracy plot experiment figure different noise rate qualitatively match expect coverage increase exponentially accuracy negative drop exponentially somewhat delay point crossover predict roughly correspond point overall accuracy start drop conclusion cotraine method use unlabeled datum example partition view view roughly sufficient achieve good classification view highly correlated previous theoretical work require instantiate condition b strong sense independence label form weak dependence work argue right condition weak expansion property underlie distribution positive example sufficient extent necessary expansion property especially interesting directly motivate iterative nature practical cotraining base algorithm work rigorous analysis iterative cotraining setting demonstrate advantage acknowledgement work support nsf grant ii reference s bootstrappe proceeding th annual meeting association computational linguistic page blum m mitchell combine label unlabeled datum cotraining proc annual conference computational learning theory page m collin singer unsupervised model entity classification empirical method nlp large page dasgupta m l littman mcallester pac generalization bound cotraining advance neural information processing system mit press r combine label unlabeled datum text classification large number category proceeding ieee international conference datum mining joachim inference text classification use support vector machine proceeding th international conference machine learn page m kearn m valiant learn freund unsupervised improvement visual detector use cotraine computer vision page r p randomize cambridge university press r analyze effectiveness applicability cotraining int information knowledge management page mccallum thrun m mitchell text classification label unlabeled document use machine learn park large scale unstructured document classification use unlabeled data syntactic information lnc vol page springer c limitation cotraine natural language learn large dataset proc conference empirical method nlp page r r sloan learn complicated concept reliably proceeding workshop computational learning theory page unsupervised word sense supervise method meeting association computational linguistic page lafferty semisupervise learning use gaussian field harmonic function machine learn page relate definition definition imply definition theorem satisfie expansion definition satisfy expansion definition proof prove suppose exist s prs min prs prs assume loss generality prs prs prs prs prs follow prs assume prs prs imply prs prs prs prs prs prs notice prs prs prs prs prs prs prs prs prs prs prs similarly prs prs failure expansion direction complete proof
analysis improvement policy gradient estimation abstract policy gradient useful modelfree reinforcement learning approach tend suffer instability gradient estimate paper analyze improve stability policy gradient method prove variance gradient estimate pgpe policy gradient exploration method small classical reinforce method mild assumption derive optimal baseline pgpe contribute far reduce variance theoretically optimal baseline preferable reinforce optimal baseline term variance gradient estimate finally demonstrate usefulness improve pgpe method experiment introduction goal reinforcement learning rl find optimal policy maximize return ie sum discount reward interaction unknown environment modelfree rl flexible framework decisionmake policy directly learn explicit modeling environment policy iteration policy search popular formulation modelfree policy iteration approach value function estimate policy determine base learn value function policy iteration demonstrate work realworld application especially problem discrete state action policy iteration naturally deal continuous state function approximation continuous action hard handle difficulty find value function respect action policy indirectly determine value function approximation value function model lead inappropriate policy simple problem limitation policy iteration especially physical control task control policy vary drastically iteration cause severe instability physical system favorable policy search approach modelfree rl overcome limitation iteration policy search approach control policy directly learn return maximize example gradient method reinforce method natural gradient method gradientbased method particularly useful physical control task policy change gradually ensure stability physical system reinforce method tend large variance estimation gradient direction naive implementation converge slowly optimal ease problem extent variance gradient estimate large furthermore performance heavily depend choice initial policy appropriate initialization straightforward practice cope problem novel policy gradient method policy gradient exploration pgpe propose recently pgpe initial policy draw prior probability distribution action choose deterministically construction contribute mitigate problem initial policy choice stabilize gradient estimate subtract movingaverage baseline variance gradient estimate far reduce experiment pgpe demonstrate achieve stable performance exist method goal paper theoretically support usefulness pgpe far improve performance specifically bound gradient estimate reinforce pgpe method theoretical analysis gradient estimate pgpe small variance reinforce mild condition baseline pgpe adopt original paper excess variance optimal baseline pgpe minimize variance follow line far theoretically pgpe optimal baseline preferable reinforce optimal baseline term variance gradient estimate finally usefulness improve pgpe method demonstrate experiment policy gradient reinforcement learn section review policy gradient method problem formulation let consider markov decision problem specify s pi r set dimensional continuous state set continuous action transition probability density current state s state s action probability initial state immediate reward transition action discount factor future reward let stochastic policy parameter represent conditional probability density action state let s trajectory length return ie discount sum future reward t t rst expect return parameter define ph ps pst goal reinforcement learning find optimal policy parameter maximize expect return review reinforce algorithm reinforce policy parameter update ascent small positive constant gradient r log log use socalled log trick log unknown expectation approximate empirical average log sn sample let employ policy model parameter mean standard deviation policy gradient explicitly log s log drawback reinforce variance policy gradient large lead slow convergence review pgpe algorithm reason large variance policy gradient reinforce empirical average time step cause stochasticity policy order mitigate problem method policy gradient exploration pgpe propose recently pgpe linear deterministic policy adopt stochasticity introduce consider prior distribution policy parameter hyperparameter p entire history solely determine single sample parameter formulation expect variance gradient estimate reduce expect return hyperparameter express differentiate respect log trick p use approximate expectation empirical average pn log trajectory sample draw parameter draw let employ prior distribution hyperparameter draw vector mean vector vector consist standard deviation element exp derivative log p respect follow log p log p variance gradient estimate section theoretically investigate variance gradient estimate reinforce multidimensional state space consider trace covariance matrix gradient vector random vector define p tr e m e e denote expectation let b p dimensionality state consider subset follow assumption assumption rs s assumption rs s assumption c exist series ct hold probability respectively choice path denote norm note assumption b strong assumption let ct t ct dt t analyze variance gradient estimate pgpe proof theorem provide supplementary theorem assumption follow upper bound t t b b var var j b proportional upper mean upper bind variance bind square reward trace inverse gaussian covariance t b sample size upper bind variance twice large infinity converge analyze variance gradient estimate theorem assumption b c following lower bind probability b var assumption c follow upper bind probability h var dn assumption upper bound reinforce similar pgpe monotone increase b nontrivial respect trajectory length lower bind variance positive satisfy ct derive lower bind variance leave open future work finally compare variance gradient estimate reinforce theorem addition assumption b c assume positive monotone increase respect t exist t var j t t probability mean pgpe favorable reinforce term variance gradient estimate mean trajectory length t large theoretical result partially support experimental success pgpe method variance reduction subtract section method reduce variance gradient estimate pgpe analyze theoretical property basic idea introduce baseline know variance gradient estimate reduce subtract baseline b reinforce pgpe modify gradient estimate log log adaptive reinforcement baseline derive exponential average past base empirical gradient estimate propose reinforce pgpe movingaverage baseline contribute reduce variance gradient estimate movingaverage baseline optimal optimal baseline definition minimizer variance gradient estimate respect baseline follow formulation optimal baseline reinforce follow arg log log movingaverage baseline introduce pgpe far suboptimal derive optimal baseline pgpe study theoretical property optimal baseline pgpe let optimal baseline pgpe minimize follow optimal baseline optimal baseline pgpe log log excess variance baseline b log pk expression optimal baseline pgpe expect return square norm characteristic eligibility independent optimal baseline reduce average expect return optimal baseline generally different average expect return excess variance proportional squared difference baseline expect square norm characteristic eligibility log sample size analyze contribution optimal baseline variance respect mean parameter rs following lower bind b b assumption follow upper bind b theorem low upper bound excess variance proportional bound squared immediate reward trace inverse sample size n t infinity converge comparison reinforce analyze contribution optimal baseline reinforce compare pgpe excess variance baseline b reinforce reinforce base follow theorem assumption b c follow bound probability var low upper bound excess variance monotone increase respect trajectory length aspect reduction variance gradient estimate theorem theorem optimal baseline reinforce contribute finally base theorem base theorem follow theorem assumption b c var ct inequality hold probability upper bind variance gradient estimate reinforce optimal baseline monotone increase respect trajectory length hand t upper bind variance gradient estimate optimal baseline far var independent t trajectory length large variance gradient estimate reinforce optimal baseline significantly large variance estimate pgpe optimal baseline experiment section experimentally investigate usefulness propose method pgpe optimal baseline illustrative datum let state space s onedimensional continuous initial state randomly choose standard normal distribution action space set onedimensional continuous transition dynamic environment set stochastic noise immediate reward define r bound discount factor set compare follow method reinforce baseline reinforce optimal baseline pgpe baseline pgpe movingaverage baseline mb pgpe optimal baseline fair comparison method use parameter setup mean standard deviation gaussian distribution set number episodic sample set length trajectory set t calculate variance gradient estimate run table summarize result variance reinforce overall large pgpe notable difference reinforce pgpe variance reinforce table variance bias estimate gradient illustrative datum t bias reinforce pgpe method pgpe iteration c reinforce pgpe good initial policy reinforce return number episode variance logscale variance logscale b pgpe pgpeob iteration reinforce iteration reinforce pgpeob return variance log scale variance log scale bias iteration pgpeob figure variance gradient estimate respect mean parameter iteration illustrative datum number episode poor initial policy figure return function number episodic sample illustrative datum significantly grow t increase pgpe influence t agree theoretical analysis section result variance pgpeob propose method small contribute highly reduce variance especially large agree theory provide small variance investigate bias gradient estimate method regard gradient estimate n true gradient compute bias gradient estimate result include table introduction baseline increase bias tend reduce bias investigate variance gradient estimate policy parameter update iteration experiment set t variance compute run policy update iteration order evaluate variance stable manner repeat experiment time random choice initial mean parameter investigate average variance gradient estimate respect mean parameter trial log scale result summarize figure figure compare variance reinforce baseline figure b compare variance pgpe baseline plot introduction baseline contribute highly reduction variance iteration figure c compare variance reinforce pgpe baseline pgpe provide stable gradient estimate reinforce figure compare variance reinforce pgpe optimal baseline gradient estimate obtain pgpeob small overall term variance gradient estimate propose pgpeob compare favorably method evaluate return obtain method trajectory length fix t maximum number iteration set investigate average return run function number episodic sample experimental result different initial policy figure result initial mean parameter choose randomly tend perform graph pgpeob perform good especially follow small margin plain pgpe work reasonably slightly unstable large variance plain reinforce highly unstable cause huge variance gradient estimate figure figure b describe result initial mean parameter choose randomly tend result poor performance setup difference compare method significant case good initial policy overall plain reinforce perform poorly tend outperform method mean reinforce sensitive choice initial policy pgpe method propose pgpeob work converge quickly cartpole finally evaluate performance propose method complex task balance pole cart goal pole cart properly try pole state space twodimensional continuous consist angle angular velocity pole action space onedimensional continuous correspond force apply cart note directly control pole indirectly cart use gaussian policy model reinforce linear policy model pgpe state s transform feature space basis function vector use gaussian standard deviation basis function kernel center distribute following grid point dynamic pole update rule angle angular velocity t t t cost action time t set problem parameter mass cart kg mass pole kg length pole m set time step t position velocity update action selection reward function define rst cost high pole reward obtain initial policy choose randomly probability density set uniform agent collect episodic sample trajectory discount factor set investigate average return trial function iteration return trial compute test episodic sample use policy learn experimental result plot figure improvement plain reinforce tend slow pgpe method outperform reinforce method overall pgpe method propose pgpeob converge fast conclusion paper analyze improve stability policy gradient method pgpe policy gradient exploration theoretically mild condition pgpe provide stable gradient estimate classical reinforce method derive optimal baseline pgpe theoretically pgpe optimal baseline preferable reinforce optimal baseline term variance gradient estimate finally demonstrate usefulness pgpe optimal baseline experiment return reinforce figure performance policy acknowledgment support hh support program support reference c c l p r m m t optimize collection use constrain reinforcement learning proceeding th conference knowledge discovery datum mining page baxter p bartlett l experiment artificial intelligence research m nonlinear stabilize control invert pendulum system proceeding ieee region volume page p dayan e hinton use expectationmaximization reinforcement learning neural computation e bartlett baxter variance reduction technique gradient estimate reinforcement learning machine learn research p m l littman moore reinforcement learn survey artificial intelligence research kakade natural policy gradient becker editor advance neural information processing system page cambridge mit press m g r leastsquare policy iteration machine learn research p approximate gradient method reward process discrete event dynamic system s schaal policy gradient method robotic processing international robot t j schmidhuber policy gradient exploration control proceeding conference artificial neural network page t schmidhuber policy gradient neural network r reinforcement learn introduction mit program achieve play neural computation l baxter reinforcement learn state temporal difference technical report computer science australian l tao optimal reward baseline gradientbased reinforcement learn processing conference uncertainty artificial intelligence page young partially observable markov decision process system computer speech language r simple statistical algorithm connectionist reinforcement learning machine learn
hierarchical coattention visual question answer number recent work propose attention model visual question answer vqa generate spatial map highlight image region relevant answer question paper argue addition modeling look visual attention equally important model word question attention present novel coattention model vqa jointly reason image question attention addition model reason question consequently image coattention mechanism hierarchical fashion novel dimensional convolution neural model improve stateoftheart vqa dataset cocoqa dataset use resnet performance far improve vqa cocoqa introduction visual question answer vqa emerge prominent research problem industry correctly answer visual question image machine need understand image question recently visual attention base model explore vqa attention mechanism typically produce spatial map highlight image region relevant answer question far attention model vqa literature focus problem identify look visual attention paper argue problem identify word question attention equally important consider question horse image horse image meaning essentially capture word machine attend word arguably robust linguistic variation irrelevant meaning answer question motivate observation addition reasoning visual attention address problem question attention specifically present novel multimodal attention model follow unique feature coattention propose novel mechanism jointly reason visual attention question attention refer coattention previous work focus visual attention model natural symmetry image question sense image representation use guide question attention question representation use guide image attention question hierarchy build hierarchical architecture image question level word level b phrase level question level word level embed word vector space embed matrix phrase level dimensional convolution neural network use capture information contain trigram source code download conference neural information processing system nip answer color stop light light color stop light light stop light color color stop light image color stop light color stop light light stop figure propose hierarchical coattention model question extract word level phrase level question level embedding level apply coattention image question final answer prediction base image question feature specifically convolve word representation temporal filter vary support combine response pool single phrase level representation question level use recurrent neural network encode entire question level question representation hierarchy construct joint question image coattention map combine recursively ultimately predict distribution answer overall main contribution work propose novel coattention mechanism vqa jointly perform visual attention question attention explore mechanism strategy parallel alternate coattention describe propose hierarchical architecture represent question consequently construct coattention map different level word level phrase level question level feature recursively combine word level question level final answer prediction phrase level propose novel strategy adaptively select phrase size representation pass question level representation finally evaluate propose model large dataset vqa perform ablation study quantify role different component model relate work recent work propose model vqa compare relate propose coattention mechanism vision language attention mechanism literature image attention instead directly use holistic embed fully connected layer deep number recent work explore image attention model vqa add spatial attention standard lstm model point ground propose scheme consist language number neural module network language predict module network instantiate answer question work perform image attention multiple time stack manner author propose stack attention network run multiple infer answer progressively capture information question propose image attention scheme align word image patch refer entire question obtain image attention map second author generate image region object proposal select region relevant question answer choice augment dynamic memory network new input fusion module retrieve answer attention base concurrent work collect human attention map use evaluate attention map generate attention model vqa note approach model visual attention model question attention model attention sequentially later attention base early attention prone error propagation contrast conduct coattention level independently language attention prior work explore question attention vqa relate work natural language processing nlp general model language attention order overcome difficulty translation long sentence propose learn alignment input sentence author propose attention model circumvent bottleneck cause fix width hide vector text reading attention mechanism propose author employ neural attention mechanism reason sentence focus modeling sentence pair author propose jointly perform attention hierarchy work attention scheme propose evaluate author propose attention mechanism project pair input common representation space method begin introduce notation use paper ease understand model describe hierarchical question representation describe propose coattention mechanism describe finally recursively combine attend question image feature output answer notation question t word representation denote q qt qt feature vector tth word denote word embed phrase embed question embed position t respectively image feature denote feature vector spatial location coattention feature image question level hierarchy denote r p s weight different denote appropriate necessary follow omit bias term b avoid notational clutter question hierarchy encoding question word q qt embed word vector space learn endtoend compute phrase feature apply convolution word embed vector word location compute inner product word vector filter window size trigram tth word convolution output window size s weight parameter feature appropriately feed trigram convolution maintain length sequence convolution convolution result apply maxpoole different word location obtain feature qt qt t pool method differ use previous work adaptively select different gram feature time step preserve original sequence length order use encode sequence maxpoole correspond feature lstm hidden vector time hierarchical representation question depict fig coattention propose coattention mechanism differ order image question attention map generate mechanism parallel coattention generate q aq q q q s c image image b figure parallel coattention mechanism b alternate coattention mechanism image question attention simultaneously second mechanism alternate coattention sequentially alternate generate image question attention fig coattention mechanism execute level question hierarchy parallel coattention parallel coattention attend image question simultaneously similar connect image question calculate similarity image question feature pair specifically image feature map question representation q affinity matrix calculate rdd contain weight compute affinity matrix possible way compute image question attention simply maximize affinity location modality maxi aq instead choose activation find performance improve consider affinity matrix feature learn predict image question attention map follow h q h q t h aq rk weight parameter av rn aq attention probability image region word respectively affinity matrix c transform question attention space image attention space vice versa t base attention weight image question attention vector calculate weighted sum image feature question feature q qt parallel coattention level hierarchy lead p alternate coattention attention mechanism sequentially alternate generate image question attention briefly consist step mark summarize question single vector q attend image base question summary q attend question base attend image feature ax g image question define attention operation feature x attention guidance g derive question image input output attend image question vector operation express follow step gt ai xi encode lter pool layer layer lter dierent width word embed color s p p figure hierarchical question encode b encoding predict answer vector element rk parameter ax attention weight feature alternate coattention process illustrate fig b step alternate coattention q second step image feature guidance intermediate attend question feature s step finally use attend image feature v guidance attend question ie q g similar parallel coattention alternate coattention level hierarchy encode predict answer follow treat vqa classification task predict answer base image question feature level use multilayer perceptron mlp recursively encode attention feature q p qs p weight parameter concatenation operation vector p probability final answer experiment dataset evaluate propose model dataset vqa dataset cocoqa dataset vqa dataset large dataset problem contain human annotate question answer dataset dataset contain training question validation question testing question total pair accord include number question answer use answer possible output similar set answer cover answer testing train model vqa report result evaluation server use evaluation protocol experiment dataset automatically generate dataset train question test question dataset question base image respectively type question include object number color location type dataset respectively answer datum set single word report classification accuracy similarity table table result vqa dataset indicate result available openende method num num lstm setup use torch develop model use optimizer base learning rate momentum e set batch size train epoch early stop validation accuracy improve epoch cocoqa size hide layer set vqa large dataset word embed hide layer vector size apply probability layer follow rescale image activation pool layer resnet feature result analysis test scenario vqa openende perform method lstm norm use baseline openende test scenario compare method recent propose multiple choice compare region compare cocoqa use refer parallel coattention oursa alternate table result vqa test set openende setting approach improve state art oursa resnet openende oursa resnet notably question type num achieve improvement openende question question resnet feature outperform match feature case improvement solely use specifically use resnet oursa resnet outperform use rest use outperform table result cocoqa test set similar result vqa model improve stateoftheart oursa resnet observe parallel coattention perform alternate coattention setup attention mechanism advantage disadvantage parallel coattention hard train dot product image text compress vector single value hand alternate coattention suffer error accumulate round ablation study section perform ablation study quantify role component model specifically retrain approach certain component image attention manner similar previous work use question attention goal comparison verify improvement result orthogonal contribution optimization feature table result cocoqa dataset indicate result available method object number color location accuracy question attention image attention perform conv convolution pooling perform represent phrase instead stack word embed layer word level output word level coattention perform replace word level attention uniform distribution phrase question level coattention model phrase level coattention perform phrase level attention set uniform word question level coattention model question level coattention perform replace question level attention uniform distribution word phrase level coattention model table comparison approach wrt ablation vqa validation set test set recommend use experiment deep lstm norm baseline report comparison improve performance holistic image feature deep lstm norm consistent finding previous attention model vqa compare model wrt version word phrase question level attention table ablation study vqa dataset use clear interesting trend attention close hierarchy tion matter drop accuracy model follow intermediate level method phrase drop finally follow hierarchy ie word image drop accuracy hypothesize question question level close answer prediction layer model note level important final model significantly outperform use linguistic model attention difference model image question attention model improvement bad image attention drop oursa far improve perform alternate coattention round improvement qualitative result visualize coattention map generate method fig word level model attend object region image eg head bird phrase level image attention different pattern image image attention transfer object background region image attention object cause different question type question model capable localize key phrase question essentially discover question type example model pay attention phrase color model successfully attend region image phrase question appropriate answer question eg color bird bird region man hold snow cover man hold snow cover man hold snow cover man hold snow cover color bird color bird color bird color bird formation snow sit formation snow sit formation snow sit formation snow sit figure visualization image question coattention map cocoqa left right original image question pair word level coattention map phrase level coattention map question level coattention map visualization image question attention scale good view color model perform coattention level capture complementary information level combine predict answer conclusion paper propose hierarchical coattention model visual question answer coattention allow model attend different region image different fragment question model question hierarchically level capture information different granularity ablation study far demonstrate role coattention question hierarchy final performance visualization model interpretable region image question predict answer model evaluate visual question answer potentially apply task involve vision language acknowledgement work support career naval p sloan fellowship institute critical technology apply science tech award p national science career award research award office naval research grant research grant support view conclusion contain author interpret necessarily represent official policy express imply government sponsor reference darrell deep question answer neural module network visual question answer machine translation jointly learn align translate r torch environment machine learn nip human attention visual question answer human deep network look region machine dataset method image question answer nip deep residual learning image recognition teaching machine read nip network architecture match natural language sentence nip focus dynamic attention model visual question answer visual connect language vision use crowdsource dense image annotation preprint common object context learn answer question image use convolutional network ask neuron approach answer question image explore model datum image question answer nip reasoning neural attention pool network look focus region visual question answer cvpr zisserman deep convolutional network largescale image recognition deep convolution cvpr dynamic memory network visual textual question answer icml ask attend answer explore spatial attention visual question answer stack attention network image question answer cvpr network modeling sentence pair balance answer binary visual question preprint ground question answer image cvpr measure machine intelligence visual question answer magazine
recursive inversion model permutation abstract develop new exponential family probabilistic model permutation capture hierarchical structure mallow generalize mallow model subclass describe parameter estimation propose approach structure search class model provide experimental evidence add flexibility improve predictive performance enable deep understanding collection permutation introduction probabilistic model permutation model base penalize inversion respect reference permutation prove particularly elegant intuitive useful typically generative model construct permutation stage insert item stage example model generalize mallow model paper propose gmm recursive inversion model rim allow flexibility original gmm preserve elegant useful property compact parametrization tractable normalization constant interpretability parameter essentially gmm construct permutation sequentially stochastic insertion sort process rim construct stochastic merge sort sense rim compactly parametrize independence ri define term inversion independence recursive inversion model interested probabilistic model permutation set element e e use denote permutation total ordering element e use ei denote element order define lower diagonal discrepancy matrix dij capture discrepancy permutation dij argument dij test permutation typically second argument reference permutation typically classic model permutation mallow generalize p mallow model model define term inversion distance ij dij total number inversion mallow mallow model p r note normalization constant depend concentration parameter generalize gmm extend mallow model introduce parameter element e decompose inversion distance element particular define p number inversion element respect dij case gmm define p p exp ve gmm think stagewise model element e insert accord reference permutation list parameter e control likely insertion element e yield inversion respect reference permutation model normalization constant compute closed form rim generalize gmm replace sequence single element insertion sequence recursive merge subsequence relative order subsequence preserve example sequence b c d e obtain merge subsequence b c d e inversion sequence d b e c obtain subsequence inversion rim generate permutation recursively merge subsequence define binary recursive decomposition element e number inversion control separate parameter associate merge operation formally rim set element e e structure represent recursive decomposition set e set parameter represent rim binary tree n e leave associate distinct element e denote set internal vertex binary tree internal vertex represent ir left right subtree control number inversion merge subsequence generate subtree traverse tree left child precede right child induce permutation e reference permutation rim denote rim define term vertex discrepancy number internal vertex p inversion p test permutation ri subset element e appear leave ir left right subtree internal vertex note sum vertex discrepancy internal vertex inversion distance reference permutation finally likelihood permutation respect rim follow p vi example element e c d figure rim preference type reference permutation model c d modal permutation c b sign root vertex test permutation b c note model capture strong preference pair b c d weak preference c ad ac b example set preference capture gmm choose strong preference pair b d induce strong preference d c b differ strength order example apple figure example rim preference apple parameter internal vertex indicate preference item left right indicate preference negative number indicate right item preferable left item naive computation partition function recursive inversion model require sum permutation use recursive structure compute follow note gmm parameterize term parameter fact proposition ri m q q m q q define q extend definition m correspond limit gaussian polynomial q approach note reference permutation modal permutation modal permutation unique note gmm represent use tree structure element reference permutation split remain element time estimate recursive inversion model section present maximum likelihood approach parameter structure estimation observe data d permutation parameter estimation straightforward structure likelihood factor accord structure particular rim product exponential family model internal node consequently negative loglikelihood decompose sum ln p d ln ri ei p d vi represent sufficient statistic node convex function parameter ml estimate obtain numerically solve set univariate minimization problem remainder paper use sum discrepancy matrix observed data respect identity permutation note matrix provide basis efficiently compute sufficient statistic rim remainder section consider problem estimate structure rim observe datum begin brief exploration degree structure rim identify identifiability consider structure rim identify datum previous section know parameter identifiable structure structure rim identify suitable assumption type occur parameter case permutation identifiable switch left right child node change distribution represent rim fact proposition leave right child change distribution sign parameter change proposition let rim e d matrix sufficient statistic internal parameter left right child denote rim obtain switch set p p permutation proposition demonstrate structure rim identify general equivalence class alternative structure distinguish particular type consider rim canonical form proposition provide way canonical form c p input internal node parameter switch leave child right child end end proposition matrix sufficient statistic d rim cal p change loglikelihood proof correctness follow repeat application proposition apply c p output algorithm non identifiability arise parameter generate model equal easy parameter equal value likelihood permutation p likelihood correspond mallow model case identifiable internal structure similarly parameter equal subtree structure subtree identifiable rim locally identifiable iff child rim identifiable unique canonical rim represent distribution follow proposition capture degree identify structure rim proposition rim identifiable iff locally identifiable estimation fixed tractable consider estimation fix reference permutation leave e remainder section assume optimal value internal node available optimization problem describe previous section remain estimate internal tree structure proposition set e permutation e observe datum maximum likelihood rim structure induce compute polynomial time dynamic programming truct proof sketch note onetoone correspondence tree structure represent alternative binary recursive partitioning fix permutation e alternative way permutation e negative loglikelihood decompose accord structure model cost subtree root depend structure subtree furthermore cost decompose recursively sum cost subtree recursion identical recursion optimal matrix chain multiplication problem inside string parse loss generality consider identity e subsequence length l m define variable costj m m m store respectively negative loglikelihood parameter root z optimal tree subsequence value costj m know m value costj l l l obtain recursively exist value maintain m indicate subtree use obtain costj m cost correspond obtain optimal structure parameter find read recursively follow m note loop quantity v algorithm implement optimization truct truct input sample discrepancy matrix d compute observed data m m end l l costj m k m calculate m r m l r calculate m costj m m s costj m costj m m store end end end end sa earch input set e discrepancy matrix d compute observe datum inverse temperature initialize gmm b b t accept false t s truct dp d p reference order truct dp dt p d u accept false end end store accept new model p dt p good good t end end output good evaluate run time truct consider inner loop z l r split l r l apparently l loop compute v time cubic l v summation lr term notice calculation value submatrix size l l element add sum sum number step remove total number addition twice ll number submatrix element estimate score involve compute z score gradient estimation r l operation iteration consider number iteration convergence constant inner loop ol operation n l subsequence length l easy run time truct order local search algorithm develop local search algorithm structure reference permutation provide approach motivate previous work structure estimation mallow model structure permutation problem researcher find approach greedily improve loglikelihood transpose adjacent element couple good initialization effective approximate optimization method approach similar approach treat problem search good reference permutation leverage truct find structure reference permutation high level initialize estimate gmm data d improve local change start rely estimation gmm initialization unfortunately ml estimation mallow model gmm initialization use fast heuristic method estimate mallow model computationally expensive search algorithm approach search space small find provably optimal permutation case return suboptimal result local search variation respect previous work add local optimization step specific class recursive inversion model replace greedy search simulated search generate proposal permutation current second proposal permutation restrict pairwise instead sample permutation current rim t reason pair e e weakly order t happen ordering e e support datum sample process likely create inversion pair conversely t high confidence e e probable ordering support datum reverse propose accept proposal permutation estimate optimal structure optimal parameter structure sample permutation rim apply c p change loglikelihood convert canonical model perform structure optimization step truct dp chance increase loglikelihood experimentally find increase loglikelihood significantly use estimate structure associate parameter sample new permutation step implement earch relate work addition mallow model rim model relate work understand connection work rim model consider restrict rim model parameter value model provide uniform distribution permutation consistent partial order define term binary recursive partition parameter value correspond parallel combination parameter value correspond series combination work consider problem learn structure estimate parameter mixture rim model use local greedy search recursive partition element close connection exist rim model independence model ri propose approach use recursive partitioning set element define distribution permutation rim model ri model define term inversion term independence process ri model require exponentially parameter meath election sushi figure loglikelihood score model gmm difference loglikelihood earch output heldout set meath election datum leave middle set split respectively random replication negative score indicate model low likelihood model obtain earch far outlier meath represent run sa score poorly test set right common structure typical parameter learn datum node contain associate parameter value high value dark green indicate strong ordering item left right subtree leave different type sushi rim model fact model define general distribution grow exponentially cardinality left right set element addition model ease interpretation rim model instance easily extract reference permutation modal permutation ri model comparison alternative ri model ri model structure limit comparison rank marginal fouri coefficient worth note wide range approach use multiple reference permutation benefit approach enable model capture multimodal distribution permutation example approach include mixture modeling approach discuss work lafferty model weighted product set mallow model reference order natural consider mixture product rim model experiment perform experiment synthetic datum realworld datum set synthetic experiment find approach typically able identify structure parameter generative model specifically run extensive experiment choose model structure vary degree balance choose parameter randomly choose use rim generate dataset contain vary number permutation investigate true model recover find model recoverable high probability use earch iteration find identification correct tree structure typically require large sample size note failure identify correct structure typically fact alternative structure high likelihood generate structure particular sample failure search algorithm experiment run time algorithm instance truct run second domain item small domain datum search accept proposal typically run minute particular search fast b search model experiment realworld datum set examine dataset data set house election meath use single vote election system rank candidate date election run candidate associate major political party number small party use roughly fully rank election detail dataset second dataset consist permutation different type permutation capture preference different type consider sake compare set alternative recursive inversion model approach identify structure baseline approach denote reference permutation fix estimate optimal structure order truct second approach gmm use b b algorithm estimate generalize mallow model approach fit optimal rim parametrization hierarchical tree structure identify datum finally search structure ordering earch iteration temperature quantitative result figure plot difference test loglikelihood model compare earch meath datum sa earch outperform run gmm sushi datum earch superior gmm high likelihood gmm run training set earch good fit investigate structure parameter learn model meath datum find significant variation learn structure run number common learn model similar finding structure learn independence model find candidate party typically separate candidate party group addition political cluster find systematic preference ordering candidate tree find tree addition find find single candidate extreme political party typically split hierarchy indicate candidate insert ranking inability gmm capture dependency lead poor empirical performance relative search capture dependency note allow reference permutation represent major sushi datum roughly run structure figure variant similar structure find interesting number different way model capture strong preference different variety correspond typical price variety second model capture preference compare type sushi variety distinct variety respectively finally people describe distinct rank independently preference sushi additionally consensus rank like direct comparison algorithm code available aim compare quality structure structure find model datum different estimation algorithm structure find earch reference experiment ranking work special issue computational social choice pp theory partition cambridge trick m vote scheme difficult tell win social choice proof consensus ordering np hard efficient contextfree parse algorithm communication acm m distance base rank model journal royal statistical society latent space model rank datum proceeding annual international conference machine learn independence structure rank datum electronic journal statistic fourier theoretic probabilistic inference permutation machine learn research independence efficient inference partial ranking artificial intelligence research collaborative filtering recommendation base order response proceeding international conference knowledge discovery datum mining aggregation model proceeding th international conference machine learn combine ranking use conditional probability model permutation proceeding th international conference machine learn pp mallow l ranking model biometrika search learn exponential model ranking max ed artificial intelligence statistic aistat number global partial order sequential datum proceeding sixth annual knowledge discovery data mining pp strong proceeding workshop experiment
edge chaos realtime computation selforganize criticality recurrent neural network mathematic science computer abstract paper analyze relationship computational capability randomly connect network threshold gate timeserie domain dynamical property particular propose complexity measure find assume high value edge chaos ie transition order chaotic dynamic furthermore propose complexity measure predict computational capability edge chaos network able perform complex computation time additionally simple synaptic scaling rule criticality present analyze introduction propose extensive computational capability achieve system dynamic chaotic order order chaos lead idea computation edge chaos early evidence hypothesis report result numerous computer simulation carry study suggest sharp transition order chaotic dynamic later confirm use idea statistical physics develop accurate meanfield theory allow determine critical parameter analytically physical background theory focus autonomous dynamic system ie relaxation initial state input terminal state output external influence contrast offline computation focus article timeserie computation ie mapping filter input signal timevarye output signal online realtime computation describe input output relation system animal autonomous robot react realtime continuously change stream sensory input purpose paper analyze computational capability randomly connected recurrent neural network domain realtime processing type dynamic induce underlie distribution synaptic weight relate particular type neural network consider paper define exist transition order chaotic dynamic phase transition determine use extension meanfield approach describe step propose novel complexity measure input network activity timestep timestep timestep order chaotic mean activity figure network randomly connect threshold gate exhibit order critical chaotic dynamic upper row example temporal evolution network input indicate different network parameter order critical chaotic regime respectively parameter u r indicate phase plot background phase plot mean activity network depend parameter calculate use meanfield theory develop serve predictor computational capability network timeserie domain employ recently develop framework analyze realtime computation investigate relationship network dynamic computational capability timeserie domain paper propose analyze synaptic scaling rule criticality soc type network consider contrast previous work check propose rule adaptation critical dynamic computational capability network actually increase rule apply relation previous work socalled state machine approach propose use analyze computational capability timeserie domain randomly connect network biologically inspire network model compose integrateandfire neuron use approach demonstrate edge chaos complex computation perform similar analysis restricted case mean synaptic weight network model consider paper find network model dynamic consider input drive recurrent network consist threshold gate state node receive nonzero incoming weight exactly randomly choose nod nonzero connection weight randomly draw distribution mean variance furthermore network drive external input signal u inject node summary update pn network apply neuron parallel h following consider randomly draw binary input signal time step ut assume value probability r value probability r network model similar consider differ important aspect use emphasis asymmetric information encoding spike biological neural system general sense gaussian distribution nonzero weight draw allow arbitrary mean r imply network activity pn vary considerably different parameter compare fig enter calculation discuss rest paper row typical example order critical chaotic dynamic section definition order chaos system parameter correspond type dynamic indicate low panel phase plot refer phase transition order chaotic regime critical line solid line phase plot note increase variance weight consistently lead chaotic behavior critical line order memory chaos define chaotic order phase input drive network use approach similar propose autonomous system consider initial network state certain normalize hamming distance state map corresponding successor state use weight matrix input case change hamming distance observe small distance tend grow sign chaos distance tend decrease order follow closely argument develop meanfield theory detail allow calculate update ut normalize t hamming distance state update network activity time step note depend input ut contrast activity contrast twodimensional map dt ut ut describe time evolution input let consider steady state average hamming distance steady state average network activity ie know state difference eventually network phase hand small difference network chaotic phase decide look slope function fix point depend calculate average steady state activity determine slope map u point accordingly network order critical chaotic regime respectively critical line phase transition order chaotic behavior occur pbf kn n u u pbf denote probability average input network activity node change output single input bit flip example denote composition map kth iteration input apply denote average possible initial condition input signal statistic determine r actual single probability q depend number input figure m separation assume high value critical line gray code image m separation dependence denote panel r solid line mark critical value critical line calculate formula mark solid line fig order phase describe use notion memory reference intuitively speak network memory state t fully determine finite history ut input slight reformulation property reference equivalent requirement state difference vanish ie order phase memory play important role state machine framework separation property principle allow appropriate readout function recent input function network state hand network memory ie chaotic regime network state contain information initial condition hard impossible feature recent input predictor computational power mention separation property especially important network useful computation input timeserie different input signal separate network state ie different input result different state possible function respond differently necessary different input series readout function produce different output drive recurrent network sufficiently different state mean field theory develop extend describe update hamming distance result apply different input mean distance b pr separation summary threedimensional map t t t ut fully describe time evolution hamming distance network activity consider steady state hamming distance network overall separation input statistic determine r b overall separation measure directly relate r external input r d denote density cumulative respectively detailed explanation value search conduct find value numerical iteration function use determine b bit random boolean function bit mean figure realtime computation edge chaos gray code image interpolation datum point mark open diamond performance train network dependence parameter p delay bit parity task performance measure memory capacity m c mutual information classifier output v target tion yt measure test set panel c panel average randomly draw function time step ie yt ut power chaotic network separate minor difference input high degree separation cause input distance b distance initial state measure state distance cause difference initial state remain long run input note order phase nonzero chaotic phase want complexity measure m separation predictor computational power correct term account separation input drive suitable measure immediate separation average increase hamming distance system run long time t equal input single step input pair v average difference pr apply measure network mediate separation input difference fig m separation result input difference b dependence network parameter note m separation peak close critical line computational importance separation property suggest computational capability network peak onset chaos confirm section realtime computation edge chaos access computational power network use state machine framework propose independently forward idea complex timeserie computation implement compose system consist conceptually different value value choose critical line value equally space logarithmic scale interval pair extensive numerical iteration map s perform obtain accurate estimate numerical estimate replace analytic result future properly choose generalpurpose recurrent network rich dynamic readout function train map network state desire output detail approach potentially successful generalpurpose network encode relevant feature input signal network state way readout function easily extract critical line network consider paper encode input way simple linear suffice implement broad range complex nonlinear filter note order train network task parameter r r linear classifier adjust actual network output close possible target value yt access computational power principled way network different parameter test delay bit parity task increase delay randomly draw boolean function input bit note task complex network consider linear separable function require memory achieve good performance necessary contain information input bit ut t t nonlinear transform form linear classifier sufficient perform nonlinear computation result summarize fig performance measure term mutual information test set network output target signal parameter setting detail high performance clearly achieve parameter value close critical line phase transition occur note contrast previous result network use optimize specific task computational capability assess evaluate different task network specifically design single task good performance setup consideration suggest follow hypothesis regard computational function generic recurrent neural circuit serve generalpurpose temporal integrator simultaneously nonlinear projection high dimensional space facilitate subsequent linear readout information need selforganize criticality synaptic scaling computational capability network depend critical dynamic adaptive system able adjust dynamic accordingly state critical dynamic achieve probability pbf single input output average external internal input statistic r respectively equal allow rule adjust weight node local estimate pbf available accomplish estimate pbf margin node distance internal activation firing threshold intuitively node activation high low firing threshold unlikely change output single bit input flip formally pbf average internal external input statistic following quantity mit denote margin node detail node apply synaptic scaling adjust critical line accordingly arrive follow socrule k wij wij t k neuron timestep b c kp timestep figure selforganize criticality time evolution network state t start chaotic regime socrule active parameter initial estimate pbf dotted line node average estimate pbf evolve time network run average estimate thick black line use socrule clearly pbf approach critical value dash line c b initial order learning rate pbf t run average formula estimate pbf apply rule parallel node network able adjust network dynamic criticality fig upper row time evolution network state socrule run clearly visible network dynamic change chaotic initial network parameter critical dynamic respect input signal low row fig average estimate probability pbf approach critical value case network start order regime critical dynamic suit information processing fig expect performance bit parity task improve soc confirm fig memory capacity m c define fig grow network initialize chaotic order regime respectively note performance reach network use socrule high network critical value choose apriori stay level rule stable sense dynamic critical computational capability discussion develop meanfield theory network allow determine position transition line chaotic dynamic respect learning rate exponentially weight run average time constant time step use start chaotic start order bit bit soc step soc step figure time evolution performance activate socrule plot memory capacity m c fig bit parity task average network standard deviation evaluate indicate time step evaluation time step network weight fix m c measure fig train correspond readout scratch network initialize chaotic regime network initialize order regime parameter control network connectivity input statistic base theory propose complexity measure m separation assume high value critical line clear correlation computational power realtime timeserie process result provide evidence idea computation edge chaos support hypothesis dynamic critical line expect general property input drive dynamical system support complex realtime computation analysis propose complexity measure provide new approach discover dynamical principle enable biological system sophisticated information process furthermore local rule synaptic scaling able adjust weight network critical dynamic additionally network adjust rule find exhibit enhance computational capability system combine taskspecific optimization provide supervised learning rule selforganization dynamic criticality provide explanation specific information process able react incoming signal flexible way acknowledgement work support pascal project ist european community reference computation edge chaos b network simple anneal dynamical phase transition spin glass phy math t realtime computation edge chaos recurrent neural network neural computation t realtime computing stable state new framework neural computation base perturbation neural computation h harness nonlinearity predict chaotic system save wireless communication science critical neural network physical review e supplementary information meanfield theory randomly connect recurrent network threshold gate
emergence conjunctive visual feature quadratic independent component analysis computer research unit previous study quadratic modelling natural image result cell model react strongly edge bar apply quadratic independent component analysis natural image patch small approximation error estimate component compute conjunction linear feature conjunctive feature appear represent edge bar inherently twodimensional stimulus corner addition component underlie linear feature essentially simple cell receptive field characteristic result indicate development v cell prefer angle corner partly principle unsupervised sparse coding natural image introduction sparse coding natural image lead model resemble receptive field primate primary visual cortex area ongoing research effort try understand model computational principle visual area follow v commonly think provide representation complicated stimulus example recently macaque monkey area follow v contain neuron respond angle corner necessarily constituent edge present behaviour easily attain linear model paper estimate quadratic model natural image use independent component analysis use quadratic function natural extension linear function x value single feature component x matrix specify weight secondorder interaction input variable stimulus class function equivalent secondorder polynomial input compute linear combination squared response linear model wellknown interpretation component quadratic model output twolayer neural network base eigenvalue decomposition discuss estimate quadratic model natural image report emergence receptive field model respond strongly stimulus contain feature correct spatial arrangement heavy dimensionality reduction feature collinear prefer edge bar small reduction additional component emerge appear prefer complex stimulus angle corner case emerge component approximately operate compute product output linear submodel simple cell characteristic rest paper organize follow section describe quadratic section outline dataset preprocessing use section describe result finally section conclude discussion future work quadratic ica let x rn vectorize grayscale input image patch basic form linear assume data point generate linear mixing matrix s vector unknown source signal independent component dimension assume equal dimension possibly x reduce small dimension ica estimation try recover s parameter matrix independent component sparse equivalent perform sparse code account propose quadratic model perform quadratic basis expansion apply standard let new data vector quadratic space z xn generate secondorder polynomial constant term dimension expansion implicit kernel method secondorder polynomial kernel use instead work traditional input transformation simplicity assume perform transform datum column quadratic component cell model polynomial filter model coefficient weight secondorder polynomial straightforward decompose response si quadratic component x wit z x symmetric square matrix correspond weight weight order know represent form eigenvalue decomposition lead expression si x sorted eigenvalue corresponding eigenvector case representation eq help understand model individual eigenvector interpret linear receptive field quadratic function form illustrate left figure model estimate quadratic ica eigenvector resemble simple cell receptive field propose decomposition lead simple network computation base linear receptive field similar simple cell assume eigenvalue hi large absolute value opposite sign refer dominant eigenvalue denote assumption turn hold empirically estimate model include correspond dominant eigenvector ignore linear term denote dd s t figure quadratic component network use eigenvalue decomposition quadratic form interpret network leave computation single component vi eigenvector eigenvalue matrix product approximation possible variance concentrate eigenvector eigenvalue opposite sign turn case natural image use simple arithmetic obtain product approximation eq approximation network right figure justify later relatively small empirical error model provide approximation good intuition component essentially compute product response linear filter analogous operation conjunction empirically vector v resemblance simple cell receptive field model respective dominant eigenvector complicated shape material method experiment use natural image dataset provide dataset contain grayscale image represent natural scene image resolution intensity distribution image set long right tail relate variation overall image contrast intensity addition know high frequency natural image contain sample artifact rectangular sampling spectral characteristic uniform frequency cause difficulty gradientbased estimation method alleviate problem adopt preprocesse raw image follow processing consider simple model physiological pathway contain address problem heavy contrast variation intensity distribution natural logarithm input image effectively compress dynamic range preprocessing similar happen stage natural visual system previously suggest current dataset correct spectral datum use whiten filter propose field whiten filter cut high frequency balance frequency distribution dominant low frequency use filter parameter whiten filter bandpass characteristic resemble centersurround behaviour lgn cell filtering approximately decorrelate datum preprocesse image sample small image patch image patch resolution subtract local mean intensity patch patch form datum use estimate model fitting transform datum quadratic space use follow use symmetric estimation component input dimension quadratic space figure quadratic ica component model size small component display dominant eigenvector row correspond vector row light dark area correspond positive negative weight respectively component sort feature use drop dimension select dominant principal axis cover approximately sum eigenvalue result estimation independent component polynomial filter perform additional experiment dominant principal axis correspond coverage space constraint unable discuss component model briefly mention main result present paper ensure research source code perform experiment describe paper publicly available result general interpret quadratic model difficult strategy propose literature current work estimate component turn fairly simple small approximation error later discuss section illustrative display estimate component term dominant eigenvector v h respective vector eq pair vector use compute approximate component response stimulus use eq analysis component base vector preferred figure quadratic ica component small model estimate component ignore linear term eq dominant eigenvector row equal stimulus component react highly note eigenvector right highly negative figure quadratic component pick bootstrap iteration component estimate run component order feature small sample tail presentation figure component prefer conjunction collinear feature component highly orthogonal feature component apparent model size large clear characteristic case vector corresponding eigenvector complex hand vector v respond stimulus nonzero value component respond strongly case small model size feature v collinear respond strongly edge stimuli feature conjunction collinear remain unstructured appear react highly stimulus component type different ordinary linear detector edge bar conjunctive nature selective following limit discussion large model consist component mention high dimensionality allow diversity emerge component increase figure quadratic component pick experiment repeat different subset input patch different random seed addition collinear conjunction image component orthogonal stimulus component appear respond intuitive visual concept angle corner case benefit decomposition vector apparent receptive field model retain resemblance filter corresponding eigenvector complicated validate characterization approximation eq hold generally small error turn eigenvalue distribution decay fast quadratic form estimate component illustrate left figure mean sorted eigenvalue component model component figure similar eigenvector equal norm eigenvalue imply magnitude contribution respective eigenvector component output value fast decay eigenvalue dominant eigenvector largely responsible component output provide linear term l discussion linear term quadratic model figure conjunctive nature component eigenvalue quadratic form typically emerge heavily dominate eigenvector eigenvalue far confirm relatively small approximation error cause ignore eigenvector linear term leave sorted eigenvalue hi average component quadratic ica quadratic pca eigenvalue distribution tend decay fast right relative mean square error product approximation quadratic ica component component sort error approximation gabor function use model v quadratic tend dominate component response covariance large quadratic dimension space linear reasoning support analysis prediction error product approximation examine sample new image patch use training compute mean square error approximation divide variance component response es v ar error right figure component average relative error respective component variance range product approximation appear capture behaviour component plot effect approximate vector v gabor function commonly use model receptive field use gabor function approximation error increase range mean understand obtain error rate fit linear model approximate estimate quadratic cell use leastsquare regression reveal quadratic component highly nonlinear behaviour component error linear approximator come close baseline error attain empirical mean use constant predictor product approximation cover dominant eigenvector possible rest eigenvector code interesting phenomenon excitatory inhibitory effect quick decay eigenvalue estimate model effect minor follow idea method explore possibility eigenvector code invariance component strong invariance find possibly local input sign change partly structural property model originate square particular observe consistent recent finding v area leave exploration role eigenvector future work finally perform experiment examine extent method quadratic hand natural image input datum responsible result example argue quadratic component similar quadratic pca component figure illustrate trivially pca component large eigenvalue component quickly lose resemblance filter eigenvalue decrease conjunctive nature estimate feature clear component left figure leave row vector v quadratic pca component row display component component quickly lose resemblance receptive field simple cell right typical quadratic ica component term vector model estimate white noise circular shape likely artifact whiten filter figure demonstrate average eigenvalue quadratic form decay slow component set component study component appear change lowpass filter filter eigenvalue decrease compare figure figure output characteristic method apply difference resemble observe linear linear use code natural image datum verify emerge component structure artifact modelling methodology generate dataset artificial image white noise luminance distribution original dataset spatial spectral structure repeat model estimation include preprocesse new dataset result component respond stimulus long clearly conjunctive eigenvalue distribution decay fast tend dominant eigenvector base vector component roughly categorize class class respond spatial form centersurround filter possibly cause use whitening filter second class preferred apparently random configuration inhibitory excitatory effect component estimate random datum display right figure term vector discussion paper specify quadratic model natural image estimate parameter independent component analysis report emergence cell model exhibit strongly nonlinear behaviour particular demonstrate estimate cell essentially compute product output linear filter simple cell characteristic feature conjunction prefer collinear feature correspond combination orthogonal stimulus react strongly angle corner result indicate sparse coding natural image partially explain development angle cell previous work describe quadratic model natural image datum ie approach resemble obermayer report curvature detect cell patch size use number component estimate small result seek replicate complex cell property algorithm base kullbackleibler divergence report conjunctive feature cell preference angle corner instead estimate quadratic form static image datum dominant eigenvector work extend previous research report emergence conjunctive component combine response linear filter difference work previous research reason number estimate component ie number principal component retain affect feature diversity component feature collinear produce highly selective edge bar detector large difference previous work likely different input preprocesse know image datum cause difficulty statistical estimation linear model preprocessing size use image patch affect estimate feature quadratic modelling product dimension input datum cause additional problem method rely estimation quadratic transform strong boost effect outlier tail marginal distribution note difference previous work invariance resemble complex cell behaviour emerge method class quadratic model contain classic model complex cell fit static image optimization sparsity work emergence invariance equivalently behaviour resemble operation model far constrain example optimize model likelihood preferable optimize output sparseness quadratic clear construct proper probabilistic model finally important open question regard current work extent obtain conjunctive feature reflect real structure present image datum time able prove possibility algorithmic artifact following sense effect pca account quadratic component combination randomly select sparse independent possible currently investigate component issue acknowledgment author wish thank anonymous reviewer helpful comment work support ist programme european community pascal network ist publication reflect author view reference olshausen field sparse code overcomplete basis set strategy employ vision research independent component filter natural image compare simple cell primary visual cortex b hyvarinen p hoyer emergence phase shift invariant feature decomposition natural image independent feature subspace neural computation selectivity complex shape primate visual area journal representation angle embed contour stimulus area monkey journal neuroscience nonlinear image operator evaluation local dimensionality ieee transaction image process quadratic form natural image network computation neural system e oja independent component analysis quadratic independent component analysis tran fundamental p j comparison model simplecell coding perception hyvarinen fast robust fixedpoint algorithm independent component analysis ieee transaction neural network p l analysis interpretation inhomogeneous quadratic form receptive field neural computation accept obermayer secondorder statistic natural image
asymptotic theory onedimensional email abstract generalization ability neural network improve dramatically regularization analyze improvement need result asymptotic distribution weight vector study simple case onedimensional linear regression quadratic regularization regression study random design case derive expansion optimal regularization parameter improvement possible construct example good use regularization introduction suppose available training datum consist pair vector try predict basis neural network weight vector popular way select criterion min loss squared error function inputoutput function neural network penalty real function small value mapping smooth high value change rapidly regularization parameter nonnegative scalar depend training sample refer setup training regularization setup choice training regularization regularization find effective improve generalization ability neural network especially sample size order magnitude dimensionality theory onedimensional linear case paper deal asymptotic case architecture network fix sample size grow fix idea let assume training datum independent identically distribute pair random vector ie distribution collection pair independent x dependent define prediction risk network weight expect value rw let denote minimizer wn minimizer risk r quantity average prediction error datum independent training sample quantity random variable describe generalization performance network bound r w rw performance quantify concentration single number expect value interested quantify gain generalization training training regularization define regularization help positive relatively little quantity specify detail regularization parameter determined section provide converge sufficiently quickly rate equal lead order turn optimal regularization parameter reside asymptotic regime reason analysis require order asymptotic approximation article derive need asymptotic expansion simple possible case onedimensional linear regression regularization parameter choose independently training sample regularization linear regression specialize setup case linear regression quadratic smoothness penalty ie t wj scalar x vector r symmetric positive definite matrix known easy minimizer wn r generalize ridge regression estimator ridge regression correspond choice r survey notice generalize ridge regression usually study fix design case far usually assume model correctly specify ie exist parameter distribution noise term depend contrast study random design case assume ie invertible minimizer risk risk write rw p sequence random variable notation mean zn converge probability notation mathematical tool need follow proposition ch proposition suppose ie invertible distribution c normal distribution mean covariance previous proposition generalize nonlinear case complicated condition proposition follow certain additional condition expansion rw admit expansion constant fi regime need consider high order expansion order compare performance wn onedimensional linear regression specialize setting previous section case scalar consider case parameter sample size deterministic especially allow depend training sample necessary coefficient follow type asymptotic expansion depend detail determine deterministic case easy analyze develop asymptotic expansion criterion regularization parameter deterministic nonnegative expansion turn valid uniformly k develop asymptotic formula minimizer ino inf n quantity interpret average improvement generalization performance gain optimal level regularization regularization constant allow depend n training sample assume arrange linear change variable refer formula previous section ink k introduce function use heavily follow arithmetic mean l vi convenience define notice mean random variable condition converge lead idea use expansion point u order expansion ink asymptotic theory onedimensional linear case outline idea let degree polynomial ie polynomial coefficient function degree respect v depend n moment v derive upper bind quantity k k upper bind error approximate ink k turn odd degree error order magnitude degree consider degree turn error bound uniform proceed need introduce assumption assumption high rand assumption constant surely density bound neighborhood assumption guarantee existence high moment value r s sufficient follow proof eg pair normal distribution distribution compact support moment order exist case assumption satisfied condition assumption ino fail meaningful finite follow technical result state proof proposition let let ie assumption hold expectation left finite assumption respectively b hold b p provide proposition let assumption hold exist constant m ink k s k k proof sketch formula ie follow easily integrate degree polynomial term term upper bind consider residual omit similar term use bound p triangle inequality inequality r ije k l proposition use following fact lemma p fact let zd zd apply inequality fact eg l e p term carefully hold proposition let assumption hold assume set exist constant ni n nl function unique minimum point admit ain far ino k ino proof base perturbation small parameter previous proposition et k sum term supremum term unique minimum differentiate sn proof sketch pk second degree polynomial polynomial root converge regular perturbation expansion root yield state formula point minimum sufficiently large far great sufficiently large estimate inf k o case follow notice ino ink k use degree expansion wk theory onedimensional linear case figure illustration asymptotic approximation situation equation horizontal axis vertical axis asymptotic legend ink solid line t kj dash line technique previous proposition ie k integrate polynomial use estimate ino s finally mean value ino ink ino ino ino ain lie ain use fact indicate derivative evaluate order moderate effort remark precede assume equal case formula divide model correctly specify sense e e independent ie e e ie ie e strictly positive expect degenerate case e probability mean regularization help provide regularization parameter choose value ain large figure illustration case e independent ink estimate basis repetition task n addition ie function ie plot ink correctly order notice good approximation ink minimizer minimizer ink minimizer lie point predict theory situation actually calculation minimizer ink exactly sample size possible construct case uniform z instance z independent case regularization use positive regularization parameter matter bad use properly choose negative regularization parameter help particular case reward rapidly change function case regularization use negative value regularization parameter catastrophic discussion obtain asymptotic approximation optimal regularization parameter improvement simple case onedimensional linear regression regularization parameter choose independently training sample turn optimal regularization parameter lead order result improvement order regularization matter bad obtain asymptotic result optimal regularization parameter consider case nonlinear network assume neural network model correctly specify generalization present result nonlinear case possible use eg technique generalization case regularization parameter choose basis sample cross validation desirable acknowledgement paper prepare author visit department statistic probability theory financial support thank useful discussion reference r validity formal expansion annal statistic m neural network pattern time series theory method springer series e ridge regression l read b editor statistical science generalization performance regularize neural network model e editor proc th ieee workshop neural network signal processing page pattern recognition neural network cambridge press approximation theorem mathematical statistic m common structure smooth technique statistic international statistical review
comparison dynamic reposing tangent distance drug activity prediction point mit artificial intelligence laboratory drug activity prediction handwritten character recognition feature extract describe training example depend pose location orientation example handwritten character recognition good technique address problem tangent distance method simard lecun denker jain b introduce new address problem dynamic reposing iteratively learn neural network repose example effort maximize predict output value new model train new pose compute model pose converge paper compare dynamic reposing tangent distance method task predict biological activity musk compound fold crossvalidation comparison dynamic reposing tangent distance drug activity prediction dynamic reposing attain correct compare tangent distance method neural network standard pose near neighbor method introduction task drug activity prediction predict activity propose drug compound learn observe activity drug compound accurate drug activity prediction save substantial time focus effort synthesis testing compound predict activity high requirement highly active bind display dimension work display design new compound high predict activity drug molecule usually act bind localize site large receptor molecule large molecule reasonable way represent drug molecule capture location surface fix frame reference hypothesize bind site learn constraint allow location molecular surface important region surface learn form model bind site yield accurate prediction support drug design training datum drug activity prediction consist molecule describe structure bond graph measure bind activity complication difficult learn bind site model datum bond graph uniquely determine shape molecule bond graph view specify possibly cyclic kinematic chain internal degree freedom bond conformation graph adopt embed space assign energy depend interaction force internal bond interaction algorithm exist search space conformation find local minima low energy conformer relatively molecule low energy conformer training datum indicate conformer conformer bind bind site produce observed bind activity second conformer know feature describe molecular measure frame reference bind molecule rotate translate space consider feature space training example bond graph induce family dimensional manifold manifold correspond conformer rotate translate degree freedom space classification task positive decision region active molecule region manifold active molecule manifold inactive molecule find decision region difficult difficult compute similar feature manifold problem arise handwritten character training example label handwritten digit feature extract grayscale picture feature value depend rotation translation camera respect character train formalize situation follow let bond graph physical handwritten digit let associate measure activity molecule identity handwritten digit suppose extract realvalue feature xi describe object xi employ example multilayer sigmoid network approximate ix ordinary supervised learn task feature manifold problem arise extract feature depend pose example define pose vector p parameter describe example rotation translation conformation molecule rotation translation scale line handwritten digit case feature vector depend example handwritten character recognition community technique develop deal feature manifold problem exist approach standardized pose tangentprop method describe new apply supervise learn simultaneously discover good pose pi training example xi learn approximation unknown function paper briefly review method compare performance standardized pose tangent distance dynamic reposing problem predict activity molecule approach feature manifold problem standardized pose simple approach select feature vector compute standard example construct function object choose example usual supervised learn training example unique feature vector approximate difficulty hard design optical character s typically work compute property principal axis choose pi translate rotate scale property standard value error ocr trace error function character incorrectly position drug activity prediction function guess conformer conformer difficult additional information d atom coordinate molecule bind bind comparison dynamic reposing tangent distance drug activity prediction site determine addition determine orientation conformer bind site conformer mutually align share potential chemical interaction bond superimpose tangent propagation tangentprop approach simard denker employ function augment learning procedure constraint output learn function p invariant respect slight change pose example p indicate euclidean norm constraint incorporate use regularizer backpropagation training tangentprop view way focus learning algorithm input feature feature invariant respect slight change pose tangentprop constraint learning identify feature discriminate class tangentprop assume standard pose correct safe assumption drug activity prediction tangent distance approach simard lecun denker variant nearestneighbor address feature manifold problem ideally good distance metric employ nearestneighbor manifold compute manifold point near approach manifold expensive compute manifold highly nonlinear shape feature space manifold distance local tangent distance approximation manifold distance compute approximate manifold tangent plane standard pose let jacobian matrix define plane tangent manifold molecule distance define p sx column vector change pose require minimize distance tangent plane approximate manifold value b minimize righthand compute fairly quickly gradient descent simard personal communication practice pose close sx consider provide opportunity object belong class adopt pose similar experiment handwritten digit simard lecun denker find tangent distance good performance method dynamic reposing precede method view attempt final predict output respect change pose standard pose permit pose change tangentprop add local invariance constraint tangent distance enforce somewhat local invariance constraint dynamic reposing j invariant define maximum value pose p auxiliary function g p function function learn neural network consider learn let consider use predict activity new molecule compute find pose p maximize perform gradient start standard pose direction gradient respect process important physical analog drug activity prediction new molecule learn model bind site vary pose p imitate process molecule choose conformation rotate translate bind site handwritten character recognition dual template model template hold fix example rotation translation scaling find good fit template function learn iteratively grow pool feature vector initially pool contain feature vector standard pose training example actually start standard pose low energy conformation training example iteration apply backpropagation learn select feature vector draw pool molecule feature vector select perform forward propagation compute pi feature vector molecule select predict activity molecule learn compute conformer pose p pi p maximize p chemical perspective permit molecule current model gj bind site feature vector correspond pose add pool pose new hypothesis learn process iterate pose comparison dynamic reposing tangent distance drug activity prediction change note algorithm analogous procedure accomplish simultaneous optimization pose conduct series separate optimization hold pi fix hold fix believe power dynamic reposing result ability identify feature critical discriminate active inactive molecule initial standard pose learning likely find feature discriminate active inactive reposing process inactive molecule able resemble active molecule respect feature iteration learning force choose feature discrimination repose active molecule able similar respect feature judge important previous iteration subsequent iteration learn tighten criterion recognize active molecule initial standard pose molecule pose resemble feature equally convergence active molecule change pose resemble feature important discrimination experimental comparison musk activity prediction compare dynamic reposing tangent distance standard pose method task musk odor prediction problem musk odor prediction focus modeling effort musk odor specific clearly identifiable mechanism underlie poorly understand determine entirely ie molecular shape effect addition deletion single group convert compound strong musk musk molecule similar size composition kind drug molecule study set diverse structure collect publish study et datum set contain molecule odor lack musk odor molecule search identify low energy conformation final data set contain conformation molecule detail datum set conformation place starting pose function apply near neighbor euclidean distance near neighbor tangent distance feedforward network repose feedforward network dynamic reposing method dynamic reposing iteration repose sufficient convergence time require compute tangent distance far exceed computation time algorithm tangent distance computation feasible table result fold crossvalidation musk molecule percent correct near neighbor euclidean distance network standard pose near neighbor tangent distance network dynamic reposing table neural network prediction percent correct molecular class standard pose dynamic reposing compute tangent distance neighbor near distance experiment subset molecule heuristic introduce error subset table result fold crossvalidation method tangent distance method improvement respect standard neural network approach respect standard near neighbor method dynamic reposing method outperform method substantially important test drug activity prediction method predict activity molecule molecular structure bond graph substantially different molecule training set weakness exist method drug activity prediction rely assumption molecule training test datum set share common structural representation molecule concern surface molecule suffer problem table structural class molecule result class holdout experiment molecule class exclude training set predict prediction standard pose particularly good dynamic reposing obtain excellent prediction demonstrate ability dynamic reposing identify critical feature note accuracy prediction generally determine size training set ie molecule performance drop exception rightmost class local geometry atom substantially different class comparison dynamic reposing tangent distance drug activity prediction conclude remark feature manifold problem arise application task include drug activity prediction handwritten character recognition new method dynamic repose exhibit performance superior good exist method tangent distance standard method problem musk activity prediction addition produce accurate prediction dynamic reposing result learn bind site model guide design new drug molecule method visualize learn model context molecule demonstrate model apply guide design compare method stateoftheart method drug activity prediction feedforward network dynamic reposing substantially superior bind task method currently apply development new compound acknowledgement people contribution project author thank effort reference b m m new r r t soc t g r chapman r e e method molecular design adaptive alignment selection submit chapman b method performance comparison benchmark submit b c chemical sense h mixture maximum likelihood simard denker tangent formalism specify select invariance adaptive network p ed advance neural information processing system simard p denker efficient pattern recognition use new transformation distance advance neural information processing system
transfer learn text classification computer computer abstract linear text classification algorithm work compute inner product test document vector parameter vector algorithm include naive baye tfidf variant parameter determine simple closedform function training set statistic mapping mapping statistic parameter parameter function research text classification decade consist manual effort identify parameter function paper propose algorithm automatically learn function relate classification problem parameter function find algorithm define new learning text classification apply novel classification task find learn classifier outperform exist method variety multiclass text classification task introduction multiclass text classification task training set document label belong disjoint class new unlabeled test document use training set guide predict likely class test document bagofword linear text classifier represent document vector word count predict pn class score linear function high arg choose parameter high classification accuracy test datum main challenge linear text classification algorithm paper focus linear text classification algorithm parameter function training set statistic function fix statistic training set discriminative learning method logistic regression support vector machine svms use numerical optimization pick parameter learner consider perform optimization technique parameter learning involve statistic vector apply closedform function g obtain parameter refer g mapping statistic parameter parameter function common text classification multinomial multivariate model naive baye vector probabilistic variant belong class algorithm pick good text classifier class equivalent find right parameter function available statistic practice researcher develop text classification algorithm guide empirical testing classification task argue year history consist manually try tfidf formula variant ie adjust parameter function optimize performance heuristic process lead good parameter function task require human risk fail find variation consider paper consider task automatically learn parameter function text classification set example text classification problem wish new learning specify parameter function apply new classification problem metalearning technique propose leverage datum variety related classification task obtain good classifier new task instance transfer learn specifically framework automate process find good parameter function text classifier replace hour straightforward optimization problem experiment demonstrate effectiveness learn classifier form low training datum classification task learning automatically learn parameter function consistently outperform parameter function base naive baye tfidf exist discriminative learning approach preliminary let fix vocabulary word let x input output space classification problem label document pair ndimensional vector indicate number occurrence word document document class label classification problem tuple distribution xi m set m train example single test example m example draw training set test input predict value test class label linear classification algorithm evaluate score assign class pick class arg high score metalearning setting define evaluation parameter function vector training set statistic vector component compute training set s provide specific example later furthermore r parameter function mapping corresponding parameter illustrate definition specific case naive baye tfidf classification method belong class algorithm describe naive baye multinomial variant naive baye classification score assign document class k pn log xi log pwi k term correspond prior document class second term pwi k smoothed relative frequency word naive baye strong independence assumption shortcoming probabilistic model text document nonetheless view naive baye simply prediction compute certain function training set view prove useful analysis naive baye probabilistic assumption hold adopt view particular probabilistic meaning empirical frequency p happen compute algorithm train document class k balanced training set term irrelevant number time appear document class number document class k contain total number word document class u total number document class total number document smooth parameter laplace smooth tfidf unnormalized tfidf classifier score assign class k pn xi log log px average term frequency average component document vector class document frequency document contain write statistic define time space constraint preclude detailed discussion classification algorithm similarly express framework use definition statistic vector include variant tfidf base different term heuristically modify version naive baye learn parameter function section example algorithm obtain parameter apply function g statistic vector case parameter handdesigned probabilistic case naive baye geometric case tfidf consideration consider problem automatically learn parameter function example classification task sequel assume fix statistic vector focus find optimal parameter function standard supervise learning set training set example sample unknown distribution d goal use training set prediction new test example sample d use training example understand statistical regularity d hope predict yt low error analogously problem metalearning supervised learning task training example classification problem sample distribution d classification problem instance text classification problem note implicitly define dot product vector component consist product term normalize tfidf classifier vector normalize unit length compute dot product modification stable document vary length represent framework consider appropriately normalize statistic vector note metalearning problem output algorithm parameter function mapping statistic parameter training datum explicitly indicate good parameter function example classification problem effectively task central problem fit unseen base test example training classification problem draw d hope learn parameter function exploit statistical regularity problem formally let m collection m classification problem sample new test classification problem sample independently d desire learn correctly classify high probability achieve goal restrict attention parameter function g linear input use linearity assumption pose optimization problem find parameter function achieve small loss test example training collection finally generalize method nonparametric setting trick allow learn complex highly nonlinear function input statistic softmax learning recall softmax regression class model p p parameter learn training datum s maximize conditional log likelihood datum approach total kn parameter train jointly use numerical optimization consider alternative approach parameter function statistic vector particular goal learn appropriate pose optimization problem start learn linear form parameterization conditional likelihood example p p xi setup natural approach learn linear function maximize regularize conditional log likelihood entire collection pm log xj p m t xi p c log p term correspond gaussian prior parameter provide means control complexity learn parameter function maximization similar softmax regression training instead optimize parameter directly optimize choice nonparametric function learn section generalize technique previous section nonlinear exist maximize solution optimal linear combination training set statistic p p reparameterize original optimization equivalent optimization training example weight notational convenience let p p figure distribution vector datum apply log transformation principle alternatively use feature vector representation use frequency directly apply log transformation yield feature space isolate point r b use feature space isolated point important topology feature space establish locality influence support vector substitute obtain p m p kj p log p m p kj m m note concave differentiable train model use standard optimization procedure conjugate gradient assumption linear function place severe restriction class learnable parameter function note statistic vector appear inner product apply trick obtain p p j function u v define inner product highdimensional mapping input particular choose nonparametric representation p weight combination value weight depend exponentially square distance statistic vector result approximate sufficiently smooth bounded function u arbitrarily sufficiently training classification problem experiment validate method evaluate ability learn parameter function variety email webpage classification task number class large number number training example class mk small use open project hierarchy newsgroup dataset dataset industry sector dataset note consequence consider statistic vector entire document table test set accuracy category column proportion correct classification use method learn naive baye tfidf respectively column corresponding value discriminative method softmax regression svms multiclass svms good accuracy row bold average project hierarchical collection webpage link organize subject matter level hierarchy consist major category contain perform testing obtain classification problem category retrieve webpage respective newsgroup industry sector dataset perform similar preprocessing dataset document sample class classification problem randomly select different class dataset pick training example class choose test example randomly choose class choice feature theoretically method describe paper sufficiently rich set feature use learn parameter function classification simplicity reduce feature vector follow twodimensional representation word document class document contain note log transformation component correspond relative term frequency document frequency word relative class figure generalization performance test metalearning algorithm classification problem category category build collection classification problem category result report average datum associate article topic label discard article topic annotation dataset discard category example select word vocabulary base information gain feature rescale mean unit variance training set table cross classification accuracy use classifier train good accuracy row bold newsgroup industry sector problem assess accuracy metalearning algorithm particular test category use learn set classification problem draw category ensure overlap training testing datum category learn parameter function g outperform naive baye tfidf addition discriminative method test softmax regression svms multiclass svms table assess ability transfer corpus newsgroup industry sector construct independent training testing dataset random classification problem train separate classifier use datum corpus test performance learn classifier remain table learn parameter function compare favorably method test single parameter function accurate classification algorithm different corpus demonstrate effectiveness approach achieve transfer relate learning task discussion related work paper present algorithm base softmax regression learn parameter function example classification problem learn define new learn apply novel classification task approach learn modify multiclass support vector machine formulation singer manner modification regression section follow quadratic program minimize subject t usual dual lead naturally procedure optimization implement method find learn softmax formulation outperform naive baye tfidf discriminative method technique describe paper approach achieve inductive transfer classifier label datum related example classification problem solve particular classification problem consider issue knowledge transfer text classification context ensemble classifier propose system use related classification problem learn reliability individual classifier ensemble approach attempt property execution learning parameter determine grid search use small holdout set classification problem holdout set use select regularization parameter discriminative learning algorithm use assess multiclass svms large value multiclass svms consistently outperform naive baye tfidf learned achieve performance par discriminative method constrain parameter explicit function train datum statistic result consistent previous study heuristically version naive baye attain text classification performance large dataset algorithm method use metalearning construct new classification directly apply text classification consider problem automatically learn term distribution use finally consider task classification robot domain work author describe learn task group near neighbor means facilitate transfer similar concept implicit parameter function learn facilitate transfer similar statistic vector acknowledgment thank abbeel useful discussion anonymous helpful comment support fellowship work support darpa contract number fa reference lafferty mccallum use maximum entropy text classification ijcai workshop machine learn information filtering page t joachim text categorization support vector machine learn relevant feature machine learn ecml page mccallum comparison event model naive baye text classification learn text categorization term weighting approach automatic text retrieval processing management t joachim probabilistic analysis tfidf text categorization proceeding icml page r tackle poor assumption naive baye text classifier icml page explore similarity space acm sigir c foundation statistical natural language processing m discriminative generative classifier comparison logistic regression naive baye nip g wahba result spline function anal nocedal s springer r classification learn singer algorithmic implementation multiclass kernelbase vector machine mach learn library support vector machine software available t joachim largescale support vector machine learn practical advance method support vector machine mit s thrun learn case study cmu tech report multitask learn machine learn inductive transfer text classification use generalize reliability indicator proceeding icml workshop label unlabeled datum machine learning datum mining r empirical development exponential probabilistic model text use textual analysis build model sigir thrun structure multiple learning task international conference machine learn page
hierarchical apprenticeship learn application locomotion computer abstract consider apprenticeship setting large complex domain work apprenticeship learn require expert demonstrate complete trajectory domain problem expert difficulty control system approach infeasible example consider task teach quadrupe robot navigate extreme terrain demonstrate optimal policy ie optimal set foot location entire terrain highly nontrivial task expert paper propose method hierarchical apprenticeship learning allow algorithm accept isolated advice different hierarchical level control task type advice feasible expert expert unable demonstrate complete trajectory allow extend apprenticeship large challenging domain particular paper apply hierarchical apprenticeship learn algorithm task quadrupe locomotion extreme terrain achieve good knowledge result superior previously publish work introduction paper consider apprenticeship learn setting large complex domain reinforcement learning algorithm operate markov decision process mdp formalism reward function typically assume priori past work note reward function difficult specify hand quantify trade feature apprenticeship learning base insight easy expert demonstrate desire behavior specify reward function induce behavior attempt apply apprenticeship learn large domain challenge arise past algorithm apprenticeship learning require expert demonstrate complete trajectory domain specifically concerned domain sufficiently complex task feasible second past require ability solve easy problem find nearly optimal policy candidate reward function challenge large domain domain necessitate hierarchical control order reduce complexity control task application consider task navigate quadrupe robot figure challenge irregular terrain figure naive approach dimensionality state space prohibitively large robot independently joint state specify current threedimensional position orientation robot lead dimensional state space capability standard fortunately control task naturally hierarchical decomposition plan general path terrain plan footstep path finally plan joint movement figure robot design build typical terrain c height map terrain black cm white cm achieve footstep challenging specify proper reward specifically high level control require quantify tradeoff feature include progress goal height differential foot slope terrain foot consider apprenticeship learn task specify complete set foot location entire terrain properly capture tradeoff highly nontrivial task motivate difficulty present unified method hierarchical apprenticeship learn approach base insight difficult expert specify entire optimal trajectory large domain easy teach hierarchically employ hierarchical control scheme solve problem easy expert advice independently level hierarchy low level control hierarchy method require expert able demonstrate good local behavior behavior optimal entire task type advice feasible expert expert entirely unable trajectory demonstration approach allow apprenticeship learn extremely complex previously intractable domain contribution paper introduce hierarchical apprenticeship learn algorithm extend apprenticeship learn paradigm complex highdimensional control task allow expert demonstrate desire behavior multiple level abstraction second apply hierarchical apprenticeship approach quadrupe locomotion problem discuss apply method achieve performance good knowledge publish result locomotion remainder paper organize follow section discuss preliminary notation section present general formulation hierarchical apprenticeship learn section present experimental result hierarchical grid world realworld quadrupe locomotion task finally section discuss related work conclude paper preliminary notation markov decision process mdp tuple s t h d r set state set action set state transition probability state transition distribution action state h horizon correspond number timestep consider d distribution initial state s r reward function concerned mdps reward function use notation mdpr denote mdp reward function policy mapping state ability distribution action value policy rst expectation respect random state sequence s draw state state draw distribution d pick action accord work robot develop system capable date believe controller present paper par good controller develop instance direct comparison difficult fast running time team achieve public evaluation second section present result cross terrain comparable difficulty distance second reward function r represent compactly function state let rn mapping state set feature consider case reward function r linear combination feature rs wt s parameter value policy linear reward function weight e t rst use linearity expectation bring expectation quantity define vector feature expectation hierarchical apprenticeship learn present hierarchical apprenticeship learn hal simplicity present level hierarchical formulation control task refer lowlevel highlevel controller extension high order hierarchy pose difficulty reward decomposition hal heart hal algorithm simple decomposition reward function link level control suppose hierarchical decomposition control task form lowlevel highlevel mdpr denote m t d hh respectively partitioning function map low level state highlevel state assumption hierarchical decomposition actually provide computational gain example case quadrupe locomotion problem lowlevel mdpr describe state foot highlevel mdpr describe position robot standard apprenticeship learn suppose reward lowlevel mdpr represent linear function state feature hal algorithm assume reward highlevel state equal average reward corresponding lowlevel state formally s s denote inverse image partitioning function ideal decomposition reward function case example want let reward highlevel state maximum low level state reward capture fact ideal agent seek maximize reward low level alternatively minimum low level state reward robust worstcase capture idea absence prior information reasonable assume uniform distribution lowlevel state correspond highlevel state important consequence high level reward linear lowlevel reward weight enable subsequent section formulate unified hierarchical apprenticeship learn able incorporate expert advice high level low level simultaneously expert advice high level similar past apprenticeship learning method expert advice high level consist policy demonstrate expert highlevel mdpr significantly simple lowlevel mdpr task substantially easy expert suggest optimal policy mdpr correspond follow constraint state expert policy outperform policy h h equivalently use formulate constraint follow wt h able obtain exact feature expectation expert policy highlevel transition stochastic observe single expert demonstration correspond receive work reinforcement learn assumption paper hierarchical decomposition control task system recent work automate discovery state abstraction find natural decomposition control task multiple level discuss specific case quadrupe locomotion sample feature expectation simply use observe expert feature count true expectation standard sample complexity argument sufficient number observed feature count converge true expectation resolve ambiguity allow expert provide noisy advice use regularization slack variable similar standard svm formulation result following formulation kwk ch h h index highlevel policy index mdps ch regularization constant fact exponential number possible policy wellknown algorithm able solve optimization problem defer discussion present complete formulation expert advice low level approach differ standard apprenticeship learning consider advice low level apprenticeship learn paradigm expert specify trajectory target domain allow expert specify single greedy action lowlevel domain specifically agent state expert suggest good greedy action state correspond directly constraint reward function state s reach current state reachable current state s result follow constraint reward function parameter s reachable resolve ambiguity allow expert provide noisy advice use regularization slack variable m kwk s index state reachable j index lowlevel demonstration provide expert unified hal high level low level reward linear combination set reward weight allow combine type expert advice present obtain follow unified optimization problem m kwk ch t h h optimization problem solve efficiently particular optimization problem exponentially large number constraint constraint policy optimum find efficiently ie polynomial time use example ellipsoid method efficiently identify constraint violate practice find follow constraint generation method efficient formulation entirely correct fact impossible separate policy policy include margin exact solution problem w deal typically scale margin slack loss function quantify different policy approach maximum margin planning alternatively abbeel ng solve optimization problem slack notice soon problem infeasible expert policy lie hull generate policy formulation lowlevel advice account issue present formulation simplicity experiment use highlevel constraint employ margin scaling alternatively advice p level action interpret expert pick action constraint s rs s rs domain consider clear set reachable state state formalism natural similar technique employ solve structured prediction problem alternatively different approach constraint objective eliminate slack variable employ subgradient method hal hal highlevel flat apprenticeship learn lowlevel constraint suboptimality policy suboptimality policy number training sample train mdps figure picture environment b performance number training sample hal flat apprenticeship learn c performance number training mdps hal use lowlevel highlevel constraint begin expert path constraint find current reward weight solve current optimization problem solve reinforcement learning problem high level hierarchy find optimal highlevel policy current reward mdpr optimal policy violate current high level constraint add constraint current optimization problem step constraint violate current reward weight solution optimization problem experimental result section present result domain unknown cost mean challenging control task allow compare performance hal traditional flat apprenticeship learn method algorithm feasible domain grid world domain natural hierarchical decomposition average cost room form highlevel approximation grid world hierarchical controller plan domain choose path room room path plan lowlevel path desire exit figure b performance number training example provide training example equal action demonstrate expert expect flat apprenticeship learn eventually converge superior policy employ value iteration find optimal policy hal use hierarchical controller small training datum hal outperform flat method able leverage small datum provide expert level hierarchy figure c performance number mdps training set hal receive training datum hal high level low level expert demonstration use hal perform substantially mean direct comparison different method hal obtain training datum mdp approach experiment illustrate situation access highlevel lowlevel advice advantageous use experimental detail consider x grid world evenly divide room size wall room size connect room neighbor picture domain figure state binary feature sample distribution particular room reward function choose randomly small negative reward medium negative reward high negative reward case generate multiple training mdps differ feature active state provide algorithm expert demonstration train mdp test holdout mdps generate process case result average run experiment fix ratio ch constraint equally weight ie typically t low level action accomplish highlevel action use ratio ch t fix scaling find algorithm generally insensitive term result suboptimality scaling slack penalty comparison hal flat apprenticeship learn figure training example correspond expert action hal number train example training mdp correspond number high level action high level demonstration equal number low level expert action provide flat apprenticeship learn number training example training mdp correspond number expert action expert trajectory demonstration figure highlevel path expert demonstration b lowlevel footstep expert demonstration especially important domain quadrupe locomotion task access training mdps ie different terrain quadrupe robot section present primary experimental result paper successful application hierarchical apprenticeship learn task quadrupe locomotion video result section available hierarchical control locomotion robot figure design build consist independently equip internal foot force sensor estimate robot state use motion capture system track robot body perform computation computer send command robot wireless connection mention introduction employ hierarchical control scheme navigate quadrupe terrain space constraint describe complete control system briefly detailed description find high level controller body path planner plan approximate trajectory robot mass terrain lowlevel controller footstep planner path robot center plan set footstep follow path footstep planner use reward function specify relative tradeoff different feature robot state include feature capture slope terrain different spatial scale robot foot ii distance foot location robot desire center area support triangle form stationary foot similar feature kinematic feasibility require candidate foot location obstacle form highlevel cost aggregate feature footstep planner particular foot consider footstep feature cm radius foot position desire position foot relative center mass absence discriminate feature aggregate feature form feature body path planner approximation find perform practice possibly ability account stochasticity domain form cost function level use value iteration find optimal policy body path planner horizon search find good set footstep footstep planner hierarchical apprenticeship learn locomotion experiment carry terrain relatively easy terrain training significantly challenging terrain testing advice high level specify complete body trajectory robot figure advice low level look situation robot step suboptimal location indicate correct greedy foot placement figure b entire training set figure snapshot quadrupe traverse testing terrain figure body footstep plan different constraint training leave test right red learn green hal blue path yellow footstep single highlevel path demonstration training terrain lowlevel footstep demonstration terrain minute collect datum small training datum learn system achieve excellent performance training board difficult testing board figure snapshot cross testing board figure result footstep different type constraint large qualitative difference footstep choose training table crossing time different type constraint hal algorithm outperform method use footstep constraint training board testing board lack highlevel training lead robot route perform bad quadrupe fail cross testing terrain learn demonstration learn finally prior work hierarchical apprenticeship learn week attempt controller capable pick good footstep challenge terrain previous effort significantly outperform controller present learn minute worth datum previous effort perform substantially bad relate work discussion work present paper relate area reinforcement learning include apprenticeship learning hierarchical reinforcement learning large body past work quadrupe locomotion introduction formulation algorithm discuss connection inverse reinforcement learn maximum margin planning algorithm addition subsequent work extend maximum margin planning framework allow automate addition new feature boost procedure recent work reinforcement learning hierarchical reinforcement learn recent survey work area aware deal standard reinforcement learning formulation know reward agent act possibly unknown environment contrast work follow apprenticeship learn paradigm model reward know agent prior work locomotion focus generate traverse fairly flat train testing time time hal foot path learn table execution time different constraint training testing terrain dash indicate robot fall reach goal terrain learning algorithm attempt generalize previously unseen terrain successfully apply consider paper difficulty level consider prior work acknowledgement gratefully acknowledge anonymous reviewer helpful suggestion work support learn locomotion program contract number reference abbeel apprenticeship learn inverse reinforcement learning proceeding international conference machine learn advance hierarchical reinforcement learn discrete event dynamic system theory application plan navigation strategy complex environment proceeding international conference humanoid robotic hierarchical reinforcement learn value function decomposition artificial intelligence research state abstraction discovery irrelevant state variable proceeding international joint conference artificial intelligence r choi planning quadrupe walk climbing robot locomotion d environment proceeding international conference robotic automation learn fast locomotion proceeding p complete control architecture quadrupe locomotion terrain proceeding international conference robotic automation appear quadrupe robot obstacle reinforcement learning proceeding international conference robotic automation minimax differential dynamic programming application robust walk neural information processing system apprenticeship learn use inverse reinforcement learning gradient method proceeding uncertainty artificial intelligence reinforcement learn machine neural information processing system margin planning proceeding international conference machine learn boost structured prediction imitation learn neural information processing system framework temporal abstraction reinforcement learn artificial intelligence structure prediction model large margin approach proceeding international conference machine learning joachim large margin method structured interdependent output variable machine learn research
online model content optimization abstract describe new content publishing system select article serve user choose program pool frequently deploy major portal select article serve million user visit day significantly increase number user click original manual approach editor select article display challenge face include dynamic content pool short article lifetime nonstationary rate extremely high traffic volume fundamental problem solve quickly identify item popular different user segment exploit remain current explore underlie pool identify promise alternative quickly discard poor approach base tracking article performance real time online model describe characteristic constraint application set discuss design choice importance effectiveness couple online model randomization procedure discuss challenge encounter production online environment highlight issue careful attention analysis application suggest number future research introduction web central distribution channel information traditional source news rapidly grow content develop effective algorithmic approach deliver content user visit web portal fundamental problem receive attention search engine use automate rank algorithm return relevant link response user query likewise online ad target use automate algorithm contrast portal user site typically program manually content hard assess relevance personal preference wide range quality reliable quality trust metric pagerank weight url manual programming content ensure high quality maintain editorial voice typical mix content user associate site hand expensive scale number article number site page wish program grow machine learn approach help scale issue seek blend strength editorial algorithmic approach optimize content programming highlevel constraint set editor system describe currently deploy major serve user visit day usual approach ranking article user use featurebased model train use offline datum datum collect past significant effort feature engineering look user activity site article category keyword entity article conclude difficult build good model base solely offline datum scenario content pool small change rapidly article lifetime short wide variability article performance share common set feature value approach track performance online model initialize use offline datum update continuously use real time datum online aspect open new modeling challenge addition classical feature base discuss paper problem description consider problem optimize content display module point major page provide service eg weather content link module panel slot label slot account large fraction click prominent article display receive click display pool available article create train human editor continually point time live article pool new article program editor push system hour replace old article editor important new story break news eliminate irrelevant story ensure pool article consistent voice site desire nature mix content personalization program system time article user visit page consider choose good set article display module user mix content available pool incorporate constraint voice focus choose article maximize overall rate total number click divide total number view time interval simplify presentation describe learn click feedback obtain important position framework system use information position system challenge setting pose challenge important follow highly dynamic system characteristic article short lifetime hour pool available article change user population dynamic article different different time day different slot module find fast user feedback dynamic model base click page view crucial good performance discuss alternate commonly pursue approach rank article base offline model section scalability portal receive thousand page view second serve million user visit day datum collection model training article scoring use model subject tight latency requirement instance millisecond decide appropriate content user visit significant effort require build scalable support realtime collection event user click page view collect large number web server continuously transfer datum collection cluster support event handle time lag user view page click article page event stream feed cluster run learn algorithm update model web server pull update model serve content base new model complete cycle datum collection model update model delivery minute datum collect impossible relate activity individual user scale scale regular remove repeat view decay figure ctr curve typical article bucket article ctr decay continuously bucket position b article ctr machine learning challenge serve scheme manual algorithm decide article different position module user prior system article choose human editor refer editorial serve scheme random sample user population refer bucket discuss issue build predictive model setting try usual approach build offline model base retrospective datum collect use editorial serve scheme user feature include age gender infer interest base user visit pattern article use feature base url article category sport keyword approach perform poorly reason include wide variability article set feature value change article ctr time fact retrospective datum collect serve scheme factor hard adjust section initial study reveal high variability article share common feature eg sport article article achieve performance seek quick convergence use online model good article user user segment lose opportunity failure detect good article quickly costly cost increase margin difference good select article discuss challenge address article strongly dependent serve scheme use especially exposure receive change dramatically time learn technique assume process stationarity order ensure webpage stability consider serve scheme alter choice user slot choice identify figure curve typical article subject serve scheme decay user expose article exposure article happen different way different degree user expose article link click read article user click multiple link associate article strong form exposure analysis consider related click single click event view exposure noisy module content piece user look weather module visit portal look article link like explain decay precisely term exposure difficult instance article user page view contain link suboptimal link click later fact large number click article occur page view depend user navigate portal instead solve problem impose serve constraint user build component dynamic model track article ctr decay time impose reasonable serve constraint provide good user article user x minute work article addition decay ctr article change time day day week figure b ctr typical article serve use randomized serve scheme article serve fashion randomly choose user population randomization remove serve bias provide unbiased estimate evident article vary dramatically time clearly need adjust time effect pattern decay obtain adjust article score decide rank article current study fit global time day curve minute resolution datum obtain randomized serve scheme periodic adaptive regression spline interaction occur article level difficult estimate offline article feature online model weight recent observation provide effective adaptive mechanism automatically account deviation global trend article push system strong serve bias model build use datum generate serve scheme bias scheme example serve scheme decide article user model build use datum learn popularity user feedback general serve scheme heavily exploit region feature space generate data point region data point region model build use datum learn little sample region serve scheme introduce factor datum adjust factor obtain unbiased article score difficult fact early experiment update model use datum editorial bucket serve experimental bucket perform poorly bias affect empirical evaluation comparison learn base retrospective datum discuss later section interaction editorial team project involve considerable interaction human editor manually successfully place article portal year understand experience leverage automate serve scheme major challenge technically editor learn ml learn subtle consideration result framework editor control pool set policy constraint serve serve algorithm choose user visit experimental setup framework experimental setup create mutually exclusive bucket roughly equal size fraction live traffic serve traffic bucket use candidate serve scheme usual bucket process ensure statistical validity result create control bucket run editorial serve scheme addition create separate bucket random bucket serve article visit fashion framework framework consist component describe batch learn time lag view subsequent click approximately minute engineering constraint impose high datum volume update model occur minute constraint add editor serve satisfy business editorial algorithmic approach instance editorial team recommendation produce machine learn algorithm instance break news story immediately position user visit portal minute article early visit constraint add editor serve satisfy online model track build online model track article ctr user segment separately bucket article currently good serve bucket explore random bucket score current good article point promote serve bucket serve bucket serve article position minute window business rule separate online model track article position bucket article promote position serve bucket subsequently score model serve bucket article play position serve bucket course score model strategy random bucket use purpose provide simple strategy random exploration small probability p serve good article rank estimate score online model large probability p addition help estimate systematic effect pattern need build elaborate statistical model adjust serve bias bucket far collect month datum continuously run random bucket prove extremely useful study offline model run offline evaluation conduct simulation study randomization procedure adopt simple prove effective set setting different study classical literature develop strategy focus ongoing work section online model track article ctr online fashion study area time series method available literature application content optimization carefully study provide description dynamic model currently use system estimate popular emp model track ctr article position time use user feature subscript t notation refer tth interval article display bucket let ct denote number click view time t work empirical logistic transform define ct approximately gaussian large variance ct ct scenario roughly observation f position article minute interval transformation appropriate emp ss user segment decay pattern ctr position increase article lifetime fit dynamic linear growth curve model t t t t t t t equation constant offset term obtain offline model correction t mean yt time t t interpretation incremental decay level series time interval t t evolve interval accord addition stochastic element t evolution error t assume uncorrelate model parameter initialize observe value t article tracking begin t general initialization place feature base offline model build use retrospective datum provide intuition state parameter t t t evolve assume evolution t time point t t linear trend fit value weight square addition nonzero evolution straight line dynamic help tracking decay time fact value state evolution variance component t t relative noise variance v wt determine temporal smoothing occur model large relative value smooth use large history predict future model conduct kalman filter base discount concept explain detail discuss saturate model model generalize emp incorporate user feature particular user covariate use create disjoint subset segment local emp model build track item performance user segment small number user segment fit separate emp model user segment item f position number user segment grow data sparseness lead high variance estimate small segment especially early article lifetime address smooth article score segment time point bayesian hierarchical model particular predict mean variance item score different user segment time t derive new score follow emp model score item constant control shrinkage popular obtain estimator widely use online logistic regression olr ss provide flexibility incorporate low order interaction work large number feature instance age gender user model consider possible combination variable possible additive model base provide performance use efficient online logistic regression approach build model update parameter label event positive negative instead achieve empirically transform datum emp ss likelihood event achieve linearity parametrize linear function feature problem nonlinear perform quadratic approximation expansion achieve linearity modeling fitting detail discuss experiment section present result experiment compare serve scheme base online model emp ss olr current editorial programming approach refer ed online model significantly outperform ed base bucket alternative concurrently live traffic portal month offline identify reason personalization base user feature olr segmentation provide mainly sufficiently diverse article exploit ss olr predictive model emp finally extensive bucket test live traffic expensive unusual opportunity evaluate algorithm cast usefulness common practice compare serve algorithm base retrospective datum collect use serve scheme suggest bucket effective correction procedure essential conduct test live traffic statistical validity bucket testing methodology conduct extensive offline analysis small different variation model include feature selection narrow candidate evaluation follow emp ss age gender segment olr feature term article position user behavioral feature period time provide statistically significant improvement ed use scheme serve traffic bucket random set user month bucket run concurrently measure performance scheme lift term relative baseline scheme obtain significant improvement relative serve scheme report avoid online model comparison online model significantly increase ctr original manual editorial scheme increase achieve mainly increase reach ie induce user click article provide evidence favor strategy constraint content programming incorporate human editor algorithm use place optimize easily measurable metric figure lift different algorithm month online model emp ss significantly ed lift range clearly demonstrate ability online model accurately track realtime model base user feature ss olr statistically different emp indicate select user segment provide additional lift relative emp ss olr predictive likelihood relative emp retrospective datum analysis day concurrent good article male article female article lift fraction lift emp serve scheme olr ctr highly polar article b lift fraction polar figure experimental result bucket test result c polar article segment yaxis ctr global good article polar article lifetime segment refer text definition polar figure b lift relative ed term fraction click user lift achieve online model confine small user reflect user analysis personalization expect find personalization user segment provide additional ctr lift relative emp fact user feature predictive article ctr close look datum reveal main cause current editorial content generation process create candidate article expect popular user different user segment fact article affinity user segment define article ctr segment article overall refer polar article polar article pool candidate article usually pool popular polar segment result choose segment figure c gender segmentation polar article article overall great ctr segment polar article segmentation article article interval maximum expect ctr lift global ranking observe similar pattern segmentation base user feature retrospective evaluation metric bucket test common practice exist literature evaluate new serve algorithm use predictive metric obtain run algorithm retrospective datum collect use serve scheme instance approach use extensively study ad matching problem set equivalent compare new serve scheme eg emp ss olr ed compute predictive metric retrospective datum obtain ed find performance difference obtain use retrospective datum correlate obtain run live traffic find need random bucket datum effective technique correct bias rapid bucket testing compare serve scheme relate work news personalization use collaborative filtering provide realtime recommendation news article user closely relate prior work select vast pool content aggregate news site recommend subset small list item choose editor hand allow build model near realtime control mix item mean article high quality hard achieve lift simply eliminate bad article recent work match ad query ad webpage relate primary emphasis construct accurate offline model update long time interval model provide good initialization online model perform poorly reason discuss section author consider active exploration strategy improve search ranking similar spirit randomization procedure problem relate rich literature multiarmed bandit problem note assumption classical multiarmed bandit reinforcement learn literature satisfied set dynamic set article short article lifetime lag response fact short article lifetime content importance learn article behaviour quickly major challenge scenario preliminary experiment perform obvious natural modification widely use ucb scheme perform poorly recent study content aggregation site build model story popularity analysis base biased retrospective datum deploy model present result test conduct live traffic discussion paper describe content optimization problem select article present user information variant problem depend setting variant select large diverse pool article example include recommend feed article feed aggregation segmentation personalization likely effective variant address involve select small homogeneous set article segmentation effective pool article choose diverse high quickly estimate track popularity work suggest offline feature base model good rank article highly dynamic content publishing system article pool small dynamic high quality lifetime short utility metric measure strong dynamic component fact biased nature datum obtain serve scheme need obtain percentage datum randomized experimental design tradeoff involve maximize utility eg total number click quickly converge good article user user segment online model effectively initialize offline feature base model adjust factor perform unbiased exploration small randomized experiment key machine learn challenge setting address sufficiently handle small content pool deal large pool require significant advance focus current research reference agarwal model content optimization report tr agarwal d m estimate rate rare event multiple resolution page optimal attention bandit process dynamic allocation index journal royal statistical society series p generalize linear model chapman bayesian forecasting dynamic model finitetime analysis multiarmed bandit problem machine learn t joachim active exploration learn ranking datum international conference knowledge discovery datum mining clinical trial control clinical trial m r predict click estimate rate new ad page p agarwal d bandit taxonomy modelbased approach proc datum mining news online collaborative filtering asymptotically efficient adaptive allocation rule advance apply mathematic
geometry eye rotation listing apply mathematic computer abstract analyse geometry eye rotation particular saccade use basic lie group theory differential geometry parameterization rotation relate unifying mathematical treatment transformation coordinate system compute use formula describe listing law mean lie enable demonstrate direct connection donder law eye orientation restrict quotient space equivalent sphere exactly space gaze direction analysis provide mathematical framework study oculomotor system extend investigate geometry arm movement introduction saccade listing law saccade fast eye movement bring object interest center visual field know eye position restrict subset possible saccade tweed accord donder law eye gaze direction determine orientation uniquely orientation depend history eye motion lead gaze direction precise specification allow subspace position listing law observed orientation eye reach orientation primary t position single rotation axis lie plane perpendicular gaze direction primary position listing plane orientation eye torsion recently domain validity listing law extend include eye employ suitable mathematical treatment tweed use quaternion demonstrate addition order allow position single rotation lie listing plane tweed normal saccade perform approximately single axis validity listing depend rotation single axis target experiment axis rotation change saccade listing law obey point trajectory trace eye previous analysis eye rotation particular listing law base representation rotation quaternion rotation vector rotation matrix relate underlie mathematical object dimensional rotation group work analyse geometry saccade use lie algebra rotation group group structure briefly describe basic mathematical notion need later follow section analyse parameterization rotation point view group theory section contain detailed mathematical analysis listing law connection donder law base group structure section briefly discuss issue angular velocity vector axis rotation end short conclusion rotation group lie algebra group rotation dimension stand special orthogonal transformation use describe actual rotation denote eye position mean unique rotation primary position identity operation leave eye primary position identify position unit element group e e rotation parameterize d axis angle rotation axis generate continuous set rotation increase angle formally unit axis rotation continuous subgroup rotation angle plane perpendicular subgroup denote c explicit representation n matrix exponent calculate expansion let look example parameter rotation plane ie rotation axis represent case matrix direct computation rotation angle o sin o sin geometry eye rotation listing law identity matrix rotation matrix r construct axis angle rotation rotation achieve use instead lx scalar rescale angle collection matrix dimensional linear space element generator rotation plane set generator constitute lie algebra group space d rotation lie dimensional vector space span standard orthonormal basis comprise direction vector principal axis axis express linear combination basis element lie algebra represent matrix form corresponding basis matrix space l l u thank linear structure lie convenient analysis group addition linear structure lie bilinear operation define element bracket bracket operation vector usual vector cross product element lie algebra write matrix bracket operation relation expect relation basis matrix lie rotation group equivalent vector product finally accordance rotation matrix obtain stand component angle coordinate system rotation linear space position point simply parameterize coordinate wrt principal axis choose orthonormal basis nonlinear space rotation group define local coordinate look piece vector space coordinate system rotation base fact group element write exponent element lie algebra angle appear exponent serve coordinate underlie property essential compare system rotation usual real number c imply c corresponding equation element formula series t expansion use repeat element lie expansion order x variable stand element lie natural parameterization use representation rotation axis angle rotation angle appear canonical coordinate kind system constitute second type parameterization overall rotation obtain series consecutive rotation principal axis component angle canonical coordinate second kind present context type coordinate advantageous correspond single axis rotation turn represent natural eye movement convenience use canonical coordinate kind second type simply commonly use study oculomotor rotation matrix order rotation different formula use general tool obtain transformation coordinate system particular apply product rightmost term product result term arrive expression form right hand expression canonical angle log exponent equation obtain transformation formula angle canonical angle repeat calculation equivalent formula angle l transformation follow equation stand angle coordinate angle sign term equation sign case angle l bh error cause approximation small degree oculomotor range mention close additional parameterization quaternion rotation vector unit quaternion lie d sphere s embed constitute manifold group rotation double covering group local structure enable use quaternion parameterize rotation popular rotation vector write axis rotation angle closely relate contrast order expansion second order approximation usually appear literature example equation geometry eye rotation listing law quaternion central projection d affine space tangent quaternion r listing law donder law choice head fix coordinate system follow ex straight ahead direction horizontal plane e lateral direction e z point vertical direction ex e z define plane e e z define plane principal axis rotation lx set parallel head fix coordinate system reference eye orientation primary position choose gaze direction coordinate listing law express term lie allow position generate linear combination subspace lie algebra l z listing plane denote span lx h decomposition lie algebra direct sum linear subspace vector e project component v vi linear structure consider addition close bracket operation close vector addition lie bracket sub contrast sub algebra close fact stand algebra imply correspond group h correspond g generate rotation axis group rotation plane group linear structure ask kind decomposition projection achieve analogy answer positive projection perform follow element group e g multiply element subgroup h subset consider single object ab set constitute quotient space write s g map group quotient space understand divide quotient space group correspond fact subspace sub algebra quotient space construct difficult visualize mathematically equivalent geometrically point q e s connect center sphere line line run direction parallel vector q tangent space intersection line project point numerically simply vector q divide scalar table summary table biological notion corresponding mathematical representation term rotation group lie general eye position primary position eye allow eye position lie h span listing plane donder sphere gaze direction space unit sphere s embed equivalence follow way unit vector rotate head reach point unit sphere s point infinitely rotation point reach rotation axis leave vector e invariant factor rotation h order eliminate obtain onetoone correspondence require subset rotation sphere achieve matrix torsion rotation generate element listing plane obtain set r sin sin ljj sin ljj sin sin ljj sin sin sin sin ljj total angle rotation ljj angle axis ie ljj polar coordinate listing notice column left constitute coordinate point sphere unit exact correspondence group level lie algebra level fact describe reality global manner table summarize important biological notion concern listing law corresponding mathematical representation connection donder law listing law clear intuitive way sphere obtain eliminate torsion space gaze direction recall donder law state orientation eye determine uniquely gaze direction listing law imply need consideration torsion order emphasize point use fact locally look product topological space p u parameterize gaze direction torsion donder law restrict eye orientation unknown d submanifold product space p listing law submanifold piece sphere representation advantageous biological modelling mathematically set degree freedom gaze orientation torsion differ functionally principal bundle geometry eye rotation listing law axis rotation listing law mention introduction position require rotation axis angular velocity vector lie listing plane result group structure axis rotation contain listing plane matrix quotient space close multiplication form subgroup word ri rj matrix represent current target orientation eye correspond axis listing plane rl matrix form explain section condition finally normal saccade involve rotation single axis subgroup generate single element lie algebra addition property geodesic curve group manifold natural metric bilinear form group et conclusion analyse geometry eye rotation use basic lie group theory differential geometry unifying view present serve improve understanding oculomotor system extend study dimensional rotation joint upper acknowledgement like thank mathematical background fruitful discussion special thank initiate work reference c m analysis manifold lie algebra application math phy neural network m brain research tweed t neurophysiology tweed t vision representation eye position dimension movement lie group lie algebra optical
bayesian model inductive generalization b department brain cognitive abstract argue human inductive generalization explain bayesian framework traditional model base similarity computation previous work learn introduce unsupervised method construct flexible hypothesis space propose version bayesian trade prior likelihood prevent flexible space analyze publish datum set inductive reasoning result new behavioral study carry introduction problem inductive reasoning particular generalize specific example novel concept computer scientist early day computational approach inductive generalization range simple heuristic base similarity match complex statistical model consider fall spectrum base classic datum set literature comprehensive datum set collect argue model base rational bayesian framework issue previous model concept learn origin learner space present simple unsupervised clustering method create hypothesis space apply human similarity judgment embed bayesian framework consistently outperform good alternative model inductive reasoning base heuristic focus related inductive generalization task introduce involve reasoning property animal task judge strength generalization specific kind mammal different kind mammal animal kind property likely animal kind property example chimp squirrel horse predicate susceptible disease know outside example work predicate ensure people induction drive deep knowledge general feature animal detail know particular property stimulus typically present form argument premise example conclusion generalization test item chimp susceptible disease squirrel susceptible disease horse susceptible disease subject ask judge strength argument likelihood conclusion line true premise line true second task form conclusion instead ask likely property hold kind mammal horse ask likely hold mammal refer kind induction task specific general task respectively present datum experiment use task datum set contain human judgment relative strength specific inference different pair mammal example premise test specie horse set contain judgment argument strength general inference different triplet mammal example test category mammal osherson publish subject judgment similarity pair mammal use generalization experiment use build model generalization previous approach attempt model datum model featurebased model bayesian model factor determine strength inductive generalization osherson et model similarity animal premise conclusion coverage define similarity animal premise large taxonomic category mammal include specific animal type domain importance coverage factor compare follow inductive generalization chance horse disease know chimp squirrel disease high know chimp disease simple similarity favor generalization horse judge similar chimp similar primate specie squirrel coverage intuitively favor generalization set chimp squirrel cover set mammal set chimp extent set example support generalization mammal support generalization horse particular type mammal similarity coverage factor mix linearly predict strength generalization mathematically prediction mammal set example premise test set conclusion free parameter similarity metric define sum element maximal similarity element specific argument test set element horse maximum similarity horse example animal type general argument mammal approximate set mammal type use experiment figure osherson consider sumsimilarity model replace maximum sum sum similarity traditionally use model human concept learning rational interpretation term nonparametric density estimation osherson favor maxsimilarity model base match intuition particular task examine model experiment develop featurebased model encode share feature premise set conclusion set weight neural network psychological model consistently fit datum set significantly bad maxsimilarity model outline bayesian framework provide qualitative explanation inductive reasoning phenomenon model constrain learner hypothesis space embody generative model datum prediction depend strictly prior probability general method set prior probability quantitative prediction compare bayesian model previously introduce bayesian framework learn concept example apply learn number concept word meaning formally specific inference task observe example concept want compute probability particular test stimulus belong concept observe example generalization probability compute average prediction set hypothesis weight posterior probability hypothesis pick subset stimulus candidate extension concept depend test stimulus fall subset general inference task interested compute probability test category fall concept crucial component model task structure learner space hypothesis space element hypothesis space represent natural subset object domain subset likely extension novel property concept goal build capture hypothesis possible people employ concept learn use procedure ideally automatic unsupervised natural way begin identify hypothesis cluster return clustering hierarchical clustering particularly appropriate people appear organize concept biological specie hierarchical taxonomic structure apply standard clustering algorithm subject similarity judgment pair animal algorithm produce output figure suggest robust cluster structure define base set cluster consist cluster tree straightforward way define hypothesis space hypothesis consist base concept learning refer taxonomic hypothesis space clear sufficient chance horse disease know cow squirrel disease high know horse cow chimp mouse squirrel figure hierarchical clustering mammal base similarity judgment node tree correspond hypothesis taxonomic hypothesis space chimp squirrel disease taxonomic hypothesis consistent example set cow squirrel chimp squirrel bayesian generalization purely taxonomic hypothesis space essentially depend similar example squirrel ignore similarity structure example set cow squirrel similar target horse sense similarity clear objective basis biology single property apply taxonomic cluster chance convergent evolution disease question distinct cluster animal cow squirrel likely horse share taxonomic cluster cow disease distinct cluster chimp squirrel consider rich hypothesis subspace consist pair taxonomic cluster ie union cluster figure include consist taxonomic cluster include low layer stop behavioral datum example total hypothesis space union layer notion hypothesis space candidate concept correspond power set base cluster single cluster broadly applicable domain biological property base system cluster sufficiently framework parameterize possible concept analogous generalpurpose representation concept normal form classconditional mixture model classification bayesian balance prior likelihood hypothesis space bayesian generalization require assign prior likelihood hypothesis let number base cluster hypothesis th layer hypothesis space correspond union base cluster simple reasonable prior assign sequence d variable success parameter probability intuitively choice prior assume generative model hypothesis base cluster small independent probability express concept correspondence exact hypothesis express union base cluster multiple way consider minimal union define instantiate preference simple hypothesis hypothesis consist disjoint cluster small complex hypothesis receive exponentially low probability penalty complexity increase small prior apply set base cluster structure currently explore sophisticated prior taxonomic cluster define stochastic mutation process branch tree follow likelihood calculate assume example random sample replacement instance concept learn let number example let size hypothesis simply number animal type contain follow size principle include example include example assign great likelihood small hypothesis factor increase exponentially number consistent example observe increase note prior likelihood implement form bayesian prior favor hypothesis consist cluster likelihood favor hypothesis consist small cluster factor typically trade set example cover single cluster cluster large cover hypothesis minimal size include animal example use singleton cluster let number cluster equal number example posterior probability proportional product term seek optimal tradeoff model result consider datum set datum set come specific general task describe section task draw stimulus set mammal figure datum set include set similarity judgment use construct model come different group subject model probability generalization specific general argument equation respectively let example set vary trial trial respectively fix test category horse mammal osherson subject provide explicit judgment generalization example set relative rank strength argument general specific set convert model prediction rank datum set enable natural comparison model datum figure rank prediction model bayesian maxsimilarity sumsimilarity human subject rank judgment specific row induction task model free parameter bayesian model similarity model tune single value maximize correlation model datum jointly datum set good correlation achieve bayesian model general specific task great achieve maxsimilarity sumsimilarity model sumsimilarity model far bad actually correlate datum general task maxsimilarity consistently score slightly bad bayesian model new experiment vary example set composition order provide comprehensive test model conduct variant specific experiment use animal type constant test category horse example set different size similarity structure datum set number example constant trial expect vary number example cause difficulty maxsimilarity model explicitly sensitive factor purpose include argument example animal specie chimp chimp chimp argument animal chimp include argument example draw lowlevel cluster specie figure eg chimp chimp increase preference small hypothesis example observe baye general different prediction case maxsimilarity manipulation allow distinguish prediction bayesian model alternative bayesian formulation include size principle predict difference generalization example generalization example kind change judgment task cover story slightly match closely natural problem inductive learning randomly sample example subject tell train observe example particular animal novel disease require judge probability horse disease example observe cover story clear subject multiple example animal type present instance refer distinct individual animal figure row model predict generalization probability datum experiment mean rating generalization subject example set use example test specie horse argument prediction good value free parameter model fit different parameter value data set task difference great range stimulus example example argument strength figure human generalization conclusion category horse example single premise type cow chimp mouse premise category maxsimilarity model come close performance bayesian inconsistent qualitative trend datum notably find difference generalization example generalization example kind direction predict bayesian model generalization test category horse great example chimp example kind chimp chimp chimp figure effect relatively small observe animal type test statistically significant number example animal type maxsimilarity model predict effect bayesian account include size principle interest ask model sufficiently robust reasonable prediction experiment use single parameter setting good prediction heldout datum free parameter tune remain datum criterion maintain advantage maxsimilarity single value baye achieve correlation datum set respectively compare maxsimilarity single good parameter value use validation estimate run datum set split baye obtain average testset correlation datum set respectively compare maxsimilarity use method tune conclusion bayesian model offer moderate consistent quantitative advantage good similaritybased model generalization predict qualitative effect vary sample size alternative approach importantly bayesian approach principle rational foundation introduce framework unsupervised construction hypothesis space apply domain contrast similaritybased approach require arbitrary assumption form similarity measure include similarity coverage term base maxsimilarity sumsimilarity choice justification run counter similarity model apply domain lead conclude rational statistical principle offer good hope explain people generalize little datum consistently good performance maxsimilarity model raise important question future study relatively small number simple heuristic provide algorithmic implement approximate rational inference brain like understand people hypothesis space origin objective structure environment plausible source hypothesis space use rule actual biological taxonomy animal base history look different taxonomy use substitute true taxonomic cluster biology base cluster model hypothesis space lead dramatically bad prediction people generalization behavior taxonomy construct linguistic cooccurrence apply clustering algorithm similarity score output lead bad prediction likely possibility test cluster simple feature size shape speed weight appropriately reproduce taxonomy construct people similarity judgment push problem question define appropriate feature feature weight offer solution merely point question salient open problem try understand computational basis inductive provide valuable help statistical analysis support grant communication science laboratory fellowship ne reference s classify nature cognitive science volume r p pattern classification e bayesian analysis form induction rational model cognition s solution problem latent semantic analysis theory acquisition induction representation knowledge psychological review machine learn d osherson e smith induction psychological review j tenenbaum capture similarity human concept learn sixth international conference cognitive neural system s induction cognitive psychology p smyth clustering use crossvalidation second international conference knowledge discovery datum mining tenenbaum rule similarity concept learn solla t editor advance neural information processing system page mit press tenenbaum learn inference proceeding annual conference cognitive science society baye general specific sumsimilarity maxsimilarity figure model prediction axis plot human score axis column result particular model row different inductive generalization experiment indicate number example premise stimulus
goaldirecte decision making prefrontal cortex computational framework computer research animal learning behavioral neuroscience distinguish form action control form rely store action value goaldirecte form forecast compare action outcome base model environment control subject extensive research computational principle underlie goaldirecte control animal far receive attention present paper advance computational framework goaldirecte control animal human empirically point premise neuron prefrontal represent action policy neuron cortex represent reward neural computation domain appropriately understand perform structured probabilistic inference purely computational level result account relate closely previous work use solve markov decision problem extend work introduce new algorithm provably converge optimal plan cognitive level theory provide unifying framework different form goaldirecte action selection place emphasis novel form reward representation directly drive policy selection d study human animal behavior idea decision making rely qualitatively different mechanism decision stimulus response shape past reinforcement goaldirecte purposive decision making hand action select base consideration possible outcome future line action past year attention cognitive computationally tend focus control large interest potential link function algorithm reinforcement learning interest purposive action selection drive innovation animal behavior research yield powerful new behavioral reveal specific effect neural damage goaldirecte behavior discuss relevant data dayan recently point close relationship purposive decision making understand science modelbase method solution markov decision problem mdps action policy derive joint analysis transition function mapping state action outcome reward function mapping state reward important insight little work characterize computation underlie goaldirecte action selection discuss great deal evidence indicate purposive action selection depend critically particular region brain prefrontal cortex currently critical open question relevant computation brain course basic computational problem formulate optimal policy model mdp extensively study algorithm consider potentially relevant prefrontal function value iteration policy backward induction linear programming cognitive perspective approach solve mdps particularly appeal consider particular researcher suggest method solve mdps probabilistic inference interest idea present context derive recent movement frame human animal information processing underlie neural computation term structured probabilistic inference perspective consider goaldirecte action selection neural mechanism underlie understand term challenge investigate possibility previous research algorithm solve mdps probabilistic inference provably yield optimal policy align know action selection brain start fill gap follow section introduce account goaldirecte action selection perform base network component map specific brain structure account introduce new algorithm solve mdps convergence proof present result set simulation illustrate framework account variety behavioral phenomenon think involve purposive action selection co m p u t l m o d note early prefrontal cortex believe play role purposive behavior indicate broad association prefrontal lesion goaldirecte action human animal recording datum suggest different sector distinct contribution particular neuron prefrontal appear encode taskspecific mapping stimulus response eg task representation language psychology policy language dynamic programming understanding policy representation guide execution little know representation select basic proposal policy representation select modelbase fashion leverage information contingency transition function incentive value associate specific outcome state reward function extensive evidence suggest association represent area cortex transition function clear brain contain detailed representation association anatomical localization entirely clear evidence suggest effect simple action represent inferior frontoparietal cortex evidence suggest medial temporal structure important forecast action detailed section model assume policy representation representation representation state action brain region coordinate network structure represent causal statistical policy selection occur network process probabilistic h t e c t u implementation form direct graphical model layout figure node represent discrete random variable state variable s represent set m possible world state serve role play parietal medial temporal cortex represent action action variable represent set available action play role highlevel cortical motor area involve programming policy variable represent set deterministic policy associate specific state capture representational role local global utility variable describe leave decision right sequential decision capture role include set m policy nod represent incentive value separate set node include discrete timestep planning horizon conditional probability associate variable represent form state probability base state action variable precede timestep encode transition function action probability depend current state associate policy variable utility depend current state represent reward magnitude continuous variable adopt approach introduce represent reward posterior probability binary variable associate large positive reward raise near state associate large negative reward reduce simulation report use simple linear transformation map scalar reward value r si p u max r j situation involve sequential action expect return different timestep integrate global representation expect value order accomplish employ technique propose introduce global utility variable u u binary random variable associate posterior probability determine p ug number nod network embody generative model instrumental action basic idea use model substrate inference order arrive optimal policy general method accomplish correspond form query desire outcome state identify treat state variable initial state variable observe application approach second expect return specific plan evaluate compare conditioning specific set value policy node focus obvious possibility condition directly utility variable u explain p l c e l e c t p b l s t e c e t e r t e l g o r t m introduce idea infer optimal decision influence diagram treat utility node binary random variable condition variable technique adopt recent work aware application guarantee optimal decision sense task introduce simple algorithm guarantee procedure follow initialize policy node set prior treat initial state u g observed variable use standard belief note temporal incorporate framework minimal modification equation situation u node variable treat observe propagation comparable algorithm infer posterior distribution policy nod set prior distribution policy node value posterior obtain step step section present proof monotonicity convergence monotonicity policy node probability associate optimal policy rise iteration define follow p current set probability distribution policy node subsequent timestep note assume simplicity unique optimal policy objective establish p t index process iteration dynamic network entail p t p t represent value policy decision node consider substitute p t p t point focus single iteration permit omit relevant subscript apply baye law yield p p u p p cancel bring denominator p p ug rewrite left hand obtain p u p p ug p far rearrange p u p ug p u p u p u p u p p p ug note inequality follow definition remark course identity depend particular policy globally optimal plan set choice optimal fortunately requirement guarantee meet long upper bind place number processing cycle recall consider problem note policy lead state successor relevant policy node fix guarantee optimal policy proof continuously rise reach maximum immediately precede decision fit globally optimal policy process work backward fashion backward induction convergence continue notation note apply baye law recursively p p pt pt p p ug p p p p p ug p p p p wish prove p p t rearrange t p ug p note relationship p processing iteration p ug previous iteration p p p u p u p pt ug pt pt mind rewrite left hand product follow p p p u p p u ug u u p ug p p p ug p p note numerator factor cancel denominator subsequent factor leave denominator expression rewrite p ug p u u p p u p ug p p objective equal p proceed directly definition p ug p ug term sum approach remain term equal p p p p simulation binary choice begin simulation simple incentive choice situation animal face press left reliably yield preferred food r right prefer food r represent contingency network structure leave employ iterative describe section yield result figure posterior probability policy press leave press right marginal value posterior label ev expect value dash horizontal line indicate expect value optimal plan model obviously converge key empirical purposive behavior involve outcome action yield previously value outcome incentive value outcome reduce example pair event simulate binary choice scenario describe reduce reward value food yield left appropriate change yield choice b purposive action causal connection outcome remove contingency degradation simulate start model fig change conditional probability t reflect left action outcome result behavior fig c fig simulation result binary choice stochastic outcome critical aspect present modeling paradigm yield choice stochastic domain property distinguish recent approach use graphical model plan illustrate use architecture figure leave simulate choice fair coin left coin yield head tail right coin head tail loss illustrate fig model maximize expect value opt left coin fig simulation result twostep sequential choice sequential decision adopt twostep scenario use fig represent task contingency graphical model base template fig right use reward value indicate fig yield choice behavior figure b follow shift state represent graphical model change reward function impose change level yield choice behavior fig model use simulate decision start scenario fig simulate insertion scalable barrier rs appropriate change result behavior fig d famous empirical demonstration purposive control involve behavior use maze fig food reward place find rat react barrier location upper route barrier b long low route simulate experiment represent correspond transition reward function graphical model form fig right represent insertion barrier appropriate change transition function result choice behavior critical fig fig simulation result behavior barrier barrier barrier b classic empirical demonstration involve latent learn allow rat explore maze fig later insertion food reward follow immediately dramatic reduction run time reflect reduction entry blind simulate effect model base template fig right represent maze layout appropriate transition function absence reward random choice occur intersection set rs result set choice indicate heavy arrow fig fig latent learning rel t t p work initial proposal solve decision problem probabilistic inference graphical model include idea encode reward posterior probability random utility variable cooper relate idea present include use node integrate information multiple utility nod recently rao use graphical model solve problem leverage probabilistic representation reward way guarantee convergence optimal reward maximize plan closely relate present research work storkey employ iterative approach introduce certain resemblance procedure evident view policy variable model parameter mapping state action possible formal equivalence algorithm propose report cognitive proposal present work bear close relation recent work address prefrontal computation underlie goaldirecte action selection present effort tie closely normative principle decisionmake work tie closely detail neural circuitry respect approach prove complementary interesting far consider simulation set state associate state node limit set reachable state relevant timestep assume initial state acknowledgment thank pereira useful comment r e f e c e s hull cl principle behavior purposive behavior animal man action development behavioral transaction goaldirecte instrumental action contingency incentive learn cortical substrate p p dayan competition prefrontal system behavioral control nature neuroscience p model prefrontal cortical mechanism goaldirecte behavior cognitive neuroscience p ad purposive behavior cognitive mapping neural network model biological cybernetic rd dynamic programming influence diagram ieee transaction system man cybernetic rao planning act uncertain use probabilistic inference international conference intelligent robot system m storkey probabilistic inference solve discrete continuous state markov decision process proceeding international conference learn h planning probabilistic inference proceeding th int workshop artificial intelligence statistic rd decision making use probabilistic inference method uncertainty artificial intelligence proceeding eighth conference tenenbaum yuille probabilistic model cognition trend cognitive ed bayesian brain probabilistic approach neural code mit miller cohen theory prefrontal cortex function annual review neuroscience taskspecific neural activity primate prefrontal neurophysiology p function cortex brain cognition neuron cortex encode economic value nature p et theory causal learning child causal map baye net psychological review outcome represent human inferior frontoparietal integrate decisionmake current opinion neurobiology bayesian network decision cooper method use belief network influence diagram fourth workshop uncertainty artificial intelligence p dayan normative perspective motivation trend cognitive effect introduction reward maze performance rat publication psychology p
grammar transfer second order recurrent neural network abstract know people expose sentence generate artificial grammar acquire implicit grammatical knowledge able transfer knowledge input generate modify grammar second order recurrent neural network able transfer grammatical knowledge language generate finite state machine language differ vocabulary syntax representation grammatical knowledge network analyze use linear discriminant analysis introduction field artificial grammar learn people know able transfer grammatical knowledge new language consist new vocabulary furthermore effect new string violate syntactic rule slightly long similar old string past study recurrent neural network ability generalize previously acquire knowledge novel input instance neural network generalize abstract knowledge acquire domain new domain train network predict input symbol grammatical sequence domain network able learn predict grammatical sequence second domain effectively learn prior learning training second domain weight network prevent catastrophic use simulation paradigm analyze domain transfer effect similarity training sequence effect information human datum hanson prior learning grammar facilitate learning new grammar case syntax vocabulary constant study investigate grammar transfer neural network syntax vocabulary different source grammar target grammar network weight network allow change e learning target grammar allow investigate interference transfer source grammar target grammar simulation design grammar transfer task follow simulation neural network train sentence generate state machine fsm test learning sentence generate fsm pair fsm use grammar transfer task fig symbol eg b c denote word number represent state state number incoming arrow state number foot left fsm initial state number circle left fsm fig accept state pair diagram transfer test direction left fsm right fsm opposite direction word sentence generate fsm present network word time time word select randomly possible word end sentence possible current state equal probability state update state sentence length limit word exclude start task network predict correct sentence network predict sentence end current input activity output node network threshold value output threshold value note fsm accept state far state sentence end prediction succeed fail network eventually learn yield high value fsm accept state network learn training sentence test randomly generate sentence training session complete network correct end point judgment sentence network train sentence generate fsm extent transfer measure reduction number sentence require train network fsm prior learning fsm compare number sentence require train network current fsm network architecture learning network second order recurrent neural network add hide layer receive order connection input layer fig network input layer node b c start output layer node input hide layer node state hide layer node feedback layer node recurrent neural network use model syntactic processing second order network suit processing language generate fsm learn carry weight update rule recurrent network develop extend second order connection necessary learning rate momentum respectively high low threshold initialize respectively adapt network process test sentence follow high threshold modify minimum value yield end point test sentence margin low threshold modify high threshold margin threshold use training test output layer state hidden r feedback r input r figure second order recurrent network use simulation network consist input layer receive word output layer predict end hidden layer input hide layer state hide layer feedback layer receive copy state hide layer activity simulation result transfer effect number require training change number training average network different initial weight fig number parenthesis standard error number training change sign increase sign reduction instance fig require sentence presentation network learn left fsm network train right fsm hand require sentence presentation network learn left fsm scratch reduction transfer direction right left note network train sentence source grammar criterion sentence target grammar completion target grammar learn knowledge source grammar extent network eventually learn grammar number require training examine cycle cycle number require training reduce representation grammatical knowledge analyze representation grammatical knowledge network discriminant analysis lda apply hide layer activity lda technique find set coefficient define linear combination input variable use discriminate set input datum belong different category linear combination hide layer node activity use coefficient provide lowdimensional view hide layer activity good separate specify category eg grammatical function respect lda similar principal component analysis pca find dimension datum large variance lda find dimension differentiate specify category l l d e gj d figure initial saving observe grammar transfer task number require number training average network different initial weight number parenthesis standard error number change number training transfer negative change mean reduction positive transfer positive change mean increase negative transfer interference m d e linear figure state space organization grammar transfer task case state space activity correspond plot square diamond circle respectively state space activity belong target fsm dot plot belong source fill pattern state input stat correspond state linear figure trajectory correspond loop fig b state hide layer state space line correspond share discrimination cue boundary describe white diamond white circle e state source grammar discrimination boundary diamond dot square dot e state target grammar triangular shape state trajectory correspond input ellipsis state space activity involve state loop state state state loop state discussion grammar transfer task fig initial accept state fsm different frequency distribution subsequence word similar short sentence case save observe transfer direction little change require training direction second grammar transfer task direction fsm reverse mirror image sentence accept grammar accept grammar grammar different significant overlap short subsequence case saving training fourth grammar transfer task source target grammar share subsequence case fig c instance subsequence different source grammar loop state word loop target grammar consist different word d e case little change number learning require transfer direction increase direction fourth case fig d reduction direction increase direction number learning require observation hypothesize case syntax transfer acquire grammar allow subsequence word appear target grammar equivalent symbol set substitute transfer easy saving source saving grammar transfer vocabulary transfer task source saving organization state hide layer activity directly reflect space organization grammar transfer fig b fig change state hide layer activity draw state space organization triangular line trajectory network receive create state loop fsm region trajectory correspond state loop loop fig trajectory line avoid figure state space activity belong different state loop tend distinct belong fsm state tendency allocate vocabulary transfer region belong different fsm loop tend region belong grammar cause state space structure furthermore find significant correlation correct rate linear discrimination respect reflect extent state space organization reflect state saving reasonably argue saving transfer grammatical knowledge lowlevel processing specific neural network instance network weight value appropriate range stage source grammar learning unnecessary target grammar conduct simulation examine effect alter initial random weight use source target grammar space limitation permit present detail observe effect initialize bias weight appropriate range state space organization statistic source saving transfer mention state space organization observe grammar transfer task observe vocabulary transfer task fig region discriminate far region represent combination current network state current vocabulary yield different network state hide node provide discrimination place boundary network state space boundary line collectively define region state space correspond set combination treat equivalently term task boundary share instance boundary line fig discrimination boundary white diamond white circle e state source grammar discrimination boundary diamond dot square dot e state target grammar share boundary source saving boundary create source grammar learning use possibly modification boundary target grammar word source saving high level low level feature syntactic processing level conclusion investigate ability recurrent neural network transfer grammatical knowledge previously acquire language find network able transfer grammatical knowledge new grammar slightly different syntax define new vocabulary grammar transfer extent transfer depend subsequence symbol generate grammar equivalence set translate result present paper restricted type syntax cover size syntactic rule vocabulary hypothesize ability network transfer grammatical knowledge come share discrimination boundary input vocabulary combination sum hope demonstrate neural network simply learn association input symbol acquire structural knowledge input reference r abstract analogy abstract grammar comment experimental psychology g mapping domain feedback neural network model transfer implicit knowledge cognitive science distribute representation simple recurrent neural network grammatical structure machine learn gile l miller c b sun learn extract state automata secondorder recurrent neural network neural computation emergence explicit knowledge symbol rule neural network submit transfer syntactic structure synthetic language experimental psychology learn algorithm continually run fully recurrent neural network neural computation
graphical model inference missing datum dept computer science dept computer science dept computer abstract address problem recoverability ie decide exist consistent estimator relation q datum miss random employ formal representation missingness graph explicitly causal mechanism responsible missingness encode dependency mechanism variable measure use representation derive condition graph satisfy ensure recoverability devise algorithm detect presence condition graph introduction miss datum problem arise value variable miss record observation extent problem evidence vast literature miss datum diverse field social science statistic biology computer science miss datum cause varied factor high cost involve measure variable failure sensor answer certain question miss datum play major role survival datum analysis treat primarily use machine learn typical example system automatically generate list product potential interest user incomplete dataset user rating online portal amazon employ system area datum mining knowledge discovery network tomography miss data problem miss datum consequence firstly significantly bias outcome research study mainly response profile significantly different ignore distort true proportion population secondly perform analysis use complete case ignore case missing value reduce sample size substantially reduce estimation efficiency finally algorithm statistical technique generally draw inference complete dataset difficult inappropriate apply algorithm statistical technique incomplete dataset exist method handle miss datum method handle miss datum describe rich literature book article software package briefly summarize listwise deletion pairwise deletion use approximately study social behavioral science listwise deletion refer simple method case missing value delete datum miss completely random listwise deletion bias outcome pairwise detailed discussion direct reader book deletion available case deletion method use estimate pairwise relation variable example compute covariance variable case observation observe use regardless variable dataset missing value expectationmaximization general technique find maximum likelihood estimate incomplete datum prove inference ignore miss data mechanism lead unbiased estimate assumption miss work machine learning assume proceed ml bayesian inference exception recent work collaborative filtering system develop probabilistic model explicitly incorporate miss data mechanism ml use conjunction imputation method term substitute reasonable guess miss value simple example mean miss observation variable substitute mean observe value imputation imputation multiple imputation example popular imputation procedure technique work practice performance guarantee eg convergence base primarily simulation experiment miss datum discuss far special case coarse datum datum contain observation power set sample space variable interest notion random car introduce identify condition mechanism ignore draw inference distribution variable notion sequential car discuss detailed discussion datum refer miss datum literature leave question regard theoretical guarantee result estimate nature assumption prior employ procedure assumption testable introduction miss datum problem issue refer paper aim miss datum problem use causal graph justification question pose target relation q estimate set assumption missingness process encode graphical model condition consistent estimate exist datum available answer question missingness mgraph short describe section furthermore review traditional taxonomy miss datum problem cast graphical term section define notion recoverability existence consistent estimate present graphical condition detect recoverability probabilistic query q conclusion draw section graphical representation missingness process missingness graph rx ry rx b c d figure mgraph datum mcar b mar c mnar solid circle denote partially fully observe variable respectively graphical model dag direct graph use encoding conditional independency causal relation graphical criterion dseparation refer appendix definition use read graph graphical model use analyze miss information form miss case sample selection bias use causal analyze missingness partially observe outcome use auxiliary variable paper miss value associate variable interaction missingness mechanism remain need exist general approach capable model arbitrary process decide missingness dataset generate process general approach allow variable govern missingness mechanism mechanism trigger potentially partially observe variable model achieve flexibility use graphical model missingness graph mgraph short dag direct graph define follow let e causal dag u r v set observable node node graph correspond variable datum set set unobserved node latent variable e set edge dag use edge shorthand notation denote existence u variable common parent variable vo r v partition vo v set variable observe record population v set variable miss record variable x term fully observe vo partially observe associate partially observe variable vm variable variable actually observe represent status mechanism responsible missingness formally m contrary conventional use treat merely missingness indicator switch enforce equality set variable r set causal mechanism responsible missingness r variable parent variable u graphical representation depict relationship variable v process account missingness variable graphical representation missingness graph mgraph short dseparation graph imply conditional independence distribution mgraph provide effective way represent statistical property missingness process potential recover statistic variable partially miss datum taxonomy missingness mechanism common classify miss datum mechanism type miss completely random mcar datum mcar probability miss independent variable study case decide reveal level base miss datum mar datum case p denote observed component miss component example woman population likely reveal age miss random mnar miss datum mcar term mnar example online rate item high probability item word probability supply rating dependent underlie like invoke specific value observed unobserved variable author find rubin definition difficult apply practice prefer work definition express term independency variable interpretation use paper mcar define total independence r vo u depict figure mar define independence r vo ie r vo depict figure b finally condition hold datum term mnar depict figure interpretation use slightly strong assumption rubin advantage user encode communicate assumption determine classification problem additionally conditional independency define class represent explicitly separation condition corresponding mgraph use taxonomy rest paper label data mcar mar mnar accord define condition u mcar u vo mar satisfied corresponding mgraph recoverability section examine condition estimate probabilistic relation q compute begin define notion definition recoverability mgraph g target relation q define variable q recoverable g exist algorithm produce consistent estimate dataset d p d compatible strictly positive complete case ie p vo r assume observed distribution complete case p vo r strictly positive render recoverability property exclusively corollary relation q recoverable q express term probability p r vo set observable variable word model m m induce distribution p m p m respectively p m o p m qm proof sketch corollary merely requirement obtain consistent estimate term observable practically recoverability mean datum generate process compatible procedure exist compute estimator limit large sample converge q procedure consistent estimator recoverability property q datum available routine choose analyze process datum recoverability datum mcar mcar datum vm write p p r p vo r r observable joint probability p consistently estimable recoverable consider complete case listwise deletion follow example example let treatment outcome depict mgraph let case delete value sample recover compute mgraph g know dseparation p y ry particular p x y p y p p ry consistently estimable p recoverable recoverability datum mar datum p vm vo p vm vo p vo joint distribution p recoverable example let treatment outcome depict mgraph fig b let case patient undergo treatment likely report outcome arrow circumstance recover compute mgraph g dseparation p p p y ry p application truncation problem certain combination event occur case definition need modify accommodate constraint appendix modification complicate definition recoverability change basic result derive paper particular p ry p p p ry p fully observable p recoverable note permit p recover listwise deletion require p estimate sample include miss paper focus recoverability large sample assumption deal shrink sample size issue recoverability datum mnar datum mar mcar term mnar generally believe relation mnar dataset recoverable follow example demonstrate example fig d depict study unit undergo treatment report outcome delete value treatment case miss value x render dataset mnar p recoverable d compute rx rx rx ry p p p rx p rx equation x p p rx p rx recoverable eq dictate p estimate dataset step delete case missing create new datum set estimate p d far prune form dataset d remove case miss p compute note order matter delete case reverse order result estimate bias dseparation need establish validity p p support sequence deletion deletion order feature worth note regard taxonomy missingness mechanism mcar mar verify mgraph general verify datum assumption mcar allow estimation procedure asymptotically listwise deletion dictate procedure listwise deletion apply mar procedure mcar problem safe conditional independency require recoverability assumption hold mcar problem r vo vo converse hold fig b apply listwise deletion likely result bias necessary condition r vo violate graph interesting property evolve discussion recoverability certain relation require vi vo subset suffice property p recoverable p proof p decompose p vi p vi p p recoverable important note recoverability p fig d feasible fact missingness model consider rubin define fact majority datum generate mnar example rubin brief discussion line refer appendix question determine relation recoverable follow provide sufficient condition recoverability condition recoverability theorem query q define variable vo recoverable decomposable term form p sj contain missingness mechanism partially observe variable appear qj proof decomposition exist qj estimable datum entire expression q recoverable example equation demonstrate decomposition p product term p x rx q p satisfy condition q recoverable example consider problem recover mgraph fig b attempt decompose q chain rule eq satisfy condition theorem write p note graph permit augment term necessary rx ry term independent rx condition partially observe independent ry condition partially observe use decomposition p rx q p rx p rx p rx p rx p ry x rx denominator obtain use independency rx graph final expression satisfie theorem render recoverable example recovery feasible datum requirement recoverability joint p mgraph g edge variable latent variable parent r variable necessary sufficient condition recover joint distribution p variable parent missingness mechanism rx recoverable p p r p q m m p ri ar p vo p ri parent ri proof observed joint distribution decompose accord p r p r p p ri ri use fact edge r variable latent variable parent r variable parent p ri ri p p ri r m p ri p ri r strictly positive p r vo probability p ri strictly positive use equation conclude ri ri p recoverable necessity parent missingness mechanism rx p recoverable base appendix joint p recoverable follow sufficient condition recover joint distribution theorem mgraph latent variable markovian joint distribution p recoverable missingness mechanism rx descendant correspond variable x recoverable p p p vi p p p parent proof refer appendix definition order factorization order factorization set o order variable denote o product conditional probability p yi minimal set sufficient condition recoverability relation q q decomposable order factorization sum factorization factor qi p satisfie factorization satisfy condition admissible rx ry rx rx rx ry c b rx rx ry figure graph p recoverable b p recoverable condition example p x recoverable p z recoverable proof follow note order factorization specific form decomposition allow confirm recoverability certain query q model fig b d satisfy requirement example apply conclude figure p recoverable figure p x z p rx p rx ry p recoverable figure p z p recoverable note condition differ way firstly decomposition limit order factorization singleton set secondly xi vo exclude r variable example consider query p fig b q decompose variety way p factorization p x px x x p order x b p p x x p order p c p p x x p order dseparate c admissible factor satisfie theorem specifically c write p x rx p estimate deletion schedule ie delete sample rx compute p rx theorem satisfied graph permit rewrite p p rx p p rx heuristic find admissible factorization consider task estimate p x set search admissible factorization p satisfie theorem possibly resort additional variable reside outside serve separate set exponentially large number order factorization helpful rule class order prior detect graph section provide lemma prune process harness information graph order set yield admissible decomposition exist partially observe variable vi order independent minimal separator refer appendix definition dseparate appear proof refer appendix b d c e ry rx rd b figure demonstrate example p x recoverable example apply require solution set constraint represent direct example let q p relation recover graph fig let c d e total number order factorization independency imply minimal separator require c d e d rd c e b d e c test potentially admissible need variable order rule soon note appear b minimal separator dseparate precede lemma violate order c e b c e b d satisfy condition state potential candidate follow lemma present simple test determine specify condition order remove set candidate order likely yield admissible factorization order set yield admissible decomposition contain partially observe variable exist set s dseparate vi proof factor p vi vi correspond vi satisfy condition require interesting consequence follow corollary sufficient condition order factorization label admissible corollary disjoint set exist admissible factorization recover relation p theorem contain partially observe variable exist set s dseparate vi conclusion demonstrate causal graphical model depict datum generate process serve powerful tool analyze miss datum problem determine theoretical exist eliminate bias datum missingness procedure produce consistent estimate procedure find formalize notion recoverability relation recoverable datum miss random mcar mar importantly commonly occur problem recoverability achieve datum miss random mnar far present sufficient condition ensure recoverability relation theorem use graphical criterion theorem summary demonstrate insight capability gain exploit causal knowledge miss datum problem research support grant nsf iis onr reference miss datum series quantitative application t n network tomography general topology acm performance evaluation review volume page acm p mind strategy handle miss health behavior rm sn use causal diagram guide analysis miss data problem statistical method medical research r temporal constraint network artificial intelligence apply miss datum analysis data mining knowledge discovery sense data ieee expert m definition diagnosis problematic randomized control experiment work paper available sequential model missingness proceeding seattle symposium page springer jm random characterization conjecture counterexample proceeding seattle symposium miss datum analysis design statistic social behavioral coarse datum annal statistic page little rubin statistical analysis miss datum rs collaborative prediction rank miss datum proceeding acm conference system page acm roweis m collaborative filtering missing assumption roweis m system miss datum statistical model estimation ijcai pe km miss datum introduction mining knowledge discovery pearl model miss datum appear proceeding aistat available pearl probabilistic reasoning intelligent system network plausible inference pearl causality model reasoning recoverability miss datum introduction summary result technical report r available m advance miss data method implication educational research real datum analysis page ck miss datum educational research review practice suggestion improvement review educational research rubin inference miss datum biometrika multiple imputation survey multiple imputation year journal association miss datum view state art psychological method rise selection auxiliary variable miss datum problem auxiliary variable create equal technical report technical report r jm unified method censor datum modeling miss data publisher
combine causal similaritybased reasoning b department brain cognitive science abstract inductive reasoning draw kind knowledge include knowledge relationship property knowledge relationship object previous account inductive reasoning generally focus kind knowledge model causal reasoning focus relationship property model similaritybased reasoning focus similarity relationship object present bayesian model inductive reasoning incorporate kind knowledge account human inference property biological introduction good table strong sit predict object unobserved property basic inductive problem kind knowledge appear relevant different researcher emphasize role causal knowledge similarity category judgment association mapping script intuitive theory approach account important subset inference isolation approach fundamentally limited human draw multiple kind knowledge integrate require eventually model attempt match ability initial step goal present model inductive reasoning sensitive causal relationship property similarity relationship object inductive problem consider formalize problem miss entry matrix figure previous account inductive reasoning generally address version problem model causal reasoning usually focus relationship property figure animal instance likely animal model usually focus relationship object figure b carry gene probably likely carry gene previous model account inference rely similarity causality carry gene x gene cause enzyme express likely express enzyme figure c develop unifying model handle inference previous probabilistic approach causal reasoning similaritybased reasoning formal framework overcome limitation approach approach rely causal graphical model typically assume feature vector object row matrix figure conditionally independent causal network feature suppose example row matrix correspond people causal network state lead lung cancer probability suppose identical twin lung cancer assumption conditional independence imply equally likely suffer long cancer conclusion assumption false variable unknown capture unknown biological environmental factor mediate relationship disease deal unknown b c o o figure model causal reasoning generally assume row matrix conditionally independent causal structure feature model use prediction unobserved feature novel object b model similaritybased reasoning generally assume column matrix conditionally independent similarity structure object model use prediction novel feature c develop generative model matrix incorporate causal relationship feature similarity relationship object model use kind information prediction matrix miss entry variable difcult suggest knowledge similarity object help similar model correctly predict likely lung cancer previous model similaritybased reasoning suffer restrictive assumption conditional independence time assumption state feature column matrix figure b conditionally independent information similarity object empirical test similaritybased model attempt satisfy assumption use example tell property p ask judge probability fox property p rst approximation inference task judgment similarity subject conclude example fox likely property p mouse similar fox mouse people natural reason property link familiar property violate assumption conditional independence suppose instance learn fox rat likely share property fox fox similar general fox rat model capture inference incorporate causal relationship property case link property live limit assumption conditional independence avoid specify joint distribution entire matrix model use distribution sensitive relationship property similarity relationship object know previous model attempt combine causality similarity set experiment suggest people difcult combine source information introduce model present experiment design test result suggest people able combine causality similarity model account capacity generative model matrix consider rst probabilistic approach similaritybased reasoning assume object structure graphical model capture relationship know set object figure suppose instance object include mouse rat squirrel sheep view graphical model capture phylogenetic relationship formalization intuitive similarity animal feature interest feature value object collect object vector distribution p vo vector work case treestructure graphical model sort previously use method phylogenetic cognitive model property induction object lie leave tree assume object vector binary vector generate mutation process tree process parameter represent base rate novel expect proportion object feature instance low model predict novel feature probably find animal feature occur exactly animal mouse rat likely pair mouse sheep mutation process formalize continuoustime markov process state innitesimal matrix q generate object vector model imagine binary feature spread tree root leave feature root probability feature switch state point branch parameter determine easy state state high easy markov process enter state difcult leave consider probabilistic approach causal reasoning assume sf feature structure graphical model capture relationship know set feature figure feature instance correspond enzyme sf capture causal relationship enzyme possible structure state enzyme involve production turn involve production enzyme feature value object collect feature vector distribution p vector suppose interested model combine knowledge represent sf figure mouse express enzyme instance combine model predict rat likely squirrel express enzyme formally seek distribution p m m matrix p m sensitive relationship feature relationship animal distribution bayesian inference use prediction miss entry partially observe matrix feature sf happen independent figure b assume column matrix generate base rate fi consider case capture causal relationship feature figure c causal relationship typically depend hide variable causal relationship enzyme instance likely depend biological variable causal link cancer mediate genetic environmental variable little know hide variable rst approximation assume respect similarity structure figure example unknown variable mediate relationship likely value formalize intuition convert probabilistic model figure equivalent model sfd figure b use deterministic combination independent random event random event include hidden relevant variable figure b example model sfd indicate effect e deterministically present cause c present transmission mechanism t active background cause b activate model sfd equivalent sf sense model induce distribution variable appear general model sfd meet condition algorithm convert sf model application desirable integrate model attempt choose model sfd variable specic deterministic model assume root variable sfd independently generate precisely suppose base rate ith variable distribution p m seek meet condition note candidate matrix m column variable sfd marginal distribution row match c c c e b c c c e b c e e b e e c c t b e c t b e c t b e c t b e figure graphical model capture probabilistic relationship cause c effect e b deterministic model sfd induce joint distribution c indicate mechanism causal transmission c e active indicate true owe background cause independent c root variable independent remain variable e deterministically specie root variable xe graphical model create combine sfd representation similarity object root variable t independently generate tree note arrow edge combine model suppress distribution specie sfd second fi root variable sfd marginal distribution column match distribution specie precisely distribution p m condition represent use graphical model combine model suppose object create combine model rst introduce copy sfd root variable sfd connect copy variable accord structure figure c result graph provide topology combine model conditional probability distribution inherit sfd node belong ith copy inherit remain node inherit deterministic distribution p m represent graphical model standard inference technique use compute miss entry matrix m result paper compute use implementation junction tree include baye net toolbox experiment inductive inference rational agent exploit information available include causal relationship feature similarity relationship object human able meet normative standard clear certainly vary task task hand motivate example case natural think causal relationship similarity relationship time hand argue causal information tend similarity information support conclusion datum task involve articial category help resolve compete view design task subject require simultaneously reason causal relationship enzyme similarity relationship animal experiment material method adult participate experiment subject ask reason presence enzyme set animal mouse rat sheep squirrel subject train causal structure involve enzyme use experiment enzyme chain condition subject tell know produce task r r r r r r r r r r task task r b r task mouse combine similarity figure experiment behavioral datum column prediction model result chain condition know test result mark arrow task subject tell mouse test positive task tell addition rat test negative error bar represent standard error mean b result pathway common pathway begin stimulate production turn lead production condition subject tell know produce pathway common pathway involve involve reinforce causal structure subject card represent animal different mammal specie specie supply card animal represent animal test positive enzyme card choose representative distribution capture causal network know structure chain known parameterization chain condition example network noisyor network form chain leak probability set probability causal link active set subject study card long like card remove subject ask question enzyme eg learn new likely mammal produce question training phase intend encourage subject causal relationship enzyme condition subject tell test animal mouse rat sheep squirrel enzyme condition include task chain condition subject tell mouse test positive ask predict outcome remain test figure c subject tell addition rat test negative ask predict outcome remain test note second task require subject integrate causal reasoning similaritybased reasoning causal reasoning predict mouse similaritybased reasoning predict condition subject tell mouse test positive tell addition rat test negative rating provide scale likely test negative likely test positive result subject use point scale differently task condition subject choose number choose number convert set rating average rst column figure remain column prediction model case model prediction convert probability allow direct comparison combine model use tree animal causal network feature use tree figure b object correspond mouse rat squirrel sheep tree component model total path length tree small path length likely animal feature value great path length likely distant animal tree mouse sheep different feature value result report use value value maximize average correlation achieve model experiment causal component model include free parameter use parameter network generate card subject training phase compare rst column figure combine model account datum column figure model prediction remove similarity component column causal component column combine model model use network describe model use tree describe model miss qualitative trend evident human datum task condition causal model identical prediction rat squirrel sheep task chain condition example use similarity rat predict rat likely test positive task condition similarity model predict unobserved feature chain condition condition distribute identically animal task chain condition example similarity model predict mouse likely sheep test positive limitation causal similarity model suggest combination causality similarity necessary account datum likely approach combine model account datum suggest accurate prediction achieve causal network similarity information tightly integrate simply average prediction causal model similarity model task chain condition example model predict rat sheep equally likely test positive compute average model result prediction experiment work hypothesis similarity causality combine context alternative suggest state similarity relationship use root variable causal structure unobserved instance similarity inference chain condition experiment root variable observe animal hypothesis correct case root variable true structure know figure instance similarity long play role root variable observe remain variable deterministically specie interested case sf contain relevant variable similarity help prediction effect unobserved variable consider example case sf state cause lung cancer root variable observe believe likely suffer lung cancer discover lung cancer case provide intuitive evidence design related experiment explore hypothesis empirically material method experiment similar experiment condition replace condition rst task condition subject tell mouse test positive second task subject tell addition rat squirrel sheep test positive mouse task r r r r r r r r r r r task r b task task mouse combine similarity figure experiment behavioral datum prediction model task condition root variable network observe animal test negative note second task value root variable provide animal adult participate experiment result figure mean subject model describe previously judgment rst task condition replicate experiment subject combine causality similarity pair observe result second task rule hypothesis chain condition example causal model predict rat sheep equally likely test positive subject predict rat likely sheep test positive combine model account prediction discussion develop model inductive reasoning sensitive causal relationship feature similarity relationship object demonstrate experiment provide good account human reasoning model contribution provide integrate view inductive reasoning similaritybased usually consider separately second previous account causal reasoning acknowledge importance unknown relevant variable use similarity constrain inference effect variable previous model similaritybased reason model handle novel property link know property convenience emphasize distinction causality similarity notion similarity need approach causal interpretation treestructure taxonomy example simple representation causal process generate biological process evolution combine model causal model relationship feature relationship specie account generally framework method build sophisticated causal model experiment suggest kind model need account complexity human causal reasoning researcher propose strategy combine probabilistic model method account datum particular product expert approach lead prediction qualitatively similar prediction combine model approach product expert model direct graphical model support prediction experiment explore inference adequate causal model able handle inference sort causal knowledge similarity variety knowledge support inductive reasoning single form knowledge topic study inference draw multiple kind knowledge provide combine arbitrary form knowledge work illustrate general apply broadly different generative model capture different aspect human knowledge model use common language language probability probabilistic model modular compose different way build integrated model inductive reasoning second stochastic component generative model expression use model similarity model constrain stochastic component model network relatively general method combine probabilistic knowledge representation focus human reasoning integrate model induction need combine model nd application computational biology predict express certain gene example rely phylogenetic relationship causal relationship gene relate model explore et develop approach protein function prediction combine phylogenetic relationship protein relationship protein function author explore model combine hide markov model combine model small step fully integrate approach probability theory provide combine different representation world acknowledgment thank valuable discussion work support reference theory cognition b r feature inference causal structure category cognitive science d osherson e e smith induction psychological review s featurebased induction cognitive psychology c b induction proceeding annual conference cognitive science society page associate b tenenbaum semisupervise learning tree advance neural information processing system mit press cambridge b similarity causality compete property generalization memory cognition p bayesian inference phylogenetic tree bioinformatics probabilistic bayesian network articial intelligence baye net toolbox compute statistic j combine probability distribution annotate statistical science e hinton model highdimensional datum combine simple expert proceeding articial intelligence b e m s e graphical model predict protein function proceeding international conference machine learn
approximate efficient solver lp round computer abstract problem machine learning solve round solution appropriate linear program lp paper recover solution comparable quality round approximate lp solution instead exact approximate lp solution compute efficiently apply parallel method formulation lp derive worstcase runtime solution quality guarantee scheme use novel perturbation convergence analysis experiment demonstrate combinatorial problem vertex cover independent set multiwaycut approximate round scheme order magnitude fast commercial lp solver produce solution similar quality introduction problem solve effectively approximation nphard combinatorial problem set cover set packing multiwaycut popular scheme solve problem lp round consist follow process construct integer binary linear program ip formulation problem relax ip lp replace constraint round optimal solution lp create feasible solution original ip problem round know work range hard problem come theoretical guarantee runtime solution quality lprounde require solution lps possibly extreme scale decade work lp solver include impressive advance commercial code capable handle problem require scale work propose approximate lp solver suitable use lprounde approach large problem intuition lp round ultimately round lp obtain approximate solution combinatorial problem solution lp suffice approach find approximate solution large lps quickly suitable inefficient obtain highly accurate solution paper focus theoretical algorithmic aspect find approximate solution lp use lprounde scheme main technical contribution follow approximately solve large lps form quadratic programming approximation apply stochastic coordinate descent approximation second derive novel convergence analysis method base perturbation theory program finally derive bound runtime worstcase approximation ratio round scheme experiment demonstrate approach thetis produce solution comparable quality stateoftheart approach task chunk entity resolution demonstrate different class combinatorial problem outperform stateoftheart commercial lp ip solver order magnitude runtime achieve comparable solution quality relate work recently focus connection relaxation maximum posteriori map estimation problem propose round scheme iterative lp solver facilitate map inference graphical model contrast propose use stochastic descent method solve qp relaxation allow advantage recent result asynchronous parallel method type recently propose intriguing parallel scheme pack cover problem contrast result apply general lp relaxation include problem multiwaycut additionally runtime algorithm sensitive approximation error error bind runtime algorithm grow bind runtime grow background approximate nphard problem round section review theory lprounde base approximation scheme combinatorial problem use vertex cover problem example simple nontrivial setting expose main idea approach preliminary minimization problem alg factor approximation solution produce objective value time value optimal low cost solution problem vertex cover approximation scheme set cover value large olog n number set lprounde base approximation scheme problem construct ip formulation denote p step typically easy perform ip formulation p theory hard solve original problem work consider application integer variable ip formulation binary variable second step lp round relax solve step relax constraint p obtain linear program lp p replace binary variable continuous variable solve lp p step round solution lp p integer solution feasible p yield candidate solution original problem focus paper relax solve step usually computational bottleneck lprounde base approximation scheme example scheme vertex cover let e denote graph vertex set undirected edge e let denote nonnegative cost associate vertex vertex cover graph subset v edge e e vertex set vertex cover minimize sum term sum vertex belong cover let review construct relax solve round phase lprounde base approximation scheme apply vertex cover construct phase introduce binary variable set vertex v select vertex cover ip formulation follow u e relaxation yield follow u e feasible solution lp relaxation fractional solution original problem round phase generate valid vertex cover simply choose vertex fractional solution easy vertex cover generate round scheme cost cost fractional solution fractional solution choose round optimal solution arrive factor approximation scheme vertex cover note important property round algorithm generate feasible integral solution oblivious fractional solution optimal solution formally define notion oblivious round scheme follow definition minimization problem ip formulation p lp relaxation denote factor oblivious round scheme convert feasible point integral solution p cost time cost problem family set cover set pack multiwaycut graphical model approximation factor logn machine learning application classification tracking natural language computer vision entity resolution semantic role label clustering figure lprounde scheme consider paper parameter refer number set refer sparse matrix refer number terminal constant factor oblivious algorithm alg problem construct factor approximation algorithm use alg round optimal fractional solution approximate solution feasible problem round produce factor approximation algorithm factor slightly large difference account approximate solution lprounde scheme include scheme vertex cover discuss section oblivious implement oblivious algorithm figure report experimental result section main result section describe solve lp relaxation approximately time traditional lp solver preserve formal guarantee round scheme define notion approximate lp solution discuss consequence oblivious round scheme use regularize quadratic penalty formulation compute approximate solution describe scd algorithm obtain approximate solution qp mention approach specifically asynchronous parallel implementation use lagrangian framework analysis yield worstcase complexity bind solution quality runtime entire lprounde scheme approximate lp solution consider lp follow standard form b x c b rm correspond u let denote optimal primal solution approximate lp solution use lprounde infeasible objective value different optimum ct quantify approximate lp solution follow definition point approximate solution lp exist constant bk use definition easy factor oblivious round scheme round approximate solution produce feasible integral solution cost time optimal solution p factor arise round access optimal fractional solution cope convert approximate solution approximate solution large vertex cover prove following result denote projection unit hypercube let x approximate solution linear program approximate solution feasible solution oblivious round scheme section result factor approximation algorithm general construct approximate solution require reasoning structure particular lp appendix c establish statement analogous lemma packing covering multiwaycut problem quadratic programming approximation lp consider follow regularize quadratic penalty approximation lp parameterize positive constant solution denote ct ax bk k rm x rn arbitrary vector practice choose approximation dual primal solution simply set quality approximation depend conditioning underlie linear program concept study denote datum problem b c consider perturbation b c linear program define d primal infeasible primal condition number p infimum ratio vector dual condition number d define analogously clearly p range small value indicate poor conditioning following result prove suppose p positive let x u solution pair define c unique solution satisfie bk p addition parameter d p k p d practice solve approximately use algorithm complexity depend threshold objective accurate seek inequality follow fact strongly convex define c kdk p d combine elementary inequality result theorem obtain bound c c c c k bk c p d p d following result immediate consequence suppose p positive let x u optimal pair suppose c define positive pair satisfy inequality definition provide satisfie follow low bound kdk min p d p c p d p c c p d instance vertex cover node m edge p m om value yield c m obtain om m ct scd method choose loop choose ij n randomly equal probability define set ij leave component unchanged end loop lmax ij solve qp approximation coordinate descent propose use stochastic coordinate descent scd solve step scd choose component step ith component partial gradient respect component project necessary retain simple procedure depend follow constant lmax bound diagonal hessian objective lmax max ai ai denote column describe scd method convergence result obtain result e denote expectation random variable indicate update index choose iteration need follow quantity l r sup denote jth iterate scd note r bound maximum distance iterate travel solution theorem algorithm l r lmax lmax obtain convergence follow sense small p provide lmax lmax l worstcase complexity bound combine analysis section derive worstcase complexity bind approximate lp solver suppose column norm lmax theorem indicate require iteration solve log term value describe section translate complexity estimate n order obtain desire accuracy term feasibility function value capture need solve qp different tight tolerance introduce tolerance relate choice penalty parameter qp ignore dependence dimension m note relationship express quantity term use theorem iteration complexity scd ignore log term linear convergence rate scd instrumental favorable value contrast standard variant descent sgd apply qp yield poor complexity variant complexity robust sgd inverse dependence square analysis method contribution order conditioning qp mention important improve efficiency approach outline asynchronous parallel implementation second use augment lagrangian framework oneshot approximation qp factor graph m m p rank p gibb sample figure solution quality approach task number primal variable number nonzero constraint matrix standard form rank indicate place participate competition asynchronous parallel scd asynchronous parallel version describe suitable execution multicore architecture core execute single thread access complete vector thread essentially run version independently choose update component ij iteration time thread read perform update usually update thread provide number thread large accord criterion depend diagonal dominance property hessian matrix step size choose appropriately convergence rate similar serial case observe augment lagrangian framework know example approach extend lagrangian framework sequence problem form solve primal dual solution estimate possibly penalty parameter update iteration proximal method multiplier describe omit discussion convergence property algorithm note quality solution depend value iteration convergence declare apply theorem note constant small close primal dual solution set improve approximation reduce need increase large value obtain approximate solution acceptable accuracy experiment experiment address main question approximate lprounde scheme useful graph analysis task arise machine learning approach compare stateoftheart commercial solver favorable answer question approximate lprounde scheme useful graph analysis task lp formulation use solve map inference problem graphical model generalpurpose lp solver rarely use reason scalability demonstrate solution obtain use thetis comparable quality obtain stateoftheart system perform experiment different task entity link text chunk task produce factor graph consist set random variable set factor describe correlation random variable run map inference factor graph use lp formulation compare quality solution obtain thetis gibbs samplingbase approach follow lprounde algorithm solve map estimation problem entity link use benchmark input graphical model boolean variable k factor text chunk use share task factor graph contain categorical random variable domain size k factor use training set provide respectively evaluate quality approach use official evaluation script evaluation datum set provide challenge figure contain description relevant quality metric precision p recall r fscore figure demonstrate produce solution quality comparable stateoftheart approach graph analysis task propose approach compare stateoftheart commercial solver conduct numerical experiment different combinatorial problem commonly arise graph analysis task machine learn vertex cover independent set cut problem compare performance lp solver lp ip solver denote cplexlp cplexip respectively main goal experiment compare quality integral solution obtain use lprounde integral solution cplexip compare time require thetis cplexlp solve lps purpose lprounde dataset task base family graph family instance obtain benchmark hide optimum solution consider difficult problem instance family similar report figure section remainder appear appendix e second family instance social graph obtain network analysis platform system setup implement use combination matlab augment lagrangian framework implementation augment framework base experiment run core ghz ram run disk use core available machine consistency implementation restrict core implement procedure detect redundancy substitute eliminate variable obtain equivalent small lps aim experiment compare algorithm use solve lps run cplexlp thetis reduce lps generate procedure cplexlp cplexlp thetis run tolerance additional experiment cplexlp run use default tolerance option report appendix e use barrier optimizer run cplexlp code provide time limit second exclude time preprocesse runtime round algorithm generate integral solution fractional solution task solve vertex cover problem use approximation algorithm describe section solve maximum independent set problem use variant approximation maximum degree node graph c detail multiwaycut problem use describe detail transformation approximate infeasible solution feasible solution provide appendix c round scheme set multiwaycut randomized choose feasible integral solution repetition problem maximization problem figure summary comparison cplexlp solution quality comparison cplexip thetis graph analysis problem code run time limit hour parallelize core indicate code reach time limit number primal variable number nonzero matrix standard form million speedup define time cplexlp divide time ratio solution objective obtain thetis report cplexip minimization problem low q maximization problem mi high q value q indicate find solution cplexip find time limit result result summarize figure additional detail figure discuss result vertex cover problem instance integral solution document optimal solution comparison cplexip produce gap gap t gap rsol t sec rsol sec rsol rsol rsol rsol figure time quality fractional integral solution graph analysis problem use thetis cplexip cplexlp code time limit hour indicate objective value good integer feasible solution find cplexip gap define know solution bind find cplexip time limit gap indicate problem solve accuracy indicate cplexip unable find valid solution bind lp objective value lp solution rsol objective value solution integral solution document optimal solution require hour instance solution obtain thetis accurate obtain cplexlp solution thetis cplexlp exactly summary approach use thetis cplexlp obtain integral solution comparable quality cplexip thetis time fast cplexlp observe similar trend large social able recover integral solution comparable quality cplexip time fast use lprounde cplexlp additional observation difference optimal fractional integral solution instance small record unpredictable performance cplexip large instance notably cplexip able find optimal solution amazon instance time comparable size instance cplexip outperform cplexlp time specialized strategy conclusion describe lp round scheme base approximate solver relaxation combinatorial problem derive worstcase runtime solution quality bound scheme demonstrate approach fast alternative base stateoftheart lp solver produce solution comparable quality acknowledgement ss generously support onr award generously support nsf award dms dms onr award work project generously support nsf career award nsf award award sloan research fellowship oracle generously support nsf award dms onr award award national laboratory recommendation finding opinion express work author necessarily reflect view sponsor reference solve pack integer program randomized round theory compute nonlinear programming set cover prototype experimental comparison vision ieee transaction pattern analysis machine intelligence improve approximation algorithm cut proceeding annual acm symposium theory compute page practical relative error criterion mathematical programming page approximation algorithm set covering vertex cover problem compute twodimensional programming image analysis problem science automatic factor graph information theory ieee transaction web scale entity resolution use relational evidence technical report research global optimization shape fit ieee conference computer vision pattern recognition cvpr page ieee asynchronous parallel stochastic coordinate descent distribute largescale generalize matching proceeding approach parallelize stochastic gradient descent nocedal springer ravikumar wainwright linear program proximal method round scheme journal machine learn research perturbation theory linear programming programming series integer linear programming inference conditional random field proceeding international conference machine learn page acm linear programming analysis loopy belief propagation weight match advance neural information processing system page improve approximation guarantee pack cover integer compute correlation cluster link detection ijcai page springer implement proximal point method programming optimization theory application couple detection multiple object tracking computer vision pattern recognition cvpr ieee conference page ieee hard example exact phase transition theoretical computer science gibb sample scale study storage proceeding
empirical risk minimization approximation probabilistic grammar computer computer abstract probabilistic grammar generative statistical model useful sequential structure present framework reminiscent structural risk minimization empirical risk minimization parameter fix probabilistic grammar use logloss derive sample complexity bound framework apply supervised setting unsupervised set introduction probabilistic grammar important statistical model family use natural language processing computer vision computational biology recently human activity analysis commonly estimate use maximum likelihood estimate variant estimation view minimize empirical risk logloss logloss bound apply probabilistic grammar hard obtain uniform convergence result result help derive sample complexity bound bound number training example require obtain accurate estimation overcome problem derive distributiondependent uniform convergence result probabilistic grammar sense learning framework relate previous work learn distributiondependent set structural risk minimization work relate discuss statistical property estimation parse model distributionfree set base notion bounded approximation define sequence increasingly approximation probabilistic grammar proper approximation derive sample complexity bound framework supervised case unsupervised case result rely exponential decay probability respect length derivation number derivation step grammar generate structure mean probability mass distribution concentrate small number grammatical derivation formalize notion use result application involve datum finite size natural language processing computational biology believe reasonable assumption rest paper organize follow overview probabilistic grammar overview learning set present proper approximation approximate concept space permit derivation sample complexity bound probabilistic grammar describe main sample complexity result discuss result conclude probabilistic grammar probabilistic grammar define probability distribution grammatical derivation generate process example probabilistic contextfree grammar generate tree recursively rewrite nonterminal symbol sequence child symbol accord fix set production rule rewrite conditionally independent previous state markov property permit efficient inference probability distribution define probabilistic grammar paper assume grammatical derivation fully determine string denote derivation z string infinitely kind grammar assume number derivation finite general probabilistic grammar define probability grammatical derivation z pk z log function count number time kth distribution event occur derivation collection multinomial kth include pk event let nk denote total number derivation event type dg denote set possible derivation define dg let pk denote length number denote length string k event token derivation parameter estimation probabilistic grammar mean choose complete datum supervise incomplete datum semisupervise usually imply string evidence derivation miss view parameter estimation identify hypothesis z equivalently log z simplicity notation assume fix grammar use refer refer parameter pk log assumption distribution generate derivation note p probabilistic grammar bound derivation length far exponential decay derivation constant r constant l exponential decay string let dg z k number derivation r assume exist constant q q k imply number derivation length exponentially large bound bound expectation rule b b note example assumption hold p support consist finite set assumption hold case p probabilistic grammar supplementary material note assumption empirical justification relationship noise learning set supervised learning set set grammatical derivation z use estimate imply choice h agree training datum mle choose h maximize likelihood datum argmax log rempn log h learn rule grammar important problem receive attention minimize empirical risk expect value particular loss function know logloss expect risk p quantity r log h log log convergence form rempn log r log h probability refer uniform convergence note rempn log r log h kind uniform convergence drive force empirical risk minimizer consistent ie empirical risk converge expect risk assume relevant literature empirical risk minimization proper approximation logloss unbounded function dg r z dg ie envelope uniformly bind difficult obtain uniform convergence result page consistency maximum likelihood estimator bind family probability distribution hand instead restriction heavy probabilistic grammar revise learn model accord wellknown result convergence stochastic process approximate concept space use sequence replace uniform convergence convergence sequence concept space concept space sequence vary function number sample construct sequence concept space return learning model approximation base concept bounded approximation let m sequence concept space contain require m grow large approximation original concept space sequence properly approximate exist nonincrease function tail m tail m nonincrease function bind m bind m operator m cm fm m large m m m e km bind m tightness p cm tail m tail m second requirement bound expect value fm value large km require obtain uniform convergence result revise model note km grow arbitrarily large requirement ensure approximation actually converge original concept space actually characterization convergence probabilistic grammar supervised set note good approximation km increase fast function m tail bind m decrease fast function arbitrarily fast convergence rate example subsequence size km great effect number sample require obtain accurate estimation construct proper approximation probabilistic grammar focus construct proper approximation probabilistic grammar assumption probabilistic grammar common grammar formalism change expressive power grammar express use express use grammar supplementary material construct define transformation t shift k probabilistic grammar h k note fix constant p define proposition exist constant q boundedness property km log m bind m m log m proof p let let p z log m log m log m km follow m second log m addition requirement p p m km log m m m constant finally l q constant m m m log m q log m m log m log m mp m tight respect tail m proposition exist m cm z tail m mp tail m tail m m log m mp proof supplementary material proper approximation probabilistic grammar point use fm denote proper approximation construct use bind m tail m proposition proposition assume p fix rest asymptotic empirical risk minimization know empirical risk minimizer asymptotic empirical risk minimizer logloss case mean converge maximum likelihood estimate conclusion section proper approximation motivate requirement pose proper approximation true unify number sample m index approximation concept space let minimizer empirical risk rempn let minimizer empirical risk gn rempn let z sample operator gn rempn asymptotic empirical risk minimizer follow proposition let d z sample derivation rempn asymptotic empirical risk minimizer denote zn set cn z denote event zi d properly approximate e rempn rempn e pan e pan expectation respect dataset supplementary material proof proof proposition let concept uniform weight k k note pan pn pan l pan s let event p p pan p p p p z z zn z come independent constant x pan z construction proper approximation proposition know derivation length log great q log constant similarly pz mean pan addition pan use proof technique use rely fact pn log sample complexity result main sample complexity result probabilistic grammar result hinge convergence rate convergence fast cover number fn grow fast brief overview cover number cover way reduce class function small finite fact representative class function original class represent use function small class let class function let distance measure function cover subset denote g exist covering number d size small cover use respect distance measure interested specific distance measure dependent empirical distribution describe data z let g use instead use g d directly bind quantity n following key consider possible sample yield result connection cover number convergence empirical process let class function fn ef bind let kn set class requirement mild regularity condition hold proper approximation refer reader detail function fn truncate p sup rempn bind provide kn proof chapter page supplementary material explanation cover number complex combinatorial quantity hard compute directly fortunately bound use pseudo dimension generalization dimension real function case probabilistic grammar pseudo dimension bound function linear parameter pseudo dimension lemma let proper approximation probabilistic grammar kn log supervise case combine main sample complexity result let grammar let proper approximation correspond family probabilistic grammar let px distribution derivation satisfy requirement let z sample derivation exist constant l q p constant m m log max l p sup rempn pn log proof omit space l q p constant proposition proof base simple algebraic manipulation right eq rely unsupervised case unsupervised setting yield derivation grammar goal identify grammar parameter yield concept class set log marginalize distribution define p denote set fn define analogously note need define operator step define proper approximation p unsupervised setting let concept z x define immediate proper approximation hard boundedness property satisfied kn form bind tion bind m m log m q p rely property bounded derivation length p supplementary material proof follow result proposition exist m m s log p tail operator define p utility bi log ai log bi exist log log bi sketch proof proposition utility lemma p tail p tail define exist z log proof proposition requirement p know exist p px inequality happen large fix m compute covering number pseudo dimension hard task function class include dasgupta overcome problem bayesian network fix structure bind covering number respective depend covering number unfortunately fully adopt approach derivation probabilistic grammar arbitrarily large overcome problem use follow restriction assume dn function mapping size sample real number sample large derivation set grammar hand accuracy desire restricted choose grammar large derivation set refer restriction condition condition following result proposition hide variable rule probabilistic grammar proof proposition identical proof hide variable rule unsupervised case follow sample complexity result let grammar let proper approximation correspond family probabilistic grammar let px distribution derivation satisfy requirement let sample string px exist constant l q p constant m m log log max dn p rempn pn log sample complexity bind nontrivial example restrict dn polynomial size number enlarge dn possible exponential function tightness proper approximation sample complexity bind increase improve dn increase effect p increase improve degrade degrade degrade table tradeoff quantity learning model effectiveness different criterion dn function condition dn discussion framework specialize improve main criterion tradeoff tightness proper approximation sample complexity example improve tightness proper approximation subsequence sample complexity bind degrade grow fast table different tradeoff parameter model effectiveness learn general want condition remove choose dn allow tn t small sample case sample complexity bound trivial supervised case result state number sample require upper bind grow term behave log n fix grammar example depend total number rule normal form grow order v vocabulary size mean bind grow order log consistent conventional wisdom grammar require datum accurate learning dependence bound suggest easy learn model small size help explain success recent advance supervised parse coarse model small size pass model easy learn require datum accurate serve base model later phase sample complexity bind unsupervised case suggest need log dn time datum achieve estimate good supervised learning interestingly grammar learn available training sentence long maximum length ignore note sample complexity measure complexity estimate probabilistic grammar unsupervised setting example computational complexity np hard probabilistic automata conclusion present framework learn parameter probabilistic grammar logloss derive sample complexity bound motivate framework empirical risk minimizer approximate framework asymptotic empirical risk minimizer framework use sequence approximation family probabilistic grammar improve datum distribution dependent sample complexity bound supervise unsupervised setting acknowledgement thank anonymous reviewer comment useful research support nsf grant iis reference m warmuth polynomial probabilistic concept respect divergence acm conference theory m warmuth computational complexity approximate distribution probabilistic machine learn network learn theoretical press e m parse maxent discriminative b cohen smith viterbi training hardness result uniform initialization proceeding b cohen smith empirical risk minimization probabilistic grammar sample complexity hardness learn preparation m collin statistical model natural language processing computational linguistic m collin parameter estimation statistical model theory practice distributionfree method text speech language technology new development parse technology page dasgupta sample complexity learn bayesian network learn j new probabilistic model dependency parse exploration e m language identification limit information control discover language human activity cognitive system haussler decisiontheoretic generalization pac model neural net learn application information computation d man induction syntactic structure model dependency v koltchinskii local rademacher complexity oracle inequality risk annal statistic stochastic graph grammar representation recognition pattern recognition d improve inference convergence stochastic process m brown r r c haussler stochastic contextfree grammar modeling research optimal aggregation classifier statistical learn annal statistic vapnik statistical learning theory d weiss taskar structure prediction cascade proceeding aistat
message pass task sparse technology clear technology clear water address dept physics normal problem resource allocation sparse graph real variable study use method statistical physics efficient distribute devise basis insight gain analysis examine use numerical simulation excellent performance agreement theoretical result introduction optimal resource allocation know problem area distribute computing significant effort computer science community problem general applicable area large number node require balance task reduce internet traffic problem usually refer computer science literature find practical heuristic solution distribution computational load computer connect manner problem address generic represent node computational power carry task computational power task choose random arbitrary distribution node locate randomly choose sparse graph connectivity goal task graph demand satisfied minimize subtask important aspect desire algorithmic solution decision message pass carry locally enable efficient implementation algorithm large distribute network focus case total computing power great demand number node involve large case address use similar technique analyze problem use bethe approximation statistical mechanic section alternatively new variant replica method section present numerical result section derive new message pass distribute basis analysis section conclude paper summary brief discussion future work statistical physics framework consider typical resource allocation task sparse graph node label n node randomly connect node capacity randomly draw distribution objective task node node capable carry task current draw aim satisfy constraint aij represent revise assignment node pair j respectively illustrate statistical mechanic approach resource allocation p consider load balance task minimize energy function cost e aij summation run pair node subject constraint general function current load balance task typically function assume study analysis graph introduce free energy temperature t partition function yz ij ij aij function return nonnegative argument connectivity low probability find loop finite length graph low bethe approximation describe local environment approximation node connect branch tree structure correlation branch tree neglect branch node arrange generation node connect node previous generation descendent node generation consider vertex t capacity current draw vertex write expression free energy function free energy descendant branch vertex z t represent tree terminate descendent vertex energy consider sum number node tree average free energy node refer vertex free energy note vertex add tree focus graph fix connectivity easily accommodate connectivity profile framework algorithm present later completely general term marginalize input current vertex leave difference potential argument terminology use change free energy add vertex number node increase vertex free energy obtain subtract free energy change average free energy allow obtain recursion relation t yk average free energy node k yk capacity vertex v feed tree h represent average distribution temperature limit eq reduce yk current distribution average free energy link derive integrate current link vertex feed tree t t respectively obtain expression p h r dy r dy statistical physics framework replica section sketch analysis problem use replica method alternative bethe approximation derivation involved detail provide facilitate derivation focus quadratic cost function result confirm validity bethe approximation sparse alternative formulation original optimization problem consider dual introduce p p multiplier function minimize l aij optimize l respect obtain refer chemical potential node current drive potential difference analysis carry space current focus optimization problem space chemical potential energy invariant addition arbitrary global constant p chemical potential node introduce extra regularization term break symmetry study characteristic problem calculate average free energy t n partition function aij ij aij calculation follow main step replica base calculation system use identity z partition function average network configuration connectivity capacity distribution consider case intensive connectivity extend analysis average connectivity matrix find rs ln z y z prs q q r prs q r rs label somewhat unusual index r s order parameter represent integer vector r s sn respectively result specific interaction consider node different index rs condition eq order parameter q set saddle point equation wrt order parameter assume saddle point equation yield recursion relation twocomponent function r related order parameter generating function ps r r z t represent tree terminate vertex node chemical potential provide input chemical potential h represent average distribution recursion relation independent replica index z t vertex node capacity d constant express term function r integrate k algebraic structure typical bethe lattice representation network connectivity node obtain input descendent node generation represent tree terminate kth descendent regularization factor r turn function interpret current draw node chemical potential chemical potential express function r product vertex partition function normalization factor limit dependence separable provide recursion relation rise vertex free t current draw vertex tree t recursive equation average free energy expression agree result approximation iterative equation directly link obtain principle bayesian approximation logarithm message pass node proportional vertex free energy numerical solution solution eq obtain numerically vertex free energy node depend capacity disorder configuration descendant generate node iteration capacity randomly draw distribution feed node randomly draw previous iteration discretize vertex free energy function vector component value function correspond current yi speed optimization search node find vertex saturation current draw node capacity node use current draw descendant node saturate capacity constraint condition satisfied separately optimize current draw descendant node vertex saturation current equal node capacity subtract current draw descendant optimal solution find use exhaustive search vary component current small discrete step approach particularly convenient search confine single parameter compute average energy randomly draw node compute optimal current flow repeat sampling obtain average figure result function iteration step t gaussian capacity distribution variance average iteration correspond add extra generation tree structure iterative process correspond approximate network increasingly extensive tree observe initial rise iteration step average energy converge value rate increase average capacity study convergence rate iteration fit average energy iteration step t use t e t asymptotic regime inset fig relaxation rate increase average capacity interesting note exist average capacity value convergence iteration slow average energy curve start develop final convergence hand disappear convergence fast convergence probably appearance increasingly large cluster nonzero current network cluster node negative capacity increasingly extensive need draw current increasingly extensive region node excess capacity satisfy demand figure b illustrate current distribution average capacity distribution p consist delta function component continuous component decrease average capacity fraction link current increase average capacity low average capacity link nonzero current form cluster high average capacity break isolate cluster distribute local nature recursion point possibility network optimization solve message pass approach successful problem code probabilistic inference major advantage message pass potential solve global optimization problem local update reduce computational complexity example computational complexity quadratic programming load balance task typically scale network topology underlie connectivity variable message pass scale important advantage relevant c c p p c c figure result system size obtain iterate function t dash line asymptotic inset function distribution p obtain iterate steady state parameter average capacity right leave inset p function hi symbol p c r pair obtain eq respectively line c function hi result eq eq inset multiply function hi condition distribution p obtain iterate steady state parameter average capacity left right inset p function hi symbol practical implementation nature require global optimizer particularly suitable control evolve network contrast message pass algorithm pass conditional probability estimate discrete variable neighbor node message present context complex function current simplify message parameter second derivative vertex free energy quadratic load balance task solution consist vertex free energy piecewise continuous slope parameter message precise approximation let aij message pass use recursion relation message aij ij ij bjk p min bjk represent second derivative forward message follow backward message update current accord bjk simulate network compute average energy network configuration generate randomly loop length exclude update perform random sequential choice node simulation result message pass algorithm excellent agreement obtain recursion quadratic load balance task consider independent exact optimization available comparison condition optimal solution yield min aij j provide local iterative method optimization problem recursion message pass yield excellent agreement iteration chemical eq allow study distribution p chemical potential fig d p consist delta function continuous component node chemical potential correspond capacity constraint fraction node increase average capacity inset fig d low average capacity saturate node form cluster high average capacity break isolated cluster interesting note average capacity start develop relaxation rate recursion fraction node close threshold case simulation result average energy use eq average energy decrease connectivity increase increase link connect provide freedom allocate resource average capacity exponential fit applicable lie range remarkably multiply factor find curve collapse regime average capacity average energy scale regime inset property optimize network study simulation present merely summarize main result average capacity drop energy rise exponential fit applicable average capacity fraction link current increase average capacity insensitive connectivity remarkably small average capacity function good fit datum limit large function approach fraction link vertex fraction node increase average capacity insensitive connectivity limit large average capacity approach upper bind probability capacity node nonnegative d convergence time eq measure time rm change chemical potential fall threshold similarly convergence time eq measure time rm sum current message direction link fall threshold average capacity find powerlaw dependence average capacity exponent range eq average capacity decrease far convergence time deviate power law study prototype problem resource allocation connect network use replica method result recursion relation interpretable use bethe approximation recursion relation lead message pass algorithm optimize average energy significantly reduce computational complexity global optimization task suitable online control suggest parameter produce result excellent agreement original recursion relation simple illustrative example letter consider quadratic cost function result exact algorithm base local iteration chemical potential message pass algorithm remarkable agreement exact result suggest simple message pass algorithm generalize realistic case nonlinear cost function additional constraint capacity node link constitute rich area investigation potential application acknowledgment work partially support research grant research grant ip reference bs computer network system approach large scale system d s acm computer review statistical physics spin glass information processing m p spin glass theory world scientific phy d phy rev opper m advanced mean field method mit information theory inference learn
oneclass lp dissimilarity representation abstract problem novel situation detect approach describe domain class typical example application come area machine diagnostic fault detection identification principle refer problem little knowledge available typical class paper explain proximity natural representation descriptor propose simple oneclass classifier dissimilarity representation use linear programming efficient oneclass description find base small number prototype object classifier robust transform dissimilarity cheap compute use reduce representation set finally comparison comparable oneclass classifier introduction problem describe class domain recently gain lot attention identify application area interest cover problem specified target recognize outlier instance detect example type fault detection behavior rare possible approach class description problem construct oneclass classifier classifier concept descriptor ie refer possible knowledge class efficient build feature space find determine minimal volume data determine hyperplane separate datum origin possible use kernel data implicitly map inner product space result original space yield nonlinear boundary approach convenient datum represent feature space case lack good suitable feature difficulty define case string graph shape avoid definition explicit feature space propose address kernel general proximity measure symmetric conditionally positive definite function variable proximity directly arise application reasoning start feature space case method proximity representation address general dissimilarity basic assumption instance belong class similar example class identification procedure realize proximity function equip threshold determine instance class member proximity function distance average representative set select prototype datum represent proximity natural build concept descriptor ie proximity function directly build paper propose simple efficient general dissimilarity representation discuss section find use linear programming lp section present method dissimilarity transformation robust object large dissimilarity section describe experiment conduct discuss result conclusion summarize section dissimilarity representation dissimilarity measure provide flexible way represent datum constraint condition essential define proper measure convenience adopt symmetry requirement require d strict metric nonmetric dissimilarity naturally find shape object image compare computer vision let refer object compare dissimilarity representation dissimilarity kernel base representation set r pn realize mapping dz r define r dz p r control dimensionality dissimilarity space d r note express space object necessarily feature space emphasize object z pi feature vector bold hypothesis ch basis object recognition state similar object close representation dissimilarity measure mean small object r s demand object r s identical imply belong class extend assume object s sufficient small similar r member class consequently ds t object t dissimilarity representation satisfy reverse ch hold object similar representation similar reality belong class object large distance assume set r contain object class interest object z large dz r outlier origin dissimilarity space characteristic use dissimilarity measure metric vector dz r lie open prism bound hyperplane object r principle place dissimilarity space d r triangle inequality completely violate possible practical point view ch reverse consequently mean d lose discriminate property small similar object measure metric slightly nonmetric ie triangle inequality somewhat violate dz r lie prism close prism bound bound linear programming dissimilarity data description describe class nonnegative dissimilarity space minimize volume prism cut hyperplane p t dz r bound datum note nonnegative dissimilarity impose feasible task natural extension minimize volume simplex main vertex origin vertex result intersection p axis dissimilarity space vector element assume nonzero weight hyperplane p effectively p construct rm geometry know volume simplex express m volume base define vertex minimization euclidean distance origin p relate minimization let dpi r dissimilarity representation bound hyperplane dpi r n lq distance origin p small q satisfie p q p mean p determine minimize require avoid arbitrary scaling construction p solve minimization mathematical programming formulation problem min dpi r p p find minimize yield quadratic optimization problem simple formulation realize p interest know assumption simple calculation find m minimize bound volume simplex consider reasoning class represent dissimilarity characterize linear proximity function weight threshold oneclass linear programming description define wj indicator function proximity function find solution soft margin formulation straightforward extension hard margin case upper bind outlier fraction target class min dpi r p formulation sparse solution obtain mean positive object correspond nonzero weight support object left plot fig d illustration lpdd data represent metric dissimilarity space triangle inequality datum prism indicate dash line lpdd boundary hyperplane close origin possible minimize accept target object discussion section outlier origin proposition upper bind outlier fraction target class fraction object lie boundary mean pn dpi p expect parallel prism hyperplane d p lpdd min t lpsd min sum dissimilarity space w similarity space figure illustration lpdd dissimilarity space leave lpsd space dash line indicate boundary area contain object lpdd try minimize distance bounding hyperplane origin lpsd try attract hyperplane average distribution proof analogously proof intuitively proof p follow assume find solution increase slightly term objective function change number point nonzero object optimum hold outlier scale dissimilarity unbounded object target class large dissimilarity badly influence solution propose nonlinear transformation distance interval locally distance scale linearly globally large distance close function property sigmoid function tangent use ie s control slope function ie size local neighborhood transformation apply elementwise way dissimilarity representation state train linear programming similarity recently propose lp formulation novelty detection start reasoning feature space spirit positive definite kernel s base set restrict modify rbf kernel s euclidean l city block distance principle refer base consistent lpdd method rewrite lp formulation hard margin formulation obvious include tradeoff parameter lack interpretation lpdd follow pn wt p wj similarity representation simplicity method linear programming description lpsd define wj right plot fig d illustration lpsd datum represent similarity space object lie hypercube object representation object close origin hyperplane optimize minimal average output target set necessarily mean good separation origin small volume possibly result high outlier acceptance rate lpdd euclidean lpsd base figure oneclass hard margin lp classifier artificial d data left right value d d d d average distance support object mark square extension lpdd lpsd define square dissimilarity matrix computation dissimilarity costly consider reduce representation set rre r consist object dissimilarity similarity representation rectangular matrix rre respectively formulation remain change rs replace rre reason consider reduce representation robustness outlier choose set scope paper experiment artificial dataset illustrate lpdd lpsd method artificial dataset originally create d feature space dataset contain cluster object represent euclidean distance second dataset contain uniform square cluster contaminate outlier p object represent slightly nonmetric l dissimilarity d fig dataset decision boundary lpdd lpsd theoretical input space parameter s use plot refer scale parameter sigmoid function lpdd base scale parameter picture similar behavior lpdd lpsd lpdd tend slightly tight target class lpdd euclidean e e e lpsd base e e e e s e s e figure oneclass lp classifier train artificial uniformly distribute datum outlier leave right value dm dm dm dm dm dm median distance distance e refer error target set support object mark square lpdd e e e lpsd base e e e s e s figure oneclass lp classifier artificial d datum setting fig use l nonmetric dissimilarity instead euclidean note median change consequently value lpdd lpsd s e e e e figure oneclass lp classifier train artificial uniformly distribute d datum outlier nonmetric rectangular dissimilarity representation upper row lpdd result row result rbf object reduce set rre mark triangle note differ left right e refer error target set support object mark square clear fig outlier lie single uniformly distribute cluster ignore soft margin train figure observe lpdd tight class description robust scale parameter robust outlier observe dissimilarity use instead present result reduce representation object randomly choose set rre left plot rre contain object uniform method perform equally right plot hand rre contain outlier judge suitable scaling s outlier support object lpdd case lpsd crucial difference lpdd lpsd observe support object case lpsd apply representation lie boundary case lpdd tend class condition monitoring fault detection important problem machine diagnostic failure detect fault lead machine damage false alarm lead unnecessary expense example consider detection type fault dataset consider datum instance consist sample acceleration preprocesse discrete fast fouri transform signal characterize attribute dataset consist category normal behavior nb correspond table error second kind lpdd lpsd dissimilarity representation datum reduce representation base object euclidean dissimilarity representation error t t error t t measurement new type t correspond damage outer badly compare lpdd method lpsd method perform experiment way describe use training set independent validation test set optimal value find lpdd train valid test lpsd method use validation set l dissimilarity representation result t present table conclude l representation far convenient fault det especially look fault type t unseen validation process lpsd perform normal instance yield small fault detection data lpdd expect boundary tight support object need contrary lpsd method deteriorate wrt detection note reduce representation base randomly choose target object yield significantly performance detection lpdd target acceptance lpsd conclude failure fault detection high cost cost target object approach recommend conclusion propose linear programming description lpdd classifier directly build dissimilarity representation method efficient mean object need computation dissimilarity test phase novelty approach lie reformulation general dissimilarity measure think natural class descriptor dissimilarity measure unbounded propose transform dissimilarity sigmoid function lpdd robust object large dissimilarity emphasize possibility use procedure rectangular representation especially useful dissimilarity costly compute lpdd apply artificial realworld dataset compare lpsd detector propose consider dataset lpdd yield compact target description lpsd lpdd robust outlier training set particular object consider reduce representation proper scaling parameter sigmoid function support object lpdd contain outlier difficult lpsd achieve original formulation support object lpsd tend lie boundary lpdd boundary mean removal object impose change lpdd summary lpdd method recommend failure detect outlier expensive cost false alarm possible enlarge description lpdd add small constant constant relate dissimilarity value neighborhood boundary choose remain open issue research acknowledgement work partly support organization scientific research fellowship author solely responsible information communicate responsible view result express reference combine support vector mathematical programming method induction burge editor advance method support vector learn page mit cambridge p harmonic analysis linear programming approach novelty detection information processing system page mp modify distance object match th conference pattern volume page complexity pattern recognition problem symposium pattern recognition page royal e complexity dissimilarity base pattern class conference image analysis classification nonmetric distance class representation ieee tran pami jain representation recognition handwritten digit use template ieee ol separate plane operation research letter e p generalize kernel approach classification machine learn research b estimate support highdimensional distribution neural computation b method novelty detection neural information processing system oneclass classification vector datum description machine learn accept vapnik nature statistical learning springer
regret low bound optimal algorithm finite stochastic partial monitoring abstract partial monitoring general model sequential learning limited feedback formalize game player game learner choose action time opponent choose outcome learner suffer loss receive feedback signal goal learner minimize total loss paper study partial monitoring finite action stochastic outcome derive logarithmic distributiondependent regret lower bind define hardness problem inspire multiarmed bandit problem propose pmdme algorithm minimize distributiondependent regret pmdme significantly outperform stateoftheart algorithm numerical experiment optimality pmdme respect regret bind slightly modify algorithm introduce hinge function pmdmedhinge derive asymptotically optimal regret upper bind pmdmedhinge match lower bind introduction partial monitoring general framework sequential decision problem feedback class problem include prediction expert advice multiarmed bandit problem dynamic pricing dark pool problem label efficient prediction linear optimization bandit feedback model instance partial monitoring partial monitoring formalize repeat game play player learner opponent round learner choose action time opponent choose outcome learner observe feedback signal set symbol suffer loss deterministic function select action outcome goal learner find optimal action minimize cumulative loss alternatively define regret difference cumulative loss learner single optimal action minimization loss equivalent minimization regret learner small regret balance exploration acquisition information strategy opponent exploitation information rate regret indicate fast learner adapt problem linear regret indicate inability learner find optimal action sublinear regret indicate learner approach optimal action sufficiently large time step study partial monitoring classify setting respect assumption outcome hand stochastic set opponent choose outcome distribution game start outcome round iid sample distribution hand adversarial set opponent choose outcome maximize regret learner paper study set relate work paper study regret finite partial monitoring problem propose attain regret problem bind later improve instance bind optimal literature partial monitoring deal minimax regret worstcase regret possible opponent strategy bartok classify partial monitoring problem category term minimax regret trivial problem regret easy problem regret hard problem regret problem t regret class partial monitoring problem limit bandit sort include large class problem dynamic pricing algorithm regret bind easy problem propose partial monitoring stateoftheart sense empirical performance distributiondependent minimax regret focus distributiondependent regret depend strategy opponent minimax regret partial monitoring extensively study little know distributiondependent regret partial monitoring author knowledge paper focus distributiondependent regret discrete partial monitoring bartok derive olog distributiondependent regret easy problem contrast situation interest regret field multiarmed bandit problem upper confidence bind wellknown algorithm multiarmed bandit distributiondependent regret bind algorithm minimize distributiondependent perform minimize minimax regret instance distribution hard distinguish scenario field partial monitoring expect algorithm minimize distributiondependent regret perform exist contribution contribution paper lie follow aspect derive regret lower bind special class partial monitor eg multiarmed bandit olog regret lower bind know achievable paper far extend low bind obtain regret lower bind general partial monitoring problem second propose algorithm partial monitoring pmdme introduce slightly modify version algorithm pmdmedhinge derive regret bind pmdmedhinge logarithmic regret bind hard problem easy hard problem algorithm optimal constant factor lead logarithmic term performance exist algorithm compare numerical experiment partial monitoring problem consist specific instance vary difficulty instance pmdme significantly outperform exist method number round large regret pmdme problem quickly approach theoretical low bind problem setup paper study finite stochastic partial monitoring problem action m symbol instance partial monitoring game define loss matrix l rn m feedback matrix h hij m beginning learner inform l h round t t learner select action time opponent select outcome m learner ignore factor note suffer loss observe information learner receive consider stochastic opponent strategy select outcome govern opponent strategy p pm pm set probability distribution outcome outcome round iid sample goal learner minimize cumulative loss round let optimal action minimize loss c expectation p ith c m row l assume unique loss generality assume let l p number round tth action select performance algorithm measure pseudo c figure cell decomposition partial monitoring difference expect loss learner instance m optimal action easy minimize loss equivalent minimize regret expectation regret measure performance algorithm learner use t action let ci set opponent strategy action optimal ci pm ci optimality cell action optimality cell convex close furthermore set optimality cell c cell decomposition figure let pm ci set strategy action optimal signal matrix action define hij true signal matrix define slightly different previous paper number row si number different symbol ith row h advantage use definition probability distribution symbol observe select action example signal matrix section instance partial monitoring globally observable pair action paper exclusively deal globally observable instance view minimax regret include trivial easy hard problem regret lower bind good algorithm work opponent strategy extend idea introduce notion strong consistency partial monitoring algorithm strongly consistent satisfie p pm l context multiarmed bandit problem robbin derive regret low bind strongly consistent algorithm algorithm select arm number draw satisfie log t divergence distribution reward action optimal action generate analogously partial monitoring problem define minimum number observation sufficiently large t strongly consistent algorithm satisfie dpi log t olog p pi log divergence discrete distribution define log pi interpret follow round t consistency require algorithm sure possible risk action optimal small large deviation principle state probability opponent strategy q behave p roughly exp need continue exploration action si log t hold q c reduce risk exp log t t proof appendix b supplementary material base technique use robbin proof consider modify game action optimal difficulty prove lower bind partial monitoring lie feedback structure complex example confirm action need use feedback action derive low bind utilize consistency algorithm original modify game derive lower bind regret base note expectation regret express p let ri ri dpi q cl denote closure let cj ri ri p ij optimal solution ri pi p cj p pi value p pi log t possible minimum regret observation minimum divergence p q cc large log t use yield follow regret low bind regret strongly consistent algorithm lower bound p pi log t olog t theorem naturally measure instance p pi past study define closeness boundary cell furthermore appendix p pi cc m regret bind quadratic dependence p cc m define appendix closeness p boundary optimal cell pmdme section describe partial monitor deterministic minimum empirical divergence pmdme algorithm inspire solve multiarmed bandit problem let empirical distribution symbol selection action kth element t hit omit pi clear context let empirical divergence pm exponential consider likelihood q opponent strategy main routine loop action current list select list action loop zn determine subroutine subroutine check empirical divergence point q cc large log t eq large exploit current information select optimal action base estimation pt minimize empirical divergence select action number observation minimum requirement empirical divergence suboptimal point q cc large n armed bandit problem reward associate action partial monitor problem action outcome feedback signal relate need solve nontrivial optimization run pmdme later section discuss practical implementation optimization main routine pmdme subroutine add action parameter c initialization select action compute arbitrary zn t t arg arbitrarily fix order select receive feedback let mini l p add action accordance action pmdme c log t pmdmedhinge t end end compute action ri log t zn necessity log exploration pmdme try observe action extent eq necessary follow reason consider game characterize p optimal action action yield useful information use action receive kind symbol estimate p p p p p jth component p algorithm find small expect loss action large action feedback action use action manner loss observation action respectively use action difference come fact p p algorithm know p need observe action source distinguish p p optimal algorithm select log t time affect factor regret fact olog observation action sufficient p reason pmdme p probability select action log t experiment follow bartok compare performance algorithm different game game section threestate game dynamic pricing experiment armed bandit game result threestate game classify easy term minimax regret characterize signal matrix game s bpmleast bpmts pmdme lb round bpmleast bpmts pmdme t round t round dynamic pricing benign regret threestate bpmleast bpmts pmdme lb bpmleast bpmts pmdme round t round e dynamic pricing intermediate t round b threestate intermediate regret threestate benign bpmleast bpmts pmdme bpmleast bpmts pmdme lb regret regret regret dynamic pricing bpmleast bpmts pmdme lb t round figure plot algorithm regret average run lb asymptotic regret lower bind dynamic pricing classify hard term minimax regret game model repeat auction seller learner buyer opponent round seller set price product time buyer set maximum price pay signal buy seller loss constant difference buyer seller price buy loss feedback matrix l c signal correspond buy signal matrix action m si follow bartok set m experiment threestate game dynamic pricing test setting regard opponent beginning simulation sample point uniformly random sort c p pi choose opponent strategy intermediate benign setting respectively compare bpmleast bpmts pmdme c random naive algorithm select action uniformly random require matrix g g l apply game algorithm logarithmic regret easy game parameter set accordance paper bpmleast bayesian t regret easy game bpmts heuristic stateoftheart performance prior set uninformative avoid recommend paper pmdmedhinge subroutine add action parameter b t log t compute arbitrary satisfy arg min let mini l p zn exist action d action zn action c log action zn compute action ri log t zn ri infeasible action computation evaluation condition involve convex optimization obtain classify linear programming problem linear programming lp finitely variable infinitely constraint follow optimization bpmleast resort finite sample approximation use solver compute ri round sample point pm relax constraint sample speed computation skip optimization round large t use result computation computation coefficient p pi regret low bind approximate sample point experimental result figure game game easy intermediate opponent pmdme outperform algorithm number round large particular dynamic pricing game intermediate opponent regret pmdme t time small algorithm setting minimax regret matter pmdme advantage algorithm bpmts sufficiently large t slope optimal algorithm converge lb game setting slope pmdme converge lb empirical evidence optimality pmdme theoretical analysis section empirical performance close regret low bind author conjecture pmdme optimal hard analyze pmdme technically hard arise case divergence action small fully converge circumvent difficulty introduce discount factor let ri ij inf ri dpi q note natural generalization section sense event log ti r mean number observation ensure discount empirical divergence q cc large log t analogous define cj pi ri ij ri lj p ij optimal solution ri ri lj p cj p pi l p define ci p pm p optimal region action margin pmdmedhinge share main routine pmdme list action pmdme discount empirical divergence d close cell boundary encourage exploration identify cell belong assume follow regularity condition hold p r p pi unique p p pi p q dp s hold s neighborhood int denote closure interior respectively p pi log t olog t prove theorem d recall r pi set optimal solution problem condition duality apply case constraint check condition hold p reference condition hold case example case state pmdmedhinge regret upper bind match lower bind corollary optimality n armed bandit problem armed bandit problem regularity condition hold coefficient lead logarithmic term regret bind partial monitoring problem equal bind robbin c p pi di p log pq p log p q kldivergence corollary prove appendix c state pmdmedhinge attain optimal regret armed bandit run armed bandit game represent partial monitoring asymptotic analysis theorem lose finitetime property continuity optimal solution set r p pi mention close r p pi r p pi m maxi m maxi obtain explicit bind need sensitivity analysis theory robustness optimal value solution small deviation parameter particular optimal solution partial monitoring involve infinite number constraint analysis hard reason perform finitetime analysis note n armed bandit problem special instance avoid solve optimization finitetime optimal bind know necessity discount factor sure discount factor pmdmedhinge necessary empirically test pmdmedhinge algorithm setting dynamic pricing intermediate opponent far bad pmdme find implementation use optimization solver inaccurate optimize case true p satisfie d solution pt obtain nonzero hinge value case list action degrade performance determine discount factor essential future work acknowledgement author gratefully acknowledge advice thank anonymous reviewer useful comment work support grant number j reference manfre warmuth weight majority algorithm comput herbert robbin asymptotically efficient adaptive allocation rule advance apply mathematic value know demand curve bound regret online postedprice auction page max optimal allocation strategy dark pool problem aistat page gabor lugosi minimize regret efficient prediction ieee transaction information theory online programming generalize gradient icml page m kakade stochastic linear optimization bandit feedback colt page discrete prediction game arbitrary feedback loss colt page gabor lugosi regret minimization partial monitor math gabor bartok regret game stochastic environment colt page gabor bartok adaptive algorithm finite stochastic partial monitoring icml bartok nearoptimal algorithm finite game adversarial opponent colt page krause efficient partial monitoring prior information nip page fischer analysis multiarmed bandit problem machine learn bounded stochastic bandit colt page large deviation technique application application asymptotically optimal bandit bound support model colt page interior point optimizer teo dual parametrization method programming annal operation research introduction sensitivity stability analysis nonlinear programming
classification rank partial feedback abstract present novel algorithm work partial information setting algorithm base descent method upperconfidence bound tradeoff exploration exploitation analyze algorithm partial adversarial setting covariate adversarial multilabel probability rule generalize linear model log t regret bound improve way exist result test effectiveness upperconfidence scheme contrast baseline dataset obtain comparable performance introduction consider book recommendation system customer profile system recommend possible book user mean limited number banner place different position webpage system goal select book user like possibly typical feedback system actual action user particular book system observe user action book recommend book ad place different order webpage problem collectively refer learn partial feedback oppose information case system learning know outcome possible response user action possible book recommendation place large banner ad partial feedback set system observe response limited option specifically option actually recommend example sort reasonable assume recommend option treatment system eg large banner display page recommendation small place plausible interpret user feedback preference restrict display alternative consider instantiation problem multilabel setting learn proceed round time step receive instance output order subset yt label finite set possible label restriction apply size yt number available slot webpage set yt correspond aforementioned recommendation intend approximate true set preference associate set observe receive yt k noisy version true set user preference restrict yt t multiclass classification problem bandit feedback relate work paper lie intersection online learning partial feedback multilabel field include substantial work outline main contribution field emphasis believe related paper wellknown standard tool face problem partial feedback online learning trade exploration exploitation upper confidence bound socalled set contextual information bandit information bandit reference online algorithm receive time step context typically form feature vector x select action label quantify predefine loss function information loss function available specific interaction model determine piece loss observe actual value loss choose action information direction action space noisy version thereof overall goal compete class function map context expect loss regret sense obtain sublinear cumulative regret bound instance work finite action space mapping action linear generalize linear function feature obtain t regret bound t time horizon extend loss function model sample gaussian process joint space use similar generalize linear modeling linear multiclass classification problem bandit feedback consider t logarithmic regret bound prove depend noise model underlie loss function paper consider structured action space learner afford select set action suitable multilabel rank problem line paper general problem online minimization submodular loss function bandit information covariate consider achieve regret t bandit case problem online learning assignment consider algorithm request assign position ranking set item eg ad constraint set item place position problem share similar motivation bandit version algorithm explicitly information account lead t regret bind aim learn suitable ordering available action thing author prove t regret bind bandit set multiplicative weight update scheme contextual information incorporate ability select set action motivate problem diverse retrieval large document collection mean live general metric space generality approach lead strong regret guarantee specific loss function use simple linear model hide utility function user interact web system provide partial feedback form allow system significant progress learn function regret bind t provide depend degree feedback experimentally argue feedback typically available user click relevant url list present search engine argument formal effort relate information context information hand way datum generate finally recent paper investigate class graphical model contextual bandit setting afford rich interaction context action lead t regret bind literature learning learn rank wide attention literature attract motivate application paper experimental nature relevant reference include reference deal multilabel typical assumption important concern model correlation class contrast specific setting consider need face model related reference learning pair example approach need assumption datum typically deliver batch learning procedure summarize technically close close result investigate multilabel problem partial feedback scenario contextual information assume probabilistic linear model label context choose adaptive adversary consider family loss function multilabel loss generalize standard hamming loss respect kind unnormalized rank loss case learning maintain generalized linear predictor probability label occur ranking produce upper estimate probability setting prove t log cumulative regret bound bound optimal log factor label probability fully linear context distinguish feature user feedback model previous paper assume algorithm observe noisy version risk function currently select action fact generalized linear model adopt mapping turn nonconvex parameter furthermore operate structured action space traditional form bandit appropriate capture typical user preference feedback approach base loss decouple label generating model user feedback noisy version gradient surrogate convex loss associate model consequence algorithm directly deal original loss exploration emphasis theoretical result validate algorithm realworld wrt number loss function good comparative performance simple baseline operate information model preliminary consider setting receive time information vector rd allow output possibly order subset k set possible label subset label yt k associate generate algorithm feedback loss suffer algorithm account thing distance yt view set cost play yt cost associate sum cost suffer class yt possibly account order occur view order list label specifically constant cost c ci s k consider loss function yt yt yt yt position class depend yt size yt term account false negative mistake specific ordering label second term collect loss contribution provide false positive class account cost cji yt order label occur constant serve weight relative importance false positive false negative mistake specific example suppose cost ci s s s play case yt yt p yt cost play class slot play class slot special case cost long need view collection loss reduce standard loss set yt yt yt yt yt yt notice partial feedback yt yt allow algorithm know choose class good bad extent select ordering algorithm observe value yt yt remain hidden work loss function output yt rank list class ranking restrict relevant class set relevance feedback select class observe set yt yt supervised rank information form pairwise preference provide algorithm set alternatively think rank framework restriction size yt set exogenous possibly timevarye parameter problem algorithm require provide rank restriction connection rank set partial feedback section problem arise noise model adopt encompass significant realworld setting time afford efficient implementation result algorithm subset yt k let k respond indicator vector easy yt yt cji yt sum yt sake optimize yt equivalently define x cji yt cji yt let shorthand conditional probability information vector principle generate adaptive adversary function past marginal xt k vector rd know function r r model define d r choose adversary assume sake simplicity xt t notice point variable need conditionally independent family allow joint distribution property marginal function instantiate negative derivative suitable convex nonincrease loss function algorithm base instance square loss g result xt assumption logistic loss ln e g e domain r set brevity xt account model allow write conditional expect loss algorithm play yt yt et cji yt pit expectation et wrt generation label yt pit condition previous x key aspect formalization baye optimal order subset et compute efficiently know t handle lemma word order minimize suffice try possible size minimize sequence value determine sequence s turn compute sort class k decrease order pit sequence class sorted list notation introduce far let pi t sequence pit sort nonincrease order et yt notice way cost ci influence baye optimal computation place class position beneficial ie lead reduction loss pit yt cji yt high slot ij large pit order inclusion convenient interpret true set user preference like compete yt cumulative regret sense ie like bind t et yt et yt high probability use similar largely general analysis s devise online secondorder descent update rule comparison vector u define baye optimal wrt surrogate convex loss l observe expect loss function generally speak margin consider instance logistic case g e directly minimize expect loss reader familiar generalized linear model recognize derivative function p inverse link function associated canonical exponential family distribution space limitation proof supplementary material notice depend actual size decompose problem independent problem decomposition occur cost ci constant independent s criterion inclusion pit constant threshold parameter loss parameter cost value ci s interval r r function r confidence level initialization ai rdd instance rd set wit r xt r r output p yt cji pbit pbit d b c c c ln d l feedback yt yt update sit x c l sit yt yt yt yt yt yt yt sit figure partial feedback algorithm order multiple label set algorithm regret bound figure bandit order multiple label algorithm base replace unknown model vector prototype vector wit approximation ui similar constraint set ui vector sake b use let b d accord upper confidence approximation scheme suitable upperconfidence level class time denote operation ie r r algorithm prediction form computation baye optimal sequence yt replace true unknown pit correspond upper confidence pbit compute yt mimic computation d b baye optimal replace pit pbit ie order running time prediction algorithm produce rank list relevant class base score pbit class deem relevant rank high relevant good approximation pit large algorithm confident approximation upper confidence level large receive input loss parameter s model function g associate margin domain r r maintain positive definite matrix dimension initially set d identity matrix weight vector wit rd initially set vector time step t receive ddimensional use weight vector wit compute prediction vector wit vector easily result project wit space distance function wit wit u vector wit use produce prediction value b involve upperconfidence calculation k feedback yt yt observe algorithm figure promote class yt yt sign sit class yt yt sign sit leave remain class yt unchanged sign sit update wit wit base gradient loss function l satisfy g hand update use rank matrix update wit involve reader observe role play sign sit finally constant cl cl occur expression relate smoothness property l let r r r r c d convex nonincrease function argument define l d u assume positive constant cl cl l l l cl cl hold d cumulative regret figure satisfie probability q d u l cl lr ln c l l l easy l square loss l l logistic loss ln e r r e e remark drawback order properly set upper confidence level assume prior knowledge norm upper bind u information present simple modification cope limitation change definition figure r c ln c cl l l lead follow result theorem assumption notation replace explain probability d cl r d l rank partial feedback point cost value ci decrease baye optimal order sequence obtain sort class decrease value pit decide cutoff point induce loss parameter tell relevant class increase order apart irrelevant turn p gg correspond sort class decrease value parameter close yt produce order subset yt receive feedback relevant class time yt yt view multilabel assignment k ranking class natural way unnormalized rank loss function rank ranking function rd rk represent degree class relevance sort decrease order count number class pair disagree p ranking rank fi notice compute incrementally time update reference use diagonal approximation thereof report good empirical performance od time update point algorithm care false negative mistake good strategy predict yt k yield regret theorem indicator function predicate argument point rank function baye optimal wrt rank y matter class label conditionally independent use algorithm tackle rank problem derive measure choice rank feedback fact partial information version easily obtain suppose time environment maximal size order subset choose adaptively adversary number available slot webpage number url return search engine response query plausible compete regret sense good offline ranking form st far ranking loss reasonably restrict count number class pair disagree quantity relate number false negative mistake eg set factor serve balance contribution main term hard class conditionally independent pit baye optimal ranking t argmin step figure constraint sort class accord decrease value pbit prove follow rank version theorem assumption let class conditionally independent ik pit t let cumulative regret wrt define t b pit probability q cl d s proof appendix similar suggest extent decouple label generating model loss function consideration notice linear dependence total class large s problem replace similar benefit finally combine theorem argument contain remark experiment conclusion experiment report mean validate tradeoff implement algorithm different condition restrict number class loss measure ac hamming loss setting square loss l logistic loss vary r dataset use multilabel dataset introduce video annotation challenge comprise training sample test number feature number class second dataset train sample test sample sample describe feature number class case feature vector normalize unit l norm parameter setting loss measure use algorithm figure different loss function square loss logistic loss vary parameter r setting cost function depend task hand preliminary experiment decide evaluate possible setting denote decrease c s second denote constant c ci s experiment parameter set ac constant c reduce half hamming loss decrease c scenario evaluate performance algorithm loss algorithm minimize ability produce meaningful partial hamming loss constant c final average hamming loss logistic logistic r logistic r logistic r final average rank loss logistic logistic r logistic r logistic r run average rank loss decrease logistic logistic r logistic r logistic r number sample figure experiment final average hamming loss logistic logistic r logistic r logistic r final average rank loss logistic logistic r logistic r logistic r run average ranking loss hamming loss constant c decrease c logistic logistic r logistic r logistic r number sample figure experiment ranking constant c setting evaluate hamming loss typical problem label density ie average fraction label associate example small instance clearly beneficial impose upper bind s constant c ranking loss experiment try different value report final performance baseline baseline consider information version use square loss receive prediction array true label yt sample algorithm online binary relevance natural online adaptation binary relevance widely use baseline multilabel literature compare stress effectiveness rule detail underlie generalize linear predictor use produce subset hamming loss case restrict ranking case result result summarize figure algorithm train sweep training datum preliminary nature experiment allow draw conclusion result yt decrease c contain left plot performance improve time dataset predict middle plot final cumulative hamming loss constant c divide number training sample function similar plot right final average ranking loss divide case optimal value allow balance exploration exploitation performance algorithm pretty close performance receive partial feedback experiment square loss result exception ranking loss dataset figure right conclusion use generalized linear model formalize tradeoff setting partial feedback provide t regret bound setting analysis decouple loss hand model thank usage calibrate score value pbit algorithm capable automatically split ranking relevant class split clearly induce loss parameter ac plan use general label model explicitly capture label correlation apply loss function fmeasure average precision plan carry thorough experimental comparison especially information multilabel method correlation account finally work extend framework structured output task hierarchical classification reference szepesvari improve algorithm linear stochastic bandit th nip m kearn u graphical model bandit problem p auer use confidence bound tradeoff jmlr k multiclass classification bandit feedback use adaptive regularization icml kakade stochastic linear optimization bandit feedback e label dependence loss machine learn appear szepesvari parametric bandit generalized linear case nip page freund r d r e schapire singer efficient boost algorithm combine preference jmlr e e calibrate label rank machine learn e hazan s online submodular minimization nip e hazan s efficient bandit algorithm online multiclass prediction nip obermayer large margin rank boundary regression advance large margin classifier mit kakade shalevshwartz tewari efficient bandit algorithm online prediction icml s l r schapire bandit problem th nip krause contextual gaussian process bandit optimization nip robbin asymptotically efficient adaptive allocation rule math p generalize linear model chapman p improve multilabel analysis music largescale validation correction approach ieee tran audio speech p t joachim online structured prediction learning icml appear slivkin learning optimally diverse ranking large document collection icml m m challenge problem automate detection semantic concept proc th international conference page m krause online learning assignment rd nip random ieee transaction knowledge datum engineering
oscillatory correlation framework computer email computer information science centre cognitive science email neural model describe use oscillatory correlation speech interfere sound source core model twolayer neural oscillator network sound stream represent synchronize population oscillator different stream represent oscillator population model evaluate use corpus speech mix interfere sound produce improvement signaltonoise ratio mixture introduction speech isolation usually mix environmental sound auditory system parse acoustic mixture reach ear order retrieve description sound source process term auditory scene conceptually regard process stage term segmentation decompose acoustic stimulus collection sensory element second stage grouping element likely arise environmental event combine perceptual structure stream stream far interpret higherlevel cognitive process recently grow interest development system mimic computational auditory system inspire auditory function model closely employ symbolic search highlevel inference engine performance system encourage match ability human tend complex computationally intensive short currently remain problem realtime application automatic speech recognition human concurrent sound apparent ease computational system closely model mechanism hearing offer performance advantage exist system observation desire understand basis lead propose neural network model recently brown account concurrent vowel separation base oscillatory correlation framework oscillator represent perceptual stream synchronize phase phase oscillator represent different stream evidence oscillatory correlation theory come study report oscillation auditory visual cortex review l paper propose neural network model use oscillatory correlation underlie neural mechanism stream form synchronize oscillator twodimensional network model evaluate task involve separation sound extend previous study consider segregation vowel sound static spectra model description input model consist mixture speech interfere sound source sample rate khz bit resolution input signal process stage describe detailed account peripheral auditory process peripheral auditory frequency selectivity model use bank filter center frequency equally distribute equivalent rectangular bandwidth scale khz subsequently output filter process model inner cell function output cell model probabilistic representation auditory nerve firing activity auditory mechanism similar underlie pitch perception contribute separation sound different fundamental frequency accordingly second stage model extract information simulate auditory nerve fire pattern achieve compute run autocorrelation auditory nerve activity channel form representation know correlogram time step autocorrelation channel time t ai r output cell model rectangular window time step use correspond window width ms autocorrelation lag t compute l step sample period use correspond maximum delay equation compute m frame interval e interval step time periodic sound characteristic appear correlogram center lag correspond stimulus period figure structure emphasize form pooled correlogram exhibit prominent peak delay correspond perceive pitch sj possible extract harmonic correlogram frequency channel acoustic component share similar pattern band coherent identify adjacent correlogram channel region high correlation indicate harmonic crosscorrelation cij channel time frame define ai t autocorrelation function normalize mean unity variance typical crosscorrelation function figure oscillatory correlation network segmentation grouping place twolayer oscillator network figure basic unit network single oscillator define connected excitatory variable x inhibitory variable layer network form grid index oscillator accord frequency channel time frame represent external input oscillator si denote coupling oscillator network parameter p amplitude noise term couple noise ignore hold constant define relaxation oscillator time scale cubic function sigmoid function point middle branch cubic choose small case oscillator exhibit stable limit cycle small value c refer enable limit cycle alternate silent active phase near behaviour compare motion phase phase place rapidly refer jump stable fix point case oscillation occur oscillation oscillator network segment layer layer network segment form block oscillator trace evolution acoustic component time frequency layer twodimensional grid oscillator global figure coupling term sij define e z function ie connection weight oscillator ij oscillator near neighbor ij threshold ex choose oscillator influence u q u nd j autocorrelation lag figure correlogram mixture speech trill telephone ms start stimulus pooled correlogram panel crosscorrelation function right b structure twolayer oscillator network l neighbor active phase weight neighboring connection time axis uniformly set connection weight oscillator ij vertical neighbor set cij exceed threshold set weight inhibition global define oscillator ij threshold small segment form correspond significant acoustic component order remove noisy fragment introduce lateral potential oscillator ij define pij l ex pij threshold potential neighborhood ij choose neighbor ij active pi approach fast time scale pij relax slow time scale determine lateral potential play role gate input oscillator specifically replace initialize follow pij drop threshold oscillator ij receive excitation entire potential neighborhood choice neighborhood imply segment extend consecutive time frame oscillator stimulate maintain high potential relegate background noisy activity oscillator ij stimulate corresponding input oscillator stimulate energy corresponding correlogram channel exceed threshold sa evident energy correlogram channel time correspond set sa figure segmentation mixture speech trill telephone network simulate produce segment represent distinct gray level background black convenience segment figure actually arise unique time interval b s c time second time second figure segment form layer network mixture speech trill telephone b categorization segment accord gray pixel represent set p white pixel represent region agree oscillatory correlation network group layer second layer twodimensional network couple oscillator global inhibition oscillator layer stimulate corresponding oscillator layer stimulate form background initially oscillator phase imply segment layer allocate stream initialization consistent psychophysical evidence suggest perceptual fusion default state auditory second layer oscillator form change ij sij p small positive parameter imply oscillator high lateral potential slightly high external input choose oscillator correspond long segment layer jump active phase long segment identify use mechanism describe coupling term consist type couple e sij sj represent mutual excitation oscillator segment set active oscillator segment occupy half length segment sj active oscillator segment s denote vertical connection oscillator correspond coupling term different frequency channel different segment time frame time frame estimate pooled correlogram use classify frequency channel category set channel p consistent set channel figure b delay tm large peak occur pooled correlogram channel time frame e p au ad energy correlogram channel time classification basis energy threshold use ad delay tm find use winnertakeall network simplicity currently apply maximum selector r r u u q q l l g c q q u c q q u u u time second time second figure snapshot activity second layer shortly start simulation active oscillator white pixel correspond speech stream b snapshot shortly active oscillator correspond telephone stream l classification process operate channel segment result channel segment particular time frame allocate different category segment decompose enforce rule channel frame segment belong category majority channel step vertical connection time frame oscillator different segment mutual excitatory link corresponding channel belong category mutual inhibitory link set ij receive input inhibitory link similarly s set ij receive input vertical excitatory link present model mechanism group segment overlap time accordingly limit operation second layer time span long segment lateral connection long segment network numerically solve use singular limit method figure response second layer mixture speech trill telephone figure snapshot second layer indicate active oscillator black pixel indicate silent oscillator network quickly form block figure snapshot oscillator block stream correspond speech active phase figure b subsequent snapshot oscillator block correspond trill telephone active phase activity layer network embody result component acoustic mixture separate use represent oscillatory correlation stage model path output divide section overlap raise cosine weighting apply section unity corresponding oscillator active phase weighted filter output sum channel yield b u b r s u type ir r r s bi d type figure snr black bar bar separation model result voice speech mix khz random noise party noise music n trill telephone female speech male speech female speech b percentage speech energy recover mixture separation model
product edgepert biological cybernetic max computer abstract image represent important source datum understand statistical structure important application image compression restoration paper propose particular kind probabilistic model product edgepert model describe structure wavelet transform image develop practical algorithm base single edgepert stateoftheart denoise performance benchmark image introduction image represent collection pixel value exhibit high degree redundancy wavelet transform capture second order dependency form basis successful image processing application image compression image restoration eg wavelet core high order dependency filter linear transform particular absolute value neighboring wavelet coefficient sign mutually dependent kind dependency cause presence edge induce clustering wavelet activity model clustering effect potentially improve performance important image processing task model build early work image processing literature particular poedge model discuss paper view generalization model propose area joint model discuss base gaussian scale mixture model gsm gsm fall category direct graphical model topdown structure poedge model good classify undirected markov random field model follow bottomup semantic main contribution paper new model describe high order statistical dependency wavelet coefficient section efficient estimation procedure fit parameter single edgepert model new technique estimate wavelet coefficient participate local model section new iterate wiener denoise section section report number experiment compare performance algorithm method literature method particular u upper left component upper leave component z center component center component figure estimate model ib conditional distribution wavelet coefficient upper left neighbor statistic collect vertical subband low level filter wavelet decomposition image note bowtie dependency capture poedge model network interpretation product edgepert topdown gaussian scale mixture model product edgepert long recognize image processing community wavelet transform form excellent basis representation image class linear transform represent compromise desirable property image representation multiscale representation locality space frequency orthogonality result decorrelation particularly suitable wavelet transform form basis good denoise algorithm today overcomplete wavelet pyramid freely experiment confirm good result obtain use wavelet pyramid following describe model statistical dependency coefficient model inspire recent study dependency represent generalization bivariate laplacian model propose probability distribution product poedge wavelet coefficient follow form h p normalization constant z depend parameter model indicate wij figure effect change parameter single ie set eqn parameter control shape contour elliptical contour contour straight line contour curve parameter control rate distribution decay ie distance contour unit vector ai determine orientation basis vector ai figure distribution symmetric wrt subset zi origin imply wavelet coefficient necessarily decorrelate high order dependency remain finally weight wij model scale inverse variance coefficient mention possible large number basis vector wavelet coefficient socalled overcomplete basis appropriate empirical joint histogram model describe important statistical property observe wavelet coefficient marginal distribution peak heavy tail high conditional distribution display bowtie dependency clustering wavelet coefficient neighbor coefficient b c d figure contour plot single edgepert model figure active phenomenon figure understand qualitative behavior model provide follow network interpretation figure input model ie wavelet coefficient undergo nonlinear transformation u z u output network interpret penalty input large penalty unlikely input probabilistic model process naturally understand enforce constraint form z penalize violation constraint reason poedge model capture clustering wavelet activity consider local model describe statistical structure patch wavelet coefficient recall weighted sum activity penalize fix position activity typically small image edge happen fall window model coefficient active jointly sparse pattern activity incur penalty instance activity distribute equally image concave shape penalty function act act act activity level relate work early wavelet denoise technique base observation marginal distribution wavelet coefficient highly heavy tail find generalize gaussian density represent good fit empirical histogram lead successful wavelet shrinkage method bivariate generalization model describe wavelet coefficient parent high level pyramid jointly propose probability density q easily special case poedge model propose model univariate model capture bowtie dependency describe result significant gain denoise performance propose model large neighborhood wavelet coefficient particular good denoise result obtain include subband neighborhood size addition parent wavelet gsm define term precision variable multiply multivariate gaussian variable u follow expression distribution wavelet coefficient u prior distribution precision variable gsm represent example generative model topdown semantic assume total variance wavelet activity fix comparison contrast poedge model interpret bottomup network proportional output difference contrast figure edgepert denoise base poedge model discuss previous section introduce simplified model form basis practical denoise algorithm recent progress field indicate important model high order dependency exist wavelet coefficient realize estimation joint model small cluster wavelet coefficient coefficient ideally like use poedge model train model datum order computation tractable proceed simplified model pz wj j compare poedge model use edgepert set model estimation task estimate parameter model efficiently learn separate model wavelet coefficient jointly small neighborhood dependent coefficient model estimate step determine coefficient participate model transform model decorrelate domain implicitly estimate estimate remain parameter decorrelate domain use moment match describe step detail denote clean noisy wavelet coefficient respectively denote decorrelate clean noisy wavelet coefficient denote random variable wavelet domain zi detail wavelet decomposition property noise assume noise correlate mean paper far assume know noise covariance image domain easily compute noise covariance wavelet domain minor change need estimate noisy image step start neighborhood adaptively select good candidate include model addition include parent coefficient subband coarser scale exist band coefficient participate model select estimate dependency relative center coefficient anticipate second order correlation remove sphere interested high order dependency particular dependency variance follow cumulant use obtain estimate hcj e c center coefficient denoise necessary average compute collect sample subband assume statistic location invariant cumulant invariant addition possibly correlate ie value zi effectively measure high order dependency squared wavelet coefficient correlation finally select participant model center rank positive hcj pick satisfy model vary number participant estimate ij correct set negative eigenvalue way sum eigenvalue invariant statistic collect sample subband perform linear transformation new basis diagonal accomplish follow procedure z new space different wavelet coefficient assume axis align basis step decorrelate space estimate single edgepert model moment match moment model space easily compute use p wj number participate coefficient model note lead follow equation e e estimate line search approximate second term right hand simplify calculation far note model symmetric wrt permutation variable wj find np e wj common strategy wavelet literature estimate average e collect sample local neighborhood coefficient consideration advantage estimate adapt local statistic image adopt strategy use box coefficient collect sample decorrelate wavelet domain coefficient e set remove consideration estimation depend fourth moment sensitive outlier commonly know problem moment match method encounter problem find estimate use eqn simply set iterate wiener filter infer wavelet coefficient noisy observation decorrelate wavelet domain maximize posteriori probability joint model equivalent argmax log log pz z assume noise translate z wj argmin z t linear wavelet transform z pseudoinverse noise covariance matrix decorrelate wavelet domain simply set construct upper bind objective use input psnr output psnr output lm input figure output psnr function input psnr method leave right image gsm gaussian scale mixture shrinkage result result dash line indicate result copy literature solid line indicate value reproduce computer bind saturate construct follow iterative guarantee converge local minimum diag t wj j algorithm natural interpretation iterate wiener filter step leave hand ordinary wiener filter second step right hand adapt variance filter summary complete algorithm provide denoise decompose image subband subband lowpass residual determine coefficient participate joint model use include parent compute compute signal covariance use coefficient subband transform coefficient decorrelate domain use estimate parameter wi local neighborhood use eqn iii denoise wavelet coefficient neighborhood use iv transform denoise cluster wavelet domain retain center coefficient reconstruct denoise image invert wavelet transform experiment denoise experiment run wavelet pyramid orient residual band use orientation describe result report image house pepper average experiment experiment image contaminate independent noise predetermine variance denoise use iteration propose algorithm reduce artifact boundary use boundary extension image obtain ensure comparison set image table compare performance poedge gsm base denoise algorithm test image different noise level figure compare result pepper table comparison image denoise result poedge close competitor gsm result average noise sample gsm result copy detail poedge describe main text note poedge outperform gsm low noise level gsm perform high noise level poedge perform good noise level barbara image gsm superior image method publish literature image experiment lead interesting conclusion compare poedge general trend poedge perform superior low noise level reverse true high noise level observe poedge significantly result barbara image publish method large accord finding author stem mainly fact parameter estimate locally particularly suit image increase estimation window algorithm let denoise result drop gsm solution report compare quality restore image detail figure conclude gsm produce slightly sharp edge expense artifact denoise pixel sized image ghz pc adaptive neighborhood selection model second second compare gsm use separable orthonormal pyramid use simple orthonormal decomposition find model outperform gsm experiment describe result significantly inferior wavelet representation play prominent role denoise performance result matlab implementation algorithm available online discussion propose general product edgepert model capture dependency structure wavelet coefficient turn practical algorithm simplify single edgepert choose parameter model adapt base noisy observation image comparison close competitor gsm find superior performance low noise level reverse true high noise level poedge model perform competitor barbara image consistency gsm image gsm model aim capture statistical regularity poedge use different modelling paradigm poedge interpret bottomup constraint satisfaction model gsm causal generative model topdown find modelling paradigm exhibit different denoise accuracy personal communication b c d figure comparison gsm parent psnr parameter setting describe text psnr barbara image crop enhance artifact noisy image b psnr result turn similar gsm slightly expense introduce artifact type image imply opportunity study improvement model extend number way example lift coefficient extend allow selection subband different scale andor orientation substantial performance gain expect extend single case approximation estimation model necessary denoise algorithm practical adaptation rely empirical estimation fourth moment sensitive outlier currently investigate robust estimator fit performance gain expect development new pyramid modelling new dependency structure phenomenon phase alignment edge acknowledgment like thank author code available online reference statistic natural image model proc computer vision pattern recognition page ft collin bivariate shrinkage local variance estimation ieee process letter m wainwright e p simoncelli image denoise use scale mixture wavelet domain ieee tran image process freeman flexible architecture multiscale derivative computation image processing simoncelli model joint statistic image wavelet domain proc annual meeting volume page teh discover multiple constraint frequently approximately satisfied proc uncertainty artificial intelligence page noise removal wavelet core image processing spatially adaptive image denoise overcomplete expansion image process p image denoise base statistical modeling wavelet coefficient proc letter
correct sample selection bias maximum entropy density estimation computer science research park abstract study problem maximum entropy density estimation presence know sample selection bias propose bias correction approach advantage unbiased sufficient statistic obtain biased sample second estimate biased distribution factor bias approximate second use sample sample distribution provide guarantee approach evaluate performance approach synthetic experiment real datum specie habitat modeling maxent successfully apply sample selection bias significant problem introduction study problem estimate probability distribution particularly context specie habitat model common distribution model assume access independent sample distribution estimate practice assumption violate reason example habitat modeling typically base know occurrence location derive collection natural history biological survey goal predict specie distribution function environmental variable achieve statistically sound manner use current method necessary assume sample distribution specie distribution correlate fact sample location easy access area close road furthermore independence assumption hold road correlate topography vegetation influence distribution new unbiased sampling expensive gain use extensive exist biased datum especially freely available online available datum collect biased manner usually information available nature bias instance case habitat model factor influence sample distribution known distance road addition list visit site available view sample sample distribution list available set site specie large group observe reasonable approximation visit location paper study probability density estimation sample selection bias assume sample distribution approximation know training require unbiased model use knowledge sample selection bias test requirement habitat modeling model apply different region different condition knowledge work address sample selection bias statistically sound manner setup suitable specie habitat model datum propose approach incorporate sample selection bias common density estimation technique base principle maximum entropy maxent maxent regularization successfully use model distribution specie assumption sample unbiased review regularize maxent unbiased datum section detail new approach section approach simple modification unbiased maxent achieve analogous provable performance guarantee approach use bias correction technique similar obtain unbiased confidence interval biased sample require version maxent prove unbiased case produce model log loss approach good possible gibb distribution increase sample size contrast second approach propose estimate biased distribution factor bias target distribution gibbs distribution solution approach log loss target distribution target distribution gibb demonstrate second approach produce optimal gibbs distribution respect log loss limit infinitely sample prove produce model good good gibbs distribution accord certain bregman divergence depend selection bias addition observe good empirical performance moderate sample size approach approximation second approach use sample sample distribution instead distribution challenge study method correct sample selection bias unbiased datum set require training need test set evaluate performance unbiased datum set difficult obtain reason study problem synthetic datum use section describe experiment evaluate performance method use fully synthetic datum biological dataset consist biased training set independently collect reasonably unbiased test set relate work sample selection bias arise econometric stem factor selection extensively study context linear regression paper bias estimate transform estimate use additional machine learn community sample selection bias recently consider classification problem goal learn decision rule biased sample problem closely relate learn technique differential weighting sample apply method previous approach apply directly density estimation setup ie dependent variable classification terminology access positive example cost log loss unbounded addition case model specie habitat face challenge sample size small machine learning standard maxent setup section describe setup unbiased maximum entropy density estimation review performance guarantee use relaxed formulation yield regularization term objective function goal estimate unknown target distribution known sample space base sample assume sample independently distribute accord denote empirical distribution m structure problem specify real value function r feature distribution q represent default estimate assume feature capture relevant information available problem hand q distribution choose sample distribution q assume uniform limited number sample expect poor estimate reasonable distance measure empirical average feature different expectation respect let pf denote expectation function choose randomly accord distribution p like find distribution p satisfie j estimate deviation empirical average expectation usually infinitely distribution satisfy constraint case default distribution q uniform maximum entropy principle tell choose distribution maximum entropy satisfy constraint general minimize relative entropy q correspond choose distribution satisfy constraint impose little additional information possible compare q allow asymmetric constraint obtain formulation min subject bj p r simplex probability distribution relative entropy kullbackleibler divergence q p information theoretic measure difference distribution nonnegative equal distribution identical convex argument problem convex program use lagrange multiplier obtain solution form q q p q normalization constant distribution q form refer q gibbs gibb ambiguity arise instead solve directly solve dual log bj bj r choose range general optimization technique use algorithm symmetric case bj j dual log q r term empirical log loss negative log likelihood second term regularization small value log loss mean good fit datum balance regularization force simple model prevent overfitte primal constraint satisfied target distribution solution q dual guarantee bad approximation good gibb distribution precisely theorem performance guarantee theorem assume distribution satisfy primal constraint let q solution dual arbitrary gibb distribution q j input finite domain feature default estimate regularization parameter sample distribution sample output q approximate target distribution let m min max min s m min max min cj dj s j s min bj cj dj c min max solve average table example distribution minimize x sx sx q q sx q feature bound symmetric box constraint log nm satisfied high probability hoeffding inequality union bind relative entropy q bad relative p entropy gibbs distribution q practice set m min max fj tune constant sample deviation fj max upper bind standard deviation refer unbiased datum nbiase m axent maxent sample selection bias biased case goal estimate target distribution sample come directly nonnegative function p p define let p p denote distribution obtain multiply weight p x p point renormalize p p p p p sample come biased distribution s sample distribution setup correspond situation event observe occur point probability perform independent observation probability probability observe event observe event equal sx empirical distribution m sample draw denote assume know principal assumption introduction strictly positive technical assumption approach average approach use algorithm unbiased case employ different method obtain confidence interval bj direct access sample use version bias correction theorem convert expectation respect expectation respect bias correction theorem translation theorem sf s ss suffice confidence interval sf ss obtain confidence interval corollary assume bound cj dj high probability c d cj s dj probability cj c bound away bound use determine cj dj theorem yield guarantee method performance converge increase sample size good gibbs distribution practice confidence interval cj determine use expression analogous random variable empirical distribution restrict confidence interval natural fashion yield alternatively use bootstrap type estimate confidence interval bias second algorithm approximate directly use maxent estimate distribution convert estimate approximation default estimate q default estimate apply unbiased maxent empirical distribution default q s obtain q distribution approximate s factor obtain ef estimate yield factor b ias approach correspond regularize maximum likelihood estimation q gibbs distribution q gibb distribution performance guarantee unbiased maxent imply estimate converge number sample increase case finite estimate obtain factor converge q gibb s q approximate q gibb distribution q increase number sample minimize q example minimizer different example consider space feature feature target distribution sample distribution biased distribution s table use uniform distribution default estimate minimizer unique distribution q similarly minimizer q unique distribution solve exactly find q table distribution differ factor b ias minimize q minimize different bregman divergence precisely minimize bregman divergence certain projection distribution bregman divergence generalize common distance measure relative entropy square distance enjoy favorable property bregman divergence associate function define proposition define rx r log function p p p p k p p p p p x x sx p p pp sx p projection p p line t r hyperplane approach iii approximating factor b ias mention introduction know sample distribution s exactly access sample approach assume unknown addition sample separate set sample use factor b ias sample distribution s replace corresponding empirical distribution simplify algorithm note instead use q default estimate s suffice replace sample space use q relative entropy target target target target number training sample m unbiased maxent factor bias approximate factor bias sample sample figure learn curve synthetic experiment use u denote uniform distribution sample distribution performance measure term relative entropy target distribution function increase number training sample number sample plot log scale return restrict default step factor equivalent use space entire space sample distribution correlate feature value cover feature range case yield poor estimate range clamp restrict value range maximum result value exponent pprox factor b ias ut experiment conduct real datum experiment evaluate bias correction technique difficult bias typically unknown sample unbiased distribution available synthetic experiment necessity precise evaluation addition synthetic experiment able conduct experiment datum habitat model synthetic experiment synthetic experiment generate target distribution domain size distribution derive feature index fi value fi choose independently uniformly set fi x fix feature generate weight distribution weight generate jointly capture range different behavior value fi range let denote random variable uniform set s instance correspond new independent variable set u u u weight choose create correlation fi observable strong dominate set respective probability maxent use subset feature specify target distribution irrelevant feature use feature fi square fi fi relevant feature fi u irrelevant feature generate use set feature experiment generate sample distribution correlate target distribution specifically s gibbs generate feature fi square fi fi fi use weight target distribution evaluate performance u nbiase m average ias pprox factor b ias sample sample distribution performance evaluate term relative entropy target distribution use training set size consider randomly generate training set average performance set setting range report result good choose separately average approach want table result real datum experiment average performance unbiased maxent bias correction approach specie region uniform distribution receive log loss auc result bias correction approach significantly set boldface significantly unbiased maxent accord pair level significance unbiased maxent average factor bias factor bias average log loss explore potential performance method figure result optimal function increase number sample b ias nbiase m axent average bad nbiase m axent small sample size number training sample increase soon outperform nbiase m axent eventually outperform factor b ias pprox factor b ias improve number sample sample distribution increase version pprox factor b ias perform bad u nbiase m axent distribution real datum experiment set experiment evaluate maxent task estimate specie habitat sample space region divide grid cell sample know occurrence cell specie observe cell describe set environmental variable categorical vegetation type continuous annual feature realvalue function derive environmental variable use binary indicator feature different value categorical variable binary threshold feature continuous variable equal value variable great fix threshold specie sample location environmental variable produce use testing alternative methodology model specie predict distribution work group national center analysis synthesis work group compare model method variety specie region training set contain datum survey record include test set contain datum plan independent survey compare performance bias correction approach unbiased maxent method comparison use dataset consist specie region training presence specie average test detail treat training occurrence location specie region sample distribution sample use directly pprox factor b ias order apply average factor b ias estimate sample distribution use unbiased maxent sampling distribution estimation step contrast work experiment use sample distribution estimate evaluation depend quality result distribution evaluate test presence accord log loss test presence absence accord area curve auc auc predict distribution rank test presence test absence value equal probability randomly choose presence rank randomly choose absence uniformly random prediction receive auc perfect prediction receive auc table performance approach compare unbiased maxent algorithm yield average bad log loss unbiased maxent attribute estimate sample distribution sample distribution large portion sample space performance measure term auc factor b ias pprox factor b ias yield average auc nbiase m axent region improvement region dramatic method perform method evaluate conclusion propose approach incorporate information sample bias maxent demonstrate utility synthetic real datum experiment experiment raise question merit far research average strong performance guarantee perform bad real datum experiment method large sample size synthetic experiment poor estimate unbiased confidence interval possibly improve use different estimation method factor b ias pprox factor b ias improve nbiase m axent term auc real datum bad term log loss disagreement suggest method aim optimize directly successful specie model possibly incorporate concept factor b ias pprox factor b ias pprox factor b ias perform good real world datum possibly direct use sample sample distribution sample distribution estimate method come performance guarantee exploit knowledge sample space prove performance guarantee pprox factor b ias remain open future acknowledgment material base work support nsf grant opinion finding conclusion recommendation express material author necessarily reflect view nsf datum share member testing alternative methodology model specie predict distribution work group support national center analysis synthesis center fund nsf grant reference quantitative method model specie habitat comparative performance application australian plant editor quantitative method biology page springerverlag predictive habitat distribution model modelling schapire maximum entropy approach specie distribution modeling proceeding international conference machine learn l m sampling bias implication barbara r mammal world example datum integration distribute network environment learning evaluate classifier sample selection bias proceeding international conference machine learn learning weight proceeding ieee international conference datum mining sample selection bias specification error survey error survey cost j little rubin statistical analysis miss datum second elkan foundation learning proceeding international joint conference artificial intelligence j schapire performance guarantee regularize maximum entropy density estimation th annual conference learn theory j work group compare methodology model specie distribution datum preparation meaning use area receiver operate curve
linear contextual bandit knapsack r abstract consider linear contextual bandit problem resource consumption addition reward generation round outcome pull arm reward vector resource consumption expect value outcome depend linearly context arm constraint require total consumption exceed budget resource objective maximize total reward problem turn common generalization classic linear contextual bandit bandit knapsack bwk online stochastic packing problem present algorithm nearoptimal regret bound problem bound compare favorably result unstructured version problem relation context outcome arbitrary algorithm compete fix set policy accessible optimization oracle combine technique work bwk ospp nontrivial manner tackle new difficulty present special case introduction contextual bandit problem decision observe sequence context feature round need pull arm observe context round outcome pull arm use context decide future arm contextual bandit problem find useful application online recommendation system online advertising clinical trial decision round need feature user serve linear contextual bandit problem special case contextual bandit problem outcome linear feature vector encode context point contextual bandit problem represent natural point supervised learning reinforcement learn use feature encode model relation feature vector outcome inherit supervise learn tradeoff necessary ensure good performance reinforcement learn linear contextual bandit problem think linear regression model supervised learning reinforcement learn recently significant interest introduce multiple global constraint standard bandit set constraint crucial important application example clinical trial treatment plan constrain total availability medical drug resource online advertising budget constraint restrict number time ad application include dynamic pricing dynamic crowdsource example paper consider linear contextual bandit knapsack henceforth lincbwk problem problem context vector generate round unknown distribution pick arm reward consumption vector observe depend research conference neural information processing system nip linearly context vector aim decision maximize total reward ensure total consumption resource remain budget precise definition problem use follow notational convention vector denote bold face low case letter matrix denote regular face case letter quantity set scalar case bold face vector column vector ie vector dimension treat matrix transpose definition lincbwk arm identify set initially input budget r round t observe m arm choose arm finally observe reward ddimensional consumption vector algorithm option pick arm reward consumption goal pick arm total reward p maximize ensure total consumption exceed budget b follow stochastic assumption context reward consumption vector round t vt generate unknown distribution d independent previous round exist unknown vector m matrix arm context history time denote tuple context arm time t column matrix similarly reward consumption vector time represent vector matrix respectively discuss later text assumption equation form primary distinction linear contextual bandit setting general contextual bandit setting consider exploit linearity assumption allow generate regret bound depend number arm k render especially useful number arm large example include recommendation system large number product product travel package ad sponsor post advantage use general contextual bandit setting need oracle access certain optimization problem case require solve nphard problem section detailed discussion compare performance algorithm optimal adaptive policy know distribution d parameter account history point current context decide possibly randomization arm pull time easy work upper bind optimal expect reward static policy require satisfy constraint expectation technique use related problem standard definition optimal static policy contextdependent policy mapping context space mk p denote probability pk play arm context probability define r v expect reward consumption vector policy respectively let rx arg max t r optimal static policy note allow feasible policy exist denote value optimal static policy opt t r follow lemma prove opt upper bound value optimal adaptive policy proof appendix b let opt denote value optimal adaptive policy know distribution d parameter opt opt definition regret let arm play time algorithm regret define t main result main result algorithm nearoptimal regret bind algorithm lincbwk m t probability p opt m relation general contextual bandit recent paper solve problem similar lincbwk general contextual bandit paper relation context outcome vector arbitrary algorithm compete arbitrary fix set context dependent policy accessible optimization oracle regret bound p approach potentially apply linear set use set p linear context dependent policy compare bound result essentially log factor replace factor m importantly dependence enable consider problem large action space far suppose want use result set linear policy policy form fix algorithm require access argmax oracle find good policy maximize total reward set context reward resource consumption fact reduction problem learn noise optimization problem underlie argmax oracle problem nphard approach computationally expensive proof appendix supplement result budget b m t similar bound budget small b m t interesting open problem indicate hard problem special case regret bound linear contextual bandit problem online algorithm achieve regret bind m t fact prove lower bind linear contextual bandit static context problem special case lincbwk problem dependence m t regret bind optimal log factor general contextual bandit resource constraint bound optimal relation bwk ospp easy lincbwk problem generalization linear contextual bandit problem outcome scalar goal simply maximize sum remarkably lincbwk problem turn common generalization bandit knapsack bwk problem consider online stochastic packing problem ospp study bwk ospp outcome round t reward vector goal algorithm maximize ensure problem differ reward vector pick ospp problem round t algorithm pick pair set dimensional vector set draw unknown distribution set vector correspond special case lincbwk m context equal vt bwk problem fix set arm arm unknown distribution pair pick arm pair draw corresponding distribution arm similar regret bound linear contextual bandit correspond special case lincbwk m context identity matrix t use technique special case algorithm follow use online learn algorithm search dual space order deal linear context use technique estimate weight matrix define optimistic estimate use technique combine objective constraint use certain tradeoff parameter introduce new difficulty arise estimate optimum value round task follow standard technique special case challenging develop new way exploration use linear structure evaluate possible choice lead optimum solution sample technique independent interest estimate optimum value problem sum t fact optimal bind lincbwk m meaningful case optimal bind hold m approach bwk extend case static context arm context change time ospp special case lincbwk static additional difficulty dynamic static context relate work recently o t regret linear contextual setting single budget constraint cost depend context arm space constraint proof main paper supplement confidence ellipsoid consider stochastic process round t generate pair observation unknown linear function mean bounded noise t rm t r et r t time t high confidence estimate unknown vector obtain build t construct confidence ellipsoid regularize leastsquare estimate observation far technique common prior work linear contextual bandit regularization parameter let p y ri following result lie high probability center positive semidefinite matrix define m norm m confidence ellipsoid time t define p t r m log m theorem t m m useful observation construction state appear prove lemma t km corollary lemma obtain bind total error estimate provide point confidence ellipsoid proof appendix d supplement incorrectly claim approach extend dynamic context modification ct point confidence ellipsoid corollary let r probability p t m logt t m online learning consider t round game play online learner adversary round t learner choose t observe linear function pick adversary learner choice t depend learner choice previous round goal learner minimize regret define difference learner objective value value good single choice t multiplicative weight update fast efficient online learn algorithm problem let gt parameter round choice algorithm following form p initialization wj provide following regret bind online learn problem describe particular q logd logd logd q rest paper refer logd online learn update ol update time optimistic estimate unknown parameter let denote arm play algorithm time beginning round use outcome context previous round construct confidence ellipsoid column construction confidence ellipsoid follow directly technique section yt xt reward time t construct confidence ellipsoid column use technique section substitute vt section let mt xi construct regularize square estimate respectively p ai ai p ai vi ai define confidence ellipsoid parameter p m m k k arm optimistic estimate arg let denote column matrix define confidence ellipsoid column p rm m m denote gt product ellipsoid wj note imply gt probability vector t rd define optimistic estimate weight matrix time wrt t arm k arg t t intuitively reward want upper confidence bind consumption want low confidence bind optimistic estimate intuition align definition use case reward minimizer use consumption utility precise meaning t clear describe algorithm present regret analysis t result corollary confidence use definition ellipsoid following derive corollary probability sequence t arm k time t t w t arm k time p t m log k m logt essentially claim ensure optimistic estimate claim ensure estimate quickly converge true parameter core algorithm section present algorithm analysis assumption parameter z satisfy certain property later use t round compute z bind additional regret t round define opt assumption let universal constant opt z c t section run t construct estimate instance online learning problem vector play time step t t observe context optimistic estimate arm construct use t define intuitively t use multipli combine different column weight matrix optimistic weight vector arm estimate reward arm define use z linearly combine optimistic estimate reward t t optimistic estimate consumption choose arm appear good accord estimate reward observe result reward consumption vector estimate update learn advance step define vector end t time step soon total consumption exceed budget dimension theorem z assumption achieve following p opt b m t logt proof sketch provide sketch proof proof appendix e supplement let stop time algorithm proof step step apply inequality small work high probability similar application instead p use bind gap t p lower bind t sufficient lower bind total reward t algorithm lincbwk initialize online learn initialize satisfie assumption t t observe respectively t k compute t t t z play arm arg observe d vt b exit use vt obtain choose t use ol update refer gt t vt end b t p t step use corollary high probability bind t sufficient work sum vector instead t instead similarly p t step proof complete desire bind opt similar online stochastic packing problem actual reward consumption vector t exactly problem adapt technique t use parameter combine constraint objective dimension consume fast multipli dimension increase algorithm pick arm likely consume dimension regret bound combination online learn regret error optimistic estimate z computation section present modification compute require parameter satisfy assumption need provide z input compute use observation t round z compute algorithm run remain time step need modify slightly account budget consume t round handle use small budget t computation remain round modify algorithm algorithm lincbwk z computation input b b b t use observation round compute z satisfie assumption run t t round budget b provide detail compute observation t round choose t provide method advantage linear structure problem explore space context weight vector obtain bound independent k round t t observe let k pk select arm probability fact matrix convexity function kxt play estimate time ai vi ai opt value define later obtain estimate intuition choice arm observe discussion section column wj guarantee lie confidence ellipsoid center column ellipsoid m logt m note ellipsoid principle axis eigenvector length axis inverse eigenvalue maximize kxt choose context close direction long principal axis confidence ellipsoid ie direction maximum uncertainty intuitively correspond pure exploration observation direction uncertainty large reduce uncertainty estimate effectively want algebraic explanation follow order good estimate opt close estimate small policy particular optimal t policy use bound km kxt define km m norm max m norm use t km bound t bound m logt term m probability lemma bound second term play policy use play policy maximize km bound actually bound combine bound m logt d deviation p t x t prove follow m logt d probability opt opt corollary set opt b b value probability o opt b mt constant factor approximation corollary imply long b ie t opt opt m t regret bind provide bind account budget consume t round consider t consume budget t round additional regret opt t far b t budget remain t t round need z opt satisfy require assumption b instead ie need b t b use time compute corollary satisfy require assumption observation theorem use t t corollary high probability regret bind opt m t b particular b m t m t use t m t regret bind opt m b reference szepesvari improve algorithm linear stochastic bandit nip agarwal l e schapire fast simple algorithm contextual bandit icml r bandit concave reward convex knapsack proceeding acm conference economic computation r fast algorithm online stochastic convex programming soda page r l efficient algorithm contextual bandit knapsack extension concave objective colt dynamic nearoptimal algorithm online linear programming operation research e hazan s multiplicative weight update method application theory compute p auer use confidence bound tradeoff mach mar r slivkin bandit knapsack foc page j langford slivkin contextual bandit proceeding conference learn theory colt page l r e schapire contextual bandit linear function aistat p s m kakade stochastic linear optimization bandit feedback colt r t p problem online match budget bidder random permutation r optimal online algorithm fast approximation algorithm resource allocation problem m n online stochastic packing apply display ad allocation proceeding th annual european conference p hardness learn compute h algorithm logarithmic sublinear regret constrain contextual bandit proceeding th international conference information processing system nip
measurement apply computational standard abstract study problem reconstruct unknown matrix m rank r dimension use ord poly log pauli measurement application tomography analogue wellknown problem compressed sensing recover sparse vector fouri coefficient set log measurement satisfy rankr restrict isometry property rip imply m recover fix universal set measurement use matrix bound error similar result hold class measurement use orthonormal operator basis element small operator norm proof use dudley inequality gaussian process bound cover number obtain duality introduction lowrank matrix recovery follow problem let m unknown matrix dimension d rank r d let set measurement matrix reconstruct m inner product problem application machine learn filtering problem remarkably turn useful choice measurement matrix possible efficiently example matrix know m ord measurement sufficient uniquely determine m furthermore m reconstruct solve convex program minimize nuclear norm example matrix completion problem measurement return random subset matrix element m case m ord poly log d measurement suffice provide m satisfie incoherence condition focus paper different class measurement know measurement ai randomly choose element pauli basis particular orthonormal basis cdd pauli basis analogue fourier basis lowrank matrix use measurement view generalization idea compressed sensing sparse vector use fourier coefficient addition problem application tomography task learn unknown quantum state perform measurement quantum state physical interest accurately describe density matrix low rank pauli measurement especially easy carry experiment tensor product structure pauli basis paper strong result lowrank matrix recovery measurement previously know rankr matrix m cdd choice m ord poly log random measurement lead successful recovery m strong statement fix universal set m ord poly log measurement rankr matrix m cdd successful recovery random pauli sample operator obey restrict isometry property rip intuitively rip sample operator approximate isometry act set lowrank matrix geometric term sample operator embed manifold lowrank matrix ord poly log d dimension low distortion norm rip lowrank matrix strong property prior work know hold unstructured type random measurement measurement application rip know fail matrix completion case hold measurement open question establish rip measurement use know result lowrank matrix recovery universal set measurement particular use bound error reconstructed density matrix datum noisy bound recovery arbitrary necessarily lowrank matrix bound qualitatively strong obtain use dual technique applicable situation rip fail context state tomography imply quantum state consist lowrank component residual fullrank component mc reconstruct error large particular let denote nuclear norm let denote frobenius norm error bound nuclear norm assume noiseless datum bound frobenius norm poly log d hold noisy datum reconstruction nearly good good rankr approximation m truncated addition completely arbitrary quantum state reconstruct error r norm lastly rip insight optimal design tomography experiment particular tradeoff number measurement setting essentially m number repetition experiment setting determine statistical noise enter result generalize class measurement essentially replace pauli basis orthonormal incoherent element basis c small operator norm order o d similar generalization note early result proof rip actually hold sense hold rankr matrix matrix satisfy kxk prove result combine number technique appear rip result previously know gaussian measurement close relative restrict strong convexity rsc similar somewhat weak property recently context matrix completion problem additional condition result follow cover argument ie use concentration inequality upperbound failure probability individual lowrank matrix union bind rip pauli measurement measurement structure randomness concentration measure weak union bind long desire result instead account favorable correlation behavior sample operator different matrix intuitively lowrank matrix m m overlap support good behavior m positively correlate good behavior m transform problem gaussian process use dudley entropy bind approach use classical compressed sensing rip fouri measurement key difference case gaussian process index matrix sparse vector bind correlation process need bind cover number nuclear norm ball matrix ball vector note universal result m slightly large factor poly log bind universal require different technique use entropy duality related work note remark matrix recovery fail exist large set d pauli matrix ie simultaneous d interest information state measure set pauli gain complete knowledge diagonal element unknown matrix m basis learn element reminiscent difficulty arise matrix completion case case turn rare unlikely random subset matrix finally note large body related work estimate lowrank matrix solve regularize program paper organize follow section state result precisely discuss specific application state tomography section prove rip matrix section discuss direction future work technical detail appear section b supplementary material notation vector denote norm matrix denote pnorm p x singular value particular trace nuclear norm norm operator norm finally matrix adjoint hilbertschmidt inner product b b letter denote act matrix map matrix cdd matrix result consider follow approach matrix recovery let cdd unknown matrix rank r let orthonormal basis cdd respect inner product b b choose m basis element sm uniformly random sample replacement observe coefficient m datum want reconstruct m possible measurement matrix incoherent respect roughly speak mean inner product wi m small formally basis incoherent small operator norm d constant assumption use proceed far let sketch connection problem tomography consider system hilbert space dimension want learn state system describe density matrix positive semidefinite trace rank r d state nearly pure class convenient experimentally feasible measurement describe matrix observable matrix form p pn denote tensor kronecker product pi matrix choose follow possibility z estimate expectation value pauli observable p pn special case measurement matrix scale pauli observable p pn d incoherent note maximum inner product rank matrix m normalize return discussion general problem choose sm uniformly random define sample operator cdd cm m m normalization choose note pm m assume datum cm unknown noise minimize nuclear norm subject constraint tion construct estimator m specify note view nuclear norm convex relaxation rank function estimator compute efficiently approach matrix selector arg min kxk m alternatively solve regularized leastsquare problem matrix lasso arg min kxk m parameter set accord strength noise component discuss later interested bound error estimator sample operator satisfie restrict isometry property rip rip measurement fix constant fix d set cdd satisfie restricted isometry property rip u u kxkf kxkf denote norm vector denote frobenius norm matrix set cdd rank r precisely notion rip study measurement satisfy rip slightly large set set cdd kxk provide number measurement m rd poly log d result generalize measurement basis small operator norm theorem fix constant let orthonormal basis incoherent sense let log d constant c depend c let define high probability choice sm satisfie rip set cdd kxk furthermore failure probability exponentially small prove theorem section remainder section discuss application lowrank matrix recovery particular application combine previous result immediately obtain bound accuracy matrix matrix lasso particular time universal recovery lowrank matrix measurement nearoptimal bound accuracy reconstruction data noisy similar result hold measurement incoherent operator basis result improve early result base dual detail sketch couple result particular interest state tomography m density matrix describe state quantum mechanical object vector expectation value state m m additional property positive semidefinite trace real vector main issue arise m precisely lowrank situation ideal state low rank instance pure state rank actual state observe experiment density matrix m fullrank decay eigenvalue typically interested obtain approximation m ignore tail spectrum secondly measurement inherently noisy observe directly estimate entry prepare copy state m measure pauli observable si copy average result observe zi distribute number experiment average large zi approximate interested estimate m stable respect noise remark reduce statistical noise perform repetition experiment suggest possibility tradeoff accuracy estimate parameter number parameter choose measure overall discuss like reconstruct m small error nuclear frobenius norm let m estimate bound error nuclear norm imply measurement allow m small bound mechanic probability distinguish state m error norm imply difference m m highly mixed contribute coherent quantum behavior system sketch result apply situation write m mc rankr approximation m correspond large singular value m residual m tail m ideally goal estimate m error large bind error nuclear norm assume datum noise proposition theorem let cdd cm random pauli sample operator m log d absolute constant c high probability choice following hold let m matrix cdd write m mc describe observe selector noise let m c absolute constant bind error frobenius norm allow noisy datum proposition assume setup observe selector let m high probability noise m c rd c r km c absolute constant term noise strength size tail mc universal bound error m sample operator work matrix m bind mix different norm useful m actually lowrank rank r bound particularly simple dependence noise strength know nearly furthermore singular value m fall noise level tight bind biasvariance theorem detail depend behavior tail hand m fullrank error m consider couple case suppose assume m fact density matrix quantum state km c m kf c rd use km arbitrary necessarily lowrank r state estimator nontrivial result r term interpret penalty measure incomplete subset observable finally consider case m fullrank know tail mc small know small nuclear norm use equation know small norm different bind use idea follow proposition let m matrix cdd singular value m m choose random pauli sample operator cdd cm m log d absolute observe let m selector lasso high probability choice noise z m c km r m c log d m ir c absolute constant bind interpret follow term express biasvariance tradeoff second term depend factor c log tight particular imply km m kf c rd c log compare equation involve bind ie tail mc eigenvalue physical term highly mixed proposition adaptation sketch proof section b note bind universal matrix m random choice sample operator likely work proof rip pauli measurement prove theorem general approach involve dudley entropy bind similar technical proof bound certain covering number use idea summarize argument detail section overview let cdd kxk let b set operator define follow norm sup mx suppose r sufficient purpose straightforward norm b banach space respect norm let define r elementary argument order prove rip suffice r proceed follow bind r concentrate mean p m use standard argument e dm r rademacher random variable round notation mean view matrix sj element vector space hilbertschmidt inner product round sj denote adjoint element dual vector use follow lemma prove later bound expect rademacher sum fix collection operator small operator norm let m fix cdd uniformly bound operator norm k let m uniform random variable m m x vi e r r r c log log m universal constant algebra q d m r log find root quadratic equation following bind let assume m dc k log desire result remain r concentrate expectation use concentration inequality sum independent symmetric random variable value banach space section detail proof bound rademacher sum let l e vi quantity want bind use standard comparison principle replace random variable variable gi m l vi random variable index u form gaussian process l expect supremum process use fact symmetric dudley inequality theorem log dg d eg sup dg covering number number ball cdd radius metric need cover set metric define new norm actually cdd follow km max m m use upperbound metric elementary calculation dg pm k vi let upperbound covering number dg cover number n r u rr dg r bind cover number introduce notation let denote pnorm cdd let unit ball norm let unit ball observe r u b k bx second inclusion follow simple bind covering number n r u dudley inequality restrict integral interval k small use follow simple bind equation large use sophisticated bind base empirical method entropy duality log log m defer proof section constant c use bind integral dudley inequality l c r rk log log m universal constant prove lemma proof equation cover number ball result follow easily bind cover number introduce appear let e banach space convexity power type constant e let e dual space let t e denote type constant let denote unit ball let e define norm max m log c e t e k log m jm universal constant proof use entropy duality reduce problem bound dual covering number m basic idea follow let m p norm consider p denote complex vector space m map e jth coordinate vector vj let n s denote number ball e need cover image map unit ball m bind use empirical method define dual map e m associate dual cover number relate finally relate entropy duality inequality detail apply lemma follow use approach let sp denote space consist matrix cdd pnorm intuitively want set e s e s work infinite instead let e sp p log e log d note kp km log e p log d e e log d appendix note k ek use lemma log log d log m prove claim random pauli measurement obey restrict isometry property rip imply strong error bound matrix recovery key technical tool bind cover number nuclear norm ball interesting question method apply problem matrix completion construct embedding lowdimensional manifold linear space slightly high dimension matrix completion compare work wainwright sample operator satisfie restrict strong convexity rsc certain set matrix manifold embedding try generalize result use rip construct metric embedding question matrix lasso compare traditional approach use maximum likelihood estimation variation basic tomography problem alternative notion sparsity sparsity known basis fully explore acknowledgement thank plan anonymous reviewer helpful suggestion work support grant number paper contribution standard technology subject reference m fazel matrix application thesis srebro learn matrix factorization thesis mit m fazel p guarantee minimum rank solution linear equation m fazel e p compress sensing robust recovery low rank matrix conference signal system computer page e cande plan tight oracle bound matrix recovery minimal number random measurement e cande exact matrix completion optimization find comput math e cande tao power relaxation nearoptimal matrix completion ieee inform theory gross recover lowrank matrix coefficient basis ieee inform theory appear arxiv simple approach matrix completion machine learn research appear m wainwright restrict strong convexity weight matrix completion optimal bound noise e cande nearoptimal signal recovery random projection universal encode strategy ieee inform theory m r sparse reconstruction fourier measurement pure apply math gross yk becker tomography compress sensing phy rev arxiv e cande plan matrix completion noise proc ieee b brown gross yk preparation s measure proportional subset bounded orthonormal system rev randomize channel short decomposition math phy m computation press estimation v koltchinskii tsybakov nuclear norm penalization optimal rate noisy low rank matrix completion recovery measurement m m talagrand probability banach space springer volume body banach space geometry cambridge r new improve embedding restrict isometry property anal r m m m p efficient measurement quantum dynamic compressive sensing rev p stability instance optimality gaussian measurement compressed sense find math
process survival analysis abstract introduce semiparametric bayesian model survival analysis model centre parametric baseline hazard use gaussian process model variation away dependence covariate oppose method survival analysis framework impose unnecessary constraint hazard rate survival function furthermore model handle leave right interval mechanism common survival analysis propose mcmc algorithm perform inference approximation scheme base random fouri feature computation fast report experimental result synthetic real datum model perform compete model cox proportional hazard random survival forest introduction survival analysis branch statistic focus study datum usually survival time type datum appear wide range application failure mechanical system time patient clinical trial duration population main objective survival analysis estimation socalled survival function hazard function random variable density function cumulative distribution function survival function s hazard s survival function probability patient time t hazard function t instant probability nature study survival analysis data contain aspect inference prediction hard important characteristic survival datum presence covariate distinctive survival datum presence censor survival time censor fully observable upper low bind instance happen clinical trial patient drop study method model type datum arguably popular estimator estimator simple nonparametric estimator survival function flexible easy compute handle censor time require knowledge nature datum handle covariate naturally prior knowledge incorporate wellknown method incorporate covariate proportional hazard model method popular useful application drawback impose strong assumption hazard curve proportional unlikely datum set vast literature bayesian nonparametric method survival analysis example include socalle prior model survival curve completely random measure r common choice dirichlet process conference neural information processing system nip process bit tractable conjugacy alternative place prior hazard function example extended gamma process weakness method natural direct way incorporate covariate extensively use survival analysis recently develop new model mix idea dirichlet process method successfully incorporate covariate impose strong constraint clear incorporate expert knowledge context model consider instance model fail proportional hazard assumption correspond aim work option describe survival model hazard covariate recently aware work use socalled accelerate failure time model dependence failure time covariate model rescale time rescaling factor model function covariate process prior model different proposal complex study work lastly wellknown method random survival forest generalisation estimator covariate fast flexible incorporate expert knowledge lack interpretation fundamental survival analysis paper introduce new semiparametric bayesian model survival analysis model able handle censor covariate approach model hazard function multiplication parametric baseline hazard nonparametric parametric model allow inclusion expert knowledge provide interpretability nonparametric allow handle covariate incorrect incomplete prior knowledge nonnegative function gaussian process r hazard function random variable t sample simulate jump poisson process intensity case intensity poisson process function gaussian process obtain gaussian main difficulty work process problem learn true intensity datum general impossible sample path gaussian process exact inference prove tractable author develop algorithm exploit nice trick allow inference sample process finite number point paper study basic property prior provide inference base sampler propose version algorithm present algorithm scale introduce random fouri feature approximate supply respective inference demonstrate performance method experimentally use synthetic real data model consider continuous random variable t r density function cumulative distribution function associate t survival function function s survival function probability patient time hazard function t instant risk patient time define gaussian process prior hazard function particular choose t t baseline hazard function centre stationary gaussian covariance function positive link function implementation choose sigmoidal function standard choice application way generate t jump poisson process intensity ie t distribution sd model data set covariate l gp t ti r sd interpret baseline hazard multiplicative nonparametric noise attractive feature expert choose particular hazard function noise incomplete incorrect prior knowledge incorporation covariate discuss later section discuss section notice gaussian variable work centre gaussian process hold t imagine model random hazard centre t multiplicative noise simple scenario constant baseline hazard t case obtain random hazard centre simply hazard function exponential random variable mean choice t t determine random function centre t correspond hazard function weibull distribution popular default distribution survival analysis addition hierarchical model include hyperparameter kernel t particular kernel common include length scale parameter overall variance finally need ensure model propose define welldefine survival function t tend infinity trivial random survival function generate gaussian process proposition prove supplemental material state suitable regularity condition prior define proper survival function proposition let gp stationary continuous gaussian process suppose sd ot assume exist t t let random survival function associate probability note proposition satisfied hazard function exponential distribution condition sd satisfied t decrease add covariate model relation time covariate kernel gaussian process prior simple way generate kernel time covariate construct kernel covariate time perform basic operation eg addition multiplication let t x denote time covariate rd pair t s construct kernel t follow kernel use experiment observe kernel establish additive relation time covariate second create interaction value covariate time complicated structure include interaction covariate consider refer work detail construction interpretation operation kernel observe new kernel produce gaussian process space time covariate real line evaluate pair time covariate new model generate ti assume covariate l gp xi ti ti e r sd construction choose kernel kj stationary square exponential stationary respect time proposition valid fix covariate ie fix covariate t pt datum scheme notice likelihood model equation deal term form exp sd expression come density jump nonhomogeneous poisson process intensity general integral analytically tractable define gaussian process numerical scheme use approximate computationally expensive follow develop datum scheme base thin poisson process allow efficiently avoid numerical method want sample time t covariate x equation use follow generative process simulate sequence point point distribute accord process intensity t assume user use wellknown parametric form sample point tractable weibull case easily start accept point probability accept set t try point gk repeat denote set reject point ie accept note sample procedure need evaluate gaussian process point instead space follow scheme sample follow proposition proposition let t gg proof sketch consider poisson process intensity t term density point exactly t second term probability point t ie e second term independent term come process point reject probability g point t accept probability t point independent poisson process result use proposition model equation reformulate follow tractable generative model l t e t t model state joint distribution pair set reject jump point thin poisson process t accept perform inference need datum ti xi receive point ti xi need sample miss datum xi proposition way proposition let t data point covariate let set reject point distribution t t distribute nonhomogeneous poisson process intensity t x interval t inference data scheme suggest follow inference algorithm datum point ti xi sample ti l sample ti number datum point observe sampling l gi ti process binary classification problem point gi represent different class variety mcmc technique use sample detail algorithm use follow notation denote ti xi set refer set reject point ti denote gi t t set reject accept point respectively point t ti denote instead remember point associate covariate set point denote t refer t denote inverse function exist t increase finally denote number iteration run algorithm pseudo code algorithm line sample set reject point survival time ti particularly line use mapping theorem tell map homogeneous poisson process inference input set time l instantiate t initial parameter poisson ti ci set ai set ni ai t ui set ai set update parameter update t hyperparameter kernel nonhomogeneous appropriate intensity observe use function inverse function provide easily computable follow line classify point draw poisson process intensity set proposition line use sample gaussian process set point value current set observe initially add censor usually survival analysis encounter type censor right left interval assume data point associate observable indicator denote type censor time censor describe algorithm describe easily handle type right presence right censor likelihood survival time relate event term reject point correspond accept location ti treat right way case sample distribution reject jump time prior ti case accept location consider set t line leave setup know survival time ti likelihood time ti treat type slightly difficult previous case event complex ask accept jump time prior ti lead large set latent variable order avoid proceed true survival time ti use truncated distribution ti proceed use instead ti sample follow sample point process current intensity point reject point repeat process imputation step repeat beginning interval know survival time lie interval ti deal interval censor way left censor survival time ti approximation scheme line need sample gaussian process set point conditional distribution line update set t line require matrix inversion scale badly massive dataset datum t generate large set g order help inference use random feature approximation idea kernel use experiment s square exponential overall variance length scale parameter j m approximation gaussian process m t bjk bjk independent sample overall variance kernel independent sample length scale parameter kernel notice gt gaussian process sum independent normally distribute random variable know m infinity kernel m approximate kernel approximation stationary kernel refer reader detail inference algorithm scheme practically small change value line easy evaluate need know value bjk matrix inversion need line need update value independent variable need matrix inversion experiment experiment perform use approximation scheme equation value m recall gaussian process use square exponential kernel overall length scale parameter set d covariate set hyperparameter associate process particular follow bayesian approach place prior length scale parameter gamma prior inverse gamma useful conjugate variance use elliptical slice sampler jointly update set coefficient bjk lengthscale parameter respect baseline hazard consider model option choose baseline hazard weibull random variable follow bayesian approach choose gamma prior uniform notice posterior distribution conjugate easily sample use step sample posterior additionally observe prior distribution constrain support reason expect size set increase respect slow computation second alternative choose baseline hazard t prior parameter posterior distribution gamma refer model weibull model exponential model esgp respectively implementation model exactly use hyperparameter describe tuning initial parameter hard use maximum likelihood estimator initial parameter model synthetic datum section present experiment synthetic datum perform experiment propose cross datum simulate n point follow density p t p t restrict datum contain sample point covariate indicate point sample pdf p p additionally data point add noisy covariate random value interval report estimation survival function weibull model figure result exponential model supplemental material clear clean datum extra noisy covariate datum estimation particular model perfectly detect cross survival function datum datum point noise effect precision estimation model point precise estimate survival curve point group centre corresponding real survival function independent noisy covariate finally remark esgp model prior hazard centre weibull exponential hazard respectively synthetic datum come figure weibull model row clean datum second row datum noise covariate column datum point group datum increase leave right dot indicate datum generate p p row interval second row curve combination distribution hard approximate true survival function datum observe model problem estimate survival function time close real datum experiment compare model use socalled index index standard measure survival analysis estimate good model rank survival time consider set survival time respective index set covariate particular context consider right censor compute cindex consider possible pair ti xi pair admissible order survival time ie impossible order problem small survival time pair censor ie ti case context admissible covariate status model predict ti way compute cindex consider number pair correctly sort model covariate number admissible pair large cindex indicate model predict patient observe covariate cindex close mean prediction model close random run experiment datum survival package consist randomized trial treatment regime lung cancer sample covariate treatment indicate type treatment patient age performance score indicator prior treatment month diagnosis contain censor time correspond right experiment run weibull model exponential model esgp proportional hazard random survival forest perform fold cross validation compute cindex fold figure report result dataset significant variable correspond performance score particular value covariate increase expect improved survival time study model achieve behaviour suggest relation hazard observable cindex observe good result proportional hazard rate datum score treatment score treatment score treatment score treatment esgp score score score score score score score score score score score score step esgp smooth esgp time time figure leave cindex middle survival curve obtain combination score treatment standard test right survival curve use score fix treatment diagnosis time age prior view score score score score score score score score score score score score score score score score score score score score score score score score score score score score score score score score score score score score time time time figure survival curve score fix treatment diagnosis time month age prior leave right random survival forest method detect difference treatment performance score figure compete model observe overall good result case observe low cindex figure overestimate survival function low score arguably survival curve proportional hazard random survival tree discussion introduce bayesian semiparametric model survival analysis model able deal censor covariate incorporate parametric expert incorporate knowledge baseline hazard time nonparametric allow model flexible future work consist create method choose initial parameter avoid sensitivity problem beginning construction kernel interpret expert desirable finally random feature approximation good approach help run algorithm large dataset sufficient dataset massive number covariate consider large number interaction covariate lead result receive funding european union seventh framework programme agreement support funding reference iain murray tractable nonparametric poisson process gaussian process intensity proceeding annual international conference machine learn page acm process regression survival datum compete risk preprint cox regression model royal statistical society series b hazard survival model random probability posterior distribution annal probability page rasmussen additive gaussian process advance neural information processing system page rl bayesian nonparametric approach reliability annal statistic page ferguson bayesian analysis nonparametric problem annal statistic page volume cambridge university press survival forest annal apply statistic page method optimize timing scan cancer patient risk recurrence analysis pooled nonparametric estimation incomplete approximate bayesian model statistic iain murray slice sample covariance hyperparameter model advance neural information processing system page iain murray elliptical slice sample aistat volume page feature neural information processing system page rao teh gaussian process modulate process advance neural information processing system page process generalization scheme annal statistic page
nonexistence universal learn recurrent neural network herbert email abstract prove loading problem recurrent neural network unsolvable extend result demonstrate training related design problem neural network npcomplete result imply impossible find formulate universal training algorithm neural network architecture determine correct set weight simple proof loading problem equivalent hilbert tenth problem know unsolvable neural network model relatively commonly accept general formal definition notion neural network result hold base formal definition try stay close original setting completeness result contrast deal simple recurrent network instead feed architecture network construct different type unit compute sum incoming signal ii unit activation node function product incoming signal depend input signal small large certain threshold parameter output unit connect link real weight connection operate note base construction general type unit usually unit furthermore replace unit wiklicky construction recurrent module simple linear threshold unit perform unary integer multiplication high order element actually need deal recurrent network behavior network simple mapping input space output space feed architecture input pattern map infinite output sequence note consider output recurrent network certain final stable output pattern return static setting main result question look difficult construct train neural network describe type actually exhibit certain desire behavior ie solve learn task investigate follow decision problem decision loading problem instance neural network architecture learning task question configuration c t realize c network configuration think certain setting weight neural network main result concern problem state unsolvable theorem exist algorithm decide learning task recurrent neural network consist ti unit architecture decision problem usual lower bind hardness relate constructive problem construct correct configuration instance trivial decide correct configuration exist corollary exist universal learn recurrent neural network proof proof construct class neural network impossible decide instance certain learning task satisfied refer hilbert tenth problem instance construct network solution loading problem lead solution original problem vice versa know hilbert tenth problem unsolvable conclude loading problem consider unsolvable tenth problem reference problem know unsolvable closely relate famous classical mathematical problem include example theorem nonexistence universal learning recurrent neural network definition diophantine equation polynomial d variable integer d t r t term form ri j m index coefficient ri e concrete problem formulate hilbert develop universal find integer solution vector e z d corresponding decision problem follow decision hilbert tenth problem instance diophantine equation question integer problem simple formulation actually short d hilbert famous problem prove unsolvable recursive computable predicate diophantine equation hold solution z exist fail network construction neural network diophantine straight forward step construction variable represent small subnetwork structure module simple left fig note connection unit module weight connection transmit signal ie w second term represent iv fig connection unit submodule represent variable correspond occurrence variable term finally output signal multiply corresponding coefficient ci sum unit submodule fundamental property network construct way simple fact behavior neural network correspond uniquely evaluation original diophantine note behavior depend weight wi variable module close look behavior submodule suppose initial moment signal value receive variable module signal reset wiklicky seed signal start circle update circle signal little bit small hand signal send central unit send signal unit long circle activation unit large threshold unit track recurrent connection count update unit drop final maximum value emit unit integer l correspondence integer large integer small equal construct appropriate weight choose interval r equivalence conclude proof demonstrate equivalence hilbert tenth problem loading problem discuss class recurrent network learn learn task consider follow map input pattern signal equal present output sequence number step nonexistence universal learning recurrent neural network constant equal note discuss consider static task final state single output network condition signal unit consider task behavior submodule trivial construct network evaluate diophantine polynomial set variable r correspond final output signal submodule uniquely weight value input network pattern s solution original diophantine equation d corresponding value accord relation weight submodule solve loading problem architecture hand know correct weight wi network correspond integer solve corresponding diophantine equation d particular possible decide correct set weight wi exist learn task decide corresponding diophantine solution e vice versa construction trivial problem equivalent conclusion demonstrate loading problem npcomplete simple feed architecture blum actually unsolvable ie training recurrent neural network problem intractable especially strong sense p related nonexistence result concern training high order neural network integer weight wiklicky wiklicky stress fact general algorithm exist high order recurrent network solve loading problem instance imply instance problem unsolvable solution exist hope relevant case mean restrict problem subclass problem thing tractable difference solvable unsolvable problem small particular know problem solve linear diophantine equation instead general computable quadratic diophantine equation problem complete general problem unsolvable know problem unsolvable consider diophantine equation maximum degree exist universal diophantine variable unsolvable think interpret negative result loading problem restriction neural network relate computational power concrete neural network construct simulate universal machine sontag et complexity problem attempt solve simply disappear intrinsic approach acknowledgement work start author artificial intelligence work support grant reference blum l blum l training network npcomplete pascal computability property lowdimensional dynamical system symposium theoretical aspect computer science page hilbert tenth problem unsolvable amer math tenth problem diophantine equation positive aspect negative solution editor mathematical development arise page computer guide theory complexity class handbook theoretical computer science volume algorithm complexity chapter page mit press cambridge network design complexity learn cambridge complexity result learn network machine learn set diophantine sontag sontag computational power neural net fifth workshop computational learning theory colt page wiklicky herbert wiklicky synthesis analysis neural network framework artificial neural network herbert wiklicky neural network loading problem conference computational learning theory page appear press theoretical analysis dynamic statistic
feature clustering accelerate parallel coordinate descent independent wa abstract regularize loss minimization problem arise application compressed sensing highdimensional supervise learning include classification regression problem algorithm implementation critical efficiently solve problem build previous work coordinate descent algorithm regularize problem introduce novel family blockgreedy coordinate descent include special case exist algorithm scd greedy cd shotgun unified convergence analysis family blockgreedy algorithm analysis suggest coordinate descent exploit parallelism feature cluster maximum inner product feature different block small theoretical convergence analysis support experimental result use datum diverse realworld application hope algorithmic approach convergence analysis provide advance field encourage researcher systematically explore design space algorithm solve regularization problem introduction consider regularize loss minimization problem design matrix weight vector estimate loss function convex differentiable function formulation include regularize square regularize logistic regression yt recent year coordinate descent efficient class problem shalevshwartz tewari motivate need solve large scale regularize problem researcher begin explore parallel algorithm instance develop shotgun recently develop generic framework express parallel coordinate descent special case include greedy cd dhillon et shotgun algorithm fact connection special case deep fundamental obvious abstraction contribution describe general randomized blockgreedy include special case blockgreedy algorithm parameter b total number feature block p size random subset block choose time step p block greedily choose parallel single feature weight update second present convergence rate analysis randomized coordinate descent algorithm general value b p number block exceed number feature p b result apply greedy cd shotgun build analysis insight previous work general convergence result particular instantiation novel base convergence rate analysis blockgreedy optimize certain block spectral radius associate design matrix parameter direct generalization similar spectral parameter appear analysis shotgun block spectral radius upper bound maximum inner product correlation feature mean feature distinct block motivate use feature cluster accelerate convergence finally conduct experimental study use simple clustering heuristic observe dramatic acceleration clustering small value regularization parameter characteristic pay particularly close attention heavily regularize problem improve future work blockgreedy coordinate descent describe generic framework parallel coordinate descent algorithm parallel coordinate descent algorithm determine specify select step accept step iteration feature choose select evaluate propose increment generate corresponding feature weight use accept step determine proposal update term consider blockgreedy algorithm input partition feature b block iteration select feature correspond set p randomly select block accept single feature block base estimate result reduction objective function pseudocode randomized blockgreedy coordinate algorithm apply function form r smooth convex r separable coordinate objective function satisfie condition greedy step choose feature block figure design space guarantee descent objective function feature blockgreedy coordinate update maximize descent quantify descent define precisely section arrive heuristic understand good think proportional absolute value jth entry gradient smooth fact r regularization heuristic exact parameter b p range b p p b set specific value exist algorithm instance p feature block set p randomly choose single coordinate update stochastic cd algorithm shalevshwartz tewari shotgun obtain p p blockgreedy coordinate descent parameter b block p b degree parallelism converge select random subset size p available block set feature select block increment accept maximal absolute value block update weight wj wj parallel parallel extreme case feature constitute single block deterministic algorithm greedy cd algorithm dhillon et finally choose nontrivial value b lie strictly p case choose update block parallel time p b arrive algorithm figure schematic representation parameterization special case convergence analysis course reason expect converge value b p section sufficient condition convergence derive convergence rate assume condition express convergence criterion shotgun algorithm term spectral radius maximal eigenvalue blockgreedy corresponding quantity bit complicated define block max m m m m set b obtain select exactly index block intuition feature different block orthogonal matrix m m close identity small m highly correlated feature block increase block assume minimize smooth separable convex function r differentiable function r satisfie second order upper bind rf t case inequality hold soon t differentiation wrt t function r separable coordinate rw function clearly separable quantity appear serve quantify guarantee descent base second order upper bind feature j update obtain solution onedimensional problem argmin note regularization simply gj denote gj brevity general case order optimality condition onedimensional optimization problem gj subgradient r imply rw wj let p choose p block suppose randomized blockgreedy coordinate run smooth separable convex function r produce iterate expect accuracy bound wk b r p constant c depend lipschitz smoothness constant function r upper bind norm kwk minimizer proof calculate expect change objective function follow shotgun analysis use denote similarly gb e p b b p p b b define b matrix m depend current iterate entry use b b b continue p t p p t t m t b abuse notation length vector component b respectively definition block m block t continue t p p gt block t b use simplify p p block note infinity norm definition follow norm block result value note second step p length w rest proof assume case convexity fact dual norm infinity norm norm inequality upper bind r p define accuracy wk translate recurrence p e br e e e p e br solve universal constant factor e p k relate argument little involve refer reader supplementary detail particular consider case block update parallel coordinate descent algorithm p b randomness algorithm yield follow corollary suppose blockgreedy coordinate p run smooth separable convex function r produce iterate block wk block feature clustering convergence analysis section need minimize block spectral radius directly find clustering minimize block computationally task block number possible partition p absence efficient search strategy space find convenient work instead term inner product feature distinct block follow proposition connection approach precise proposition let positive semidefinite sij spectral radius upper bound proof let eigenvector correspond large eigenvalue scale xi b proposition tell partition feature cluster use heuristic approach minimize maximum absolute inner product feature column design feature different block cluster heuristic p feature b block wish distribute feature evenly block attempt minimize absolute inner product feature block require approach efficient time spend clustering instead use iteration main algorithm describe simple heuristic build cluster feature construct block select feature seed assign near feature seed term absolute inner product block inner product sparse feature result large number choose step dense feature seed provide detailed description heuristic require computation inner product practice second large dataset heuristic cluster p feature b block base correlation p b b arg u cj yield large value return r euter r s m feature sample parallel source table summary input characteristic experimental setup platform experiment conduct core system bank memory processor processor core chip single core processor equip memory hierarchy follow l cache datum private core mb l cache share core core processor link memory bank independent memory controller lead total system memory globally address core interconnect use technology dataset variety dataset choose summarize table consider dataset n contain newsgroup datum gather r euter text datum describe matrix example training document feature term nonzero value matrix correspond term frequency transform use standard tfidf normalization iii r s m consist article discussion group simulate auto simulate real auto real datum gather research consider classify real simulate datum irrespective iv represent datum challenge educational datum datum represent process version training set problem provide winner input cover broad spectrum size structural property implementation current work empirical result focus coordinate descent block iteration thread step nonzero feature compute propose increment section estimate benefit choose feature complete thread wait enter line search phase remain thread update specify tolerance finally update perform concurrently use atomic maintain consistency testing compare effect clustering randomization ie feature randomly assign block variety value regularization parameter test effect clustering detail find rel rel nnz time min time min b r euter time min rel rel nnz time min r s m figure convergence result dataset regularize expect loss number use power regularization parameter result randomized feature black cluster feature red note allow run time time dataset active block iteration second nnz objective nnz objective randomize cluster randomize cluster randomize cluster table effect feature clustering r euter sparse weight let large power lead nonzero weight estimate follow consecutive power run measure regularize expect loss number interval time require clustering randomization negligible report result figure regularize expect loss number value regularization parameter black red curve indicate feature cluster feature respectively starting value datum require order yield nonzero weight upper plot color order curve correspond successively decrease value note large value result solution great regularize expect loss small number figure order curve low plot reverse upper plot overall result dataset consistent large value simple cluster heuristic result slow convergence small value considerable benefit space limitation choose single dataset explore result great detail dataset test r euter reasonably lead great concern dataset cluster feature lead slow convergence large fast convergence small r euter particularly interesting cluster feature provide initial benefit second run randomized feature block density nnz block nnz iteration regularize expect loss iteration number figure close look performance characteristic r euter table detailed summary result r euter large value row table number active block mean number block contain nonzero inactive block corresponding thread repeatedly confirm weight remain contribute convergence regularize case cluster datum result active block case block active case feature correspond nonzero weight block severely limit advantage parallel update second row randomized feature algorithm able time iteration second note work thread linear function number feature block block great number serve bottleneck middle row figure summarize state run second note test randomized feature result fast convergence large value comparison final row figure provide similar summary base instead number iteration term cluster advantageous large value figure source problem figure number nonzero feature block clearly simple heuristic result poor comparison figure b c convergence rate function number iteration conclusion present convergence result family randomized coordinate descent blockgreedy coordinate descent family include greedy shotgun stochastic cd convergence depend block maximal spectral radius result choice feature block simple clustering heuristic help small value regularization result importance consider issue distribution weight problem clear goal work development clustering heuristic relatively distribute weight problem evenly block maintain good computational acknowledgment author grateful helpful suggestion anonymous reviewer funding work provide center adaptive super compute software laboratory operate contract reference hastie tibshirani coordinate optimization annal apply statistic t coordinate descent algorithm lasso penalize regression annal apply statistic s shalevshwartz tewari stochastic method regularize loss machine learn research d parallel coordinate descent loss minimization proceeding th international conference machine learn page c tewari m scale parallel coordinate descent algorithm international conference machine learn s coordinate descent optimization minimization application compress sense greedy algorithm solve unconstrained problem inverse problem imaging s tewari near neighbor base greedy coordinate descent advance neural information processing system page rise new benchmark collection text research machine learn research modify finite newton method fast solution large scale linear svms journal machine learn research document classification datum gather feature engineering classifier ensemble appear jmlr workshop conference proceeding
automatic online tuning fast gaussian college park abstract machine learn algorithm require summation function expensive operation implement straightforwardly method propose reduce computational complexity evaluate sum include tree analysis base method achieve vary speedup depend bandwidth dimension prescribed error choice method difficult machine learning task provide combine tree method improved fast gauss transform ifgt originally propose ifgt suffer problem expansion perform low bandwidth parameter selection trivial drastically affect performance ease use address problem employ tree data structure result evaluation method performance vary base distribution source target input parameter desire accuracy bandwidth solve second problem present online tuning approach result black box method automatically choose evaluation method parameter yield good performance input datum desire accuracy bandwidth addition new ifgt parameter selection approach allow tight error bound approach choose fast method negligible additional cost superior performance comparison previous approach introduction summation occur machine learn algorithm include density estimation gaussian process regression fast particle smooth kernel base machine learn technique need solve linear system similarity matrix algorithm sum compute m ddimensional source target reference query point respectively weight associate bandwidth straightforward computation sum computationally intensive n time reduce computational complexity propose fast gauss transform use expansion expansion local expansion translation process convert yield overall complexity expensive translation operation constant term box base structure method effective high dimension method approach problem build separate tree source target point respectively recursively consider contribution node source tree node target tree recent work present new expansion control scheme yield improve result bandwidth large range optimal bandwidth determine standard leastsquare crossvalidation score efficiency bandwidth scale important case optimal bandwidth search code available download open source approach improved fast gauss transform ifgt use expansion space different original allow efficient evaluation high dimension approach achieve asymptotic computational complexity approach initially present accompany automatic parameter selection parameter interact nontrivial way author design simple parameter selection method meet error bound maximize performance attempt choose parameter report time recently present approach select parameter minimize constant term appear asymptotic complexity method guarantee error bound satisfied approach automatic work uniformly distribute source situation meet practice fact gaussian summation use simple distribution assume addition ifgt perform poorly low bandwidth number expansion term retain meet error bound address problem ifgt small bandwidth performance parameter selection employ tree data structure allow fast neighbor search greatly speed computation low bandwidth rise possible evaluation method choose base input parameter datum distribution direct evaluation direct evaluation use tree datum structure ifgt evaluation ifgt evaluation use tree datum structure denote direct directtree ifgt respectively improve parameter selection remove assumption data uniformly distribute provide method select individual source target truncation number allow tight error bound finally provide algorithm automatically select evaluation method likely fast datum bandwidth error tolerance way automatic transparent user software package test dataset include case find perform expect improve fast gauss transform briefly summarize ifgt describe detail speedup achieve employ truncate series factorization use space reduce number term need satisfy error bind ignore source contribution negligible approximation guarantee satisfy absolute error qi factorization ifgt use involve truncate xi ij p notation ij error induce truncate series exclude term degree p high bound p p p ij h h reduce distance reduce error bind source divide k cluster expansion source center cluster source belong rapid decay function contribution source ignore log cluster center radius respectively author ensure error bind meet choose truncation number pi pn source guarantee qi compute ij term prevent quadratic complexity author use bad case scenario xi ck bind h error term ij maximize small target far ry consider nonnegative integer length factorial define d xd rd target target c r r c source source direct directtree target target r c source ifgt c c source figure evaluation method target display separate source proceed follow number cluster maximum truncation number pmax cutoff radius r select assume source uniformly distribute clustering perform obtain c set source partition use max cluster radius rx truncation number pmax find satisfie worstcase error bind choose pi source xi source contribution accumulate cluster center xi influential cluster r find contribution cluster evaluate clustering step perform time use simple algorithm optimal log time use algorithm number value p d total complexity algorithm m number cluster center target point note fix p polynomial dimension exponential search cluster cutoff radius target time efficient use reduce cost log fast search tree datum structure problem apparent pointwise error bind ij bandwidth decrease error bind increase xi ck decrease increase number cluster maximum truncation number pmax increase continue satisfy desire error increase k pmax increase total cost consequently originally present perform small bandwidth source contribution great low bandwidth cutoff small number cluster increase bandwidth decrease need efficient way search cluster cutoff radius reason tree datum structure use allow efficient near neighbor search low tree data structure build cluster center influential cluster cutoff radius find log k time bandwidth low efficient simply find source point influence target perform exact evaluation source point source point radius time build structure log time perform query log n target method use evaluation gauss transform direct evaluation direct evaluation tree datum structure ifgt evaluation tree data structure cluster center figure graphical representation method running time method parameter differ greatly ie use directtree evaluation optimal result run time order magnitude large need efficient online method selection approach present section actual predict d d d d speedup radius rx number cluster bandwidth h figure select pmax use cluster radius m source dist mixture d target leave predict cluster radius actual cluster radius right speedup use actual cluster radius choose ifgt parameter mention section process choose parameter nontrivial pointwise error bound describe use automatic parameter selection scheme optimize source uniformly distribute remove uniformity assumption error bound tight select individual source target truncation number satisfy clusterwise error bound instead worstcase pointwise error bound provide significant speedup case source uniformly distribute second improvement result general long consider error contribution bad source point consider total error cluster instead number cluster maximum truncation number task select number cluster maximum truncation number pmax difficult depend indirectly source distribution example increase k decrease cluster radius allow low truncation number satisfy error bind conversely increase pmax allow cluster large radius allow small k ideally parameter low possible affect computational complexity unfortunately find balance analyze source distribution influence rate cluster radius decrease uniformity assumption lead estimate maximum cluster radius rx d interesting dataset uniformly distribute assumption violate actual rx decrease fast d lead increase running time solution perform clustering parameter selection process obtain actual cluster value use approach parameter select way tune actual distribution source advantage incremental nature cluster algorithm greedy algorithm propose phase provide approximation approximation optimal cluster respectively increment value obtain maximum cluster radius find low p satisfy error bind pick final value yield low computational note simply set maximum number cluster klimit spend n time estimate parameter practice optimal value low relative possible detect lower cost far increase lower pmax allow search terminate early addition section data distribution allow choose klimit individual truncation number clusterwise error bound maximum truncation number pmax select guarantee bad pairwise error desire error bind simply set source target truncation number pmax waste computational resource pair contribute error problem address allow source truncation number base distance cluster center assume bad placement d d d d bandwidth h figure speedup obtain use clusterwise instead pointwise truncation number m source dist mixture target d gain lower truncation large overhead cost target mean cluster compute d coefficient truncation number point propose method far decrease individual source target truncation number consider total error incur evaluation target left term rhs error truncate cluster cutoff radius right term bound error ignore cluster cutoff radius instead ensure ij pair ensure p cluster case cluster cutoff radius error incur great clusterwise error bound guarantee error great sum cluster p desire error bind low truncation number satisfy clusterwise error cluster find time evaluate clusterwise error cluster value p pmax addition find individual target point truncation number consider bad case target distance compute cluster error contribution consider target error source vary distance range cluster center yield region cluster truncation number use target region approach satisfy error bind tight reduce computational cost cluster maximum truncation number long depend point point cluster close center maximum truncation low weight source point consider error contribution source point far away weight error contribution ignore finally target use truncation number depend distance cluster automatic tune method selection input source target point distribution request absolute error option evaluate gauss transform use method direct directtree ifgt fig choose wrong method result order magnitude time evaluate sum require efficient scheme automatically choose good method online base input scheme use distribution source target point decision time avoid long computation purpose automatic method selection note know m ns pmax calculate cost method m m ns m ns log m pmax m dm m pmax m direct directtree auto time ratio cpu time second bandwidth auto good auto good auto good auto good auto good auto good auto bad auto bad auto bad auto bad auto bad auto bad bandwidth figure run time method automatic method selection source dist mixture target leave example d right ratio automatic fast method automatic slow method method selection incur small overhead prevent potentially large method selection calculate s estimate ns calculate m m calculate high compute pmax klimit minimize estimate cost ifgt calculate c estimate calculate m c pmax m c pmax end return precise equation correct constant relate cost obtain directly specific implementation method inspection automatically offline account hardware simple approach estimate distribution dependent build tree sample source point compute average number neighbor sample set target asymptotic complexity approximation directtree sublinear sampling use expense accuracy predict cost estimate time sample use technique field database management system estimate spatial selectivity ns predict cost directtree estimate klimit high value yield low cost direct directtree estimate parameter cost ifgt finally pick method low cost figure method selection approach choose correct method bandwidth low computational cost experiment performance bandwidth empirically evaluate method realworld dataset compare author report result scale datum fit unit hypercube evaluate gauss transform use k point source target bandwidth vary time optimal bandwidth method satisfy absolute error use absolute high value guarantee relative error achieve range factor include time require choose evaluate run time method relative error include time automatically select method parameter code currently available experiment use machine cpu time scale base time need naive approach corresponding machine figure normalize running time method method bandwidth method generally fast order magnitude time fast nearoptimal bandwidth approach fast comparable approach regression gaussian process regression gpr provide bayesian framework nonparametric regression computational complexity straightforward method cpu time naive cpu time cpu time naive cpu time sj bandwidth scale hh cpu time naive cpu time cpu time naive cpu time bandwidth scale hh h d bandwidth scale bandwidth scale cpu time naive cpu time cpu time naive cpu time bandwidth scale hh bandwidth scale hh figure comparison method realworld dataset lower fast undesirable large dataset core computation gpr involve solution linear system dense covariance matrix kxi method use accelerate solution gaussian process gaussian covariance training set xi new point training phase involve compute prediction t xn system solve efficiently conjugate gradient method use ifgt multiplication far accuracy matrixvector product reduce iteration proceed ie modify iteration use krylov subspace conjugate gradient iteration apply method process regression standard dataset present result training phase speed prediction phase dataset run experiment fix method direct directtree ifgt use conjugate gradient iteration fifth automatically select good method iteration denote auto figure validate solution measure relative error vector find direct method approximate method small range expect auto choose correct method dataset incur small overhead cost auto outperform fix method experiment right figure half way iteration require accuracy decrease ifgt fast direct evaluation switch method dynamically automatic selection approach outperform fix method far demonstrate usefulness online tuning approach fast particle smooth finally embed automatic method selection particle smoothing provide author data size tolerance set runtime s direct automatic ifgt choose method respectively rm error method ground truth value observe dataset download synthetic dataset generate code download accuracy size directtree auto direct method iteration number figure gpr result leave cpu time right desire accuracy iteration dataset conclusion present automatic online tuning approach summation combine tree datum structure ifgt suited high low bandwidth user treat black box approach tune ifgt parameter source distribution provide tight error bound experiment demonstrate approach outperform compete method bandwidth setting dynamically adapt dataset input parameter acknowledgment like thank government program support work work support grant reference chapman k e rasmussen gaussian process regression nip m freitas doucet fast particle smooth particle icml fast krylov method learning nip l fast gauss transform comput improve fast gauss transform efficient density estimation gray moore problem statistical learning nip gray moore nonparametric density estimation computational tractability mining d gray moore fast gauss transform nip gray fast theory experiment b density estimation statistic datum analysis chapman hal machine use improved fast gauss transform nip fast computation sum gaussian high dimension d m freitas empirical testing fast density estimation technical approximate near neighbor query fix dimension s m r optimal algorithm approximate near neighbor search fix dimension journal acm m s design implementation proceeding ieee r c automate empirical optimization software project parallel compute t cluster minimize maximum distance theoretical computer science number page optimal algorithm approximate clustering stoc seeger c spatial selectivity use power law c e rasmussen process machine learn mit theory subspace method application scientific compute technical report university
combine fully convolutional recurrent neural network biomedical image segmentation abstract segmentation image fundamental problem biomedical image analysis deep learning dl approach achieve stateoftheart segmentation performance exploit d context use neural network know method include convolution convolution plane orthogonal d image slice lstm multiple direction suffer highly anisotropic dimension common biomedical image paper propose new dl framework image segmentation base combination fully convolutional network recurrent neural network rnn responsible exploit intraslice interslice context respectively good knowledge dl framework image segmentation explicitly leverage d image use dataset neuronal structure segmentation challenge image stack fungus segmentation approach achieve promise result compare know d segmentation approach introduction biomedical image analysis fundamental problem segmentation d image identify target object neuronal structure biomedical imaging image consist highly anisotropic dimension scale depth large time biomedical image segmentation task deep learn method achieve success term accuracy outperform classic method large margin generality d segmentation know scheme broadly classify category fully convolutional network fcn unet apply image slice segmentation generate concatenate d result convolution employ replace d convolution combine d convolution hybrid network iii scheme apply d convolutional network base orthogonal plane plane perform classification d segmentation conduct recurrent neural network rnn representative rnn base scheme pyramidlstm use generalized long short term memory network exploit d context conference neural information processing system nip figure overview dl framework d segmentation key component architecture kunet bdclstm type fcn apply slice exploit intraslice context bdclstm generalize lstm network apply sequence d feature map d extract kunet extract hierarchical feature d context finally softmax function green arrow apply result slice order build segmentation probability map mainly issue know segmentation method simply link d segmentation d leverage spatial correlation second incorporate convolution incur extremely high computation cost high memory consumption long training time convolution solution reduce intensive computation convolution scheme pyramidlstm perform d convolution isotropic kernel anisotropic d image problematic especially image substantially low resolution depth instance scheme pyramidlstm perform convolution yz plane orthogonal wide line plane correspond structure different scale consequently correspond different type object correspond object interest convolution plane isotropic kernel able differentiate line hand object type rotate different appearance yz plane fact feature extract isotropic convolution yz plane suffer poor generality cause overfitte common practice biomedical image represent sequence slice recurrent neural network especially lstm effective model process inspire fact propose new framework combine component fully convolutional network extract intraslice context recurrent neural network rnn extract interslice context framework base follow idea fcn component employ new deep architecture d feature extraction aim efficiently compress intraslice information hierarchical feature compare know fcn biomedical imaging unet new fcn considerably effective deal object different scale simulate human behavior perceive multiscale information introduce generalized rnn exploit context essentially apply series convolution plane recurrent fashion interpret d context propagate contextual information key idea hierarchically intraslice context d leverage interslice correlation insight rnn distill context spirit convolutional neural network extract hierarchy context d image compare know rnn model segmentation pyramidlstm rnn model free problematic isotropic convolution anisotropic image exploit d context efficiently combine essential difference new framework know segmentation approach explicitly leverage d image efficiently construct hierarchy discriminative feature d perform systematic operation framework serve new paradigm architecture effectively exploit context solve image segmentation problem methodology schematic view dl framework fig framework combination key component fcn kunet rnn bdclstm exploit intraslice figure illustrate different way organize submodule unet kunet unet work coarser scale downsample original image unet work fine scale directly crop original image kunet propagate high level information extract unet unet unet fuse output unet downsample stream b unet fuse output unet stream c unet fuse intermediate result unet abstract layer d unet piece information unet layer architecture finally adopt kunet interslice context respectively section present kunet section introduce derivation bdclstm combine component framework conduct segmentation finally discuss training strategy fcn component kunet fcn component aim construct feature map d slice information texture shape extract information illumination imaging contrast discard rnn component concentrate interslice context key challenge fcn component multiscale issue object biomedical image specifically slice different scale shape common know variant segment biomedical image unet work perception field region d slice object large scale predefined perception field size fcn method capture high level context overall shape literature fcn propose address multiscale issue natural scene image image resize different scale feed parallel share fcn parameter mechanism share parameter suitable biomedical image object different scale different appearance require different process propose new fcn architecture simulate human expert perceive information multiple submodule employ work different image scale systematically use unet submodule fcn new architecture kunet unet choose wellknown fcn achieve huge success biomedical image segmentation unet consist downsample step follow step connection exist downsample feature map feature map refer detailed structure unet observe human expert label ground truth tend image figure target object label accurate boundary target critical mechanism kunet simulate human behavior employ sequence submodule extract information different scale sequentially coarse scale fine scale information extract submodule responsible coarser scale propagate subsequent submodule fcn feature extraction fine scale create different scale original input image series connection k maxpooling layer let image scale result t maxpoole layer original image pixel correspond t pixel original image use tth submodule process input window size unet use crop layer intuitively unet input size unet view small region high resolution view large region low resolution word t t responsible large image scale second need propagate high level information extract k submodule coarser scale work fine scale natural strategy copy result layer fig typical way achieve use final result use use final result use end use abstract information use piece information base trial study type type d achieve good performance type parameter d choose type final architecture organize sequence submodule different perspective submodule unet view super layer kunet deep deep learning model parameter exponentially increase input window size network small k sufficient handle biomedical image use experiment convolution convert number channel feature map softmax layer kunet use d segmentation problem table kunet ie sequence collaborative unet achieve performance single unet term segmentation accuracy rnn component bdclstm section review classic lstm network generalized convolutional lstm denote describe rnn component bdclstm extend finally propose deep architecture discuss advantage variant rnn neural network maintain internal status act memory ability remember allow rnn attain performance process sequential datum recently generalized lstm denote develop explicitly assume input image replace vector multiplication lstm gate convolutional operator particularly efficient exploit image sequence instance use image sequence prediction framework combine flow specifically formulate follow xz bi xz bf cz cz xz denote convolution denote elementwise product tanh logistic sigmoid hyperbolic tangent function input gate forget gate output gate bf bc bias term xz input cell activation state hidden state diagonal weight matrix govern value transition instance control forget gate value hide state input feature map size output feature map size win depend size convolution kernel use bdclstm extend convolutional lstm bdclstm key extension stack layer work opposite direction fig contextual information carry layer direction direction concatenate output interpret follow determine hide state slice d hierarchical feature contextual information direction layer integrate information z direction direction xz capture context fig b context z fuse fact pyramidlstm view different extension employ different direction sum output useful information lose output summation intuitively sum output inform simplified context instead exact situation different direction note concatenate output greatly increase memory consumption impractical pyramidlstm avoid problematic convolution yz plane discuss section bdclstm principle effective exploit interslice context deep architecture multiple stack deep structure output feature map bdclstm input bdclstm sense bdclstm view super layer deep structure simply output input insert operation maxpooling deconvolution bdclstm layer consequence deep architecture easily generalize build deep architecture bdclstm underlie relationship deep bdclstm deep deep extract hierarchy nonlinear feature d image deep layer aim interpret high level information image deep bdclstm extract hierarchy hierarchical contextual feature context deep bdclstm layer seek interpret high level d context multiple simply stack different size layer view bdclstm layer consider problem context discuss feature hierarchy form simple architecture usually convolutional layer follow subsample maxpoole order form hierarchy propose deep architecture combine maxpooling dropout deconvolution layer bdclstm layer detailed structure follow number parenthesis indicate size change feature map slice input dropout layer p hide unit kernel maxpoole dropout layer p hide unit kernel deconvolution dropout layer p convolution layer recurrent connection convolution layer recurrent connection note convolution bdclstm use kernel size indicate layer predict probability map region need region center position input evaluation stage feature map process use strategy deep bdclstm fully convolutional suppose feature map slice size h input tensor border resize h sequence patch process time result form d segmentation figure structure bdclstm layer module connect manner graphical illustration information propagation bdclstm green arrow represent recurrent connection opposite direction rotate diagram degree similar structure layer recurrent connection deep structure bdclstm use method bdclstm stack way analogous layer red arrow convolution yellow arrow indicate maxpooling deconvolution respectively rightmost blue arrow indicate convolution dropout apply input layer maxpoole layer deconvolution layer combine kunet bdclstm motivation solve segmentation combine rnn bdclstm distribute exploit d context kunet extract compress hierarchy intraslice context feature map bdclstm distill d context sequence abstract context component work follow suppose d image consist slice size kunet extract feature map size denote slice strategy adopt d image big process kunet second bdclstm work build hierarchy nonlinear feature d generate feature map denote slice z z serve context implementation finally softmax function apply generate d segmentation probability map training strategy network include kunet bdclstm train endtoend manner biomedical image big process common approach reduce range context utilize network training train kunet bdclstm separately especially useful situation effective context large compute resource memory allocate resource train component kunet bdclstm large input practice endtoend training advantage simplicity consistency decouple training strategy prefer challenge problem kunet initialize use strategy train use moment coefficient second moment coefficient e constant learning rate e training method bdclstm smooth constant initial learning rate set e half iteration e iteration training example randomly select training datum augment rotation flip mirror avoid gradient gradient clip iteration parameter bdclstm initialize random value uniformly select use weight crossentropy loss kunet bdclstm training biomedical image segmentation certain important region error reduce table experimental result d fungus fungus method pixel error conv fcn rnn rnn possible instance object tightly important correct segmentation separate boundary object error boundary importance adopt idea assign unique weight voxel loss calculation experiment framework implement torch rnn package conduct experiment use v acceleration approach evaluate d segmentation application compare stateoftheart structure evaluation dataset challenge segmentation neuronal structure electron image objective segment boundary briefly image stack voxel measure m noise section alignment error exist stack stack ground truth use training evaluation adopt metric ie rand score information theoretic score border good approximation difficulty correct segmentation error robust border variation fungus structure method evaluate dataset segmentation fungus structure d image serial scan electron ratio scale z stack slice grayscale image pixel manually label slice stack training datum use stack contain section evaluation metric quantify segmentation accuracy pixel error define euclidean distance ground label segmentation probability value range note use metric neuron dataset border applicable fungus dataset error actually adopt time challenge metric quantify pixellevel accuracy worth mention impractical label stack evaluation intensive prepare ground truth section evaluation stack ie totally section select estimate performance stack section stack segment use compute evaluation score corresponding stack report performance average score stack recall category know deep learning base segmentation method describe section select typical method category comparison unet achieve stateoftheart segmentation accuracy biomedical image select representative scheme link d segmentation d result note aware method variant fcn achieve excellent performance different unet generality different application clear test dataset low fscore unet decide unet representative method category method use d convolution classic solution avoid high computing figure region d fungus image b result use fcn component result combine fcn rnn true fungus segment cost convolution replace convolution d convolution orthogonal plane pyramidlstm know generalized lstm network segmentation result result neuron fungus dataset table evident propose kunet use achieve considerable improvement unet approach outperform know dl method utilize propose deep architecture achieve performance simply stack multiple discuss section add subsample layer rnn component able perceive high level d context worth mention evaluation dataset representative fungus datum small z resolution close resolution large z resolution resolution effectiveness framework handle leverage demonstrate mention pyramidlstm torch test dataset memory requirement pyramidlstm implement torch large original network structure large possible region process time memory capacity use hyperparameter obtain acceptable result limited processing cube result challenge board method efficient memory implement deep learning framework test machine result fig qualitatively compare result use fcn component result combine rnn fcn general method nearly false negative error rnn component help suppress false positive error maintain interslice consistency confident prediction ambiguous case leverage d context fcn collect discriminative information possible slice rnn refinement accord interslice correlation accurate segmentation conclusion future work paper introduce new deep learning framework image segmentation base combination kunet exploit d rnn bdclstm integrate contextual information evaluate different biomedical image segmentation application propose approach achieve stateoftheart performance outperform know scheme utilize context framework provide new paradigm superior performance deep architecture exploit context follow new paradigm explore different deep architecture achieve improvement conduct extensive evaluation different dataset acknowledgement research support nsf grant like thank discussion bdclstm provide fungus dataset reference schmid p integrated analysis brain plo h deep network accurate preprint h p deep contextual network neuronal segmentation artificial intelligence l m j schmidhuber deep neural network segment neuronal membrane electron microscopy image nip page r collobert torch environment machine learn nip workshop bengio adaptive learning rate deep surpass performance imagenet classification cvpr page s schmidhuber long shortterm memory neural computation d method stochastic optimization m deep learning medical image segmentation preprint seung recursive training dd convolutional network neuronal boundary prediction nip page rnn recurrent library torch long e darrell fully convolutional network semantic segmentation page r spatiotemporal video autoencoder differentiable e m deep feature learn segmentation use convolutional network page p fischer unet convolutional network biomedical image segmentation page dy convolutional lstm network machine learning approach preprint m schmidhuber parallel multidimensional application fast biomedical image segmentation nip page c sun m r collobert r l learn propose box cascade neural network preprint
stationarity stability autoregressive neural network process abstract analyze asymptotic behavior autoregressive neural network process use technique markov chain nonlinear analysis standard shortcut connection asymptotically stationary linear shortcut connection allow shortcut weight determine overall system stationary standard condition linear ar process use introduction paper consider popular class nonlinear autoregressive process drive additive noise define stochastic difference equation form noise process feedforward neural network parameter weight vector equation autoregressive neural network process order p short follow natural generalization classic linear autoregressive process comprehensive introduction autoregressive average model central question linear time theory stationarity model ie probabilistic structure series constant time asymptotically constant start equilibrium surprisingly question gain interest nn literature especially result condition stationarity model result stationarity net net use estimate conditional expectation rest paper organize follow section recall result time analysis markov chain theory define relationship time series associate markov chain section use result establish standard model shortcut connection stationary condition model shortcut connection stationary section examine nn modeling important class time integrate series proof defer appendix time series chain stationarity let t denote time series generate possibly nonlinear autoregressive process define leave equal conditional expectation good prediction t square interested long term property series ask certain feature mean variance change time remain constant time series weakly stationary let h mean covariance depend time t strong criterion distribution mean covariance process depend time case series strictly stationary strong stationarity imply weak stationarity second moment series exist detail standard time series t strictly stationary ip t e rr stationary distribution series obviously series stationary begin start stationary distribution rr start constant series asymptotically stationary converge stationary distribution e time series markov chain use notation write scalar autoregressive model order p order vector model stationarity stability neural network process et e p write p e probability point set e b step form markov chain state space b set lr p usual measure markov chain measure b essentially state space reach markov chain irrespective starting point important property markov chain speak mean infinitely repeat cycle detail geometrically ergodic exist probability measure p b p e lr p denote total variation satisfy invariance equation close relationship time series associate markov chain markov chain geometrically ergodic distribution converge time series asymptotically stationary time series start distribution ie series d strictly stationary stationarity model apply concept define section case define neural network let denote input vector consider follow standard network architecture single hide layer perceptron ax ai scalar weight weight vector bounded sigmoid function single hide layer perceptron shortcut connection c additional weight vector shortcut connection input output case define characteristic polynomial cz associate linear shortcut cz cz c radial basis function network mj center vector usual bounded radial basis function let define let let pdf positive define aperiodic basically state space markov chain ie point reach reduce depend starting point example markov chain series positive negative happen case additive noise term let define far let pdf positive network linear shortcut define geometrically ergodic asymptotically stationary network linear shortcut define additionally e c geometrically ergodic asymptotically stationary time t remain stationary allow hide layer layer perceptron mlp nonlinear output unit long overall mapping bound range mlp shortcut connection combine possibly nonstationary linear process nonlinear stationary nn use model nonlinear fluctuation linear process random walk network control overall process stationary linear shortcut connection present shortcut process stationary shortcut usual test stability linear system apply integrate model important method classic time series analysis transform nonstationary series stationary model remainder stationary process probably popular model kind autoregressive integrated average model transform stationary process simple difference let denote kth order difference operator stationarity stability neural network process eg standard random walk ft nonstationary grow variance transform iid stationary noise process ft difference time series nonstationary transform stationary series kth difference series integrate order standard mlp shortcut asymptotically stationary important care network use model stationary process course network train mimic nonstationary process finite interval prediction performance poor network inherently capture important feature process way overcome problem transform process stationary series difference integrate series train network linear operation transformation easily incorporate network choose shortcut connection weight input hide unit accordingly assume want model integrate series integration order ft stationary equivalent t ft ft p k model mlp shortcut connection define shortcut weight vector c fix n possible basically obtain add c weight input hide layer model integrate series integration order p order integration know shortcut weight fix series use input order unknown train complete network include shortcut connection implicitly estimate order integration train final model check stationarity look characteristic root polynomial define shortcut connection fractional integration consider integrated series positive integer order integration ie year model fractional integration order popular series integration order exhibit fractal behavior long memory type process introduce series paper modeling flow recently process use model traffic et financial time series datum series exhibit long memory fractional operator define tn obtain use equation series fractional remainder practical computation series course truncate term model shortcut connection approximate series p term summary model use standard architecture shortcut asymptotically stationary linear shortcut input output popular software package weight shortcut connection determine overall system stationary possible model integrate time series kind network asymptotic behavior especially important parameter estimation prediction large interval time use network generate artificial time series limit normal distribution parameter estimate guarantee stationary series recommend transform nonstationary series stationary series possible difference train network important aspect stationarity single trajectory display complete probability law process observe long trajectory process theory estimate interesting quantity process average time need true nonstationary process general quantity estimate average independent trajectory train network available sample use train network artificial noise random number generate new datum similar property training sample asymptotic stationarity guarantee behavior grow variance time currently work extension paper direction process strong mix memory process vanishe exponentially fast autocorrelation exponential rate question thorough analysis property parameter estimate weight test order integration finally want extend univariate result multivariate case special interest process acknowledgement piece research support science foundation grant adaptive information system modeling economic management science stationarity stability neural network process appendix mathematical proof proof easily support probability density function pdf t real line ie pdf positive tong case hypercube reach p step positive probability set necessary sufficient condition aperiodic exist set positive integer e p case true unbounded additive noise proof use following result nonlinear let define let compact preserve compact set bounded range continuous homogeneous origin fix point uniform asymptotically stable pdf t positive ir geometrically ergodic noise process t condition assumption clearly network continuous compact function standard mlp shortcut connection bounded range d series asymptotically stationary allow linear shortcut connection input output cx linear shortcut network standard mlp shortcut connection clearly continuous homogeneous origin fix point series asymptotically stationary asymptotically stable ie characteristic root magnitude unity obviously true shortcut connection note model reduce standard linear model reference p time series theory method springer h use deterministic lyapunov function stochastic difference equation advance apply probability s gradient radial basis function network nonlinear nonstationary time prediction ieee transaction neural network predict conditional probability density stationary time nonlinear autoregressive process proceeding e m s d nature traffic extend version transaction network fractional brownian motion fractional noise application nonlinear time series dynamical system approach asymptotic stationarity discretetime network network
function bethe free energy loopy belief propagation statistical mathematic statistical mathematic abstract propose new approach analysis loopy belief propagation establish formula connect hessian bethe free energy edge function formula number theoretical implication apply sufficient condition hessian bethe positive definite graph multiple cycle formula clarify relation local stability fix point local minima bethe free energy propose new approach uniqueness lbp fix point condition uniqueness introduction pearl belief propagation provide efficient method exact computation inference probabilistic model associate tree extension general graph allow cycle loopy belief propagation propose successful performance problem computer vision error code interesting theoretical aspect connection bethe free energy know example fix point correspond stationary point energy nonetheless property lbp convergence stability unclear theoretical understanding need paper theoretically analyze establish formula assert determinant hessian bethe free energy equal reciprocal edge function positive factor formula derive variety result property lbp stability uniqueness function direct link dynamic application formula condition positive definiteness hessian bethe free energy bethe free energy necessarily convex cause behavior oscillation multiple fix point clarify region hessian positive definite importance problem previous approach consider global structure bethe free energy focus local structure provide simple sufficient condition determine positive definite region correlation coefficient pseudomarginal small value characteristic graph hessian positive definite additionally hessian negative eigenvalue boundary domain graph cycle second clarify relation local stability lbp fix point local structure bethe free energy relation necessarily obvious descent bethe free energy line study locally stable fix point local minimum bethe free energy interesting ask local minima bethe free energy stable unstable fix point lbp answer question condition local stability positive definiteness free energy term eigenvalue matrix appear finally discuss uniqueness lbp fix point develop differential topological result bethe free energy result determinant hessian fix point appear formula function satisfy strong constraint consequence addition known result case lbp fix point unique connect graph cycle restrict strength interaction loopy belief propagation bethe free energy paper e connected undirected graph vertex e edge cardinality v e denote m respectively article focus binary variable suppose probability distribution set variable xi follow factorization form respect xi normalization constant ij positive function ij loss generality application computation marginal distribution px pij px require exact computation intractable large graph graph tree efficiently compute pearl belief propagation graph cycle empirically know direct application loopy belief propagation good approximation message pass algorithm direct edge message vector ij assign initialize arbitrarily update rule message xi ij neighborhood order edge update arbitrary paper consider parallel update edge update simultaneously message converge fix point approximation pij calculate belief ij xi normalization constraint automatically satisfied introduce bethe free energy tractable approximation gibbs free energy exact distribution characterize variational problem p minimum probability distribution p gibb free gibb energy define p kl pp log pp p log kullbackleibler divergence p p note p convex function p bethe approximation confine minimization distribution form ije iv bi degree constraint satisfied set satisfy constraint pseudomarginal computational tractability modify gibb free energy objective function bethe free energy log log ij xi b iv xi log ije iv xi log bi xi domain objective function set pseudomarginal function necessarily unique minimum outcome modify variational problem precisely onetoone correspondence set stationary point bethe free energy set fix point convenient work minimal parameter mean xi correlation effective parametrization pseudomarginal mj mi bethe rewrite jij m ije mj x x ije iv j log domain f write m mj e xi hessian consist second derivative respect square matrix size m denote consider function note depend jij function hessian function formula undirected edge g pair direct edge form set e m direct edge e e origin e direct edge e v e e e inverse edge denote e correspond undirected edge e e e close geodesic sequence e direct edge ei ei closed geodesic equivalent obtain cyclic permutation equivalent class closed geodesic prime cycle repeat concatenation short close geodesic let p set prime cycle weight edge function define gp gp p e c assume sufficiently small convergence analogue function represent product prime number example tree prime cycle cycle graph prime cycle e e e cn u type graph number prime cycle e know edge function follow simple determinant formula set function direct edge analytical continuation cm let determine graph define matrix e e u m diagonal matrix define need determinant formula edge function use proof leave proof supplementary theorem version formula let set function define linear operator e d edge function function set u e denote single variable case theorem reduce formula degree matrix adjacency matrix define main formula theorem main formula follow equality hold point det m bi mi mj mj proof detail computation supplementary material easy hessian diagonal matrix mj mj mj ij use diagonal block hessian word choose square matrix computation supplementary material ij m mj mi define easy check diagonal matrix define mi rhs left equality use determinant hessian bethe free energy essentially equal reciprocal edge function matrix direct connection section formula derive consequence rest paper application positive definiteness condition convexity bethe free energy important issue guarantee uniqueness fix point derive sufficient condition convexity free energy convex tree graph cycle section instead global structure focus local structure bethe free energy application main formula square matrix c denote set eigenvalue spectral radius matrix ie maximum eigenvalue let m matrix mi define r positive definite matrix mi proof define t mi t t define t way ij t t det hold interval use mi check positive definite eigenvalue t real continuous respect t eigenvalue positive real define mi mj ij ij ij final expression ij define diagonal matrix z bee e respectively z follow corollary explicit condition region hessian positive definite term correlation coefficient pseudomarginal corollary let frobenius eigenvalue m define l g hessian positive definite e proof e bm m theorem r distance origin near pole example tree g cycle graph respectively case strictly function hold reproduce result general use useful corollary let t t t t m n m g t number span tree particular convex connect graph linearly independent cycle m proof equation obtain theorem function supplementary material detail m right hand negative approach determinant hessian diverge hessian positive definite point summarize result section conclude convex tree graph cycle good knowledge proof fact application stability analysis section discuss local stability local structure bethe lbp fix point locally stable fix point sufficiently local minima bethe free energy converse necessarily true general gap property regard lbp update dynamical system model binary message parametrize parameter state express update rule identify transform t new t set fix point fix point locally stable start point sufficiently close converge local stability determine t fix point discuss locally stable c suppress oscillatory behavior lbp update t t useful damp strength identity fix point locally stable representation linearization derivative update choose good coordinate follow section transform ij function bij bi bj bi ij message fix point change representation message function affect essentially transformation cause t p t p invertible matrix p use transformation following fact hold supplementary material proposition let lbp fix point derivative t similar p t p invertible matrix p formula imply direct link linearization t local structure bethe free energy fix point local minimum bethe free energy c r clear condition positive definiteness local stability damp lbp local stability term set eigenvalue r respectively locally stable fix point sufficiently local minimum bethe free energy include c r reproduce result gap locally stable fix point local minima bethe free energy include r fix point local minimum bethe locally stable fix point damp interesting ask condition local minimum bethe free energy stable fix point lbp know complete answer attractive model define follow imply stable fix point unstable change jij correspond local minimum disappear let consider continuously parametrize attractive model temperature ij t jij t t run find stable fix point continuously change t lbp fix point unstable t correspond local minimum bethe saddle point t proof xi mi lbp fix point unstable eigenvalue mean cross theorem det positive negative extend theorem discuss case vanish local field trivial fix point mi application uniqueness lbp fix point uniqueness lbp fix point concern study property guarantee find global minimum bethe free energy converge major approach uniqueness consider equivalent minimax problem contraction property dynamic use theory gibb measure propose different differential topological approach problem approach combination follow basic theorem q q q q index note set stationary point bethe free energy coincide fix point lbp assert sum index fix point consequence number fix point odd note index local quantity express global structure function proof theorem prepare lemma proof supplementary material standard result differential topology refer theorem comment p proof sequence converge point q boundary m let m compact connected manifold boundary assume dimension m m let m m smooth map satisfy m regular value p m q p define degree map p depend choice regular value p m sketch proof define map rn m note depend j prove q unique element positive definite right equal define sequence manifold increasingly converge satisfy sufficiently large b k closed ball origin let rn m k smooth map identity monotonically increase k x k obtain map apply yield guarantee index fix point advance run lbp conclude fix point unique follow priori information let ij fix point hold proof xi straightforward computation obtain bound attain immediately obtain uniqueness condition strong property prove condition figure graph example figure graph figure type corollary m fix point unique diagonal matrix define proof ij m theorem imply index lbp fix point proof corollary use bound following case corollary utilize information sign state corollary need terminology interaction jij jij equivalent exist jij si sj equivalent model obtain transformation uniqueness property equivalent model unchanged corollary number linearly independent cycle interaction equivalent attractive model lbp fix point unique proof supplementary material example illustrate outline example let e interaction arbitrary j figure check arbitrary prime figure cycle correspond e e b m e e e e e e e e e e e e e e e e e e case graph figure similarly supplementary material reduce graph attractive model fix point necessarily unique graph multiple cycle exist result uniqueness assumption upperbound jij essentially contrast corollary apply arbitrary strength interaction graph cycle interaction attractive corollary bethe free energy nonconvex situation corollary fix point unique remark binary pairwise model connection edge function free proof version formula theorem essential initial paper find extend general class model include multinomial model gaussian model represent arbitrary factor discuss extended formula application future paper recent research suggest importance function context important application connection edge function lbp product formula partition function directly relate work pursue cover connection interesting future research topic acknowledgement work support scientific research c reference pearl probabilistic reasoning intelligent system network plausible belief propagation approximate empirical study proc uncertainty weiss generalized belief propagation information processing system weiss correctness local probability propagation graphical model loop neural computation t uniqueness loopy belief propagation fix point neural computation t stable fix point loopy belief propagation neural information processing system page function finite graph representation group form geometry arithmetic variety aa function finite graph covering advance mathematic discrete subgroup linear group mathematical society function tree lattice p belief propagation statistical physics conference information science system press finite graph property bethe approximation loopy belief propagation binary network journal statistical mechanic theory experiment p t amari information geometry lowdensity code ieee transaction information theory c jm belief propagation bethe approximation traffic prediction preprint physics willsky loopy belief propagation convergence effect message error journal machine learn research j sufficient condition convergence transaction information theory belief propagation gibb measure uncertainty ai sp modern geometry method application geometry topology springerverlag r cycle code ieee information theory workshop page r characterization lowdensity code advance mathematic m representation correction propagation proceeding th international conference learn
multiscale adaptive network model motion computation primate science computation neural abstract demonstrate multiscale adaptive network model computation primate area model consist stage l local velocity measure multiple spatiotemporal channel optical flow field compute network neuron multiple spatial resolution model embed computational efficiency algorithm parallel network adaptively compute reliable estimate flow field different spatial scale model neuron nonclassical receptive field property type neuron local velocity measure multiple channel channel provide measurement network incorporate scheme conflict resolution mechanism provide novel explanation spatial frequency dependency psychophysical phenomenon motion capture motivation previously develop model motion computation visual system primate le pathway v algorithm deficiency issue optimal spatial scale velocity measurement issue optimal spatial scale smoothness motion field address deficiency implement multiscale network base algorithm method estimate optical flow basic assumption scale velocity relative spatial neighborhood temporal discretization step delay velocity pattern large ratio spatial temporal sampling step incorrect velocity value obtain propose strategy adaptively optimal discretization grid evaluate local estimate relative error flow field discretization optimal spatial grid minimize error strategy lead superior estimate optical flow field achieve speedup associate method important large number iteration need algorithm remarkable speed human reliably estimate velocity order neuronal time constant previous model base standard regularization approach involve smooth weight parameter control smoothness compute motion field scale velocity field smooth depend size object large object large value real life vision system deal object size simultaneously exist optimal smoothness parameter network architecture allow circumvent problem smoothing weight different resolution grid network architecture overall architecture model figure stage local velocity measure multiple spatial resolution spatial resolution p local velocity represent set neuron preferred direction direction component cell second stage optical flow field compute network neuron pattern cell multiple spatial resolution vijkp following briefly summarize network use multiresolution population code p number direction grid number resolution network linear interpolation operator single resolution model input source pattern cell l t u l unit vector direction local velocity local edge strength multiscale network use convergent source term pattern cell vijkp p p p p r d restriction operator use weighting operator instead operator sparse nature input datum computational efficiency algorithm embed multiresolution network set synapsis si write multiscale adaptive network model motion computation p motion field iid normal velocity measurement figure network architecture figure scheme constant discuss section scale velocity field smooth depend size object consider example object certain size velocity field view multiresolution representation spatial frequency filtering connection force velocity field represent neuron resolution grid match size object smoothness constraint enforce individual resolution grid membrane potential use source smoothness term s resolution grid p write l vijkp vijkp vijkp vijkp k smoothness parameter smoothing weight formulation grid independent object size network equation multiresolution network architecture considerably complicated synaptic connection pattern neuron compare single resolution model convergence improve order magnitude measure number iteration need conflict velocity estimate motion depend spatial ax temporal discretization step use et derive follow expression relative error velocity incorrect derivative estimation r m velocity spatial frequency pattern velocity deviate velocity measurement accurate scaling factor depend spatial filtering choice spatial discretization spatial filter satisfy requirement sample velocity measurement accuracy derive base gradient model believe similar constraint apply correlation model model receptive field profile primate retinal ganglion cell log operator require accuracy velocity measurement standard deviation great equal happen velocity measurement scale inconsistent result consider example object speed multiscale adaptive network model motion computation figure channel p p correct measurement reliable range channel depict fill circle fine channel hand erroneous reading suggest scheme conflict resolution incorporate strategy network architecture implement term way erroneous input signal component cell grid open circle figure component cell fill circle motion capture human visual system deal potential conflict spatial channel evidence use conflict resolution scheme believe wellknown psychophysical phenomenon motion capture strategy human subject present sequence randomly random dot pattern perceive random motion find surprisingly perception greatly influence movement superimpose low contrast low spatial frequency find human subject tendency perceive random dot spatial random dot spatial frequency percentage capture high phase shift frame surprisingly low spatial frequency high percentage capture researcher attempt explain phenomenon base smoothness constraint velocity field smoothness explain dependency spatial frequency phase shift scheme provide natural explanation dependency simulate spatial frequency phase shift dependency result figure simulation plot relative uniformity velocity field uniformity total capture clearly spatial frequency effect capture increase phase shift phase shift effect capture increase spatial frequency low low spatial frequency effective coarser channel fine component cell effectively clear receptive field relationship figure nonclassical receptive field traditionally use isolate bar map classical receptive field neuron portion visual field directly stimulate recently evidence visual neuron stimuli present crf strongly selectively influence neural response stimulus present crf nonclassical receptive field find true receptive field neuron area extend crf surround commonly directional influence spatial phase degree figure spatial frequency dependency motion capture ino e l c q l z center dot background stationary direction movement center dot neuron center dot optimum direction vary direction movement background dot figure simulation type nonclassical receptive field property multiscale adaptive network model motion computation response crf base surround neuron classify type model neuron type nonclassical receptive field selectivity type neuron perform series simulation similar original experiment crf model determine optimal motion stimulus present crf surround direction motion surround effect activity cell monitor effect surround motion cell function direction surround motion plot figure b surround similar direction center neuron activity cell totally suppress hand surround opposite center cell activity enhance superimpose figure similar plot conclusion conclusion develop multichannel multiresolution network model motion computation primate model neuron similar nonclassical surround property type cell propose novel explanation motion capture phenomenon base strategy conflict input channel acknowledgement acknowledge nsf support research reference e direction response classical receptive field middle temporal visual area perception r compute optical flow multiple scale adaptive approach appear computer guide development method wt analysis visual pattern pattern recognition mechanism gross cg sm threshold coherent apparent motion random vision nm computational theory perception coherent visual motion nature c compute optical flow primate visual system neural computation
iterative scale trustregion learn krylov subspace pearlmutter implicit sparse multiply computer science mathematic computer abstract online incremental gradient backpropagation widely consider fast method solve learning problem contrast appropriately implement iterative batchmode learning method fast example time fast uci letter problem output datum item parameter time fast nonlinear regression problem arise color prediction output datum item parameter modular network principal algorithm follow use scale trustregion regularization iteration solve associate nonlinear square problem inner iteration perform truncated method second employ implicit sparse construct krylov subspace use solve truncate update exploit sparsity precondition matrix result nn output introduction objective function minimize optimize ndimensional sum d data vector f output r rk mf d r e r m dimensional residual vector compose m residual element ri m rk ddimensional residual vector evaluate vector hessian matrix e r h respectively residual jacobian matrix r readily process matrix term r m r ri nonlinear square algorithm information cross product fisher information matrix e learn important portion h weak weak residual small learn progress multiple f output nonlinear model fullyconnecte nn know m block angular matrix form reference instance consider layer mlp input hidden output node terminal parameter include threshold parameter direct connection nod direct connection rest parameter directly connect terminal node nb hide parameter word model parameter nb total separate b vector kth subset terminal parameter directly link associate residual jacobian matrix form leave hessian matrix sparse block arrow form right denote nonzero block b b bf ak bk d nb jacobian matrix respectively ddimensional residual vector rk evaluate notice diagonal block terminal parameter exclude rk vertical bk block correspond nb hide parameter contribute minimize residual rk evaluate terminal node pose problem m hold addition terminal nod linear identity function terminal parameter linear block identical output include constant output row small problem direct batchmode learning suitable direct matrix factorization attention pay exploit obvious sparsity h render algorithm memory operation count notice dense nice sparsity structure problem subspace method circumvent need perform direct matrix factorization employ realize iterative batchmode learning row column matrix bk need explicitly pearlmutter method automatically exploit sparsity perform sparse product construct subspace parameter optimization describe follow numerical evidence iterative scale trustregion method practical newton method enjoy global convergence property cauchy steepest descent method fast local convergence newton method outer iteration process trustregion method consider convex combination cauchy step cauchy step use scalar parameter know step step yield good approximate solution socalle scale norm m norm multiplier min q subject r min m r distance measure norm mx symmetric positive matrix m r trustregion radius trustregion size local quadratic model e gt radius r control accord q predict behavior e check error reduction ratio actual error reduction predict error reduction e detail refer pose constrain quadratic minimization solve multiplier solution pose problem formula h m g m r h m positive nonlinear square context nonnegative scalar parameter know parameter m trustregion step step g increase ie r decrease close cauchy step cauchy cauchy gt m m r cauchy m trustregion step reduce restrict cauchy step r cauchy m cauchy cauchy m r step intermediate cauchy scalar h h positive root r def def cauchy p cauchy way trial step subject trustregion problem solution sequence generate iteratively seek trial step inner iteration process parameter sequence consecutive element denote produce outer iteration batch mode outer iterative process update parameter movement step satisfactory r decrease realize important concept fail step direction simultaneously purpose trustregion method compute gradient vector batch mode large data block ie block mode demonstration section inner iteration process truncate precondition linear cg employ conjugate gradient krylov subspace method section chapter symmetric positive preconditioner m solve m norm truncated know cg applicable nonconvex problem solve formula inner iterative process pp base page inner iteration process precondition set solve m compute set z proceed step matrixvector product hdk curvature check hdk continue step compute hdk m r terminate hdk step size approximate solution dk m r step terminate r k m residual g k small k terminate m compute factor search direction klimit set return step terminate k step k p dk plug likewise place eq step use eq hdk hdk m r computation identical r cauchy m expensive tend bias direction process terminate ie stop inner iteration condition hold hdk k m r c h k d condition d step likely meet prior knowledge limit klimit inner iteration usually klimit long hold work properly hit trustregion boundary condition b step norm residual small condition c step xe condition step hold local model strictly positive direction negative curvature typical exploitation curvature set equal step trustregion boundary curvature segment step model minimizer trust region way terminate kth cg step yield approximate solution trustregion belong krylov space span m m m g m m g result application cg multiply m symmetric formula m m m g m h system m m unlikely symmetric page m diagonal matrix m overall memory requirement ve implement matrixvector product hdk step dominant operation cost entire process employ pearlmutter method h explicitly require understand method rst describe straightforward implicit sparse evaluate form matrixvector product exploit work block ak bk manner implicit ie sparse step f output model inner iteration start p d sweep d training datum forward pass compute nal output terminal b backward pass obtain pth row ak pth row bk compute scalar add corresponding element end end p step cost detail step b cost nb nb step c cost overall cost linear note sparsity ignore cost quadratic df nb extract explicitly pair row vector storage datum easy apply numerical linear algebra approach precondition reduce number inner iteration row vector need explicitly pearlmutter method calculate k step forward pass eq page easy simplify backward pass eq page eliminate term involve residual r node function multiply vector ak bk scalar k implicitly method pearlmutter run time dn df pearlmutter method time fast furthermore pearlmutter original method matrix m d m row vector notably method automatically exploit sparsity eq right essentially way standard deal sparsity leave perform product r experiment discussion simulation compare follow ve ie xe momentum pearlmutter method obtain preconditioner m j pearlmutter method algorithm d hessian test speed comparison purpose work probably fast d e employ obtain diagonal preconditioner m preconditioner b m apply performance comparison nonlinear regression task benchmark letter recognition problem machine learn repository experiment conduct pc optimization rst regression task realworld application color prediction problem determine mix proportion available reproduce target color require mapping input spectral signal target color output proportion use training datum m test datum table result average trial single mlp optimize train application requirement convergence relatively early stop occur clearly pose regression task nontrivial roughly day average trial nearly time slow fast algorithm d generalization performance pose algorithm equivalent total time stop test rmse single mlp d e mixed d observe use hessian matrix help reduce inner iteration epoch total convergence time turn great obtain algorithm presumably suitable hessian inner iterative process terminate detection curvature extra chance increase total epoch help reduce time remarkably time inner iteration d owe pearlmutter method fact preconditioner m merely need time inner iteration need nearly time measure rate code lie roughly range typically peak machine speed improve performance employ layer hidden node large hide layer letter problem increase nb reduce render slow alternatively introduce direct connection input terminal output layer increase column size retain nice parameter approach applicable use operation count measure use performance application programming interface mixture model modular network residual combine mlp associate scalar minimize ith output integrate unit ith normalize mix proportion assign output vector note expert learn residual desire output committee method sense nal combine output come close desire strong coupling page expert crucial consider global hessian expert optimize simultaneously correspond j form leave bk bk residual jacobian portion parameter integrate unit omit merely type method owe design avoid local expert spectral signal input convert angle input integrate unit consist ve basis function partition fuzzy fashion ve color region red yellow green blue ve receive spectral signal input localize model learn fast learn table particular model d work time fast hour time fast single mlp complementary pearlmutter method readily applicable rw yield second letter benchmark problem involve input feature output alphabet training datum d m test datum use mlp set dierent initial parameter randomly generate uniformly range implement learning batch mode split training datum set data block data block employ compute eq evaluation e involve d training datum notice learn scheme update model parameter twice epoch update datum time epoch observe possible redundancy datum set appear help reduce number inner iteration speed iterative batchmode learning use precondition table average performance trial good testset performance obtain learning mode average result total time stop inner error committee error mode block mode block batch mode batch average algorithm mode work time fast work fast batchmode report work fast test committee method merely combine output mlp optimize independently experiment committee error average error expect learn scheme introduce small bias improve performance mode yield good test error rate simple committee method conclusion future direction pearlmutter method construct krylov subspace implement iterative batch learning simulation example simple version pearlmutter method work interest investigate reallife largescale problem nd strength base method e elaborate preconditioner reduce total time dramatically need deal balance act simple committee method worth examine algorithm implement statistical learning method boost conjunction appropriate numerical linear technique goal attack practical largescale problem reference amari natural gradient work learn neural computation pp r m p l trustregion method apply numerical e d m r e adaptive nonlinear leastsquare acm tran mathematical software r m nowlan adaptive mixture local expert neural computation vol trustregion regularize nonlinear square algorithm learning neural network science vol separable nonlinear square algorithm modular network learn proceeding joint neural network vol available e complexity analysis supervised algorithmic comparison proceeding neural network vol j compute trust region step pp conjugate gradient method trust region large scale optimization anal pp vol pearlmutter fast exact multiplication hessian neural computation vol bengio boost neural network neural computation vol element statistical learn correct
combine graph laplacian semisupervise learn mark herbster computer problem semisupervised learning construction graph underlie datum propose use method optimally combine number differently construct graph graph associate basic graph kernel compute optimal combine kernel kernel solve extended regularization problem require joint minimization datum set graph kernel present encourage result different ocr task optimal combine kernel compute construct variety distance function near neighbor introduction semisupervise learning receive significant attention machine learning recent year example reference define insight semisupervise method unlabeled datum use improve performance learner supervised task key semisupervise learning method build assumption datum low dimensional manifold ambient space datum manifold approximate weighted discrete graph vertex identify empirical label unlabeled data graph construction consist stage selection distance function application determine graph edge weight thereof example paper consider distance image base euclidean distance distance combine image transformation relate tangent distance determine edge set graph neighbor common choice weight edge decrease function distance d surplus unlabeled datum improve quality empirical approximation manifold graph lead improved performance practical experience method indicate performance significantly depend graph construct model selection problem consider selection distance function parameter use graph building process describe diversity method propose graph construction paper select single graph propose combine number solution implement method base regularization build work dataset combination distance function edge set specification distance lead specific graph graph associate kernel apply regularization select good convex combination kernel function trade fit datum norm unique regularization minimization single kernel space space correspond combination kernel datum label vertex conserve training crossvalidation option number label vertex class small figure section illustrate algorithm simple example different distance image digit depict euclidean distance distance invariant small center image rotation distance invariant rotation clearly distance problematic similar performance graph learn discuss section distance report plot expect performance low case distance use paper construct follow section discuss regularization apply single graph review regularization context reproduce hilbert space section section specialize discussion space function define graph review normalize laplacian graph kernel pseudoinverse graph section detail algorithm learn optimal convex combination laplacian kernel finally section present experiment usp dataset algorithm train different class laplacian background graph regularization section review graph perspective reproduce space reproduce hilbert space let set kernel function reproduce rkh function ir reproduce kernel property hold inner product particular tell kt imply matrix symmetric positive semidefinite set input ti p use notation regularization rkh learn function basis available inputoutput example solve variational problem e min ir loss function positive parameter solution problem form real vector coefficient c example denote vector find replace right hand equation equation optimize respect c practical situation convenient compute c solve dual problem e c yi ci c ir min kxi xj ij function ir ir conjugate loss function define z ir z ir example discussion choice loss function v lead different learning method prominent square loss regularization support vector machine example graph regularization let undirected graph m vertex m m adjacency matrix aij edge connect vertex graph laplacian m m matrix define l d m degree vertex identify linear space realvalued function define graph irm introduce product u irm induce hv vi irm pm v constant vector verify note aij recall connect component r eigenvector eigenvalue eigenvector piecewise constant connected component graph particular connect constant vector eigenvector eigenvalue let ui m system l eigenvalue order define linear subspace irm orthogonal eigenvector eigenvalue framework wish learn function basis set label vertex loss generality assume vertex label let corresponding label follow prescribe loss function v compute function solve optimization problem note similar approach present v essentially obtain minimal norm label vertex functional balance error label point smoothness term measure complexity v graph note term contain information label unlabeled vertex graph idea discuss naturally extend weight graph method special case problem restriction norm pseudoinverse laplacian l reproduce kernel example proof mean hold reproduce kernel property vi column l set vi ij note analysis naturally extend case replace positive semidefinite matrix particular experiment use normalize laplacian matrix ld typically problem solve optimize particular square loss minimal norm interpolation require solve square linear system m m equation respectively contrary paper use theorem express v ij cj approach advantageous l compute offline typically m advantage approach multiple problem solve laplacian coefficient ci obtain solve problem example square loss regularization computation ci involve solve linear system equation e learn convex combination laplacian kernel describe framework learn multiple graph laplacian assume gq inn vertex correspond laplacian lq kernel q lq hilbert space norm lq v propose learn optimal convex combination graph kernel solve optimization problem min pn define set q q pn k q problem motivate observe inn optimal convex combination kernel small right hand individual kernel motivate expectation improve performance furthermore large value component identify relevant problem special case problem jointly minimize functional hull kernel prescribed set problem discuss detail case finite consider practical experience method indicate enhance performance learning computationally efficient solve solve problem important require normalization condition trace norm discussion initialization choose q q inn t compute ct solution problem find q inn ct q t ct q exist terminate compute p argmin e q pk t p set q pk t figure algorithm compute optimal convex combination kernel set q q inn use dual problem formulation discuss equation inner minimum rewrite problem e max min ci c ir variational problem express optimal convex combination kernel solution saddle point problem problem simple solve original problem objective function linear discussion ir adapt algorithm use compute saddle point algorithm alternately optimize c compute minimizer report figure note problem particular square loss regularization require e solve equation k experiment section present experiment optical character recognition observe follow optimal convex combination kernel compute competitive good base kernel second observe weight combination distinguish strong weak candidate kernel proceed discuss detail experimental design interleave result use usp dataset image handwritten digit pixel value range present result pairwise classification task vary difficulty odd digit classification pairwise classification training set consist image digit usp training set number label point choose equal number digit odd digit classification training set consist image digit usp training set number label point equal number digit performance average random selection number label point experiment construct gq q inn combine neighbor different distance corresponding laplacian compute associate kernel choose loss function square loss kernel obtain different type graph vary widely necessary renormalize choose normalize kernel available euclidean kernel kernel tangent dist kernel kernel task label label odd table misclassification error percentage standard deviation good combination kernel different handwritten digit recognition task use different distance text description training process frobenius norm submatrix correspond label datum observe similar result obtain normalize trace submatrix regularization parameter set algorithm minimization start kernel algorithm figure use average kernel maximum number iteration t table result obtain use distance combine distance euclidean distance image second method transformation distance image small distance pair transform image determine apply number affine transformation transformation information distance tangent distance describe firstorder approximation transformation column table euclidean use column image transformation distance use column tangent distance use finally column method jointly compare result indicate combine different type kernel tend select effective case tangent distance kernel degree transformation distance kernel work routine use note method performance combination comparable good kernel figure report weight individual kernel learn label use pairwise task label use odd exception easy task large weight associate build tangent distance effectiveness algorithm select good demonstrate figure euclidean transformation kernel combine kernel kernel induce consider distance invariant rotation range image easily small distance image t image t image obtain rotate degree set t tk distance approximate use matlab constrain minimization function figure distance matrix set label unlabeled datum euclidean transformation distance respectively good error different value k method error learn convex combination total learn weight method plot clear solution algorithm dominate good kernel influence low performance result error combination comparable euclidean transformation method final experiment figure demonstrate unlabeled datum improve performance method figure kernel weight euclidean transformation middle tangent text information euclidean transformation distance error p error p combination error error p figure similarity matrix corresponding learn coefficient convex combination task text description conclusion present method compute optimal kernel framework regularization graph method consist minimax problem efficiently solve use algorithm test optical character recognition task method exhibit competitive performance able select good graph structure future work focus extension algorithm continuous optimization version particular consider continuous family correspond different weight matrix study graph combination class figure misclassification error number training point odd classification number label point left right reference m learn combination continuously parameterize basic kernel learn theory m regularization semisupervise learn large theory colt m p semisupervise learning manifold mach learn blum s learn label unlabeled datum use graph proc learn theory theory regional conference series hastie p simard model metric handwritten character recognition statistical science m herbster m online learn learn t joachim learn spectral graph partition proc int machine learning icml ri j lafferty diffusion kernel graph discrete input space machine learn cristianini p m learn programming machine learn research hh component selection smooth smooth spline analysis variance model series m learn function learn advance neural information processing system becker ri kernel regularization graph theory vapnik statistical learning theory scholkopf learn local global consistency advance neural information processing system thrun mit cambridge lafferty semisupervise learning use gaussian field harmonic function machine learn lafferty nonparametric transform graph kernel semisupervise learn advance neural information processing system
robust regression twin gaussian process abstract propose gaussian process gp framework robust inference gp prior mix weight twocomponent noise model augment standard process latent function value approach generalization mixture likelihood use traditional robust gp regression gp mixture model suggest tresp rasmussen ghahramani value restriction tractable expectation propagation update allow fast inference model selection convergence standard mixture additional benefit method lie ability incorporate knowledge noise domain influence prediction recover predictive distribution information outlier distribution process model asymptotic complexity equal conventional robust method yield confident prediction benchmark problem classical heavytailed model exhibit improve stability datum cluster corruption fail altogether far approach use adjustment smoothly datum suggest extend general model address similarity work regression datum model noisy observation underlying process simple assumption noise independent identically distribute typical set sample appear latent function bayesian framework process condition computation remain tractable figure furthermore noise model enjoy theoretical justification central limit theorem state sum sufficiently variable finite variance distribute normally rarely perturbation affect datum real world argue originate addition iid source random component signal cause human measurement error systematic variation simplified model case possibility encounter small quantity highly datum require robustness ie model prediction greatly affect outlier demand render standard gp inappropriate light tail gaussian explain large nongaussian deviation mean away majority datum force infer large global noise variance figure b robust method use heavytailed likelihood allow effectively smoothness ignore erroneous datum figure c achieve use twocomponent noise model r fn b c d figure black dot noisy sample function panel behaviour gp gaussian noise assumption illustrate shaded region confidence interval presence single outlier highly influential model likelihood panel c unfortunately model fail cluster outlier panel line repeat run black line shaded region average mean confidence interval odd latent generative model observation gaussian corruption draw probability large variance outlier distribution r inference model tractable impractical small problem exponential term product paper address fundamental gp assumption research motivate observe predictive distribution suffer heavytailed model outlier appear figure d replicate figure c introduce additional outlier parameter optimal solution challenge optimization considerable uncertainty posterior compete interpretation cluster signal noise similar posterior mass view way tail effective log likelihood cluster observation approximately weight single outlier magnitude posterior peak associate robust solution comparably reduce simple tail likelihood noise model global entire data space potentially cause real datum relegate tail establish optimal choice parameter gradient ascent marginal likelihood entirely possible single setting satisfactory model introduce paper twin gaussian process generalize noise model use gp function choose real outlier distribution region confidence tail light encourage data point tightly observation treat appropriately broaden noise distribution model propose tresp rasmussen ghahramani automatically infer correct number component use possibly gain restrict simple architecture answer overhead require different approach general model require inference method argue twocomponent mixture sensible distribution model real datum natural interpretation heavy tail require weakness expose primarily noise distribution largely solve problem allow inference efficient expectation propagation procedure resort heavy monte method provide twocomponent mixture likely reflect noise datum similar result generalized mixture mention fraction suggest approach noise spirit log variance observation model gp logarithm noise variance nonnegative property inference analytically intractable gibbs sampling use generate noise vector posterior distribution alternately fit signal process fit noise process stage gibb sample require test point estimate predictive variance testing slow model selection slow metropolishasting suggest update hyperparameter twin process domain x covariance function r gaussian process gp space realvalued function specify joint distribution finite set x pf kf latent value associate matrix evaluation covariance function pair apply baye rule obtain posterior distribution observe x assumption corrupt observation normally distribute prediction marginalize joint thorough introduction robust gp regression achieve use likelihood distribution ie tail mass common choice laplace double exponential distribution student t distribution mixture model product prior heavytailed likelihood observation strong pull posterior noise model describe inference perform likelihood case performance broadly comparable bear close resemblance twin gp particularly interested mixture section include result laplace model log concave distribution guarantee posterior allow reliable convergence case method global assumption noise distribution inappropriate model beneficial graphical model tgp figure b augment standard process gp set variable u act gate function probabilistically divide domain real outlier component noise model r fn z likelihood mix form gaussian corruption strongly peak observation broad distribution provide heavy tail proportion determine intuitive sense retain advantage tractability respect update prior different covariance structure reflect different belief correlation signal noise domain addition accommodate prior belief outlier nonzero mean process pux u pf model understand lie extreme observe recover heavytailed mixture gp force absolute correlation adjust mean e conversely remove correlation u return standard mixture model independently decide component input belong begin brief account detail suppose intractable distribution form factorize product term dense series independent likelihood tn ep construct approximate posterior product scale site function tn computational tractability site usually choose exponential family natural parameter case product retain functional form component natural parameterization b prior form site function exact u u u b figure panel graphical model gaussian process datum observation gp latent bold black line indicate fullyconnecte set panel b graphical model twin gaussian process auxiliary set hide variable describe datum marginal likelihood scale parameter ideally choose global minimum divergence measure necessary optimization intractable iterative procedure find minimizer kl pointwise basis iteration select new site product cavity distribution form current marginal site true term tn obtain socalle tilt distribution q fn simple optimization fit parameter equivalent moment match distribution scale choose match zerothorder moment site update moment remain site change iteration require convergence prior independent expect correlation posterior condition observation understand consider single observation principle admit explanation correspond classification outlier real datum general term respect global structure signal diagram visualization behaviour posterior provide figure recall prior kf likelihood factorize product term site approximation tn gaussian importance moment tilt distribution seek match easily obtain differentiation zeroth moment component find r r write inner gaussian z r br q u q b r integral outlier component similar q partial log z log t equip algorithmic detail appear seeger note efficiency update approximate covariance u loop posterior end cycle avoid loss precision log p prior posterior u u replacement log p log p prior posterior prior posterior prior posterior u u figure use twin gaussian process provide natural cluster noisy datum column illustrate behaviour fix heavytailed likelihood repeat observation outlier real datum necessarily tightly pack symmetry approximation allow treat single unit posterior example mean posteriori belief observation identical latent context provide prior confidence datum topleft box illustrate influence isolated outlier mitigate standard mixture repeat observation box left cause solution collapse spike datum log scale second peak contribute posterior mass twin gp preserve marginal distribution maintain joint distribution u second column respectively contour true log joint use broad prior infer marginal posterior fifth context essentially approximation thick bar central column mark correspond unnormalized posterior column prediction outlier component describe noise eliminate require test input marginal distribution pf y obtain marginalize u approximate posterior z pf pf x noise process interest case need marginalize p p p p distribution long gaussian moment recover easily method use obtain moment tilt distribution provide addition approximate moment posterior distribution estimate marginal likelihood derivative respect kernel hyperparameter refer interested reader algorithm present add implementation use log noise value r allow unconstrained optimization complexity dominate update covariance update o iteration posterior require inverse n positive semidefinite matrix efficiently achieve factorization factor retain use calculate approximate marginal likelihood total number loop require convergence typically independent upper bound small constant entire inference process algorithm limit time complexity robust regression mask large coefficient appear approximate simultaneously additionally body loop slightly slow precision matrix standard gp obtain single division model require inversion matrix experiment identify general noise characteristic model suitable observation appear cluster figure d occurrence affect standard mixture model fact problem severe convergence possibility conflict gradient information procedure evidence maximization figure illustrate succeed mixture laplace model fail note mean process fall sharply region stable solution hyperparameter fit reliably data set exhibit superior predictive modelling tgp domain robust method expect perform provide variation set friedman sample draw function vector depend component mixture noise b laplace noise c tgp figure corruption iid highly correlated generate set training example test example sample uniformly add training datum noise experiment replicate procedure training point add random output sample value likely lie range result appear friedman figure observe rm error robust method similar tgp able fit variance far accurately second experiment training set augment gaussian cluster noisy observation cluster centre draw uniformly variance fix output value draw point highly correlated value distant underlie function friedman method offer improvement standard gp yield confident prediction cf outlier account corrupted region furthermore estimate datum corrupt recover consider process u experiment training datum renormalize mean unit variance use anisotropic square exponential process implement socalle relevance determination isotropic version u approximate marginal likelihood maximize randomly initialize model choose test second domain application noise datum believe priori function input ie twin gp simulate change variance modulate process allocate vary weight component way example behaviour onedimensional set fig c noise model directly notable associate approach predictive variance saturate weight component second component dominate variance estimate mixture particularly problematic variance datum range order magnitude comparably broad real component case extreme value u small error predict consequence process tend sweep region sensitivity variance prediction accurately circumvent problem employ gp rescale process supervised manner explore idea far test error gp log probability friedman test error gp log probability friedman c figure result friedman datum prediction tgp set extension prior knowledge nature corruption affect signal seek model noise distribution accurately example introduce p compound likelihood p component constrain relative weight outlier corruption constant entire domain rich alternative provide extend single noise series u noise process broaden likelihood function appropriately example write u fn case precede analysis apply small change component outlier distribution contribute moment independently second model introduce significant computational difficulty firstly maintain posterior distribution yield space requirement time complexity importantly moment need loop intractable inner ep loop use approximate product behave essence standard model classification omit detail defer experiment model future work conclusion present method robust gp regression improve classical approach allow noise variance vary input space find improved convergence problem standard mixture model predictive certainty improve adopt tgp problem model allow arbitrary process specialize prior knowledge use drive inference respect region consider erroneous generalization idea appear mixture infinite mixture involve slow inference procedure fast solution require robust inference twocomponent mixture adequate model task believe attractive option reference tresp mixture process advance neural information processing system page gaussian process expert advance neural information processing system regression noise gaussian process treatment advance neural information processing system mit process use neural information processing system mit press minka family algorithm approximate machine learning process model robust regression classification reinforcement learn expectation propagation exponential family available friedman multivariate adaptive regression spline annal statistic aspect spline smoothing approach nonparametric regression curve royal statistical society warp gaussian process advance neural information processing system
neural network base head tracking system seung technology abstract construct inexpensive tracking system learn track head use real time graphical user input auxiliary detector signal train convolutional neural network input network consist normalize luminance image motion information frame subsample image use provide scale invariance training phase neural network rapidly adjust input weight depend reliability erent channel surround environment quick adaptation allow system robustly track head object background introduction inexpensive computer peripheral video appear enter personal video system typically use stationary camera tie user location telephone telephone simple solution problem use video camera track person present continually control movement camera communicate paper present prototype neural network base system learn characteristic person head real time automatically track room alleviate user camera movement video system closely resemble movement human eye task biological oculomotor system direct directional microphone ear pc frame sound card signal muscle ir detector figure schematic hardware diagram head tracking system interesting visual world small high resolution area task complex neural circuit evolve order control eye movement example include smooth pursuit system allow eye rapidly acquire track object similarly active video system need determine appropriate face feature follow video stream camera track person movement time transmit image party past year problem face detection image video attract considerable attention method concentrate look generic characteristic face shape type algorithm fairly simple implement commonly find system object similar shape color face system easily potentially robust approach use convolutional neural network learn appropriate feature face implementation learn batch mode construct large training set label image face paper present video base system use online signal train convolutional neural network fast online adaptation network weight allow neural network learn discriminate individual head beginning session enable system robustly track head presence object hardware implementation figure schematic tracking system construct early version similarity character eye consist small camera view platform ability rapidly pan tilt wide range view angle typical maximum velocity system include microphone ear ability locate auditory cue integrate auditory information visual input allow system object sound video proceeding focus exclusively visual representation learn image u figure preprocesse video stream luminance chromatic motion information separately represent d channel multiple resolution able learn track visual target use erent source signal method training use small khz modulate light nm object need track lter render light video camera system merely learn follow signal camera small detector signal object locate narrow angular cone direction camera point reinforcement signal use train weight neural network natural way system learn occur actual video scenario situation user actively video stream manual control camera use graphical user interface input user camera new location neural network adjust weight track center portion image build readily available commercial component cost system include pc input device control computer use matlab available window operating system image processing computation graphical user interface easily implement simple matlab operation function follow section describe head track neural network detail use convolutional neural network architecture detect head view video stream camera video capture board series raw image figure image convert representation d hide unit map winner d figure neural network use convolutional architecture integrate erent source information determine maximally salient object image compute absolute value precede frame result image component represent luminance grayscale information channel contain chromatic color information motion information video stream capture d image object appear highlight channel subsample successively yield representation low low resolution result image pyramid allow network achieve recognition invariance di erent scale train separate neural network resolution instead single neural network set weight run erent resolution input maximally active resolution position select use convolutional neural network architecture figure locate salient object erent resolution input image ltere separate kernel denote respectively result image wa s denote scale resolution input u d channel image represent single layer hide unit network hide unit combine form saliency map s follow manner s s c sigmoidal saliency s compute nonlinear combination hidden unit scalar variable represent relative importance erent luminance chromatic motion channel overall saliency object bias term c function think relative probability head exist location input resolution s nal output neural network determine competitive manner nde location m jm scale sm good possible match gx m process visual input manner camera movement generate order maximally salient object locate center view training result user input detector use signal train kernel wa scalar weight neural network neural network update maximally salient location neural network m jm correspond desire object true position external signal cost function proportional sum square error term maximal location new desire location use train m min gx follow example constant use gradient eq backpropagate convolutional network result follow update rule emg jm gain typical batch learning application neural network learning rate set small positive number case desirable learn track head new environment quickly possible rapid adaptation weight single training example need natural way use fairly large learning rate repeatedly apply update rule eq calculate maximally salient location close actual desire position example quickly able learn track author learn curve figure weight initialize small random value correct online fashion use mouse input look author head second training processing time loop system able locate head pixel accuracy determine label video datum eye movement initiate error frame number figure fast online adaptation neural network head location error pixel image plot function frame number time indicate arrow fig new environment sample large error time error correct neural network learn robustly discriminate head figure input weight network minute training author walk kernel necessarily appear little invariant slight change head position rotation scale clearly depict dark facial feature color head relative weighting cd erent input channel luminance color information reliable track head probably relatively distinguish frame image head body currently consider complicated neural network architecture combine erent input stream tracking performance example simple convolutional architecture use automatically integrate di erent visual cue robustly track head use fast online adaptation neural network weight system able learn need large training set able rapidly change environment future improvement hardware neural network architecture algorithm necessary order approach human speed performance type sensory processing recognition acknowledge support laboratory technology thank m e helpful discussion u figure example input weight use track head head position calculate neural network mark box reference tk koch c analog vlsi eye movement system advance neural information processing system model target visual search advance neural information process system sing poggio t learning human face detection rd image understanding workshop automatic face location detection track coding video sequence low signal processing image communication e robust face feature analysis automatic character automatic face recognition darrell novel environment vision behavior proc ieee workshop visual behavior realtime face tracker application computer vision nowlan sj convolutional neural network hand advance neural information processing system human face detection visual scene advance neural information processing system le handwritten digit recognition network advance neural information processing system
minimize quadratic function constant time advanced industrial science preferred samplingbase optimization method quadratic function propose method approximately solve follow ndimensional quadratic minimization problem constant time independent z vi rnn matrix d rn vector theoretical analysis specify number sample solution z satisfy probability empirical performance accuracy runtime positively confirm numerical experiment introduction quadratic function important function class machine learn statistic datum fundamental problem linear regression kmean cluster principal component analysis support vector machine method formulate minimization problem quadratic function application sufficient compute minimum value quadratic function solution example propose efficient method estimate divergence provide useful information datum density ratio formulate estimation problem minimization squared loss divergence estimate minimum value leastsquare mutual information example compute similar manner importance minimization quadratic function issue scalability let number variable dimension problem general minimization problem solve quadratic programming require time problem constraint problem reduce solve system linear equation require time method easily infeasible problem n technique propose accelerate quadratic function minimization require linear time n problematic handle problem dimension linear time slow example optimization method widely use largescale problem nice property method objective function strongly convex output point sufficiently close optimal solution constant number iteration iteration need time access variable technique lowrank approximation nystrom method underlying idea approximation problem use lowrank matrix drastically reduce time conference neural information processing system nip complexity need compute matrixvector product size require time propose algorithm special case quadratic function minimization sublinear respect number pairwise interaction variable algorithm require time c contribution let rnn matrix b rn vector consider follow quadratic problem minimize pnadb pnadb h denote inner product denote matrix diagonal entry specify note constant term include irrelevant optimize ignore let r optimal value let parameter main goal paper computation z probability constant time independent n assume real ram model perform basic algebraic operation real number step assume query access b d obtain entry specify index note typically n consist term vi consist n term regard error error term reasonably small typical situation let operator extract submatrix specify index set algorithm define follow parameter determine later input integer query access matrix rnn vector sequence index independently uniformly sample return word sample constant number index set solve problem restrict index note number query time ok respectively order analyze difference optimal value want measure distance d bs want small end exploit graph limit theory initiate szegedy refer book measure distance graph different number vertex consider continuous version primary interest graph limit theory graph extend argument analyze matrix vector use synthetic real setting demonstrate method order magnitude fast standard algorithm accuracy method sufficiently high relate work approximation algorithm know combinatorial optimization problem max cut problem dense constraint satisfaction problem vertex cover problem far know know continuous optimization problem related notion property testing aim design algorithm distinguish input satisfy predetermine property input far satisfy characterization testable property know property dense graph property function finite field preliminary integer let denote set notation b c mean c b c paper consider function set measurable let xk sequence k index vector denote restriction v s rk k matrix rnn denote restriction s ij k dikernel follow measurable function r dikernel dikernel generalization symmetric range bound regard dikernel matrix index specify real value stress term r function g r define inner product r function r define function r let r dikernel norm p cut norm r r r r p define x respectively supremum pair subset note norm satisfy triangle inequality let measure map preimage measurable measurable set bijection map inverse map exist measurable measure preserve bijection dikernel r define r matrix dikernel let r dikernel s xk sequence element define matrix r matrix rnn follow let construct dikernel define unique b ai main motivation create integer define dikernel matrix define distance matrix b bk b b different size cut norm note distribution sequence index uniformly s sequence independently sample exactly match distribution k element uniformly independently sample sampling property cut norm section prove follow theorem state sequence dikernel l l obtain good approximation sample sequence small number element formally prove follow let l l dikernel let sequence element uniformly independently sample probability log k exist bijection function f t p t log k start follow lemma state dikernel r small cut norm negligible matter focus cut norm prove lemma let r dikernel function proof r function h r let l h level set h gk z x d k z l g z d d k z introduce technical tool need definition partition q refinement partition p obtain split set vi partition p interval p p r p define r function obtain average vj p formally define dy dx dy vj vj unique index vi respectively follow state function r approximate p small number weak regularity lemma function let p k set dikernel r exist refinement q p q constant c corollary let w r dikernel exist p p ct constant c t t t proof let p trivial partition partition consist single t iteratively apply p t t obtain partition p t p t c t t refinement wpi t wpi t k t t satisfy desire property p t t ct d long sufficiently large close cut norm let l l dikernel s sequence element uniformly independently sample d es finally need follow concentration inequality inequality let p probability space positive integer let z z z independent random variable zi value measure space ai let r function suppose c differ coordinate c e k prove counterpart let l l dikernel let sequence element uniformly independently sample probability log k exist bijection t t p k l t log k proof bind expectation prove concentration apply corollary t let p obtain partition p ct l t t t t t p k l p k bijection t k t t k e t t p k l leave problem sample p let xk sequence independent random variable uniformly distribute let zi number point fall set easy compute p partition p construct set t t t construct dikernel r value vj value agree set q vi exist bijection p t t t t t t p k l q z z l min l min zi l rewrite p k l p ip p z expectation right hand l pk ip l pk k l pk schwartz inequality t p p insert obtain r k l ct choose obtain upper bind k l t s k change element change observe p apply inequality log union bind complete proof proof immediately follow analysis section analyze want use dikernel analysis introduce continuous version pnadb recall realvalued function pnadb function r define d pnadb r function r constant function value follow state minimization pnadb pnadb equivalent let rnn matrix rnn vector min pnadb inf k proof inf v vector kn define z x b aij z d hv ij z bi hv bi n pnadb pnadb inf let measurable function ij note form partial derivative depend optimal solution assume word constant interval f define vector rn x element z aij vj aij ij dx d bi xz bi b finally pnadb pnadb f optimal value follow sense theorem let optimal solution optimal value respectively choose log log log probability sequence k index independently uniformly sample satisfie following let v optimal solution optimal value respectively problem v z z vi aij maxi di maxi bi b proof instantiate theorem log log log dikernel d d probability exist measure preserve bijection d d d b b max function min inf inf inf min v d b b d d d b b d b pnadb k pnadb k rearrange inequality obtain desire result bound symmetric rank note assume positivedefinite pnadb bound problem set positivedefinite principal submatrix mean bounded experiment section demonstrate effectiveness method experiment experiment conduct amazon instance error bar indicate standard deviation trial different numerical simulation investigate actual relationship end prepare synthetic datum follow randomly generate input aij u u bi u denote uniform distribution support solve use compare exact solution obtain qp result figure approximation error evenly control regardless meet error analysis theorem program code available use qp table pearson divergence runtime propose z figure numerical simulation absolute approximation error scale propose table pearson divergence absolute approximation error application kernel method consider approximation divergence problem define follow suppose different datum set let rnn gram matrix xi xi parameter let r vector estimator relative pearson divergence distribution obtain hh hv vi regularization parameter experiment use set choose fold crossvalidation suggest randomly generate data set denote gaussian distribution mean variance encode problem set h b h denote ndimensional vector element compute second step pseudoinverse absolute approximation error runtime compare nystrom method rank set term accuracy method clearly outperform nystrom method table addition runtime method nearly constant runtime nystrom method grow linearly table acknowledgment like thank suggest problem method support support scientific research area foundation big datum large graph project reference r m random sampling approximation problem stoc page e fischer combinatorial characterization testable graph property regularity compute c l b szegedy graph limit parameter testing stoc page c t convergent sequence dense subgraph metric property testing advance bottou stochastic learning advanced lecture machine learn page p feasible real random access machine journal complexity e hazan sublinear optimization machine learning acm r regularity approximation scheme dense problem foc page ron property testing connection learning approximation journal acm l large network graph limit society l szegedy limit dense theory series l graph property testing probability compute c algorithm dense max cut greedy soda page machine learn probabilistic perspective mit nguyen approximation algorithm local improvement foc page k m r nearoptimal algorithm approximate minimum vertex cover size soda page r m robust characterization polynomial application program compute density ratio estimation machine learn press m leastsquare independent component analysis neural computation m seeger use nystrom method speed machine nip m relative estimation robust distribution comparison nip optimal approximation algorithm result stoc page characterization locally testable property decomposition theorem stoc page norm function limit parameter estimation soda page m improve approximation algorithm maximum matching optimization problem compute
e m o o cognitive abstract individual independently event retrieve fact memory aggregate retrieve memory reconstruct actual set event fact research report performance individual series general knowledge task goal reconstruct memory order event order item physical dimension introduce bayesian model aggregate order information base thurstonian approach mallow model assume individual reconstruction base random permutation unobserved ground truth pure guessing strategy apply mcmc inference underlie truth strategy employ individual model demonstrate wisdom crowd effect aggregated close true ordering ordering good individual demonstration aggregate judgment number individual result estimate close true answer phenomenon come know wisdom crowd demonstrate d estimate weight average individual closely approximate true weight similarly game want opportunity ask member answer multiple choice question modal response correspond correct answer time sophisticated aggregation approach develop multiple choice task theory additionally difference item account wisdom crowd idea currently use realworld application prediction market spam filtering prediction preference collaborative filtering recently form wisdom crowd phenomenon occur single person average multiple guess person provide estimate individual guess interested apply wisdom crowd phenomenon human memory involve situation individual retrieve information complex single numerical estimate answer multiple choice question focus memory order information example test individual ability reconstruct memory order event eg order president magnitude physical dimension eg order large city develop computational model infer distribution ordering explain observed ordering individual goal demonstrate wisdom crowd effect infer ordering close actual ordering ordering produce majority individual aggregate rank order datum new problem social choice theory number system develop aggregate rank order preference group voting system explicitly rank order candidate preference design pick candidate field man system perform aggregate individual rank order datum inherent bias determine member list voting means express individual preference ground truth goal system determine aggregate preference sense fair member group rank aggregation problem study machine learning example present rank list webpage search engine combine create single ranking accurate sensitive spam relatively little research rank order aggregation problem goal approximate know ground truth work experiment perform test ability individual magnitude psychophysical experiment aggregation model rank order datum develop consensus theory use factor analysis covariance structure order judgment use partially recover order cause basis individual ordering present empirical theoretical research wisdom crowd phenomenon rank order aggregation communication people allow task aggregation method operate datum produce b independent decisionmaker importantly problem know ground truth compare heuristic computational vote theory exist model social analyze individual judgment provide single answer output compare ground truth refer synthesize answer group answer capture wisdom group communication group member occur apply probabilistic model base thurstonian approach mallow model thurstonian model represent group knowledge item distribution interval dimension mallow model model represent group answer modal ordering item assume individual ordering close modal order thurstonian mallow type model use analyze preference ranking apply far aware order problem ground truth present extension model allow possibility different response individual purely guess knowledge problem partial knowledge ground truth develop efficient mcmc algorithm infer group ordering assignment individual response strategy advantage mcmc estimation procedure probability distribution group ordering assess likelihood particular group order r m e t d participant student t experiment compose question involve general knowledge regard population statistic question question date release date movie book question president material commandment amendment co interactive interface present computer screen participant order present item order book release date early recent respond individual item screen desire location order initial order item question randomize question participant table unique ordering individual state president order problem j j aa b d e e g j aa b j aa b e e e j aa b d e g j j aa j d h h j aa d e j aa e aa e d e e d d f e gg j aa b hh aa b aa b e e e h aa b e g aa d d e e d j d b e e h aa d aa b e e aa c e e e j aa b d e aa ed d hh b d d e e f b e ad e c c ad d d dg e aa aa e b bh e aa e aa e b d d h e b row d r e t evaluate performance participant model measure distance reconstructed correct order commonly use distance metric ordering kendall distance metric count number adjacent pairwise disagreement ordering value range number item order question value mean ordering exactly right value mean ordering correct neighboring item transpose maximum value table unique ordering column produce problem arrange state location sort president time serve office correct ordering right column sort kendall distance second number order correspond kendall distance number participant produce ordering respectively example small number participant reproduce correct order fact problem participant correct answer ordering produce multiple participant problem participant produce unique ordering summarize result participant column label pc table proportion individual ordering exactly right ordering task question average percent participant correct rank order perfectly column mean value population participant sort task question prior knowledge task interesting note good performance overall achieve president state movie movie release date question relate educational knowledge likely share subject finally important summary statistic performance good individual instead pick good individual separately problem find individual score problem table row individual average distance demonstrate wisdom crowd effect synthesize group order outperform ordering average good individual evaluate number aggregation model ability reconstruct ground truth base group order infer individual ordering evaluate heuristic method social choice theory base mode count drawback heuristic aggregation model create explicit representation individual work knowledge method aggregate table performance model human participant city population city population city population movie movie president state amendment commandment average good individual human pc thurstonian rank mallow rank rank rank individual piece knowledge individual explain individual rank item particular way address potential weakness develop simple probabilistic model base thurstonian approach mallow model e s test heuristic aggregation model simple heuristic base mode group answer base frequently occur sequence observed sequence case different sequence correspond mode choose modal sequence pick second method use widely use technique vote theory weight count assign choice candidate receive count number candidate second choice candidate receive count count sum candidate candidate high count consider prefer use create ordering item order table report performance aggregation model check infer group order correct c measure kendall report rank column percentage participant perform bad group answer measure rank statistic verify wisdom crowd effect ideal model aggregate answer good individual group table result separately problem average problem result mode heuristic lead bad performance overall average mode good estimate participant mean participant come solution individually surprising ordering item possible participant agree ordering item difficulty infer mode unreliable method construct group answer problem ordering involve item number possible ordering grow count method perform relatively term kendall overall performance average method perform rank indicate group answer method score good individual average average distance outperform good individual problem guess model z b c c b c b c c b b b c c b c b c b figure illustration extended thurstonian model guessing component thurstonian approach overall item knowledge group represent explicitly set coordinate interval dimension interval representation problem study involve dimensional concept relative timing event length item introduce extension thurstonian approach ordering individual draw thurstonian model draw base guessing process relation underlie interval representation introduce basic thurstonian approach let number item order task m number individual order item item represent value dimension individual assume access information individual precise knowledge exact location item model individual location item single sample normal distribution center item group location specifically model determine order item person sample value item standard deviation capture uncertainty individual item sample represent mental representation individual ordering individual base ordering sample let observed ordering item individual figure leave panel example basic thurstonian model information item b c individual illustration large degree overlap representation b likely item b c transpose illustrate second individual extend basic thurstonian model incorporate guessing component find necessary extension individual ordering task actually b xj figure graphical model extended thurstonian model mallow model president freedom speech right bear arm unreasonable search process trial trial right right amendment power state people large small figure sample infer distribution vertical order ground truth order number parenthesis infer group order familiar item ordering task commandment extended thurstonian model ordering case assume originate single distribution distinction different sample come distribution parameter ordering produce individual model completely random example figure right panel ordering produce guess model associate latent state individual determine ordering individual produce guess model complete model place standard prior normal distribution figure graphical model thurstonian model model look straightforward hierarchical model inference model prove difficult observed variable deterministic rank function indicate double border circle underlie latent variable basic thurstonian introduce recently mcmc method develop estimation develop simplified mcmc procedure describe supplementary material allow efficient estimation underlie true order assignment individual response strategy result extended thurstonian model table model perform approximately model recover exact answer problem base knowledge provide participant possible large sample size need order achieve perfect reconstruction ground truth model average distance actual truth good individual problem advantage probabilistic approach insight difficulty task response strategy individual problem commandment individual assign guess strategy problem president individual assign guess strategy indicate knowledge domain widely distribute group individual extension model eliminate individual purely guess answer advantage representation underlie thurstonian model allow visualization group knowledge term order item term uncertainty associate item interval scale figure infer distribution problem model perform relatively correspond mean sample error bar represent standard deviation base geometric average sample visualization intuitive item group population instance nearly participant able identify state list central state large agreement proper placement right bear arm amendment question amendment refer second amendment m drawback thurstonian model analog representation item inappropriate problem example amendment commandment represent coordinate interval scale apply probabilistic model group answer base pure rank order model mallow model assume observe ordering close group ordering likely far away instantiation mallow model base kendall distance measure number pairwise permutation group order individual order specifically probability observed order group order kendall distance scaling parameter determine close observe order group order describe normalization function depend calculate efficiently model state describe standard mallow model use model preference rank datum introduce variant model allow idea individual ordering originate common group knowledge instead base guessing process extended model introduce latent state individual produce ordering base mallow model individual guess model guess choose ordering uniformly possible ordering item extended model complete model place bernoulli prior estimate distribution observe datum base early work extend estimate allow efficient estimation detail inference procedure describe material result inference probability distribution group answer mode single answer particular problem note infer group ordering correspond ordering particular individual model find ordering close observe ordering explain guessing process figure illustrate model solution base single mcmc sample commandment amendment sort task figure distribution distance infer group order circle correspond individual individual assign mallow model guess model illustrate fill circle respectively solid dash red line expect distribution base model parameter note mallow model describe exponential probability base distance group order expect distribution account number ordering exist distance page recursive algorithm compute commandment number individual amendment number individual figure distribution distance group answer example problem figure distribution individual capture route model individual kendall tend assign mallow route individual assign guessing route interestingly distribution distance appear bimodal especially commandment middle peak distribution occur close expect value base guess result intuitively plausible study commandment let order occur table result extended mallow model problem overall performance term kendall rank comparable model method appear overall advantage particular approach commandment amendment sort task mallow model perform thurstonian model suggest particular ordering task arguably analog representation pure representation mallow model advantage present heuristic aggregation approach probabilistic approach problem aggregate rank order ground truth problem find individual perform aggregation model identify individual fact problem person consistently outperform model aggregation method mode demonstrate wisdom crowd effect average performance model good individual problem present probabilistic approach base classic thurstonian mallow approach model outperform simple heuristic model advantage thurstonian model extract group ordering representation uncertainty associate order visualize gain insight mental representation process addition thurstonian mallow model extend guess component allow possibility individual simply know answer particular problem finally explore bayesian approach potentially offer advantage heuristic approach probabilistic model easily expand additional source knowledge confidence judgment participant background knowledge t item r e e nce wisdom crowd p nature c recent application dm b empirical comparison algorithm aggregate expert prediction proceeding conference uncertainty artificial intelligence uai e measure crowd probabilistic representation individual psychological science lafferty combine ranking use conditional model permutation machine learn nonparametric modeling partially rank datum journal machine learn research group judgment field lift weight experimental psychology law comparative judgement psychological review mallow ranking model biometrika analyze model rank chapman bayesian estimation thurstonian ranking model base gibbs sampler mathematical statistical psychology
network complexity abstract design fabricate test series compact integrate circuit realize winnertakeall function analog continuoustime circuit use interconnect perform function modify circuit realize circuit compute local nonlinear inhibition general type inhibition mediate activity neural system inhibition set level computation multiplicative nonlinear inhibition gain computation report physical realization general nonlinear inhibition extreme form know design fabricate series compact completely functional integrate circuit realize winnertakeall function use analog nature medium circuit use successfully component vlsi sensory system perform auditory localization mead press visual winnertakeall circuit input function correctly sensory system modify global winnertakeall realize circuit compute local nonlinear inhibition circuit allow multiple winner network suited use system represent feature space topographically process feature parallel design fabricate test integrate circuit compute locally function spatially order input mead winnertakeall figure schematic diagram single wire associate potential compute inhibition entire circuit wire long compute global inhibition contribute current common wire use transistor t apply global inhibition locally neuron respond common wire voltage use transistor computation continuous time use circuit exhibit operate time constant relate size large input output representation binary win output encode logarithm associated input figure schematic diagram neuron receive current input output voltage vb represent result winnertakeall computation b logarithmic function static dynamic analysis circuit illustrate system property figure schematic diagram understand behavior circuit consider input condition m transistor identical potential gate source sink m potential vi v equal transistor identical source gate potential sink identical current c region operation equation m describe transistor parameter vo likewise equation v describe transistor solve yield winnertakeall network complexity equal input current circuit produce equal output voltage behavior desirable addition output voltage encode magnitude input current m figure schematic diagram input condition m m illustrate inhibitory action sink current previous example result gate voltage rise transistor tit ti share common gate sink m m present compensate voltage v decrease small early effect serve decrease current decrease v linearly large leave saturation drive v approximately desire output associate small input large equation m describe transistor equation describe transistor t solve yield win output encode logarithm associated input circuit topology ensure similar behavior increase relative equation predict win response circuit complex expression derive predict lose crossover response figure plot analysis fit experimental datum figure wide dynamic range logarithmic property experiment figure repeat value range order conductance transistor determine lose response variant ai achieve lose response wide narrow figure use circuit mask layout technique mead good stable exhibit damp oscillation ring response input change section explore dynamic property predict temporal response circuit figure add model dynamic behavior t ie figure schematic diagram add dynamic analysis large add neuron model contribute gate t t interconnect analysis circuit transfer function circuit real pole circuit stable ring rl figure compare bind experimental ie circuit exhibit firstorder behavior time constant set dynamic win neuron vo time constant set dynamic lose neuron ve rl figure compare prediction experimental datum winnertakeall network complexity iii figure experimental datum circle theory solid line input current neuron sweep value input current output vi v plot normalize input current figure experiment figure repeat value experimental datum output voltage response plot absolute input current log scale output voltage v highlight circle experiment dash line theoretical expression confirm logarithmic behavior order magnitude equation mead figure experimental datum circle theoretical statement solid line small necessary firstorder response step input figure experimental datum symbol theoretical statement solid line circuit time constant response step input win response fill circle lose response triangle winnertakeall circuit time constant differ order network complexity local nonlinear inhibition figure previously explain locate large input circuit certain application require form nonlinear inhibition circuit represent multiple intensity scale necessary circuit modification figure perform task explain mode operation application require local winnertakeall computation winner limited spatial area figure circuit compute local function circuit identical original neuron connect near neighbor nonlinear circuit mead press conduct current ir response voltage v saturate current parameter current source present original distribute local winnertakeall figure schematic diagram section local neuron receive current input iii output voltage represent result local winnertakeall computation understand operation local winnertakeall circuit consider response spatial impulse define h ve large circuit connect neuron saturate sink current transistor conduct current region operation equation describe transistor ti equation ie describe transistor mead t solve yield original winnertakeall output win neuron encode logarithm neuron associate input mention connect current source associate neuron ie ie supply current current source supply current transistor carry current neuron output approach way win neuron inhibit neighboring neuron inhibitory action extend network need current neuron ie ie neuron sufficiently distant maintain service current source output distant neuron active spatial impulse neuron ie equal input current distant neuron equal output similar reasoning apply neuron ie ie relative value determine spatial extent inhibitory action figure spatial impulse response local winnertakeall different setting ie figure experimental datum spatial impulse response local value range factor wide inhibitory response correspond large ratio clarity plot increment winnertakeall network complexity conclusion circuit describe paper use analog nature device realize interesting class neural computation efficiently circuit exploit physics medium way use single wire compute communicate inhibition entire circuit transistor use physical phenomenon computation exponential current function encode logarithm input conductance transistor define lose output response evolution exploit physical property neural device optimize system performance synthetic neural system harness potential physics medium provide mathematical insight analysis document thank compute support darpa chip work sponsor office naval research system development foundation reference winnertakeall network caltech computer p mead press model auditory ti analog vlsi implementation correspondence abstract annual inn meeting vol supplement p press analog vlsi neural system read
bayesian set computational neuroscience unit abstract inspire set consider problem retrieve item concept cluster query consist item cluster formulate bayesian inference problem describe simple algorithm solve algorithm use modelbase concept cluster rank item use score evaluate marginal probability item belong cluster contain query item exponential family model conjugate prior marginal probability simple function sufficient statistic focus sparse binary datum score evaluate exactly use single sparse matrix multiplication possible apply algorithm large dataset evaluate algorithm dataset retrieve movie eachmovie finding completion author set nip dataset find completion set word appear grolier compare set bayesian set reasonable set completion introduction common associate different view origin man college cambridge university enter query set return list college set remarkably useful tool practical interesting problem machine learning consider item depend application set consist web page movie people word protein image object wish form query user provide query form small subset item assumption element dc example concept class cluster datum provide completion set set presumably include element dc element d concept class cluster set largescale cluster algorithm use million data instance extract web datum tong personal communication unable describe detail work nature use term cluster refer target concept view problem perspective query interpret element unknown cluster output algorithm completion cluster clustering algorithm completely unsupervised query provide supervised hint constraint membership particular cluster view cluster demand involve form cluster element cluster reveal important advantage approach traditional clustering element query useful information feature relevant form cluster example suggest feature president relevant cluster query suggest current world relevant huge number feature real world datum set hint feature relevance produce sensible cluster second think goal algorithm solve particular information retrieval problem retrieval problem output relevant query sense limit output item rank relevance query experiment approach report item rank relevance relevance criterion closely relate bayesian framework understand pattern generalization human cognition bayesian set let data set item d item set assume user provide query set dc small subset d goal rank element fit set include dc intuitively task clear set set movie query set consist movie expect movie rank highly use modelbased probabilistic criterion measure item fit observe dc belong concept want know probable belong dc measure rank item simply probability sensible item probable regardless dc example sensible model probability string decrease number character probability image decrease number pixel probability continuous variable decrease precision measure want remove effect compute ratio scorex px denominator prior probability sensible model scale exactly correctly number character discretization level use baye rule score rewrite scorex interpret ratio joint probability observe probability independently observe intuitively ratio compare probability dc generate model unknown parameter probability dc come model different parameter figure finally multiplicative constant independent score write scorex x probability observe query set likelihood discussion clear compute quantity px natural modelbase way define cluster assume figure bayesian score compare hypothesis datum generate graphical model datum point cluster come independently identically distribute simple parameterize statistical model assume parameterized model px parameter datum point belong cluster definition generate setting parameter set unknown need average possible parameter value weight prior density parameter value p use consideration basic rule probability arrive p d z pxi p d xi dc d p fully equip describe bayesian set set set item probabilistic model px d prior model parameter p input query d end output return element sort decrease score mention property algorithm common sensitivity prior simple model consider integral analytical fact model consider section compute score reduce single sparse matrix multiplication clearly sense thought choose sensible model px prior p simple model tuning prior competitive retrieval result practice use simple empirical heuristic set prior center mean datum sparse binary datum derive detail application bayesian set algorithm sparse binary datum type datum natural representation large dataset use evaluation section application bayesian set form datum realvalue discrete string possible especially practical statistical model member exponential family section assume item binary vector xi element independent ij conjugate prior parameter bernoulli distribution beta distribution j hyperparameter gamma function generalization factorial function query xi consist vector easy j pn n xij item score write hyperparameter explicit compute follow scorex n expression dramatically simplify use fact consider case separately contribution contribution scorex log score linear log scorex c c log log log log log log entire data set large matrix column compute vector log score point use single matrix vector multiplication c sparse datum set linear operation implement efficiently query dc correspond compute vector scalar c efficiently query sparse element q equal log independent query exponential family generalize result model exponential family distribution model write form px kdimensional vector sufficient statistic natural parameter nonnegative function conjugate prior p hyperparameter normalize distribution query xi item candidate hard score candidate p p scorex h expression help understand score compute efficiently score depend size query sufficient statistic compute candidate query sense matrix sufficient statistic correspond second score linear operation u depend log h linear second argument case distribution exponential family distribution distribution diagonal covariance gaussian score nonlinear u compute apply nonlinearity elementwise u sparse matrix score compute time linear number nonzero element u result run bayesian set algorithm different dataset grolier dataset consist text article consist movie rating user eachmovie service nip author dataset consist text article publish nip volume span conference grolier dataset article word entry number time word appear document datum column normalize word thresholde entry word frequency article mean essentially tuning hyperparameter use broad empirical prior cm c m m mean vector article c analogous prior use dataset dataset preprocesse remove movie rate people people rate movie dataset person movie entry value person movie rating possible datum column normalize account overall movie popularity size dataset preprocesse people movie finally nip author dataset word author preprocesse similarly grolier dataset column normalize author thresholde entry author use word frequently word mean author result experiment comparison set word movie query table unfortunately nip author achieve kind popularity web necessary set work effectively instead list word associate cluster author table run time algorithm dataset table experiment run matlab ghz algorithm fast preprocesse datum answer query e p t e m nip s s table dataset size dataset time preprocessing time query second q s q water c et baye et et baye et et baye et command animal animal mammal vegetation plant history art ft table cluster word find set bayesian set base query query algorithm bayesian set run use datum difficult evaluate result ground truth task person idea good query cluster differ drastically person choose compare algorithm set main currently public commonly use algorithm perform task access set algorithm impossible run method dataset set rely vast web datum important set clearly know lot movie word comparison bayesian set informative find set perform query consist item find list web cambridge college hand abstract concept table algorithm return sensible completion believe result detail like elaborate query table consist classic movie fact example query set website query movie et save et story fair rain happen query m toy l action et baye et et baye et toy toy fast space m frame toy story bug parent sound music action girl end day color family single fair game table cluster movie find set bayesian set base query query algorithm bayesian set run use dataset movie return bayesian set classic movie return set difficult save classic action movie movie query baye set movie associate query toy story movie set movie m appear movie actually r rate movie involve query bs m sm word solve minimize min query m word expectation parameter estimation query g m dm word decision action reward reward start return receive select table nip author find bayesian set base query query word associate cluster author bayesian set run use nip datum conference nip author small nip paper appear similar author find algorithm nip paper query author example case believe publish nip paper nip paper variational learning graphical model evaluation algorithm subject unlabeled result bayesian set set query eachmovie grolier dataset ask choose prefer result study table q baye et p value m table evaluate query list query item percentage prefer result bayesian set pvalue reject null hypothesis set preferable bayesian set particular query case binary datum method reduce matrixvector multiplication come heuristic matrixvector method run query use dataset description result find supplemental material author website conclusion describe algorithm query consist small set item return additional item belong set algorithm compute score item compare posterior probability item set prior probability item probability compute respect statistical model datum parameter model unknown marginalize exponential family model conjugate prior score compute exactly efficiently fact sparse binary datum score item large datum set accomplish use single sparse matrixvector multiplication fast practical bayesian algorithm need resort approximate inference example sparse datum set nonzero entry grolier query second method compare set term set completion demonstrate bayesian criterion useful realistic problem domain problem address decide size response set score probabilistic interpretation possible find suitable threshold probability future incorporate threshold problem retrieve set item clearly relevant application domain algorithm flexible combine wide variety type sequence image probabilistic model plan explore efficient implementation extension believe large dataset bayesian set useful tool application area acknowledgement thank blum useful discussion roweis datum partially support cmu reference set lafferty c probabilistic relevance model base document query generation language modeling language modeling approach relevance weighting search term j soc info griffith t l generalization similarity bayesian inference behavioral science personal communication
recover community block model know parameter electrical engineering abstract stochastic block model sbm recently gather significant attention new threshold phenomenon development rely knowledge model parameter number community paper introduce efficient algorithm require knowledge achieve optimal informationtheoretic tradeoff identify constant degree regime algorithm develop require lowerbound relative size community achieve optimal accuracy scaling large degree lowerbound requirement remove regime diverge degree logarithmic degree regime far enhance fully agnostic algorithm simultaneously learn model parameter achieve optimal exact recovery run time provide algorithm afford efficiency informationtheoretic optimality strong weak consistency sbm introduction paper study problem recover community general stochastic block linear size community constant logarithmic degree regime contrast paper require knowledge parameter learn graph provide motivation problem background model detect community cluster graph fundamental problem network computer science machine learn apply large variety complex network social biological network datum set network similarly graph attempt datum try identify group similar behavior particular finding community allow find people social network improve recommendation system segment classify image detect protein complex find relate discover new tumor subclass reference large variety community detection algorithm deploy past decade understanding fundamental limit community detection appear recently particular sbm sbm canonical model community detection use notation sbmn p refer random graph ensemble vertex v assign independently hide plant label v k probability distribution p p pk pair node connect independently probability symmetric k matrix entry note sbmn p denote random graph draw model hidden plant cluster ie label reveal goal recover label observe graph recently sbm come center attention practical level extension allow overlap community prove fit real datum set massive network theoretical level new phase transition phenomenon work focus exclusively sbm symmetric community ie community size connectivity community identical denote p probability result concern figure merit recovery exact recovery strong consistency investigate regime p q exist algorithm recover high probability community completely detection investigate regime exist recover high probability positively correlated partition sharp threshold exact recovery obtain b exact recovery solvable b efficient algorithm achieve threshold addition introduce sdp prove achieve threshold spectral algorithm achieve threshold sharp threshold detection obtain detection solvable efficiently b b p q settle conjecture detection recovery property ask partial recovery community study particular interest paper case strong recovery weak consistency vanish fraction node allow misclassifie community strong recovery possible q p q diverge extend general section discuss result general sbm interest paper problem learn model parameter conclude section provide motivation problem achieve threshold efficient universal threshold phenomenon long study field information theory capacity constrain satisfaction problem eg sit threshold particular achieve threshold generate major algorithmic development field code survey propagation likewise identify threshold community detection model key benchmark guide development cluster particularly crucial develop benchmark depend knowledge model parameter natural question solve recovery problem sbm access parameter paper answer question exact strong recovery community prior result general sbm know parameter previous work concern sbm symmetric community mainly exception provide general result sbm recently study fundamental limit general model sbmn p p independent result summarize recall recovery requirement definition recovery requirement algorithm recover detect community sbmn p accuracy output labelling node agree true labelling fraction node probability agreement maximize community strong recovery refer exact recovery refer problem solvable exist algorithm solve efficiently run note exact recovery require graph vertex degree multiple community high probability exact recovery focus fix partial strong recovery general sbm result concern regime connectivity matrix scale positive symmetric matrix generalize b study variation model average degree constant follow notion snr introduce min max min max respectively small large eigenvalue propose solve partial recovery complexity snr diverge k z p k p symmetric matrix q row equal let large eigenvalue p q eigenvalue p q small nonzero magnitude draw provide large min time run arbitrarily small independent note symmetric cluster reduce quantity interest detection snr diverge ensure strong recovery symmetric case follow important consequence previous state solve strong recovery entry q corollary z p p symmetric matrix q row equal exist c lnc sufficiently large detect community complexity optimal scaling accuracy exact recovery general sbm second result regime connectivity matrix scale q independent exact recovery sharp threshold characterize divergence function max specifically pair column distance exact recovery solvable general sbm refer section discussion connection shannon channel code ch divergence algorithm develop solve exact recovery d limit time exact recovery exact recovery solvable d p qi p qj algorithm solve exact recovery solvable run time exact strong recovery solve general sbm community parameter know remove assumption estimate parameter estimation parameter result know community logarithmic degree regime sdp agnostic parameter relaxation parameter estimate recover community regime parameter estimate threshold cycle efficiently approximate count walk community aware parallel work consider private estimation include particular logarithmic degree regime obtain procedure estimate parameter appropriate version norm general sbm learn model date mainly open small eigenvalue magnitude result agnostic algorithm develop constant diverge node degree p independent afford optimal accuracy complexity scale large node degree achieve limit logarithmic node degree particular sbm learn efficiently diverge degree note assumption p independent slightly relax example slowly grow k leave future work partial recovery main result partial recovery hold constant degree regime require lower bind relative size community requirement remove work diverge degree state corollary k z p k min symmetric matrix q row equal entry positive word nonzero probability path vertex community graph draw exist c lnc sufficiently large detect community graph draw sbmn accuracy e n time note vertex community degree probability exponential way differentiate vertex degree different community error rate decrease exponentially c optimal provide detailed version yield statement accuracy algorithm term snr general knowledge requirement remove exact recovery recall exact recovery computationally solvable sbmn p p qi p qj ij achieve knowledge parameter sbmn section solve exact recovery p exact recovery solvable use input graph question run time particular exact recovery efficiently solvable solvable proof technique partial recovery simplify version algorithm symmetric case ease presentation algorithm focus symmetric case sbm community relative size k probability connect inside community nb community let k average degree definition vertex let set vertex short path v length r drop subscript g graph question original sbm refer r v vector entry number vertex v community arbitrary vertex reasonably small r typically vertex r ab community community course hold log log d vertex graph obvious way try determine vertex community guess community n different community unfortunately vertex v independent compromise plan instead propose rely follow step randomly assign edge set e fix probability c count number edge e connect nrge formally definition v r r z subset edge let e number pair nrge note disjoint sparse graph generate independently pair vertex edge graph probability approximately independent r denote k bk p q symmetric case expect number neighbor depth r approximately r k r expect number neighbor depth r v approximately r r community scale c computation use independence e assume v community expect number edge e connect nrge approximately inner product p r k r r r r r p q matrix diagonal different community inner product permutation u simplification rr rr c b e v community order e v v large depend relative community v crr ie r r need log log k difficulty specific pair vertex term multiply random factor dependent degree v nearby vertex order stop variation term term necessary cancel dominant term bring introduce follow statistic e e e e rr d particular r odd e tend positive community negative irrespective specific value b k suggest follow partial recovery require knowledge k constant degree regime regime b scale set r log log d edge e probability set kmax select kmax random vertex compute possible assignment vertex community e vi community randomly select vertex apparent community v fail graph guess community vi maximize value e algorithm succeed long k bk ensure estimate e reliable far b scale set log allow removal knowledge requirement alternative approach count walk length like instead use e prove number walk close expect value difficult prove e desire range substantially easy v edge v v directly effect value r algorithm base short path study general case r v e previous section general case define nrge e probability approximately result e c c ev e nrge figure edge represent edge count v let h distinct eigenvalue p q order define h h h h p q correspond eigenvalue pwi projection operator wi e c crr c crr pwi p pwi ev final equality hold pwi p pwj ev p p pwj ev pwi p pwj ev imply pwi p pwj ev definition let v pwi p pwi v equation dominate rr term good estimate h term require cancel start e e e crr rr e e note left hand expression equal det e e definition let e m m matrix e e exist constant m e m cm v m m assume m simplify discussion case m m similar suggest follow plan estimate eigenvalue correspond graph pick vertex random use fact nrge c r good vertex estimate ratio m m r r look small m ratio small use estimate estimate h value estimate p qs eigenvalue select vertex use ratio finally median estimate general m m m e cm m qm e e cm m qm e m m m m cm rr m m m m m fact use approximate v arbitrary v course require r rr r large v large relative error term rr require v pwi ev p pwi ev v v v equality sufficiently good approximation v use determine pair vertex community generate reasonable classification base solely method compare vertex appropriate choice parameter later detail require compute e v vertex graph fairly large r r slow instead use fact vertex v v v v v inequality strict subtract v v v v inequality strict representative vertex community determine vertex v community need know value run fairly quickly r large r small algorithm require focus vertex lead follow plan partial recovery randomly select set vertex large contain vertex community high probability compare select vertex attempt determine community pick community nod use algorithm refer determine community remain vertex long actually vertex community initial set approximation particularly bad reasonable classification risk randomly bad classification bad set initial vertex mitigate repeat previous classification procedure time discuss complete refer detail exact recovery exact recovery similar use fact good clustering obtain classification local improvement base node neighborhood similar technique use establish sharp characterization local procedure error key result test multivariate poisson distribution mean logn logn respectively probability error posteriori decode prove case unknown parameter algorithmic approach largely unchanged add step know classification use estimate p q prior local improvement step analysis require careful necessary prove labelling error rate compute approximation p q true value probability secondly need modify hypothesis testing estimate control error probability attempt determine vertex community base estimate p q p q classification neighbor error rate vertex error rate log time high parameter p q vertex neighbor correctly need p qi p qj differentiable respect element p q error rate parameter p q bad log high error rate actual parameter combine yield conclusion error estimate parameter vertex classification bad error preliminary classification input g graph set specifically algorithm output node define graph vertex set select edge independently probability define graph contain edge g run log logn obtain classification determine size community edge density pair community node determine likely community label base degree v compute preliminary classification profile use v new estimate p q node determine likely community label base degree v compute output labelling profile step likely label maximize probability degree profile come multivariate distribution mean note require lower bind set slowly decrease function result acceptable range sufficiently large n data implementation open problem test simplified version algorithm real datum network glance obtain error rate good trial bad achieve stateoftheart describe result paper extend directly slowly grow number community eg logarithmic interesting extend current approach small sized community complexity scale overlap community approach paper apply overlap reference e c community detection general stochastic block model fundamental limit efficient recovery algorithm appear asymptotic analysis stochastic block model modular network algorithmic application phy rev e community detection threshold weak property stoc annual symposium theory compute page proof block model threshold conjecture available online e exact recovery stochastic block model appear ieee transaction information theory available e consistency threshold binary symmetric block model appear stoc tradeoff plant problem submatrix localization grow number cluster p m blei efficient discovery overlap community massive network proceeding stochastic step social network m graph algorithm good average case behavior solution random nphard problem polynomial expect time algorithm mark graph discrete apply mathematic r m algorithm graph partition plant partition model lecture note computer science t b estimation prediction stochastic graph latent block structure journal classification spectral partitioning random graph foundation computer science proceeding ieee symposium page p nonparametric view network model proceeding spectral clustering annal statistic d choi m grow number class biometrika page simple algorithm find hide partition available online achieve exact cluster recovery threshold random laplacian matrix relaxation arxiv accurate community detection stochastic block model spectral algorithm e belief propagation robust reconstruction optimal recovery block model r community detection sparse network inequality p rao stochastic block model community detection sparse spectral algorithm optimal rate recovery e stochastic block model reconstruction available online smith private estimation sparse graph preparation e c recover community general stochastic block model know parameter c m l spectrum random detection graph available p community detection network use graph distance eprint n spectral technique color random compute page h h c achieve optimal misclassification proportion block model
emergence topography complex cell property natural image use extension abstract independent component analysis natural image lead emergence simple cell property le linear filter resemble wavelet gabor function paper extend explain property cell decompose natural image independent subspace instead scalar component model lead emergence phase shift invariant feature similar complex cell second define topography linear component obtain topographic distance component define higherorder correlation component close topography strongly dependent lead simultaneous emergence invariance similar complex cell property introduction fundamental approach signal processing design statistical generative model observe signal approach useful model property neuron primary sensory area basic model consider express static image x linear superposition feature basis function bi si stochastic coefficient different image model consist determine value si sufficient number observation image practice image patch restrict basic case form invertible linear system invert inverse filter wi denote p hoyer identify receptive field model simple cell si activity present image patch basic case assume si nongaussian mutually independent type decomposition independent component analysis sparse code olshausen field model estimate input datum consist patch natural scene obtain filter principal property simple cell vi localize orient bandpass selective compare quantitatively obtain filter measure recording macaque cortex find good match parameter paper simple extension basic model explain emergence property cell topography invariance complex cell space limitation basic idea paper detail find use method feature subspace model response complex cell norm projection input vector image patch linear subspace equivalent classical energy model maximize independence norm projection energy obtain feature localize space orient bandpass simple cell gabor analysis contrast simple linear filter obtain feature subspace emergence phase invariance limited shift translation invariance maximize independence equivalently sparseness norm projection feature subspace allow emergence exactly invariance encounter complex cell second extend model independent subspace overlap subspace subspace correspond neighborhood topographic grid topographic define topographic organization component component far grid independent contrast component near independent strong higherorder correlation model emergence complex cell property topography image data independent subspace complex cell addition simple cell model basic important class cell vi complex cell principal property distinguish complex cell simple cell phase invariance limited shift invariance purpose model paper explain emergence phase shift invariant feature use modification model modification base combine principle subspace model multidimensional independent component analysis invariant feature subspace principle subspace state consider invariant feature linear subspace feature space value invariant higherorder feature square norm projection data point subspace typically span feature feature subspace linear subspace represent set orthogonal basis vector m m dimension subspace value fi feature input vector fi wi square root emergence property use extension fact equivalent compute distance input vector general linear combination basis vector filter feature subspace principle combine competitive learning technique lead emergence invariant image feature multidimensional independent component analysis multidimensional independent component analysis linear generative model assume contrast ordinary lea component response assume mutually independent instead assume si divide couple triplet general mtuple si mtuple dependent different mtuple allow mtuple si correspond m basis vector probability density mtuple si specify advance general definition multidimensional following let denote j number independent feature subspace set index si belong subspace index independent feature subspace subspace embed multidimensional independent component analysis consider probability distribution mtuple si symmetric ie depend norm word probability density mtuple index express function sum square e sj simplicity assume far equal subspace assume data consist observe image patch logarithm likelihood l datum model express l sj probability density mtuple si matrix contain filter column basic lea datum allow consider orthonormal imply log det likelihood function norm projection subspace index span orthonormal basis set e sj norm projection visual datum practically subspace distribution need choose probability density p model sparse ie example use follow probability distribution logp consider multidimensional version exponential distribution estimation model consist find subspace norm projection whitened datum subspace maximally sparse distribution introduce independent feature subspace analysis natural generalization ordinary lea fact projection subspace reduce ie projection d sub model reduce ordinary lea p hoyer provide addition independent component assume distribution expect norm projection subspace represent higherorder invariant feature exact nature invariance specify model emerge input datum use prior information independence independent subspace analysis apply natural image datum identify norm projection response complex cell individual filter vector identify receptive field simple cell interpret hierarchical model complex cell response compute simple cell response si manner similar classical energy model complex cell experiment model lead emergence invariance encounter complex cell topographic lea independent subspace analysis model introduce certain dependence structure component si let assume distribution subspace sparse mean norm projection time near case example density subspace specify model imply component si sj belong subspace tend nonzero simultaneously word s positively correlate structure dependency natural datum image datum note s generalize model define model kind dependence mtuple neighboring component neighborhood relation define topographic order different generalization base explicit generative model define model follow neighborhood function express strength connection ith jth unit neighborhood function define way selforganize map neighborhood define onedimensional twodimensional d neighborhood square simple example define d neighborhood relation hij m constant m define width neighborhood function similar role independent component classic image datum datum sparse structure g choose independent subspace analysis property topographic lea model consider simplicity case sparse datum basic property component uncorrelate easily prove symmetry argument variance define equal unity classic second component s near h significantly nonzero emergence property use extension tend active nonzero time word energy s positively correlate latent variable far practically independent higherorder correlation decrease function distance assume neighborhood define way similar detail let note definition topography higherorder correlation different use practically exist topographic mapping method usually distance define basic relation euclidean correlation interestingly principle possible define topography set orthogonal vector euclidean distance equal orthogonal vector actually encounter lea basis vector filter constrain orthogonal whitened space experiment natural image datum apply method natural image datum datum obtain pixel image patch random location depict scene animal forest preprocesse consist remove component reduce dimension datum detail experiment fig basis vector feature subspace complex cell subspace dimension choose basis vector associate single complex cell approximately orientation frequency location identical close phase differ considerably feature subspace consider generalization filter pair find classical energy model enable cell selective orientation frequency invariant phase somewhat invariant shift use dimension instead greatly enhance shift invariance feature subspace topographic lea neighborhood function define neighborhood consist square unit d torus lattice obtain basis vector fig basis vector similar obtain ordinary lea image datum addition clear organization addition connection independent subspace analysis clear neighbor basis vector fig tend orientation frequency location near contrast phase different mean neighborhood basis vector simple cell similar independent subspace function complex cell demonstrate detail discussion introduce extension lea especially useful image model model use subspace representation model invariant feature turn independent subspace natural image similar complex cell second model extension independent subspace model topographic lea model generative model combine topographic mapping lea topographic mapping distance representation space topographic grid relate measure distance represent component topographic lea distance represent component define higherorder correlation p hoyer natural distance measure context approach closely relate adaptive subspace selforganize map emergence shift invariance conditional restrict consecutive patch come nearby location image input datum temporal structure smoothly change image sequence similar development contrast theory formulate explicit image model independent subspace analysis model emergence complex cell property possible use patch independently select location prove information static image explain property complex cell extend subspace model model topography emergence topography complex cell property explain single principle neighboring cell strong higherorder correlation reference sejnowski independent component natural scene edge filter vision research cardoso multidimensional independent component analysis acoustic speech signal processing seattle wa p independent component analysis new concept signal process p learn invariance transformation sequence neural computation hyvarinen p hoyer topographic independent component analysis submit available hyvarinen p hoyer emergence phase shift invariant feature decomposition natural image independent feature subspace neural computation press hyvarinen p hoyer m independence assumption analyze independence component topography m advance independent component analysis springerverlag press hyvarinen e oja fast fixedpoint algorithm independent component analysis neural computation c j blind separation source adaptive base architecture signal process t selforganize emergence detector selforganize map biological cybernetic factorize multivariate function class advance neural information processing system volume page mit press b olshausen field emergence simplecell receptive field property learn sparse code natural image nature e p simoncelli schwartz model surround neuron normalization model advance neural information processing system page mit press independent component filter natural image compare simple cell primary visual cortex emergence property use extension lea figure independent subspace natural image datum model basis vector image window group basis vector correspond independent feature subspace complex cell basis vector subspace similar orientation location frequency contrast phase different figure topographic lea natural image datum basis vector basis vector similar orientation location close phase near basis vector different neighborhood property similar complex cell
exponentially local minima single neuron manfre mark herbster abstract single neuron logistic function transfer function number local minima error function base square loss grow exponentially dimension introduction consider single artificial neuron input neuron weight e rd output neuron input pattern e rd w r r transfer function sequence training example consist pattern e r d desire output yt e r goal training phase neural network consist minimize error function respect weight vector e rd function sum loss output neuron desire output sum training example notation error function m l t r x r loss function common example transfer function logistic function logistic z bounded range contrast identity function z range common loss function square loss ly example absolute loss l square loss logistic function error function single neuron training example ln dj local minima generally hold loss transfer function composition loss function transfer function notation ly continuous bound range exponentially local minima single neuron figure error function local minima visible generate twodimensional example prove transfer function bounded range exponentially local occur loss function square loss sequence example use proof property sense weight vector e r d error function neuron produce desire output example minimal assumption loss transfer function single neuron local minima global minimum example realizable transfer function logistic function suggest literature use entropic loss artificial neural network place square loss case error function single neuron minimum case generalize observation define matching loss differentiable increase transfer function ly loss area depict figure identity function l square loss likewise logistic function l entropic loss matching loss gradient descent update minimize error function sequence example simply t positive learning rate second derivative easy calculate general set hessian respect w t p auer m herbster m figure b matching loss function l b square loss saturate entropic loss t positive semidefinite increase differentiable transfer function clearly hessian error function sequence m example positive semidefinite follow differentiable increase transfer function error function respect matching loss case neuron logistic function pair square loss lead exponentially minima open number grow exponentially natural datum problem pair logistic square loss hard optimize error function gradient base method problem flat region consider example consist pattern equal vector desire output square loss y e r d turn flat function logistic w approach example figure b easy bounded transfer function finite number corresponding bound loss function square phenomenon occur word composition ly loss bounded transfer function finite number turn flat large similarly multiple example error function e w define flat flat region gradient respect weight vector small update weight vector hard time weight vector flat region phenomenon easily observe practice saturation contrast logistic function pair entropic loss figure b error function turn flat global minimum hold increase differentiable transfer function matching loss function number previous paper discuss condition necessary sufficient multiple local error function single neuron small network ss gt previous work discuss occurrence multiple paper number minima grow exponentially dimension previous work mainly limit demonstration local minima network neuron use hyperbolic tangent logistic function square loss exponentially occur composition loss function transfer function continuous bounded paper outline follow preliminary section gi ve formal exponentially local minima single neuron ll e log figure error function logistic transfer function square loss example set combine statement proof result mention section section onedimensional example result local minima error function figure error function onedimensional example local minima dimension follow easily ddimensional example result ln local minima error function figure discussion section consider neuron bias section ie add additional input clamp error function sequence example m b denote bias weight input clamp prove error function local minima loss transfer function symmetric hold example square loss logistic transfer function proof omit space constraint paper additional result general loss transfer function finally section minimal assumption transfer loss function minimum error function sequence example realizable essence proof simple observe transfer function bound domain unbounded exist area saturation error function essentially flat furthermore error function additive ie error function produce example simply error function produce example add error function produce example local minima remain local minima fall area saturation similarly local minima remain local minima figure b way set local minima combine preliminary introduce notion minimumcontaine set prove useful count error function p auer m herbster m definition let continuous function open bounded set e rd set w boundary u w e u obviously set contain local minimum ve function furthermore disjoint set contain distinct local minimum sufficient find disjoint minimumcontaine set order function neuron consider transfer function loss function l follow transfer function loss function r x property ly e r r continuous bounded minimum example dimension theorem let l satisfy pi n sequence n example xi e r y e r esw distinct continuous e r value w distinct furthermore assume loss generality set error function infinitely local follow immediately set finitely ly oo exist bound continuous use fact follow lemma state new minimumcontaine set add example area saturation error function assume ly exist let xi sequence example wi esw t let sufficiently large wi proof sufficiently large condition l satisfied ie oo recall esw ly s analogously exponentially local minima single neuron equation hold t t lemma follow proof theorem follow induction interval minimumcontaine set error function remark proof require magnitude example arbitrarily large sized figure curse dimensionality number grow exponentially dimension combine obtain let r continuous function disjoint minimumcontaine set set e n d disjoint set function rd r proof omit let l satisfy pi sequence example rd e r esw distinct local exist sequence onedimensional example disjoint set error function es disjoint minimumcontaine set s proof minima neuron bias let transfer function loss function satisfy b e r e r y e r furthermore let continuous second derivative assume derivative nonzero let ly continuous sequence example xi r d e r r es b o l distinct local note square loss hyperbolic logistic transfer function satisfy condition parallel proof magnitude example arbitrarily small p auer m herbster m minimum realizable case transfer loss function monotone example realizable single minimal surface sequence example realizable esw rd theorem let l satisfy p furthermore let r r r assume sequence example s weight e rd wi e rd function h increase minimum connect line segment esw proof let suffice monotonically increase r e r let monotone z r r r ar acknowledgment thank valuable discussion gratefully acknowledge support grant manfre warmuth support nsf grant reference p auer m herbster m exponentially local minima single neuron technical report approximation boolean function sigmoidal network function neural computation j propagation fail separate perceptron succeed ieee transaction circuit system e supervise learning probability distribution network information processing system page physics problem local transaction pattern analysis machine intelligence s neural network comprehensive s solla e m accelerate learn layer complex system ss rise network hide layer complex system propagation separate perceptron network r l comparison square error relative entropy metric use optimization algorithm complex system bs s denker strategy teach layer network classification task information processing system page new physics
control selective visual attention model pathway computation abstract intermediate high vision process require selection subset available sensory information processing usually selection implement form spatially region visual field socalled focus attention scan visual scene dependent input attentional state subject present model control focus attention primate base saliency map mechanism expect model functionality biological vision essential understand complex scene machine vision introduction vision generally accept fact computation early vision massively parallel operation ie apply parallel visual field high degree parallelism high vision number different possible combination feature necessary select instantaneous sensory input detailed processing discard rest mechanism visual selective attention present address selective visual attention model pathway clear similar selection mechanism require machine vision analysis simple visual scene attentional mechanism slowly introduce field e g use sequential focus attention context face recognition model eye path generation characterize strong topdown influence present rao volume sequential apply abstract space dynamic complex system optimization problem large number volume primate vision organize major anatomical pathway concerned mainly object recognition reason pathway anatomical reason know ventral principal task major pathway determination location object pathway anatomical reason previous work koch present model implementation pathway underlying mechanism temporal assume attend region visual field distinguish temporal neuronal spike train temporal achieve introduce moderate level correlation neuron respond attend stimulus obtain suggest simple neurally plausible mechanism common input cell respond attend stimulus excitatory input increase postsynaptic cell fire short time receive input increase correlation spike train necessarily increase average firing rate subject present study provide model control system generate input possible construct integrate system attentional control base neurally plausible element compatible physiology primate visual system system scan visual scene identify salient possible task find face image confident model far understanding function biological vision relevant technical application simple model overall structure figure overview model pathway input provide form image camera analyze feature map map organize know operation early visual cortex implement different spatial scale centersurround structure visual receptive field different spatial scale implement center similar model develop use periodic modulation present model adapt type modulation koch receptive field correspond value pixel level surround corresponding pixel level level image normal size feature implement far principal component primate color vision intensity orientation temporal change short description different feature map present section section address question integration input saliency map topographically organize map code instantaneous different visual field feature map multiscale figure overview model feature compute centersurround difference different spatial scale feature map combine integrate saliency map sm provide input array integrateandfire neuron global inhibition array functionality winnertakeall network provide output v feedback saliency map curve arrow input feature intensity intensity information obtain chromatic information r g red green blue channel respectively intensity obtain g b entry feature map control selective visual attention model pathway contrast correspond roughly sum cell opposite phase ie note present version model reproduce temporal behavior update activity feature map change visual input neglect temporal filter property input neuron chromatic input red green blue pixel value compute g pixel compute quantity correspond cell primary visual cortex instance filter compute pixel value subtract surround finally absolute value result orientation intensity image convolve gabor patch angle degree respectively result convolution array scalar level pyramid average orientation compute weighted vector sum component sum unit vector correspond orientation weight weight result convolution respective gabor patch image let vector center pixel iii c average orientation vector surround compute analogously enter centersurround difference ie scalar product c scalar quantity correspond centersurround difference orientation location account relative strength orient edge change appearance object segregation object background capture attention stimulus background incorporate attentional capture visual onset motion add temporal derivative input image sequence account chromatic information precisely compute time time difference tit tit gt tit bt tit topdown information model implement essentially bottomup strategy rapid selection visual field model high cognitive function straightforward incorporate topdown influence instance task subject attend selectively visual field instruction implement additional input corresponding saliency map e saliency map existence saliency map suggest master map idea topographically organize map encode information salient object locate visual field object task saliency map computation location visual field subsequent selection salient area object time area select feature map provide current input saliency map output saliency map consist spike train neuron correspond select area topographic map project ventral pathway mechanism tag modulate temporal structure neuronal signal correspond attend koch fusion information relevant feature compute feature map combine yield scalar quantity model solve task simply add activity different feature map compute section constant weight choose weight identical input obtain temporal change obvious great importance change stimulus capture attention select weight time large internal dynamic trajectory generation definition activity location saliency map represent relative corresponding location visual field time maximum map salient stimulus consequence stimulus focus attention direct allow detailed inspection powerful high process available massively parallel feature map mean determine instantaneous maximum map maximum select application winnertakeall mechanism different mechanism suggest implementation neural network model use integrateandfire neuron strong global inhibition inhibitory population reliably activate neuron layer cell fire inhibit cell include neuron strong input generate sequence action potential neuron static image system far attend continuously stimulus observe biological vision desirable functional point view instead inspection point usually reason long salient point attend achieve behavior introduce feedback spike occur network integrator saliency map control selective visual attention model pathway receive additional input spatial structure ie difference gaussian inhibitory center location winner inhibit saliency map consequently attention switch location function ofthe positive invert avoid excessive focus attention location nearly equal close present focus attention jump close location distant result study system input construct analogously typical visual psychophysical stimulus obtain result agreement experimental space limitation prevent detailed presentation result report fig example realworld image choose example image caltech trajectory focus attention follow model salient feature image red banner wall building center image focus attention direct salient feature system start scan image order decrease saliency jump follow initial red banner jump drive strong mechanism experimental evidence obtain recently area figure example image black line trajectory simulated focus attention time ms jump center red banner building different location decrease saliency conclusion present report prototype integrate system mimic control visual selective attention model compatible known physiology primate visual system different communicate signal neurally plausible model identify salient point visual scene scan scene order e koch decrease saliency allow control subsequently activate processor specialize detailed object recognition present saliency determine combine input set map fix weight future work generalize approach introduce plasticity weight adapt system task acknowledgement work support office naval research research national science foundation center engineering national science center program office strategic technology reference p pyramid method image process visual motion attentional capture perception shift selective visual attention underlie neural circuitry human koch c model neuronal implementation selective visual attention base temporal correlation neuron computational c model neural basis attention vision research m attention m attention neuronal response area posterior parietal neurophysiology feature object e multiscale attentional framework relaxation neural network d mozer m c m e ed advance neural information processing system vol model scan path apply face recognition n winnertakeall mechanism base presynaptic inhibition feedback neural computation
low bound rate convergence cut plane method dept compute dept computer dept statistic dept computer abstract recent paper joachim present svmperf cut plane method train linear support vector machine svms converge accurate solution iteration tighten analysis teo iteration suffice impressive convergence speed number practical problem conjecture rate far improve paper conjecture present counter example applicable train linear svms hinge loss hold support vector method optimize multivariate performance score surprisingly problem inherently hard exploit structure objective function devise algorithm converge iteration introduction interest machine learn past decade success binary support vector machine svms drive numerous application recently increase interest support vector learn linear model heart svms follow regularize risk minimization problem min kwk remp z z regularizer empirical risk assume access training set label example yi rd p use square euclidean norm kwk wi regularizer parameter control tradeoff empirical risk regularizer significant research develop specialized optimizer minimize jw efficiently award win paper joachim present cut plane method svmperf converge accurate solution iteration iteration require ond effort improve teo bundle method regularize risk minimization encompass svmperf special case converge accurate solution ond time online learning method increasingly popular solve key advantage svmperf ability directly optimize nonlinear performance measure score regression loss widely use application area case remp decompose sum loss individual datum point employ batch algorithm let denote multivariate discrepancy correct label candidate later remp multivariate measure formulate label paper use term cut plane method denote specialized solver employ machine learn clearly relate cut plane method use optimization award win paper joachim regularize risk minimization problem correspond measure optimize use widespread use machine learn important understand convergence guarantee term upper low bound number iteration need converge accurate solution tight o upper bound convergence speed teo analyze restricted version optimize dual variable iteration practical problem observe rate convergence significantly fast predict theory conjecture upper bound far tighten analysis paper construct counter example decomposable remp equation remp equation require iteration converge conjecture work prototypical point include svmperf special case result lead follow natural question low bound hold regularize risk minimization problem fundamentally hard inherent limitation word solve problem exist solver require effort d provide partial answer understand contribution need understand standard assumption prove convergence rate data point lie l euclidean ball radius r r subgradient remp bound ie point w exist subgradient remp g clearly assumption restrictive adapt result devise ond algorithm case assumption hold find fast optimizer assumption remain open problem notation low bold case letter denote vector wi denote ith component refer vector component ei coordinate vector s ith coordinate refer dimensional simplex specify p h denote euclidean dot product xi refer euclidean norm kwk wi denote r r paper structure follow briefly review section type low bound subsequently define section section contain description counter example construct section describe algorithm provably converge accurate solution iteration assumption paper conclude discussion section technical proof concept use paper find appendix iteration replace remp piecewise linear lower bind optimize kwk max ai bi ik obtain iterate ai remp denote arbitrary subgradient bi remp ai piecewise linear lower bind successively tighten gap min fall predefined tolerance convex objective function compute dual instead minimize respect equivalently maximize dual dimensional simplex specialized nature solver low bound general optimizer study nemirovski apply qpbmrm solve inner loop solve inner loop exactly qp approximately line search require previous subgradient ai intercept bi set require previous subgradient ai intercept bi set bk set argmax h k return wk return wk set note bk define maximize quadratic programming qp problem find note iteration dual qp variable number iteration increase size increase order avoid grow cost dual optimization iteration propose use onedimensional line search calculate approximate k line segment variant pseudocode find refer reader detail solve expensive optimization problem iteration teo variant converge rate suppose assumption hold converge accurate solution measure follow number step log generality thank formulation use remp applicable wide variety remp example use train binary svms remp specify yield exactly svmperf apply optimize multivariate score score remp specify immediately lead optimizer upper low bound rate convergence discuss machine learn community upper bound important define meaning low bind respect study relationship upper bound important clarify important technical point instead minimize objective function define minimize scale version scale approximation gap c assumption fix degree freedom bound scale objective function function optimization algorithm suppose iterate produce algorithm minimize define t step index accurate solution min upper low bound property pair function upper bind function order step reduce gap t g hand low bound define different way depend universal flip initial point matter good case start optimal solution quantity interest actually start point loss generality assume way initialization assume assume wlb open o open table summary know upper bound low bound note versa wlb vice versa tight match wlb strong low bound exist function step find accurate solution h weak lower bind wlb wlb exist function depend step find accurate solution h clearly existence imply wlb usually hard establish wlb fortunately wlb sufficient upper bound establish tightness size function class affect upper low bound opposite way suppose prove upper resp low bound usually easy resp hard prove upper resp low bound construct low bound let minimizer jw interested bound primal gap iterate jwk dataset construct explicitly result objective attain low bound algorithm remp hinge loss score cover result summarize table note assumption imply imply wlb entry table imply strong low bound solve linear svms use prove lower bind problem assumption consider dimensional training set example set regularize risk write use instead w scalar h minimizer verify fact subdifferential jw choose k ie jwk converge proof rely lemma iterate generate satisfy follow recursive relation follow recursive relation hold true k k wk coordinate proof available appendix b recursive relation allow derive convergence rate proof appendix combine approach rate finally straightforward translate rate jwk approach proof weak low bound solve linear svms use theorem upper bind convergence rate assume remp satisfie assumption section far demonstrate rate wlb tight remp specialize svm objective satisfy define construct yi correspond objective function jw remp easy minimizer fact simply pn check xi set yield subgradient key result follow theorem let suppose run qpbmrm objective function produce iterate wk qpbmrm step find accurate solution formally k k n step wn cut subgradient minimizer exactly pn proof remp yi choose remp argmin general claim kth iterate produce qpbmrm prove claim induction assume claim hold true step p easy check remp remp choose remp wk copy argmin o verify check ai kwk max bi remain observe jwk claim z follow subgradient remp euclidean norm run remp contain subgradient norm restrict feasible region jw satisfy assumption optimal solution change essentially local satisfaction fact bounded subgradient remp sufficient qpbmrm converge rate assume restrictive remain open question determine rate optimal qpbmrm objective leave open qpbmrm svms weak low bound optimize score use score define use contingency table define construct yi follow e ei y d e contingency table positive training example correspond objective function jw max step find accurate solution let e qpbmrm jwk min ik proof rigorous proof find appendix e provide sketch copy prove induction assume hold step step k xi convenience define term max xi maximize hard follow assignment correct labeling positive training example negative training example positive equal misclassifie positive training example assignment proof consider case xi correctly label positive example misclassifie t example xk suppose t t example positive t k ik t t t t t t t t t t k t copy nk copy misclassifie xk pick e remp wk z argmin max bi verify ai set hold step end induction remain observe n follow claim ond algorithm train binary linear svms low bound prove require iteration converge inherent limitation artifact problem demonstrate devise algorithm problem converge iteration key difficulty stem objective function render second high order algorithm thank theorem dual function lipschitz continuous gradient easy optimize formalize idea use dual abstract objective composite form objective function use machine learn model g q close convex set strongly convex function correspond regularizer stand output linear model g encode empirical risk measure discrepancy correct label output linear model let domain know theorem mild constraint adjoint form g q satisfie example binary svms bias let pn yi b correspond adjoint form turn know svm dual objective function row example multivariate score denote matrix kwk p recover primal objective correspond g variate performance measure adjoint form q n series paper develop optimal gradient base method minimize composite objective primal adjoint sequence produce assumption duality gap jwk reduce step refer reader detail efficient projection training model optimal gradient method apply nesterovs challenge require efficient subroutine compute projection set constraint q projection euclidean projection bregman projection example binary svms bias case need compute euclidean projection q define entail solve quadratic programming problem diagonal hessian box constraint single equality constraint present algorithm task section plug algorithm describe note intermediate step algorithm compute ond time directly yield ond detailed description algorithm available example multivariate score dimension q exponentially large euclidean projection intractable resort bregman projection differentiable function q point direction define bregman projection argmin p scale factor choose negative entropy log distribution possible labeling ai hxi ai constant scalar solver request expectation ai turn require marginal distribution p straightforward graphical model decompose fortunately multivariate score define contingency table possible compute marginal time use dynamic programming cost similar propose detail dynamic programming section conclusion widely employ machine learn especially context structured prediction upper bound rate convergence know low bound study paper set fill gap exhibit counter example binary classification require iteration example substantially different require increase number class lower bind fundamental algorithm artifact problem devise algorithm technique algorithm assume dataset contain ball bounded radius assumption section devise o algorithm restrictive assumption remain open problem important note linear time section key obtain computational complexity binary svms bias mention section method independently author include early know reference good knowledge recent work optimization focus improve practical performance machine learn expect linear time algorithm randomized median find choose optimizer machine learning task tradeoff number potentially requirement popular choice interested classification accuracy require deterministic guarantee online batch technique combine stochastic subgradient descent good choice dependence bad bound independent point early algorithm applicable empirical risk decompose example hand employ coordinate descent dual sequential minimal optimization algorithm matrix obtain stack matrix strictly positive definite require iteration iteration cost ond effort matrix strictly positive definite obtain log bind number iteration dependence prohibitively expensive large dependence achieve use interior point method require iteration time complexity iteration reference joachim train linear svms linear time datum mining page h teo v q bundle method regularized risk minimization learn t joachim support vector method multivariate performance measure machine learn page lecture optimization basic course springer nemirovski problem complexity method efficiency son method unconstrained minimization problem rate convergence math low bound rate convergence cut plane method long version analysis nonlinear optimization theory example book mathematical society excessive gap technique nonsmooth optimization gradient method minimize composite objective function technical report core discussion paper regularize risk minimization nesterovs accelerate gradient method algorithmic extension empirical study httparxivorgab joachim large margin method structured interdependent output variable mach learn training structural svms machine joachim learn p n algorithm constrain class quadratic program subject upper low bound mathematical programming r new algorithm linearly constrain quadratic program subject low upper bound mathematical programming series b duchi shalevshwartz singer efficient projection ball learn high dimension machine learn page shalevshwartz singer srebro primal estimate subgradient machine learn page agarwal p ravikumar m wainwright low bound oracle complexity optimization neural information process system page minimal optimization fast algorithm train support list u line search editor proc annual computational theory m method massive support optimization
organize principle layered network height informationtheoretic optimization principle propose development processing stage perceptual network principle maximum information preservation state signal transformation realize stage maximize information output signal value stage convey input signal value stage subject certain constraint presence processing noise quantity maximize shannon information rate provide motivation principle simple model case derive consequence discuss algorithmic implementation principle lead biologically relevant neural feature topographic map map distortion orientation selectivity extraction spatial temporal signal correlation possible connection informationtheoretic principle principle minimum entropy production suggest introduction paper describe property propose informationtheoretic organize principle development layered perceptual network purpose paper provide intuitive qualitative understanding principle lead specific property signal transformation simple model case detailed analysis require order apply principle case involve realistic pattern signal activity specific constraint network connectivity section brief summary result motivate formulation organize principle principle maximum information preservation later section principle state consequence study previous work l analyze development layered network model cell feedforward connection strength change accordance hebbtype synaptic modification rule find development process produce cell selectively certain input feature property progressively sophisticated proceed deep cell layer property include analysis contrast edge orientation qualitatively similar property observe layer happen hebbtype adjust synaptic strength depend correlation signal activity cause develop perceptual network optimize property connect network function information processing system analysis suitable hebbtype rule cause cell layered feedforward network lateral connection develop statistical variance output activity response ensemble input previous layer maximize subject certain constraint cell perform operation similar principal component analysis pca approach use statistic expose regularity clustering present highdimensional input data oja early demonstrate particular form hebbtype rule produce model cell implement pca exactly furthermore linear device transform input output particular output value use optimal estimation theory good estimate input value rise output device find appropriate hebbtype rule generate device good estimate come close match input value s certain condition cell property output preserve maximum information input value maximum information preservation result suggest possible principle development layer perceptual network s principle apply cell network respond input nonlinear fashion lateral feedforward connection present feedback later early layer absent formulation principle maximum information preservation state layer cell connect provide input layer m connection develop transformation signal l m presence processing noise property set output value m convey maximum information input value l subject constraint range lateral connection processing power cell statistical property ensemble input assume stationary particular achieve maximization depend statistical property quantity maximize shannon information rate equivalent statement principle transformation choose minimize information convey input value l know output value m regard set input signal value l time input message message process output message m message set realvalued signal activity noise introduce process input message generate range different output message process set connection information rate ie average information transmit m message r log discrete message space pel pm probability input resp output message l resp m joint probability input l output m continuous message space probability replace probability density sum state integral rate write pl log average information convey message land average information convey message l know m fix property input ensemble maximize r mean minimize state information rate r write l define land m form heuristically useful suggest attempt r large possible simultaneously m large l small term m large message m occur equal probability term small transform unique m generally small sharpen pm distribution l pm small set message m gain insight biologically relevant property l m transformation follow principle maximum information preservation infomax principle network l m transformation function value variable connection strength allow connection layer cell search space large particularly gain intuitive qualitative understanding network behavior consider simple model dimensionality land signal space greatly reduce infomax analysis exhibit feature important general condition relevant biological synthetic network section organize follow model introduce land m message transformation simple form principle find satisfied simple geometric condition transformation meet relate model analysis signal processing noise network formation topographic map discuss model apply simplify version biologically relevant problem emergence orientation selectivity main property principle model realize certain local algorithm propose generate topographic map use lateral interaction simple geometric model model input message l describe point lowdimensional vector space output message m number discrete state definiteness l space twodimensional extension high dimensionality straightforward l m transformation consist step noise process alter l message l lie neighborhood radius center alter message l map deterministically output message m l m mapping correspond partitioning l space region label output state m exclude priori possibility multiple disjoint region label m let denote total area state space m let m denote area l space label m let denote total border length region label m share region m label point l lie distance border map noise process l l point l distance border map region contain suppose v sufficiently small partitioning interest area occupy l state small compare total area consider case pel uniform l information rate r use approximately term order note pm pm log l log log positive number value depend detail noise process determine pm l l function distance border small low noise term m rhs dominate maximize pm value equal m second term sign equal maximize sum border length m region minimize correspond sharpen pm distribution early general discussion suggest infomax solution obtain partition l space m value substantially equal area tend border length simple analysis apply case plausible comparable spatial scale m region favor m region approximately extent direction order sharpen l reduce probability noise process mapping l different m state pel nonuniform result equal area minimum border obtain area element weight local value pel principle tend produce map great representation output space region input space activate frequently lateral interaction m layer affect result let suppose l m mapping process step l m m step step change output m number state m definition comprise mneighborhood m consider case mneighborhood relation symmetric type lateral interaction state cause favor solution m region share border l space sense define simple example state m include equal chance final state m infomax tend favor mneighborhood similar extent direction l space relation geometric model network property previous section deal certain class transformation message space specific reference implementation transformation network processor cell feature discuss previous section relate network property simplicity suppose twodimensional layer uniformly distribute cell signal activity cell time active need specify ensemble input pattern let consider simple case pattern consist disk activity fix radius arbitrary center position background case pattern fully define specify coordinate disk center twodimensional space previous section pattern represent point coordinate suppose input pattern consist sharply define disk activity fuzzy disk boundary center position sharply define pattern generate choose specify distribution position disk center set activity cell position probability decrease distance pattern describe coordinate center activity value describe example moment activity pattern relative noise process l l suppose activity l cell cell layer probability set distort activity value message l suppose set output activity m deterministic function l construct situation appropriate choice noise level dimension state space define disk center coordinate large variance compare variance induce noise process dimension variance comparable induce noise word center position pattern change small noise process compare typical difference center position pattern value attribute input pattern differ value typical input pattern differ attribute lose noise distance l state geometric model previous section correspond likelihood l state change noise process heuristically regard state space present example dimension thin dimension general space complicated topology noise process treat define simple metric structure state space complication scope present discussion example simple illustrate feature key understand operation principle character ensemble statistic noise process jointly determine attribute input pattern statistically significant large variance relative variance induce noise principle select number significant attribute encode l m transformation turn description output state space m assume space low dimensionality example m pattern disk activity center define tolerance discrete set value use label restrict form output activity particular way restrict consider encoding l m encoding use shape output pattern detailed activity value restriction form output determine feature input pattern encode topographic mapping use property emerge operation previous section infomax principle tend lead partitioning l space m region equal area pel uniform coordinate disk center minimum border length present case mean m region tend long dimension state single m value represent point ill l space differ coordinate nonuniform area region l tend inversely proportional pel furthermore local lateral connection m cell depend particular form interaction m state correspond nearby localized region activity sense previous section case mapping coordinate l space m space tend topographic example orientation selectivity temporal feature map simple example previous section illustrate lead topographic map map distortion provide great representation region l large pel let consider case information input feature encode output layer result consider model case ensemble pattern present input layer l pattern consist rectangular bar activity fix length width background bar center position orientation choose pattern uniform distribution spatial interval position orientation angle ie bar sharply define fuzzy sense describe assume property distinguish different pattern ensemble position orientation lose noise sense discuss simplify representation solution far assume coordinate need describe center position bar ensemble example ensemble consist bar pattern coordinate center position differ x coordinate orientation represent input state point rectangle state space define previous section coordinate angle horizontal rectangle identify orientation identical interior rectangle think surface horizontal number different x position range x value input ensemble divide tolerance measure noise process l l similarly relative length mj state space rectangle discuss case role result mapping reverse complicate feature note interest clarity include present analysis horizontal bar pattern horizontal distance small compare bar length likely render noise process vertical bar pattern horizontal distance large compare bar width hamming distance number binary activity value need alter change pattern great case distance l state space receptive field figure orientation selectivity simple model input domain size text reduce upper left upper right c low left figure infomax favor emergence l m map low right figure solution obtain apply relaxation algorithm dot mapping problem state great case lead simple rectangular state space ignore effect account treatment emergence orientation consider l m transformation consist process discuss deterministic l m m m step map twodimensional state space point onedimensional m state space present discussion consider l m map satisfy follow point correspond m state space uniformly topographic order line state space recall represent surface horizontal pitch slope remain determine principle mneighborhood m state previous section correspond interval state l map state particular mneighborhood close l space corresponding interval portion set l state mneighborhood center m input domain m rectangular shape lie surface previous section tend produce map equal topographic organization input domain mneighborhood similar extent direction space choice enforce explicitly satisfied choose dx input domain square mneighborhood size figure map output m encode information bar center position independent bar orientation size m neighborhood relatively large case input domain state m denote x enclose dotted line particular value choose draw m line fig irrelevant mneighborhood size length border input domain small m neighborhood size reduce dotted line close input domain result satisfy criterion input domain square small choice mneighborhood size fig m state solution encode information bar orientation center position m state correspond localize output activity pattern center position onedimensional array m cell solution correspond cell organize orientation column orientation interval onedimensional model labeling linear array cell accord orientation preference lie degree indicate bold light dotted line segment rectangle fig mneighborhood size decrease far mapping favor fig lb orientation column low portion fig narrow detailed analysis information rate function mapping confirm main feature obtain simple geometric argument type analysis apply different type input pattern ensemble example consider network receive ensemble simple pattern acoustic input pattern consist frequency sense ear time delay suppose initial network layer organize information ear separately map mean connection range different time delay signal receive ear time interval appear pattern cell activity intermediate layer l apply principle signal transformation layer l layer m state space represent rectangle axis frequency delay spatial position bar orientation apart certain difference density l state nonuniform state rectangle long identical infomax analysis carry simplified case orientation selectivity local algorithm information rate eqn principle state maximize subject constraint possibly optimization function contain cost term discuss complicated mathematical form optimization process approximation implement network cell connection limit computational power geometric form cast infomax principle simple model case suggest accomplish algorithm demonstrate topographic map emerge result lateral interaction output layer apply algorithm onedimensional m layer twodimensional l layer use euclidean metric impose periodic boundary condition short dimension layer result map fig d map similar fig band direction surprising involve local principle globally optimal solution generally tend empirically produce map property construct diagram correspond position assign point m region base close obtain set m region tend area inversely proportional pl neighborhood correspond input domain tend similar extent direction reference noise information content optimization principle appear implement qualitative way geometric condition impose simple case suggest local algorithm similar line capable implement principle general situation geometric formulation principle suggest connection algorithm propose generate topographic map trade model neighborhood relationship postulate source target space operation lead mapping source target space neighborhood relationship arise naturally analysis principle apply noise process induce l l m m transformation neighborhood relation space lateral connection m cell layer induce neighborhood relation recently devise approach solve certain geometric optimization problem travel problem gradient descent method bear similarity complementary relationship principle local find implement hand principle explain algorithm algorithm contribute generation useful perceptual system turn shed light role lateral connection synaptic modification mechanism biological network hand existence local algorithm important demonstrate network relatively simple processor biological synthetic fact find global information rate possible connection principle principle maximum preservation information view equivalently principle minimum information principle satisfied loss information layer layer minimize flow information sense nearly constraint allow resemblance principle principle minimum entropy production suggest principle important understand selforganization complex system resemblance algorithmic level hebbtype modification rule process l consider certain model evolution natural selection raise possibility connection draw synaptic modification rule informationtheoretic optimization principle example general relationship important emergence complex apparently structure behavior relatively simple local interaction neural system reference r linsker wiesel organization behavior r linsker r simulation brain science cambridge r linsker computer press e math tech selforganization associative memory r nature p theory fluctuation m p
model associative multiplication abstract fact mental arithmetic base basic fact simple algorithm human difficult time master subject individual mistake associative multiplication process multiplication memory use rule especially problematic human exhibit certain characteristic phenomenon perform associative multiplication type error error frequency propose model process associative multiplication compare performance phenomenon datum normal human model propose introduction associative multiplication define multiplication computational algorithm mainly concern recall basic time table learn time table require learn fact fact assume normal human use simple rule number fact learn reduce theory associative multiplication simple problem reality school child find difficult learn train adult relatively high rate error especially performance associative addition similar problem surprisingly little work method human perform basic multiplication problem excellent review current literature provide model consider plausible error characteristic similar author correspondence address current address computation s human task arithmetic entail account minimum phenomenon problem size effect note study ai response time error rate increase problem large operand secondly human characteristic distribution type error specifically error classify follow type suggest operand answer correct operand replace category account error normal adult result percent correct response table result correct problem operand replace result time table operation answer correct different arithmetic operation addition reasonable assume human use distinct representation deal number work model performance specie include human monkey judgment task suggest case coarse coding use hand human capable deal number abstract symbolic concept suggest use precise code previous work use code idea example et single representation combine aspect document patient suffer follow retain normal intelligence numerical arithmetic concept present arithmetic problem capable rapidly provide approximate answer press precise answer resort explicit computational count possible interpretation case study retain ability work number fashion lose ability treat number symbolic concept suggest hypothesis human use separate concurrent representation number coarse coding symbolic precise coding course associative arithmetic general multiplication particular switch coding point process hypothesis form basis modeling work guide placement transition representation assume constraint coarse coding preferred coding conserve wide variety specie tend express precise code figure coarse coding digit number left digit number position number region grid represent activity model associative multiplication methodology follow work coarse coding consist dimensional vector slide correspond magnitude digit represent size decrease degree overlap increase magnitude digit increase figure noise representation simulate probability bit wrong state precise representation intend symbolic manipulation number consist dimensional vector value code digit dimension great activity representation vector code number concatenation vector use number great direction flow figure schematic network architecture coarse code b precise coding d table text detail model train distinct phase simple perceptron train winnertakeall competitive learning use map input operand original coarse code precise representation network train epoch different set sample noisy digit end train perform level translate operand present twolayer feedforward network logistic activation function train backpropagation number hide unit equal number problem training set case force lookup table behaviour lookup table train independently vary number iteration use learning rate constant output lookup table coarse code figure final phase table output translate network provide final answer precise code schematic network architecture figure operand vector use training network noise parameter vector use analysis noise training testing problem set consist copy problem list table problem use simulation way result oo r operand normal human iteration train model iteration train model iteration training table operation error category figure error distribution human datum model model model train error training datum categorize accord error type list introduction section summary performance model present table comparison plot datum generate model model human datum figure case model generate operation error expect model train multiplication permit way operation error set result obtain model training iteration present table table error rate generate model column operation error include instance model generate operation error iteration error trial operand table set problem answer great model associative multiplication table result trial run model training iteration error mark boldface problem convention current arithmetic literature test existence effect fit line error sum operand problem positive slope fit demonstrate existence problem size effect result analysis figure model problem size effect instance note claim linear model datum conclusion draw specific parameter fit especially sparsity datum point analysis highlight generally increase trend discussion note result section model demonstrate effect number error figure choose architecture permit response time effect presence effect surprising model use representation similar coarse code display effect becker u c sum operand figure demonstration problem size effect datum plot model train iteration prove good fit distribution error human figure similar analysis slope train iteration training iteration suggest researcher e effect simply frequency effect human encounter problem involve small operand real life evidence contrary remain possibility immediately apparent figure model distribution error normal human superior model regard model implement autoassociative network use brain state box architecture generate operand error table operation error deficiency predict nature autoassociative network process translate representation digit possibility error believe allow model produce category error interesting aspect model reveal figure table increase training lookup table improve overall performance model error distribution remain relatively constant length training study suggest model error distribution inherent feature architecture training artifact correspond datum normal human error distribution remain relatively constant individual note design model permit occurrence error type save operation error point clear understanding exact feature generate error distribution define model associative multiplication single step goal understand human perform general arithmetic propose mechanism arithmetic operation mechanism operation address issue model implement addition fact human operation error suggest interaction mechanism model associative multiplication associative multiplication associative addition conversely error task occur different processing level entirely summary model question great potential description associative multiplication process eventually expect form basis complete model arithmetic human cognition acknowledgement author acknowledge financial support second author acknowledge financial support like thank m feedback comment work reference neural network knowledge inference representation m introduction neural network mit journal advance connectionist neural computation theory pollack ed publishing cognition instruction bj journal experimental psychology general m experimental psychology learn memory cognition parallel distribute processing exploration cognition vol psychological biological model research group ed mit journal experimental psychology experimental psychology learn cognition journal experimental psychology
accelerate descent continuous discrete time alexandre m abstract study accelerate descent dynamic continuous discrete time combine original continuoustime motivation mirror descent recent ode interpretation nesterovs accelerate method propose family continuoustime descent dynamic function lipschitz gradient solution trajectory converge optimum ot rate large family firstorder accelerate method obtain discretization ode method converge ok rate connection accelerate descent ode provide intuitive approach design analysis accelerate firstorder introduction consider optimization problem rn close c convex function assume lipschitz let minimum convex optimization method interpret discretization ordinary differential equation solution guarantee converge set minimizer simple method gradient descent iteration step size interpret discretization discretization step s theory ordinary differential equation provide guidance design analysis optimization algorithm use unconstrained optimization constrain optimization stochastic optimization particular prove convergence solution trajectory ode achieve use simple elegant lyapunov argument ode carefully discretize obtain optimization algorithm convergence rate analyze use analogous lyapunov argument discrete time article focus family firstorder method nesterovs accelerate method descent method firstorder method increasingly important largescale optimization problem arise machine learning application accelerate method apply problem extend number way example mirror descent method provide important generalization gradient descent method geometry discuss application optimization online learn intuitive understanding method particular importance design analysis new algorithm nesterovs method hard explain intuitively progress recently ode interpretation nesterovs method interpretation restrict original method apply extension geometry interpretation nesterovs method perform iteration convex combination mirror step gradient step cover broad family algorithm include geometry interpretation require involved analysis lack simplicity ode provide new interpretation benefit approach broad family accelerate method include study obtain discretization simple ode converge ot rate provide unified interpretation potentially simplify design analysis firstorder accelerate method continuoustime interpretation nesterovs method continuoustime motivation descent rely lyapunov argument review section combine idea propose section candidate lyapunov function depend state variable evolve primal space e evolve dual space e design couple dynamic z guarantee function lyapunov function reference lead new family system equation prove existence uniqueness solution prove use lyapunov function solution trajectory section discretization continuoustime dynamic obtain family accelerate descent method prove convergence rate theorem use lyapunov argument analogous involved continuoustime case example new accelerate method simplex view perform step convex combination entropic projection different step size interpretation accelerate descent new insight allow extend recent result adaptive restart heuristic propose cande know empirically improve convergence rate test method numerical example section comment performance ode interpretation descent method nesterovs accelerate method prove convergence solution trajectory ode involve lyapunov argument example prove convergence solution consider lyapunov function kxt k minimizer time derivative d dt inequality convexity integrate v r t t prove d converge rate previous argument extend family method descent idea start nonnegative function design dynamic v lyapunov function nemirovski argue replace function kxt k function dual space d zt z e dual variable design dynamic z value z equilibrium corresponding trajectory primal space convex function define map e zt bregman divergence associate define h yi function strongly convex wrt reference norm k d z wrt k yk review property bregman divergence chapter appendix definition bregman divergence d d d d zt z h z e d e zt dual variable z obey dynamic z d argument descent v lyapunov function r converge rate system summarize z z z z note map zt remain finally unconstrained descent obtain special case descent z identity case z coincide ode interpretation nesterovs accelerate method nesterovs accelerate method interpret discretization differential equation r x t argument use follow lyapunov function reparameterization tr r prove lyapunov function ode r decrease trajectory system follow t et e r tr tr e x prove converge ot rate note particular euclidean norm use definition et consequence lead family unconstrained euclidean accelerate method section combine argument nemirovski idea use general bregman divergence lyapunov function construct general family system convergence guarantee discretize result dynamic obtain general family method restrict unconstrained euclidean geometry continuoustime accelerate descent derivation accelerate descent consider pair dual function define define e assume l smooth respect k reference norm dual space consider function rd zt r dual variable design dynamic z value equilibrium d z r z r assume v z z r d t r follow v lyapunov function r propose ode system r t t z r x z z z unconstrained euclidean case z z r x system equivalent r r equivalent study recover special case interpretation ode equation equivalent tr integral form tr r r z write z r couple dynamic interpret follow dual variable accumulate gradient rt rate primal variable weighted average dual trajectory weight proportional r interpretation r parameter control weight distribution interesting observe weight increase r finally averaging interpretation clear primal trajectory remain map solution propose dynamic prove existence uniqueness solution system define t assumption l smooth wrt equivalent function lipschitz unfortunately term expression z lipschitz t directly apply existence work consider sequence approximate ode similarly argument use suppose c lipschitz let x z e accelerate descent system initial condition z unique solution z c existence solution interval t uniqueness prove supplementary material let consider smoothed ode system r t z r x z z z r function z x z z lipschitz t t theorem system unique solution z c t order existence solution original use follow lemma prove supplementary material let t family solution z t t l continuous uniformly bound proof existence consider family solution zi t restrict t family uniformly bound theorem exist subsequence converge uniformly t z limit prove z infinite set index let solution original ode t zi follow z satisfy initial z z solution ode t t condition let t let x z t initial condition continuity solution wrt initial condition theorem uniformly t t x uniformly t xi coincide t t satisfy ode t t arbitrary t conclude proof existence convergence rate straightforward establish convergence rate solution suppose lipschitz gradient smooth distance generate function let solution descent r t r d z z t proof construction tr rd lyapunov function follow t tr v rd z discretization careful discretization continuoustime dynamic obtain general family accelerate descent method constrained optimization use mixed scheme chapter discretize let t use step size follow solution z propose discretization approximate r equation rewrite note independence s invariance ode word combination xk coefficient rk rk summarize discrete scheme write k rk r map feasible set start guarantee remain x convexity note duality arg additionally assume differentiable image theorem write z second equation write z r d r eventually modify scheme order able prove desire rate start analyze version motivate continuoustime lyapunov function use correspondence k consider potential function e z z z z k r r z z k r r e e k simple algebraic manipulation term bound follow z k z d e d z z z z definition bregman divergence z z r r d z z r r convexity e z z d continuoustime case expression expression obtain analogous expression additional bregman divergence term z immediately conclude v lyapunov function follow modification discretization scheme family accelerate descent method expression k propose x k replace solution minimization problem arg s rx r regularization function satisfy follow assumption exist lr r rx euclidean case rx case r update general case rx d distance generating function strongly convex lr smooth case update mirror update result method summarize generalization interpretation nesterovs method convex combination mirror descent update gradient descent update accelerate descent distance generating function regularizer r step size s parameter r initialize r k k rk d z r x z z d x r consistency modify scheme assumption r x k xk r xk k prove claim use observation modify discretization scheme consistent original ode difference equation define xk converge tend ordinary differential equation continuoustime system difference equation equivalent replace x k ie k z r suppose exist function z define r z k k s use fact xk o difference equation system write zt r r converge ode convergence rate prove convergence consider modify potential function k z z s r k e r lyapunov function consequence r e lemma prove supplementary theorem discretetime accelerate descent parameter r step size guarantee r z z e proof inequality follow immediately second inequality follow prove supplementary material simple bind example accelerate entropic descent p instance problem suppose r negative entropy e ln ln xi pn indicator function simplex u rn normal vector affine hull simplex result descent update simple entropy projection compute exactly operation smooth wrt example second update rx x pn smoothed negative entropy function define follow let let simple closedform expression know compute efficiently log time use deterministic expect time use randomized algorithm additionally satisfie assumption strongly smooth wrt result accelerate descent method simplex implement efficiently guarantee converge ok s nl numerical experiment test accelerate descent method problem rn different objective function simple quadratic random positive semidefinite matrix function accelerate descent accelerate descent weakly convex quadratic rank effect parameter r figure evolution problem use different accelerate descent method entropy distance generating function accelerate descent initialize x z k r xk l d x d z d e r l condition xk l p f ln xi bi entry r bi r iid normal implement accelerate entropic descent algorithm propose section include entropic descent reference adapt propose cande speed restart heuristic propose generic restart method algorithm tion follow gradient xk speed kxk kxk result figure accelerate descent method exhibit polynomial convergence rate empirically fast rate predict method exhibit oscillation set minimizer increase parameter r reduce period oscillation result trajectory initially slow fast large figure c restart heuristic alleviate oscillation empirically speed convergence visualize experiment trajectory iterate xk method project dimensional hyperplane corresponding video include supplementary material conclusion combine lyapunov argument motivate descent recent interpretation nesterovs method propose family system minimize function lipschitz gradient guarantee converge ot rate prove existence uniqueness solution discretize ode propose family accelerate descent method constrained optimization prove analogous ok rate step size small connection continuoustime dynamic motivate detailed study ode study oscillatory behavior solution convergence rate additional assumption strong convexity rigorous study heuristic acknowledgment gratefully acknowledge fall algorithmic spectral graph theory program reference linear couple gradient descent dhillon clustering divergence learn descent nonlinear project subgradient method optimization fast iterative algorithm linear inverse problem image science nemirovski lecture modern nemirovski order subset descent optimization method application optimization gradient flow society brown m c effective method unconstrained optimization base solution system ordinary differential equation journal optimization theory application regret analysis stochastic multiarmed bandit problem foundation trend machine learn numerical method ordinary differential equation gabor lugosi prediction learning game cambridge dekel run distribute online prediction proceeding th international conference machine learning icml optimization dynamical system communication control lecture note nemirovski solve variational inequality alexandre efficient bregman projection conference decision control lyapunov general problem stability motion control theory application b problem complexity method efficiency optimization series discrete nonsmooth function mathematical programming gradient method minimize composite function mathematical programming method solve convex programming problem convergence rate mathematic lecture optimization volume springer science business medium cande gradient scheme foundation computational mathematic m continuoustime stochastic descent network variance reduction consensus convergence page press j singer dynamical system approach constrain numerical functional analysis optimization cande differential equation model nesterovs accelerate gradient method theory insight nip ordinary differential equation dynamical system volume mathematical soc
forage uncertain environment use learning p read lab rd abstract survival enhance ability predict availability likelihood presence present concrete model use diffuse neurotransmitter system implement predictive version learn rule embed neural architecture base anatomical physiological study bee model capture strategy behavior bee number animal forage uncertain environment predictive model suggest unified way influence use bias action control successful prediction enhance adaptive behavior allow prepare future action reward possible improve behavioral choice consequence execute different action reliably predict classical instrumental conditioning result psychological literature demonstrate vertebrate brain capable reliable prediction prediction compute brain know brain vertebrate invertebrate possess small nucleus project axon large target tissue deliver neurotransmitter activity system report reinforce stimulus world reflect expectation future reward division neuroscience college forage uncertain environment use predictive hebbian learning particularly example condition sensory stimulus color visual pattern sensory stimulus pair application identify project widely entire bee brain active firing substitute odor stimulus classical conditioning experiment similar project neuron substitute reward pair visual stimulus paper suggest role diffuse neurotransmitter system learning behavior analogous function previously postulate developmental selforganization specifically identify neural know exist vertebrate invertebrate deliver information widespread region brain describe algorithm mathematically sound biologically feasible iii version local algorithm context neural architecture reproduce decision behavior observe bee number animal premise predictive relationship sensory stimulus reward construct diffuse system use shape ongoing behavior synaptic plasticity illustrate use simple example literature constraint available number different level problem real perform series experiment bee forage artificial flower color blue predict delivery nectar examine bee respond mean variability reward delivery forage version stochastic bandit problem blue flower contain nectar yellow flower contain l remain yellow flower contain nectar practice bee visit constant yield blue flower equivalent mean return variable yellow flower contingency reward reverse bee switch preference flower color visit flower far demonstrate bee induce visit variable constant flower equal frequency mean reward variable flower type sufficiently high l experimental finding bee learn associate color reward color odor learning approximately time course shift preference bee indicate condition task bee prefer variable reward compute reward availability short term behavioral strategy utilize variety animal similar condition reward suggest common set constraint underlie neural substrate diagram model architecture base consideration diffuse system sensory input drive unit b represent yellow flower neuron output respectively time project xi montague dayan sejnowski action system inhibition figure neural architecture prediction future expect reinforcement brain use diffuse neurotransmitter system context bee forage sensory input drive unit band represent yellow flower unit project reinforcement neuron p set variable weight fill circle action selection system unit provide input n fire bee nectar r project output fix weight p variable weight p implement prediction future reward text output sensitive temporal change input output projection p line arrow influence learning selection action equation text modulate lateral inhibition dark circle action selection layer encounter flower nectar output p reflect temporal difference sensory input band encounter flower nectar prediction error determine output b y r learning occur connection b strength modify accord correlation presynaptic activity prediction error produce p equation text learning restrict visit flower excitatory connection weight project p weight processing stage control selection action land p receive additional input absence net input p t assumption construction model learn adjustment weight approach land flower assumption support specifically datum learn color learning flower restrict final second prior land flower experience nectar fact suggest simple model strength variable connection adjust accord presynaptic rule learning rate problem formulation learn occur contingency presence reinforce stimulus forage uncertain environment use predictive hebbian learning s nectar volume trial figure simulation bee forage behavior use predictive hebbian learn reinforcement neuron output function nectar volume fix concentration b proportion visit blue flower trial represent approximately flower visit average real bee exactly flower visit single model trial real model bee blue flower constant type remain trial yellow flower constant beginning trial set consistent evidence information past forage use real bee variable model bee source twodimensional ground represent real bee slight preference blue flower note slow drop flower switch allow sensory event predict future delivery reinforcement problem equation inconsistent substantial volume datum classical instrumental conditioning add postsynaptic factor equation alter conclusion suggest form learn rule model p direct input assume firing rate p sensitive change input time constant slowly vary input ganglion cell assumption output reflect temporal derivative net input approximate factor control weighting near distant reward current discussion presence reinforcement weight w adjust accord simple rule permit weight p act prediction expect reward land flower derive general way prediction future value scalar quantity montague dayan sejnowski r v mean mean figure tradeoff mean variance nectar method select indifference point indifference point mean variance bold v legend stochastic trial demonstrate indifference method calculation tend bias indifference point left b indifference plot model real bee point represent mean variance pair sample flower type equally circle bee actually land flower sample nectar r influence output p fix connection fig suppose prior sample nectar switch view blue flower example wi way term w prediction value t t difference wt error prediction adjust weight accord rule equation allow weight output report rest brain reinforcement expect blue flower sense model bee fly flower reinforcement nectar present light proportional t t b use prediction modulation action choice example suppose learning process equation set b switching view yellow flower view blue flower cause light positive bias activity action selection unit drive connection b bee likely chance land blue flower discussion offer accurate model choice simply indicate output diffuse system use influence action choice biological assumption neural architecture explicit project neuron change firing accord temporal difference input output p use adjust weight iii output bias selection action modulate activity target neuron particular case bee learning rule describe equation action selection describe far simplify purpose forage uncertain environment use predictive hebbian learning simple demonstration mention significant learning particular flower color occur second prior encounter restrict weight change encounter allow sensory input precede delivery t drive synaptic plasticity learning rule update weight flower flower basis encounter environment produce prediction error t l actual reward time flower color bee time blue cause prediction future reward weight input activity l weight update use form delta rule time constant control rate forget rule weight sensory input p mediate prediction r temporal component choose land remove model temporal action probabilistic use weight p choose flower actually visit trial flower visit prediction use directly choose action accord probability choose yellow flower value difference prediction large value likely large prediction result choice associate flower color limit approach winnertakeall rule simulation varied comparable result obtain change alter magnitude weight develop p different value enforce different degree competition prediction apply model experiment necessary specify nectar particular flower report p assume reinforcement deliver signal function nectar volume hard real suggest sort function nectar volume justify biomechanical ground fig behavior model bee compare real bee experiment test extent prefer constant reward variable reward mean detail present figure behavior model match observed datum suggest real bee utilize information small time window control forage value average proportion visit real bee model bee constant variable flower type switch trial bee switch flower preference subsequent visit average proportion visit change respectively real model bee reason real bee apparent preference blue come inherent bias bee instance know learn short wavelength quickly model measure length time observation influence flower selection measure bee time term mean rate sejnowski real bee induce forage equally constant variable flower type mean reward variable type sufficiently large fig b variance mean reward increase bee appear flower experiment constant flower type contain nectar datum real bee point connect solid line order clear envelope real datum indifference point circle demonstrate high value reproduce bee behavior model capture functional relationship spread real datum diffuse neurotransmitter system report prediction error control learning bias selection action distribute signal large set target structure permit prediction error influence learn generally factor rule signal second role bias activity action selection system favor behavior model construction prediction error require convergent input sensory representation neuron neuron output temporal derivative input output neuron use associate sensory stimulus predict reward relatively simple predictive learning system closely simulate behavior bee task acknowledgement work support mental health science resource supercomputer like thank alexandre pouget helpful comment reference condition press dayan p society neuroscience abstract montague dayan p nowlan sl pouget sejnowski advance neural information processing system hanson pl trend neuroscience behavioral brain science robbin neuroscience thesis real la science forage uncertain environment use predictive real hard ld real la real la hard ld b bandit problem sequential allocation experiment forage nature functional menzel r scientific journal neuroscience nj conditioning associative learning proceeding annual conference cognitive science society seattle wa annual review neuroscience overall algorithm temporal difference learn rule relate devise teach program development suggest present form use classical ieee transaction system man cybernetic use variant form instrumental conditioning watkin report computer information sj sp technical report computer information science relationship dynamic programming engineering method optimal control ar classical conditioning current research theory black editor p b adaptive signal processing prenticehall menzel r j experimental analysis insect behavior p
speech recognition use svms engineering engineering important issue apply svms speech recognition ability classify variable length sequence paper present extension standard scheme handle variable length datum fisher score useful mapping introduce base likelihoodratio scorespace define mapping avoid limitation score classconditional generative model directly incorporate definition scorespace mapping appropriate normalisation scheme evaluate isolate letter task new mapping outperform fisher score hmms train maximise likelihood introduction speech recognition complex dynamic classification task stateoftheart system use hide markov model hmms train maximise likelihood discriminatively achieve good level performance reason popularity hmms readily handle variable length sequence result variation word sequence speaker rate support vector machine svms powerful technique work variety task typically apply static binary classification task paper examine application svms speech recognition major problem address handle variable length sequence second handle multiclass decision paper concentrate deal variable length develop early research detail fully similar approach protein classification adopt variety method propose map variable length sequence vector fix dimension include vector averaging select representative number observation utterance method discard useful information paper adopt approach similar use available datum scheme use generative probability model datum define mapping fix dimension space incorporate kernel kernel know relevant issue discuss paper examine suitability classification propose alternative useful kernel addition normalisation issue associate use kernel speech recognition address initially general framework define alternative scorespace require define observation sequence ot set generative probability model observation sequence p k vector parameter kth member set observation sequence map vector fix dimension ij function member set generative model p define use scoreoperator f ij score occupy scorespace investigation scorespace fall division good generative model scoreoperator use scorespace hmms prove successful speech recognition natural choice generative model task particular hmms state output distribution form model choice twoclass problem let represent generative model denote generative model denote classconditional generative model compete class previous scheme use log single generative model represent class original fisher score g class scorespace term likelihood scorespace scorespace propose paper use log ratio classconditional generative model p corresponding scorespace likelihoodratio scorespace likelihoodratio scorespace avoid limitation likelihood scorespace view generalisation standard generative model classifier issue discuss later propose form generative model scoreoperator select original scoreoperator storder derivative operator apply hmms discrete output distribution consider continuous density hmm emit state state output distribution form mixture e component parameter weight mean covariance storder derivative log probability sequence respect model parameter derivative operator define column vector t l detail derivation posterior probability component state time t assume hmm skip assume state appear hmm ie storder derivative probability state storder derivative parameter probability hmm score definition score utterance weighted sum score individual observation score utterance speak different speak rate calculate lie different region scorespace simply differ number observation ease task scorespace scorespace normalise number observation length normalisation duration information retain derivative transition method normalisation scorespace use generative model train maximise modify log likelihood function consider state entry time duration number observation output probability observation l possible maximise use algorithm hillclimbe technique use paper simple normalisation method employ generative model train maximise standard likelihood function define scorespace use standard state posterior posterior probability state time t define state posterior normalise total state utterance standard component posterior replace equation normalise form t l effect derivative divide sum state posterior preferred division total number observation t assume vary state state sequence scale ratio necessarily case nature scorespace affect discriminative power classifier build scorespace example likelihood scorespace define twoclass sum unity constraint weight parameter mixture discard definition forward transition hmm skip generative model susceptible occur different location map single point scoresubspace example consider class model gaussian observation generate peak gaussian derivative relative mean equation derivative relative mean distant second gaussian component posterior similar problem occur observation generate peak second gaussian ambiguity mapping possible location scoresubspace mean represent acoustic space subspace occur subspace variance increase class confusion likelihoodratio scorespace define gaussian suffer component posterior gaussian force unity far storder derivative scoreoperator consider possible include zeroth nd higherorder derivative course interaction scoreoperator zerothorder derivative likelihood scorespace expect useful counterpart likelihoodratio scorespace great sensitivity acoustic condition principled approach use derivative scorespace useful consider simple case true classconditional generative model respective estimate functional form p d p express true model expansion parameter estimate detail tv t t v t output operator square bracket infinite number derivative arrange column vector column vector expression true model incorporate optimal minimum baye error decision rule follow prior encode class b t l t t b vec b score likelihoodratio scorespace form infinite number derivative respect parameter estimate optimal decision rule recover construct linear classifier case standard svm margin interpret log posterior margin justify use likelihoodratio scorespace encourage use higherorder derivative hmms use speech storder markov process speech markov process linear decision boundary likelihoodratio scorespace define storder markov model estimate unlikely sufficient recover optimal decision rule model powerful nonlinear classifier train likelihoodratio scorespace try compensate model approximate optimal decision rule svms nonlinear kernel polynomial radial basis function use gain expect incorporate higherorder derivative scorespace size scorespace dramatically increase practical system truncate likelihoodratio scorespace storder derivative use linear approximation expansion example derivative t l t l simplicity component posterior assume independent scorespace define svm classifier build scorespace standard linear polynomial kernel use scorespace space assume euclidean metric tensor scorespace whiten decorrelate scale standard apply failure perform scorespace normalisation scorespace result kernel similar plain kernel expect perform poorly different dimension scorespace different dynamic range simple scaling find reasonable approximation whitening avoid invert large matrix single observation sequence different database use information matrix normalise scorespace acceptable normalisation likelihood scorespace condition expectation scorespace appropriate svm kernel use utterance oj normalise scorespace normalise covariance matrix scorespace experimental result isolated letter database use evaluation datum code msec frame rate msec datum parameterise dimensional feature vector include log energy term corresponding delta acceleration parameter utterance letter use training utterance letter testing overlap training test speaker set letter test highly eset b e g p t z letter baseline system maximise likelihood letter model emit state continuous density skip single hmm skip state output distribution number gaussian component diagonal covariance matrix model test use viterbi constrain network useful note linear decision boundary bias construct likelihoodratio scorespace form zerothorder derivative operator equal class prior standard minimum baye error classifier baseline hmms use generative model modify version version use train svm classifier possible class pair sequence length normalisation equation simple scaling scorespace normalisation use training test linear kernel use normalise scorespace performance variable width polynomial kernel degree include homogeneous inhomogeneous inhomogeneous scorespace linear kernel require initial experiment find fairly insensitive variation parameter fix biased hyperplane permit variety scoresubspace examine refer scoresubspace vec respectively refer log likelihood r log likelihoodratio binary svm classification result baseline binary hmm result combine obtain single classification utterance use simple majority vote scheme set binary classifier tie letter relevant classifier necessary random selection perform table hmm baseline scorespace eset class state hmm vote sign table compare baseline hmm svm classifier complexity generative model varied statistical significance confidence level bracket compare baseline hmm classifier generative model significant result confidence level define p p test percentage probability classifier error rate difference simply random error decision random selection tie letter assign class baseline hmms comparable report result eset different database majority voting scheme performance minimum baye error scheme indicate majority voting acceptable multiclass scheme eset experiment svms likelihoodratio scorespace define use compete classconditional generative model project scorespace likelihood define use generative model class project scorespace likelihood class scorespace define use generative model class project scorespace original fisher score projection rn scoresubspace test find yield slightly high error rate svms build use likelihoodratio scorespace achieve low error rate system low likelihood scorespace perform slightly bad likelihoodratio scorespace contain half information contain log likelihoodratio case optimum number component generative model state possibly reflect gender division class likelihood class scorespace perform poorly possibly tion generative model component class state total model component state model class component state generally reflect gender division class datum suppose class division possible explanation gaussian component model class bimodal distribution cause gender difference datum model sit peak gaussian map ambiguous scoresubspace sufficient class discrimination scorespace task small fully assess possible decorrelation error structure classifier scale normalisation scorespace define model component state increase likelihoodratio scorespace augment derivative result classifier increase error rate performance probably simplicity task independence assumption component posterior component mean effect noise training score large scorespace know dimension noisy degrade classification performance reason experiment perform select subset likelihoodratio scorespace build svm classifier scoresubspace scoresubspace select parameter type error rate result classifier identical baseline svms detail table generative model classconditional hmms component state log likelihoodratio powerful discriminate feature increase number dimension scorespace allow discriminative classifier discrimination noise derivative component mean component variance expect dynamic task derivative transition useful contain duration information table error rate subspace likelihoodratio scorespace eset scorespace error rate m dimensionality subset scorespace select accord dimension high low error rate scorespace respectively dimension dimension respectively significant confidence level relative good hmm system component state generally add discriminative dimension lower discriminative dimension add binary classifier discriminative dimension log likelihoodratio expect eset discriminative dimension dependent initial hmm state energy term important coefficient static delta acceleration stream useful error rate differ hmm baseline nonzero bias svms hmm classifier run alphabet good hmm classifier component state error rate computational preclude optimisation generative model component state scorespace prune dimension low error rate significant confidence level relative hmm system preliminary experiment evaluate sequence normalisation alphabet eset detail conclusion work svms successfully apply classification datum paper concentrate nature scorespace handle variable sequence standard likelihood scorespace extend likelihoodratio scorespace normalisation scheme introduce new scorespace avoid limitation incorporate classconditional generative model directly classifier different scorespace compare isolate letter task likelihoodratio scorespace outperform likelihood scorespace hmms train maximise acknowledgement like thank case sponsor speech group laboratory joachim university reference vapnik nature statistical learning theory m speech pattern new discriminative kernel probabilistic model becker editor advance neural information processing system mit press t haussler exploit generative model discriminative classifier sa editor advance information processing system mit scholkopf smola advance classifier natural regularization generative model mit s fine j r hybrid gmm svm approach speaker identification proceeding volume international conference acoustic speech signal processing m use svms classify variable length speech pattern university m speak letter recognition touretzky editor neural information processing system page joachim largescale svm learn practical b c burge smola editor advance method support learn pc recognition ieee transaction speech audio processing
deep network internal selective attention feedback connection abstract traditional convolutional network stationary feedforward change parameter evaluation use feedback high low layer real brain deep attention selective network dasnet architecture dasnet feedback structure dynamically alter convolutional filter sensitivity classification harness power sequential processing improve classification performance allow network iteratively focus internal attention convolutional filter feedback train direct policy search huge parameter space scalable natural evolution strategy sne cifar cifar dataset dasnet outperform previous stateoftheart model dataset introduction deep convolutional neural network cnn maxpooling layer train stateoftheart object recognition scene parse extensive review architecture consist stack feedforward layer mimic bottomup path visual cortex layer learn progressively abstract representation input datum lowlevel stage tend learn biologically plausible feature detector gabor filter detector high layer learn respond concrete visual object eg train change weight filter evaluation evolution discover efficient feedforward pathway recognize certain object eye expert ask classify bird belong similar specie think millisecond answer imply feedforward evaluation perform evaluation try different information image human benefit greatly strategy cnn require formulation nonstationary adapt behaviour process decide adapt cnn behaviour paper introduce deep attention selective network dasnet model selective attention deep cnn allow layer influence layer successive pass image special connection bottomup topdown modulate activity convolutional filter weight special connection implement control policy learn reinforcement learning train usual way supervised learn input image attentional policy enhance suppress feature multiple pass improve classification difficult case capture initially supervise share author train aim let system check usefulness internal filter automatically omit manual inspection current implementation attentional policy evolve use separable natural evolution strategy instead conventional single agent reinforcement learning iteration temporal difference policy gradient large number parameter require control cnn size typically use image classification experiment cifar cifar difficult classification instance network correct certain filter outperform previous stateoftheart network work use maxout network combine dropout underlie model dasnet network represent stateoftheart object recognition task outperform small margin average committee convolutional network similar approach reduce dimensionality favor sparsity representation recently present cnn consist stack alternate convolutional layer final classification layer convolutional layer input layer image output previous layer consist c input map width m height output consist set output map m convolutional layer parameterise c c filter size k denote filter index input output map denote layer index input output map respectively convolutional operator elementwise nonlinear function use index layer size output determine size use convolution pool layer pool layer use reduce dimensionality output convolutional layer usual approach maximum value non patch map reduce dimensionality height width instead pool layer reduce consecutive map map maximum value block size map reduce c input map c output map use index layer output pool layer use input pair convolutional pool layer form input final classification layer classification layer finally classification step perform output pool layer large vector form input follow equation max choose softmax activation function produce class probability input project reduce use maxout similar pool layer reinforcement learning reinforcement learning rl general framework learn sequential decision order maximise external reward signal learn agent ability act perceive environment time agent receive observation current state environment select action choose policy s o space possible state observation action respectively agent enter receive reward r objective find policy maximise expect future discount reward t discount future model agent dasnet observation action space real value policy represent function approximator neural network parameterise policy use control attention dasnet state action space close dimension policy parameter vector contain close weight impractical standard rl method instead evolve policy use variant natural evolution strategy ne separable sne ne family optimization algorithm use parameterise probability distribution search space instead explicit population conventional typically distribution multivariate gaussian parameterise mean covariance matrix epoch generation sample distribution update direction natural gradient expect fitness distribution sne differ standard ne instead maintain covariance matrix search distribution use diagonal entry sne theoretically powerful standard ne substantially efficient deep attention selective network dasnet idea dasnet harness power sequential processing improve classification performance allow network iteratively focus attention filter section augment allow filter weight differently different pass image compare equation weight jth output map layer change strength activation apply maxout pool operator vector ac represent action learn policy select order sequentially focus attention maxout net discriminative feature image process change action alter behaviour result different output image change indicate follow notation mt parameter vector policy output network describe dasnet training algorithm maxout net m train classify image use training set x policy evolve use sne focus attention m pass loop represent generation sne generation start select subset image p sample draw sne search distribution mean covariance represent parameter candidate policy trial image batch trial image present maxout net t time pass t action set ai maxout network function normally work deterministic policy observation output action extend stochastic policy t rain et m true image p gate identity activation t t xi end boost end end detail supplementary material end action effect image propagate net observation vector construct concatenate follow value extract m average activation output map equation maxout layer intermediate activation classification layer class probability average map activation provide partial state information value meaningful allow selection good action candidate policy map observation action weight matrix neural network softmax note softmax function scale dimensionality action space element average instead regular softmax sum ensure network output positive filter activation stable pass image process time use filter weighting cycle repeat figure illustration process time performance network score boost boost correct misclassifie v output m end pass t correct classification correct constant measure weight loss misclassifie sample weight high correctly classify sample ie correct simple form boost use focus difficult misclassifie image input image process policy assign fitness cumulative score z regularization l class class softmax softmax s map map map gate filter gate filter m policy map map gate filter map gate filter observation action t t m filter e action m figure dasnet network image classify t pass network forward propagation maxout net output classification vector output second layer average feature map combine observation vector use deterministic policy choose action change weight feature map pass image pass output maxout net finally use classify image l regularization parameter candidate policy evaluate sne update distribution parameter accord natural gradient calculate sample fitness value sne repeatedly update distribution course generation expect fitness distribution improve stop criterion meet improvement consecutive epoch relate work human vision advanced flexible perceptual system know visual cortex area highly connected include direct connection multiple level topdown connection construct famous hierarchy diagram different visual cortical area macaque visual cortex pair area consider connected connected area connect connection numerous bottomup connection generally diffuse think play primarily role feedforward connection serve direct information carrier analysis response latency image theory stage visual processing fast phase feedforward processing follow attentional phase influence recurrent processing feedforward pass simple salient stimulus response time increase regardless number effect basic feature orientation categorical stimulus face effect remain regard attentional phase feedback connection know play important role feature group differentiate background especially highly salient perceptual fill work bar et support idea topdown projection prefrontal play important role object recognition quickly extract lowlevel spatial frequency information provide initial guess potential category form topdown expectation bias recognition recurrent connection rely heavily competitive inhibition feedback object recognition robust context computer vision rl able learn saccade visual scene learn selective attention learn feedback low level improve face recognition effective object recognition method train cifar cifar correct table classification result cifar cifar dataset error testset method note result average model method improve stateoftheart reference implementation feedback connection add recent network network architecture result apply step step step step number step evaluate figure dasnet train cifar different value t allow run iteration image performance number step network train performance drop dynamic stable combine traditional computer vision primitive iterative processing image use successfully use image reconstruction compression approach processing rl perspective lead novel algorithm improve performance research apply simplify dataset demonstration purpose computation constraint aim improve stateoftheart contrast apply perspective directly known stateoftheart neural network approach feasible actually increase performance experiment cifar experimental evaluation dasnet focus ambiguous classification case cifar cifar datum set high number common feature class interesting case approach learn train model dasnet aim fix erroneous prediction forget learn cifar dataset compose image split training testing sample image assign class cifar similarly compose contain class number step experimentally determined fix small computationally tractable allow interaction experiment set correct misclassifie l maxout network m train datum follow global contrast normalization normalization model consist convolutional maxout layer follow fully connect maxout softmax output dropout use layer input layer input layer population size sne set training dasnet day ti exclude original time use train m table performance dasnet method achieve relative improvement respect establish new stateoftheart result challenge datum figure classification testset output map activation final step difference activation compare step ie map left class probability timestep step classification dog cat note step network receive feedback step probability cat dramatically subsequently drop bit follow step network successfully cat dog investigate filter low layer emphasis change significantly change layer filter focus eye layer layer layer change layer change layer change layer timestep class probability figure classification cat dasnet output map activation final step change relative initial activation step white emphasis black change normalise effect clearly class probability time left network classify image dog wrong correct convolutional filter actually cat example include supplementary material second layer output map high convolutional layer complex change network level correspondence largely lose filter know code high level feature layer change influential close final output layer hard qualitatively analyze effect compare final activation layer corresponding change figure activation simply uniformly enhance instead complex pattern find increase decrease activation specific pixel highlevel actually open problem deep learning dynamic investigate dynamic small layer dasnet network train different value t evaluate allow run step figure result train dasnet cifar t performance peak step t expect reduce stay stable dasnet train use small number step dynamic stay stable evaluate step verify dasnet policy actually good use gate estimate information content follow way gate value final step use directly classification hypothesis gate use properly activation contain information relevant classification purpose dasnet train t use final output classification layer classification use near logistic regression perform result performance correct respectively similar performance dasnet confirm contain significant information conclusion dasnet deep neural network feedback connection learn reinforcement learning direct selective internal attention certain feature extract image rapid image classification standard stack feedforward filter feedback actively alter importance certain filter hindsight correct initial guess additional internal thought dasnet successfully learn correct image misclassification produce fully train feedforward network active selective internal attention enable stateoftheart result future research consider complex action spatially focus alter observed image acknowledgment acknowledge matthew discussion provide short literature review include relate work section reference network model mechanism pattern recognition shift position selforganize neural network grow adaptively vol ieee apply handwritten code recognition neural computation flexible high performance convolutional neural network image classification u schmidhuber deep neural network image classification cvpr long preprint krizhevsky hinton imagenet classification deep convolutional neural network nip p network icml network network ab detection cancer image deep neural network deep neural network segment neuronal membrane electron microscopy image nip learn hierarchical feature scene label pami p et detection unsupervised feature learn schmidhuber deep learning neural network overview tech gabor theory communication analysis information electrical journal communication engineering fergus understand convolutional network computer springer recognition human loop computer springer p tech m r fergus understand convolutional network tech t schmidhuber high dimension heavy tail natural evolution strategy krizhevsky learn multiple layer feature tiny image thesis et improve neural network prevent feature detector srivastava compete compute nip l p m l littman moore reinforcement learn survey artificial intelligence research pp r reinforcement learning introduction et natural evolution strategy ieee computation ieee t natural evolution strategy pp evolution publish distribute hierarchical processing primate cortex pp excitation circuit science pp j hierarchy cortical area primate visual system ed collin pp p r distinct mode vision offer recurrent processing trend neuroscience visual r second glance highlevel effect face c m topdown influence sensory processing m et feedback improve discrimination figure background v neuron nature pp role feedforward feedback connection m bar visual recognition proceeding science d et role competitive inhibition topdown feedback bind object recognition frontier psychology d t r oreilly limit feedforward vision recurrent processing promote robust object recognition object degrade journal cognitive neuroscience pp j schmidhuber r learn generate artificial trajectory detection international pp attend deep architecture image tracking neural pp r c oreilly biologically plausible learn use local activation difference generalized algorithm neural computation restore partly occlude pattern neural network model backward path springer larochelle combine foveal boltzmann machine image p m m l r b use guide autoencoder face recognition master thesis oreilly r et recurrent processing object recognition frontier psychology reinforcement learning adaptive control perception learn iterative image reconstruction neural abstraction pyramid face localization tracking neural abstraction pyramid compute application b freitas machine learn perspective predictive coding datum compression conference l neural network use icml pp fergus stochastic pooling regularization deep convolutional neural network
homotopy algorithm lasso observation theoretical abstract problem penalize leastsquare regression commonly refer lasso basis pursuit denoise lead solution sparse achieve model selection propose paper algorithm solve lasso online sequential observation introduce optimization problem allow compute homotopy current solution solution observe new data point compare method lar coordinate descent present application compressive sense sequential observation approach easily extend compute homotopy current solution solution correspond remove data point lead efficient algorithm leaveoneout crossvalidation propose algorithm automatically update regularization parameter observe new data point introduction regularization use norm attract lot interest statistic signal processing machine learn community penalty lead sparse solution desirable property achieve model selection datum compression obtain interpretable result paper focus problem penalize leastsquare regression commonly refer lasso set training example observation xi r rm wish fit linear model predict response function feature vector represent noise observation optimization problem min regularization parameter solution typically sparse ie solution entry nonzero identify dimension xi useful predict regularize leastsquare problem formulate convex quadratic problem qp linear equality constraint equivalent qp solve use standard method handle problem specialized problem recently introduce homotopy method apply lasso compute regularization path vary particularly efficient solution sparse method solve include iterative thresholding search bind optimization method gradient projection algorithm propose algorithm compute solution lasso training example obtain sequentially let solution lasso observe training example solution observe new data point rm introduce optimization problem allow compute homotopy use previously compute solution method particularly efficient support close section review optimality condition lasso use section derive algorithm test section algorithm numerically application compressive sense sequential observation leaveoneout crossvalidation propose algorithm automatically select regularization parameter time observe new datum point optimality condition lasso objective function convex nonsmooth norm differentiable global minimum subdifferential objective function contain vector subdifferential norm follow set rm let x matrix row equal optimality condition lasso t define active set index element nonzero simplify notation assume active set appear ie t t let x partitioning accord active set solution unique invertible rewrite optimality condition note know active set sign coefficient solution compute closed form propose homotopy outline suppose compute solution lasso observation additional observation rm goal compute solution augment problem introduce follow optimization problem n step propose algorithm compute path step vary regularization parameter t compute regularization path n lar solution path piecewise review paper step vary parameter t section compute path algorithm derivation section piecewise smooth function t notation light write t t section solution lasso easily compute active set sign coefficient know information available t active set sign remain t interval t solution t smooth denote point active set change transition point compute analytically t update active set sign remain valid t reach transition point process iterate know active set sign solution t compute desire solution suppose section loss generality solution t t t satisfy optimality condition suppose j exist t solution support sign p optimality condition t exist solution t satisfy optimality condition t sufficiently small partition accord active set rewrite optimality condition t t t t t t t solve t use equation t t t t continuous function t element strictly positive exist t t t element t remain positive change sign t t t t similarly t continuous function t exist t t element t strictly small absolute value t t obtain desire result solution t smooth t reach transition point component t component reach absolute value compute value transition point let partition accord active set use formula rewrite t t e t t e xn t t xn let ti value t t examine case component reach absolute value notice t e t u t rewrite e t e et t t element th component let cj column absolute value soon cj e et u t let j resp t value resp wj t t t e u t transition point equal t ti t t restrict real solution lie necessary derive propose algorithm homotopy algorithm online lasso compute path initialize active set nonzero coefficient let sign let v v xn correspond active set submatrix column correspond active set t initialize x initialize transition point t compute transition point t small previous transition point great step case component t correspond coefficient remove active set update v set case component correspond th coefficient reach absolute value add active set component reach resp set accord update active set update t t rank update update x step compute final value t value active set initialization compute solution lasso datum point rm case active set element let illustrate algorithm solution path regularization parameter successively varied simple numerical example figure complexity t transition complexity algorithm dominate inversion matrix point size matrix bound m update matrix figure solution path step algorithm set m value draw random left homotopy regularization parameter transition point inactive right piecewise smooth path t t positive remain active sign unchanged transition point black dot transition point rank cost compute inverse let total number transition point vary regularization parameter t complexity algorithm practice size active set low remain d homotopy complexity compare complexity recursive leastsquare correspond n m problem solution typically nonzero element cost update solution new observation om solution sparse small active set change k small update solution lasso fast update solution leastsquare problem suppose apply lar directly problem observation use knowledge vary regularization parameter large value size active set n let number transition point complexity approach ok q compare efficiency approach compare number transition point application compressive let rm unknown vector wish reconstruct observe linear projection gaussian noise variance general need m measurement reconstruct sparse representation coefficient noiseless case sufficient use k log m measurement approach know compressive sense generate interest signal processing community reconstruction solution basis subject measurement obtain sequentially advantageous start estimate unknown sparse signal measurement arrive oppose wait specify number measurement solve sequential measurement propose change active set criterion measurement need recover underlie signal case measurement noisy standard approach recover solve basis pursuit denoise problem instead algorithm suited compressive sense sequential noisy measurement compare propose lar apply entire dataset time receive new measurement compare method coordinate descent start receive new measurement initialize coordinate descent actual solution sample measurement model vector use sample datum non element value m set reconstruction error decrease number measurement grow plot parameter control complexity lar number transition point figure quantity consistently small measurement support solution change typically transition point figure time comparison algorithm implement observe require lot iteration converge optimal solution m find difficult set stop criterion ensure convergence algorithm consistently fast lar cd start figure compressive sense result x axis plot iteration iteration receive new measurement left comparison number transition point lar right timing comparison algorithm simulation repeat time shade area represent standard deviation selection regularization parameter suppose pre determine regularization schedule assumption practical regularization depend variance noise present datum know priori obvious determine regularization write weighting factor average error norm propose algorithm select data drive manner problem observation arg min previously piecewise linear compute gradient transition point let err error new observation propose following update rule select log log o solution observation correspond regularization parameter t sign use new observation test set allow update regularization parameter introduce new observation vary t perform update log domain ensure positive perform simulation use experimental setup section use figure representative example converge compare value obtain training test set observation fit model training set value small prediction error test set obtain similar result understand convergence property propose update rule regularization parameter object current research leaveoneout crossvalidation suppose section access dataset parameter tie noise datum know priori standard approach select parameter leaveoneout crossvalidation range value use datum point solve lasso regularization parameter compute prediction error data point leave repeat time datum point serve test set good value lead small prediction error propose algorithm adapt case wish update solution data point remove compute homotopy vary parameter compute second homotopy vary t effect remove data point use testing algorithm similar propose section omit derivation sample model vector use generate datum nonzero element add variance observation select range value figure histogram number transition point algorithm solve datum point solve problem time note majority case transition point approach efficient setting figure evolution regularization parameter use propose update rule figure histogram number transition point remove observation conclusion present algorithm solve penalize leastsquare regression online observation use current solution introduce optimization problem allow compute homotopy current solution solution observe new datum point algorithm particularly efficient active set change computational advantage compare lar coordinate descent application compressive sensing sequential observation leaveoneout crossvalidation propose algorithm automatically select regularization parameter new measurement use test set acknowledgment wish acknowledge support nsf grant fruitful discussion reference r tibshirani regression shrinkage selection royal statistical society series s donoho m atomic decomposition basis pursuit review l optimization sj m method square ieee select topic signal process b t hastie tibshirani angle regression annal statistic new approach variable selection square problem m journal numerical analysis dm m willsky homotopy continuation proceeding international conference acoustic speech signal processing donoho solution minimization problem method proceeding international conference acoustic speech signal processing m iterative thresholding algorithm linear inverse problem sparsity constraint communication pure mathematic cj olshausen locally competitive algorithm sparse approximation proceeding international conference image processing friedman hastie tibshirani coordinate optimization annal apply statistic r ng efficient sparse code algorithm proceeding neural information processing system nip m r bind optimization approach image deconvolution proceeding international conference image processing m gradient projection sparse reconstruction application compressed sensing inverse problem ieee journal select topic signal processing m effective method compute regression quantile m journal numerical e cande compressive sample proceeding international donoho compress sense ieee transaction information theory method compressed sensing proceeding international conference acoustic speech signal processing d willsky compress sense sequential observation proceeding international conference acoustic speech signal processing donoho extension compressed sensing signal processing
improved estimator variance explain presence noise laboratory laboratory abstract crucial develop mathematical model information processing brain success metric yield percentage variance datum explain model unfortunately metric bias intrinsic variability datum derive simple analytical modification traditional formula significantly improve accuracy measure bias similar precision measure error estimate true underlying variance explain model class estimator advance previous work account overfitte free model parameter mitigate need separate validation datum set b adjust uncertainty noise estimate add conditioning term apply new estimator binocular disparity tune curve set macaque neuron find population level variance unexplained gabor function noise introduction construct model biological system system neuroscience aim provide functional description fundamental physical law likely parametric signal processing single neuron rule sufficient datum test statistical validity particular mathematical formulation datum use test equally important know signal variance datum explain model commonly measure variance explain ve determination r statistic fundamental problem traditional estimator ve bias presence noise datum noise measurement error sample noise owe high intrinsic variability underlie datum especially important try model cortical neuron variability kind noise principle model need account evaluate quality model total variance datum consist true underlying variance noise traditional estimator yield systematic true ve model absence noise note author compute traditional measure noise level extrapolate noisefree condition method rely repeat stimulus impractical linden add analytical correction traditional formula order reduce bias number subsequent study use correction evaluate model eg far improve correspond author linden formula way importantly account number parameter model add correction term uncertainty noise estimation include conditioning term improve performance presence excessive noise propose principle method choose conditioning term order minimize bias mse estimator numerical simulation find analytical correction capable drastically reduce bias moderate high noise level maintain good traditional formula high level noise advantageous use conditioning term test effect improved formula data set disparity selective neuron find cell noise account unexplained variance population level find adjust noise gabor function explain underlie response variance derivation improved estimator traditional explain set measurement process d model prediction traditional variance explain compute difference total variance variance residual model mi usually report fraction total p mi case average individual measurement subject sample error variance independent random variable add measurement noise lead additive noise term numerator denominator equation noise level increase number model parameter equation consequence systematic true variance explain typically n usually small true ve effect figure example simulation simulation fit model simulate noisy datum sample different know underlie function allow compare estimate ve true absence noise average bias estimate ve true ve traditional variance explain instantiation simulation triangle simulate increase sample noise variance explain decrease significantly true ve example let ri dij ri number observation variable far assume measure dij draw gaussian distribution true mean di variance ri draw simplify presentation assume variable transform r ri follow r dij estimate base measurement r degree freedom term linden noise power estimator direct accurate especially small n r let good fitting model model class parameter explain absence noise p mi d pn true value variance explain like know base good fit model class underlie datum absence measurement sample noise course unknown value obtain draw probability distribution true variance explain normalize denominator numerator formula leave unchanged clear result denominator draw f distribution d p p r p dij r n dd r degree freedom parameter dd pn n mean distribution d d dd e n n unbiased estimator pn di pn dd d d dd reasoning find numerator equation dd mi follow f distribution n degree freedom di unbiased estimator di mi dm dm combine yield estimator numerator denominator individually unbiased mi d note apart difference noise estimation estimator propose contain special case identical uncertainty test model free parameter excellent approximation case fitting receptive field long series datum case fitting tuning curve limited number datum point fact account overfitte free parameter model mean formula overestimate true variance explain require separate validation datum set costly obtain point wish note readily generalize case noise level number observation ri mean base differ datum point conditioning term important note numerator denominator formula unbiased ratio generally fact ratio welldefine arbitrary measurement denominator negative practice avoid implicit explicit selection criterion impose require minimum snr datum analysis example criterion base significance level modulation datum assess way test criterion use context framework describe long use consistently effect criterion cut low tail distribution denominator draw exclude introduce bias denominator size depend noise criterion use recognize bias strong data ratio close singular propose additive conditioning term c denominator d n depend application optimal c choose minimize mse bias estimator generally optimal level conditioning scenario different come expense increase mse vice versa individual estimate small bias acceptable order improve accuracy minimize mse average large number estimate population neuron important estimator unbiased dm dd function number variable dm dd unknown approximate estimate equation optimal c determine case simple minimization large number random sample draw appropriate distribution compare equation e dm dd nn min dd dd nn dm min e dd dd note distribution numerator denominator sample vary estimate underlie noise share formula share minimization problem easily solve sample probability distribution subsequently find minimum mse bias respectively sample application simulate datum figure demonstrate performance estimator ve synthetic example left column result test model consist rd degree polynomial fit noisy datum sample gaussian distribution underlying domain study true ve model fit datum noiseless condition center right column case gabor function fit noisy datum sample reality true ve center column simulate right column gamma noise factor confirm traditional ve measure triangle increasingly negative bias increase noise level apply correction square negative bias turn positive overfitting noise free parameter model consideration lead true ve apply fitting datum instead separate set validation datum account number parameter greatly reduce bias close large range noise level dot bias notable bias bias rmse figure simulation result leave column rd degree polynomial fit noise datum draw underlie center right column gabor function fit noisy datum linear combination excitatory inhibitory leave center right gamma distribute noise factor row datum model line noisefree condition true ve respectively row bias define estimate true ve rmse function noise traditional estimator triangle correction square estimator eq dot row enforce prior knowledge estimator conditioning term eq optimize bias mse dash restrict ve reason bias estimator right column fourth panel datum sample variation datum exclude analysis note different scale panel figure tradeoff number condition number repetition r condition traditional measure triangle unbiased estimate dot total number measurement fix n r number different condition varied high noise level large number data sample pass significant modulation remain small traditional estimator reason decrease bias estimator high noise level cancellation bias term negative bias high noise level estimator difference gaussian general positive bias overfitting parameter account compare mse rmse different estimator similar case fit polynomial left column significantly improve case fit gabor function center right column note different yaxis scale column row simulate situation prior knowledge explicitly enforce numerator unbiased estimator eq yield value noiseless value positive negative estimator negative great restrict estimator interfere test conditioning term improve performance estimator find case gabor fit polynomial fit case gabor fit improvement conditioning term great high noise level expect bias decrease high noise level test mse slightly decrease high noise level conditioning purely analytical formula outperform conditioning approximation determine optimal c great inaccuracy analytical formula noise level especially true rd column strongly nongaussian gaussian assumption computation c conclude estimate ve presence extremely high noise confirm conditioning provide improvement particular situation consideration analytical estimator preferable note different yaxis scale nd row use estimator account noise major benefit total number measurement r usually limit tradeoff number condition number repeat r equal result traditional estimator ve depend strongly choice condition repeat high standard error mean noise low estimate ve regardless model figure demonstrate behavior case fit gabor exactly figure total number measurement constant traditional ve triangle decrease drastically number condition increase new unbiased estimator dot comparison reduce bias depend weakly r mean relatively repeat necessary allow condition test previously increase resolution surprising precise behavior respective estimator vary example approximation analytical derivation model linear parameter denominator ratio approximation accurate small noise regime noise level increase introduce bias interact depend situation disparity d ve min mse ve unbiased disparity ve old figure disparity tune curve neuron fit gabor function data neuron standard error mean estimate ve fit solid line change noise adjust datum gabor fit change test compatibility datum p c unbiased ve function signaltonoise power outlier traditional ve estimate unbiased ve conditioning minimize mse ve value limit range c fill symbol denote cell response gabor model evaluate test p application experimental datum method datum record isolated neuron awake macaque monkey previously publish stimulus consist dynamic random dot rd binocular disparity apply perpendicular preferred orientation cell include neuron analysis significantly modulate binocular disparity evaluate neuron pass test neuronal spike count approximately poisson distribute perform subsequent analysis use square root spike rate approximately variance fit gabor function parameter spike rate cell perform test residual minimum number different condition median number repeat result disparity tune curve v reasonably gabor function explain variance neuron remain reflect failure model merely consequence noise datum open question panel b figure response example cell gabor function traditional ve panel data significantly different model p adjust noise unbiased ve ie half unexplained variance attribute response variability measurement panel b opposite situation variance explain accord traditional measure additional attribute noise high ve measurement error relatively small model reject high significance p panel c unbiased estimate ve entire population neuron depend noise power relative signal power high relative noise level wide spread value decrease noise ve value fact overall population mean unbiased ve compare traditional estimate mean entire population variance previously deem unexplained model fact account uncertainty datum cell reject model denote fill circle panel demonstrate effect new measure individual cell estimation true ve neuron individually incorporate knowledge bound optimize conditioning term minimum mse exception neuron new estimate true ve great traditional average unexplained variance individual neuron account noise conclusion derive new estimator variance explain model describe noisy datum estimator improve previous work way account overfitte free model parameter adjust uncertainty estimate noise describe way add appropriate level conditioning case low signaltonoise datum impose constraint furthermore estimator rely large number repetition stimulus order perform extrapolation noise numerical simulation gaussian strongly skewed noise confirm correction capable noise level provide estimate greatly improve bias compare previous estimator note result simulation differ realistic simulation new estimator perform good important benefit new estimator address classical tradeoff number condition number repeat r condition result traditional estimator quickly deteriorate increase decrease r new estimator close invariant respect allow choose great n high resolution apply new ve estimator data set macaque disparity tune curve find variance previously gabor fit attribute sample noise population neuron find variance explain gabor model high previous estimate precisely account variability datum illustrate importance correction especially case model good improvement present limit neuronal tuning curve valuable model testing noise important factor acknowledgment thank helpful discussion reference network m advance neural information processing system m sj
use aperiodic reinforcement selforganization development dayan sj nowlan pouget sejnowski rd abstract present local learning rule hebbian learning conditional incorrect prediction reinforcement signal propose biological interpretation framework display utility example reinforcement signal cast delivery target present illustrate framework apply development oculomotor system activitydependent account selforganization vertebrate brain rely mainly hebbian rule drive synaptic learning brain major problem unsupervised rule different kind correlation exist approximately time scale effectively noise example relationship variable color motion topography mask appropriate segregation level thalamus know variable cortical area suggest certain set correlated input separate temporal noise input form supervised learning appear require unfortunately detailed dayan nowlan pouget sejnowski selection brain region feasible mechanism vertebrate brain question arise kind biological mechanism signal selectively bias synaptic learn particular subset correlation answer lie possible role play diffuse system know multiple diffuse system involve selforganization cortical structure bear singer deliver reward andor signal cortex structure influence learn adult recent datum suggest influence qualitatively similar predict classical conditioning theory system large cortical subcortical extensive axonal projection originate nucleus deliver compound target small number neuron comprise subcortical nucleus relative extent axon suggest nucleus report scalar signal target structure paper fact synthesize single framework relate development brain structure conditioning adult brain postulate modification hebbian account conditional incorrect prediction future deliver reinforcement diffuse system reinforcement signal derive drive contingency eye movement internal pathway lead cortical area section present framework propose specific model prediction future reinforcement vertebrate brain utilize firing diffuse system figure use model illustrate framework example suggest mapping oculomotor system develop example eye movement command appropriately calibrate absence visual experience figure second example demonstrate development mapping select visual target eye movement acquire target example describe framework permit development alignment multimodal map visual auditory superior colliculus example transformation auditory signal coordinate result implicitly development mapping parietal cortex colliculus theory consider class reinforcement learning rule static dynamic static reinforcement learning simple learning rule incorporate reinforcement signal u sing aperiodic reinforcement direct selforganization development connection weight input measure yt output measure t reinforcement measure ex learning rate case l drive external event world cortical projection internal event pick correlation system learn learning occur independently judge significant ie event l dynamic reinforcement learning learning drive prediction error informative way utilize reinforcement signal incorporate form prediction predictive form temporal difference learn specify weight change accord reward deliver instant time v value function value time t estimate future reward framework closely relate dynamic programming ai body theory build prediction error measure degree prediction future reward high low combination actual future reward t expectation reward time place dynamic rl biological context start simple hebbian rule learn prediction error learning slow prediction future reward contrast static rl account value l important system able predict anticipate future value r weight change include measure postsynaptic prediction brain account rl brain cortex structure t prediction future reinforcement reinforcement output subcortical nucleus deliver cortex permit learn experiment nucleus access cortical representation complex sensory input necessary instrumental classical conditioning occur figure scenario pattern activity region prediction future expect reinforcement time prediction future view excitatory drive cortex subcortical nucleus pathway high degree convergence b ensure drive predict scalar output consider pattern activity layer provide excitatory drive r cause output movement time movement provide separate source excitatory drive nucleus independent montague dayan nowlan pouget sejnowski layer layer b ci contingency figure prediction future reinforcement layer array unit project topographically layer weight develop accord equation represent value function weight r fix prediction future reward weight scalar highly convergent excitatory drive reinforcement nucleus r effectively sum input c external event world provide independent excitatory drive reinforcement nucleus scalar signal result output firing r activity deliver require output fire r control temporal change excitatory input constant slowly vary input learn layer accord equation text connection convey information sensory structure stretch receptor pathway c time excitatory input r sum immediate reward new prediction future reward vt reinforcement nucleus drive primarily change input time window difference excitatory drive time output reflect output distribute region permit hebbian weight change individual connection determine value function example hinge assumption hebbian learning cortex delivery reinforcement nucleus sensitive temporal change input constant slowly vary input initially system capable predict future delivery reinforcement correctly arrival cause large learning signal prediction error large error drive weight change synaptic connection correlated pre postsynaptic element prediction come actual future deliver reinforcement prediction accurate learning point system learn contingency currently control reinforcement delivery case delivery reinforcement control predictable contingency hebbian learning occur fluctuation prediction error positive mean use aperiodic reinforcement direct drive selforganization development l r d eye muscle e figure upper layer input array centersurround filter position project topographically middle layer middle layer project randomly motoneuron layer code equilibrium eye position signal example set equilibrium muscle muscle reinforcement signal originate eye movement muscle stretch foveation eye accord t dg respectively average activity right leave motoneuron layer fix gain parameter hand linearly combine eye position presence multiple statistically independent source control reinforcement signal pathway r system separately learn away contingency source control delivery allow development connection region stage control reinforcement pass contingency manner nucleus use deliver information globally different circumstance illustrate point development sensorimotor map example learn calibrate sensory experience figure architecture briefly cortical layer drive motor layer unit provide equilibrium command muscle mapping cortical layer layer random sparse connectivity plastic accord learning rule describe external event control delivery reinforcement eye movement foveation high contrast object visual input minimum eye movement necessary cause reinforcement change pixel direction figure begin demonstrate mapping motoneuron montague dayan nowlan pouget sejnowski figure learn calibrate eye movement command example illustrate reinforcement signal help organize appropriate balance sensorimotor mapping visual experience dark bounding box represent work area x foveal position cycle learn architecture figure weight pair balanced random activity layer provide drive motoneuron initially drive eye extreme position upper right position movement eye occur reinforcement deliver feedback cause weight begin decrease time weight motoneuron balanced eye foveal position cycle learning increase gain time initial value weight muscle balanced net eye small require increase order allow eye explore work range c size foveal region relative work range eye cover x region work area eye learning rate ex varied change result layer automatically calibrate absence visual experience imagine weight pair initially happen muscle weak effective drive muscle figure position learning indicate initially weight cause eye immediately extreme position figure reinforcement control eye movement foveation occur state roughly randomly generate activity motoneuron continually prediction reinforcement deliver weight begin decrease mediate condition decrease fast balance achieve path eye reach equilibrium random noise cause mean net eye movement mapping motoneuron layer balance large amplitude eye movement center figure label b result increase gain figure use aperiodic reinforcement direct selforganization figure development foveation map map learn cycle approximate movement vector stimulation position visual field length normalize size large movement qualitatively similar note scheme account activity contrast difference input assume normalize learn rate connectivity middle layer motoneuron randomize previous example weight layer motoneuron initially balanced learn foveation map sensory experience reinforcement deliver foveation successful expect rare event equilibrium achieve reinforcement come eye movement fully predict prior activity motoneuron contingency case foveation control delivery reinforcement result signal provide information link visual input layer figure result command system learn correctly figure motor map develop learn cycle current example weight layer motoneuron initially gain time large calibration figure learning currently assume cortical area select salient target learn align separate mapping primate superior colliculus know cell respond multiple modality include auditory input define head center coordinate system auditory receptive field shift position colliculus change eye position suggest existence mechanism maintain auditory visual map framework suggest developmental explanation finding term activitydependent selforganize principle consider intermediate layer model parietal cortex receive signal represent eye position retinal position visual target select visual input head position auditory target project superior colliculus visualize use figure parietal cortex layer colliculus drive motoneuron figure assume foveation target auditory visual deliver reinforcement learn layer colliculus follow equation manner example figure combination retinal eye position head center signal parietal layer predict eye movement select learning rule weight layer colliculus prediction future reinforcement figure foveation map develop code eye movement absolute coordinate relative equilibrium position eye current example foveation map inappropriate require persistent activity collicular layer maintain fix eye position instead collicular motoneuron mapping represent change balance muscle system code current eye position montague dayan nowlan pouget sejnowski initial architecture act learning express equation develop collicular mapping observe experiment combination signal parietal layer correctly predict foveation connection collicular layer stabilize current representation foveation target occur correct change firing motoneuron occur learn slow parietal layer leave cell visual auditory response modulate eye position signal collicular layer visual response cell modulate eye position signal auditory response modulate eye position reason difference layer parietal layer implicit new motoneuron model way equation learn collicular layer drive combination signal learning rule enforce common frame reference combination foveation target source reinforcement consider example visual target region different eye position change balance right leave muscle require retinal target eye position projection parietal collicular layer develop influence eye position fix retinal target eliminate influence eye position auditory target remain successful foveation auditory target require different region collicular map active function eye position example illustrate diffuse system employ single framework guide activitydependent map development vertebrate brain framework natural role diffuse system development conditioning adult brain illustrate external contingency incorporate cortical representation scalar signal reference watkin learning sequential decision technical report computer information science mf singer modulation visual cortical plasticity nature auditory receptive field primate superior shift change eye position nature response monkey neuron learning behavioral journal neurophysiology learn predict method temporal difference machine learn modern theory adaptive network expectation prediction psychological review model classical conditioning proceeding annual conference cognitive science society seattle wa
simple weight decay improve generalization connect abstract observe numerical simulation weight decay improve generalization feedforward neural network paper explain prove weight decay effect linear network suppress irrelevant component weight vector choose small vector solve learning problem second size choose right weight decay suppress effect static noise target improve generalization lot extend result network hide layer nonlinear unit finally theory confirm numerical simulation use datum introduction recent study generalization ability neural network learn machine depend balance information training example complexity network instance bad generalization occur information match complexity network complex little information training set instance network overfitte datum opposite situation correspond present address computer information science simple weight decay improve generalization number free parameter e number weight threshold use measure network complexity algorithm develop minimize number weight error training example small minimization number free parameter need different way constrain network decrease complexity limit growth weight kind weight decay prevent weight grow large necessary realize add term cost function penalize large weight wi error measure usually sum squared error parameter govern strongly large weight penalize vector contain free parameter network weight vector gradient use learn term cost function lead new term weight update wi formulate continuous time gradient force term present equation lead exponential decay weight obviously infinitely possibility choose form additional term concentrate simple form know long time weight decay form improve generalization widely recognize aim paper analyze effect theoretically experimentally weight decay special kind regularization discuss feedforward network feedforward neural network implement function input depend weight vector simplicity assume output unit input output e note input vector vector ndimensional input space weight vector vector weight space different dimension aim learning learn example learn underlie function produce target learning process assume target function actually implement network mean exist weight vector u target function equal network parameter teacher input vector produce right target sum square error p hertz p number training pattern learning equation write idea expand solution linear case analyze detail linear perceptron simple kind network linear perceptron characterize nl convenient normalization factor dimension weight space dimension input space learning equation simple form l define aij l transform equation basis diagonal yield ar eigenvalue subscript r indicate transformation basis generalization error define error average distribution input vector ij assume generalization error proportional natural eigenvalue covariance matrix nonnegative rank easily equal p easily eigenvector belong eigenvalue large lie subspace weight space span simple weight decay improve generalization e e input pattern subspace pattern subspace denote orthogonal subspace denote vl sufficiently example span space eigenvalue happen p solution inside simple exponential decay pattern subspace corresponding constant weight vector projection pattern subspace learning error think valley error surface u training set contain information help choose solution learning problem learn weight decay constant decay asymptotically t time weight decay choose solution small norm solution valley describe solution optimal average learn unreliable teacher random error teacher model add random term target variance assume mean note target exactly realizable network q simple model study learning function noise learning equation transform basis diagonal ex l asymptotic solution equation nl ar contribution generalization error square sum r average noise bar r expression minimum find derivative respect equal u remarkably depend hertz figure generalization error function line u dash line dotted line generalization error noise o variance noise assume random average u yield optimal independent r u average case weight decay extent prevent network fit equation noise project pattern contribution generalization error contribution average minimize weight decay size equation derive context particular eigenvalue spectrum figure dramatic improvement generalization error optimal weight decay use case present treatment independent spectrum conclude weight decay positive effect generalization linear network suppress irrelevant component weight vector choose small vector solve learning problem size choose right suppress effect static noise target nonlinear network possible analyze general nonlinear network exactly linear case local linearization possible draw interesting conclusion result previous section assume function realizable learn correspond solve p equation simple weight decay improve generalization variable number weight p equation define manifold weight space dimension p point manifold learning error expand expand v use yield vi derivative equation analogue define l outer product form rank r p rank rank course equal dimension manifold mention simple observation argue good generalization expect p accordance result cf current difference linear case rain need probably linear curve case fact valley rain disconnect contain u rank point immediate neighborhood u learning error large ie simple minimum assume learning find valley small weight decay pick point valley small norm point valley general prove pick solution good strategy point view sensible loose sense solution small probably choose value weight decay evident small error target case exactly line argument case weight decay improve generalization optimal choice strictly true small error linear approximation valid numerical experiment weight decay test problem simulation backpropagation derive entropic error measure momentum term fix use network input unit hide unit output unit weight train random word datum base word test different set random word training set test set independent run run w p figure line correspond generalization error epoch cycle training set weight decay low line weight decay dotted line low error learn weight decay lower dotted weight decay size weight decay insert figure error rate instead square error error rate fraction wrong phoneme phoneme vector small angle actual output choose result fig clear improvement generalization error weight decay use improvement error rate insert fig term relative improvement result weight decay value try basically conclusion weight decay improve generalization way suppress irrelevant component weight vector choose small vector solve learning problem size choose right weight decay suppress effect static noise target static noise target view model learn function analysis assume network expand optimal weight vector simple weight decay improve generalization strictly valid little neighborhood improvement weight decay test simulation datum weight decay decrease generalization error square error significantly actual mistake rate network phoneme close output choose acknowledgement acknowledge support natural science computational connect reference schwartz sa solla denker exhaustive learn neural computation tishby e sa solla consistent inference probability layered network prediction generalization international joint conference neural network page haussler size net valid generalization neural computation denker sa solla optimal brain damage advance neural information processing system page improve generalization neural network prune international system application forecasting editor advance neural information processing system page translation invariant recognition massively parallel network editor parallel architecture language lecture note computer science page generalization weight decay architecture selection nonlinear learning system proceeding practical bayesian framework network proceeding hertz generalization linear perceptron presence noise appear journal physics sejnowski parallel network learn english text complex system hertz r g introduction theory computation city
integrated perception recurrent multitask visual geometry group abstract modern discriminative predictor match natural intelligence specific perceptual task image classification object detection boundary extraction major advantage natural intelligence work perceptual problem solve efficiently integrate manner order capture advantage machine perception ask question deep neural network learn universal image representation useful single task solution different task integrate framework answer propose new architecture multinet deep image feature share task task interact recurrent manner encode result analysis common share representation datum manner performance individual task standard benchmark improve share feature significantly integrate solution common representation introduction natural perception extract complete interpretation sensory datum coherent efficient manner contrast machine perception remain collection disjoint algorithm solve specific information extraction subproblem recent advance modern convolutional network dramatically improve performance machine individual perceptual task remain unclear integrate way natural perception paper consider problem learn datum representation integrated perception question ask possible learn universal datum representation use solve subproblem interest computer vision finetune effective method transfer deep convolutional network different task fact possible learn single share representation perform subproblem simultaneously second question complementary feature sharing different perceptual subtask combine subtask extract partial interpretation datum problem form coherent picture datum consider incremental interpretation scenario subtask parallel sequentially order gradually share interpretation datum contribute dimension computer vision system operate manner different module run parallel object detection follow instance segmentation question endtoend systematically paper develop architecture multinet fig provide answer question multinet build idea share representation integration space reflect conference neural information processing system nip class location figure multinet propose modular multitask architecture perceptual task integrate manner subnetwork encode datum image example produce representation h share different task task estimate different label object class location example use decoder function task contribute share representation mean correspond encoder function loop close recurrent configuration mean suitable integrator function avoid clutter diagram statistic extract datum result analysis carry individual subtask loose think integration space progressively update information obtain solve subproblem representation distill information available task resolution recurrent configuration multinet advantage learn latent integration space automatically task discover automatically second task treat symmetric manner associate encoder decoder integrator function system modular easily new task architecture support incremental understanding task contribute latent representation output available task far processing finally multinet apply image understanding set architecture general apply numerous domain new architecture describe detail sect instance specialize computer application sect empirical evaluation sect demonstrate benefit approach include share feature different task accuracy integrate output different task share representation yield accuracy improvement sect summarize finding relate work multiple task learn multitask learn method study decade machine learn community method base key idea task share common lowdimensional representation jointly learn task specific parameter train task parallel mitchell thrun propose sequential transfer method neural net exploit previously domain knowledge constraint parameter current task devise hybrid method learn separate model improve generalisation exploit correlation prediction multitask learn computer vision improve result computer vision problem typically researcher incorporate auxiliary task target task jointly train parallel achieve performance gain object track object detection facial differently propose multitask network cascade convolutional layer parameter share task task predict sequentially method train multiple task parallel require specification recurrent network work relate recurrent neural network rnn successfully use language model speech recognition handwritten recognition semantic image segmentation human pose estimation relate work propose iterative segmentation model progressively update initial figure multinet recurrent architecture component box repeat task solution feed error propose efficient grid base object detector iteratively refine predict object coordinate minimise training error method base iterative solution mechanism main goal improve generalisation performance multiple task share previous prediction learn output correlation method section introduce multinet architecture integrated multitask prediction sect discuss ordinary multitask prediction special case multinet sect integrate prediction propose recurrent neural network architecture address simultaneously multiple datum labelling task symmetry drop usual distinction input output space consider instead space label th space denote symbol follow use input eg image network infer label estimate neural network object class location reason useful notation symmetric possible ground label treat input instead task associate corresponding encoder function map label representation r r enc task decoder function direction common representation space h label information r r r extract datum different task encoder integrate share representation use integrator function update operation incremental associate iteration number t update equation write r rt rk t note equation r constant correspond variable input network ground update overall task specify triplet t contribution update rule task achieve decompose integrator function sequence update k rk task option investigate task variant recurrent architecture possible natural process task sequentially add complication choose particular order case suboptimal instead propose update task recurrent iteration follow ordinary multitask prediction iteration measurement acquire share representation initialize enc r symbol denote initial value variable practice output x enc task compute step correspond ordinary multitask prediction discuss later sect t update task k use equation share representation update use r label predict use x t idea feed network output processing exist exist recurrent architecture case use process sequential datum pass output obtain process element sequence instead feedback use integrate different complementary labelling task model reminiscent architecture case encoder decoder function associate output label input datum ordinary multitask learn learn base share feature parameter different task multinet reduce ordinary multitask learn recurrence fact multinet simply evaluate function task share common subnetwork multitask learn representation sharing conceptually simple practically important allow learn universal representation function enc work task simultaneously possibility learn representation verify empirically nontrivial useful fact particular experiment image understand sect certain image analysis task possible efficient learn share representation case feature sharing improve performance individual subproblem multinet classification localization detection section instantiate multinet complementary task computer vision object classification object detection detection main advantage multinet compare ordinary multitask prediction share parameter related task improve generalization capture correlation task input space example computer vision application ordinary multitask prediction able ensure detect contain detect object multinet instead capture interaction different label potentially learn enforce constraint soft distribute manner integrate output individual task share representation discuss detail specific architecture component use application start point consider standard image classification powerful network exist choose good perform model time reasonably efficient train evaluate vggm network model pretraine image classification imagenet datum extend object detection follow particular fast method design subnetwork task component describe detail focus component correspond ordinary multitask prediction use multiple task integration ordinary component layer vggm network group convolutional section comprise linear convolution nonlinear activation function case max pooling normalization follow fullyconnected section convolutional filter support size corresponding input layer softmax compute posterior probability vector imagenet class vggm adapt different task follow clarity use symbolic task index consider cls det instead convolutional section vggm use image encoder compute initial value h share representation cut vggm level convolutional layer motivate fact fullyconnected layer remove dramatically blur spatial information like preserve object localization share representation tensor h c spatial dimension c number feature channel determine vggm configuration sect branch direction choose decoder task image classification cls object detection detection image classification branch choose enc rest original vggm network image classification word decoder function label initialize cls fullyconnected layer original vggm difference fullyconnected layer randomly predict different number c possible object instead imagenet class second difference final output vector binary probability obtain use sigmoid instead softmax object detection decoder instead base fast architecture classify individual image region belong object class type background selective search window method use generate m region bound box proposal image set spatial pyramid pool layer extract subset feature map h correspondence region use max pool object detection similarly detector contain fully connect layer initialize manner classification decoder train exception layer need use region class far describe image decoder branch task component sufficient ordinary multitask learn correspond initial multinet iteration specify component allow iterate multinet time recurrent component integrate multiple task task integration need construct encoder function enc task integrator function construction possible experiment simple order encode image label encoder rcls cls binary vector probability r possible object class cls corresponding value spatial location u v formally rcls c encode object detection label similar reflect geometric information capture label particular bounding box m extract associate vector cls cls probability object class background m r cls decode map rcls c max pooling box cls m u label encode entirely analogous manner lastly need construct integrator function experiment simple design simply stack evidence different source rcls update equation det rcls cls note formulation require modify fullyconnected layer decoder share representation c c channel instead c figure illustration multinet instantiation tackle computer vision problem image classification object detection detection original vggm architecture initialize randomly additional dimension linear map experiment second update equation cls cls det rcls r cls c c filter bank purpose reduce stack representation original c channel useful design maintain representation dimensionality regardless number task add compression perform experiment implementation detail training image encoder initialize pretraine vggm model use section conv conv input network image downsample h spatial dimension h c number feature channel c note decoder contain respectively subnetwork cls det comprise layer vggm follow linear predictor output dimension equal respectively c cls c cls c max pooling perform grid spatial bin enc sect contain parameter train task associate corresponding loss function classification task objective minimize sum negative posterior image contain certain object type allow different object present single image combine fact classification branch use sigmoid binary logistic regression object detection task decoder optimize classify target region cls c class background label class label mutually exclusive furthermore train branch perform bound box improve fit selective search region propose fully connect layer use softmax classification regression object detection task initialize distribution standard deviation respectively fully connect layer use object classification task adaptation layer eq initialize gaussian standard deviation layer use learning rate filter bias use sgd optimize parameter learning rate epoch lower epoch observe run iteration recursion sufficient reach performance marginal gain possible use publicly available toolbox experiment result section describe discuss experimental result model benchmark pascal voc dataset contain training validation image object category ground truth bounding box annotation target category use dataset obtain bound box annotation object consist annotate category engine bird person remove annotation small pixel category training sample number category reduce dataset provide annotation training validation split train model train split report result validation split task follow standard pascal voc evaluation report average precision iou detect box ground object classification detection respectively detection follow report relaxed iou threshold result task report order establish baseline train independent network task network initialize vggm model classification regression layer initialize random noise layer finetune respective task object detection use implementation note consistency baseline method minimum dimension image scale pixel task include object classification layer employ scale feature map dimensionality second baseline train multitask network share convolutional layer task setting ordinary multitask prediction sect observe multitask model perform comparable independent network efficient share convolutional computation training image case combine multiple label improve efficiency case performance finally test multinet model setting define update rule correspond eq respectively model outperform independent network multitask network remarkable model consist small number parameter sum independent network good model update consistently outperform roughly point mean furthermore multinet improve ordinary multitask prediction exploit correlation solution individual task addition observe update perform update constraint share representation space dimension regardless number task expect large capacity bottleneck observe improvement compare ordinary multitask prediction run test case verify multinet learn mix information extract task exploit prediction perform task able improve ground truth label test time test time ground label rcls iteration multinet ground truth class label read prediction iteration performance task improve respectively feedback class information strong effect class prediction modest significant effect task pascal voc consist train validation test image contain bound box annotation object category annotation available dataset exclude detection task run baseline good model object classification detection result report test split depict note individual network obtain detection score method task classification independent bottleneck table object classification detection detection result pascal voc validation split method task classification independent table object classification detection result pascal voc test split parallel result method consistently outperform baseline classification detection task conclusion paper present multinet recurrent neural network architecture solve multiple perceptual task efficient manner addition feature parameter share common multitask learning method multinet combine output different task update share representation iteratively result encourage architecture successfully integrate multiple task share large subset datum representation match outperform network second iterative update common representation effective method share information different task far improve performance acknowledgment work acknowledge support start grant integrate detailed image understanding reference baxter model inductive bias learn recurrent human pose estimation friedman predict multivariate response multiple linear royal statistical society series statistical methodology pose estimation iterative error feedback cvpr r multitask learn machine learn zisserman return detail deep convolutional net r r l yuille detect detect represent object use holistic model body cvpr page semantic segmentation cascade cvpr darrell deep convolutional activation feature generic visual recognition ab m zisserman pascal visual object class voc challenge r fast m j schmidhuber novel connectionist system unconstrained recognition pattern analysis machine intelligence ieee transaction recognition deep recurrent neural network page ieee spatial pyramid pool deep convolutional network visual recognition eccv page e hinton r r salakhutdinov reduce dimensionality datum neural network science schmidhuber long shortterm memory neural computation t statistical language model base neural network m mitchell s thrun neural network learn nip page m m iterative grid base object detector cvpr p h r collobert recurrent convolutional neural network scene parse m unsupervised learning invariant feature hierarchy application object recognition cvpr page d e rumelhart e hinton r learn representation backpropagate error cognitive model large scale visual recognition challenge q sequence sequence learn neural network nip page thrun l editor learn learn academic publisher segmentation selective search object recognition convolutional neural network proceed acm int bengio p extract compose robust feature denoise autoencoder icml page acm m r fergus understand convolutional network ab t robust visual tracking structured sparse learn z p facial detection deep multitask learn page springer
spike neuron computational neuroscience unit abstract propose new interpretation spike neuron bayesian integrator accumulate evidence time event external world body communicate neuron certainty event model spike signal occurrence new information ie predict past activity result fire statistic close poisson provide deterministic representation probability proceed develop theory spike neural network recurrent interaction implement variant belief propagation perceptual motor task perform central nervous system probabilistic describe bayesian framework important hidden property direction motion appropriate motor command infer noisy local ambiguous sensory cue evidence combine prior sensory world body importantly inference lead quick decision change world noisy cue integrate online way account unpredictable event change motion direction appearance new stimulus raise question temporal integration perform level propose single neuron sensory cortex represent compute log probability sensory variable certain value visual motion neuron prefer direction alternatively avoid normalization issue provide appropriate signal decision neuron represent log probability ratio particular hypothesis motion likely right left log probability convenient assumption independent noisy cue simply combine linearly physiological evidence neural representation log probability log probability ratio model assume neuron represent probability firing rate argue important study probabilistic information encode spike idea online integration noisy cue underlie rate code require average large population noisy neuron long period time particular natural task require integration place time scale interval spike efficiently signal event cognitive science analog quantity addition neural theory inference spike bring close physiological level generate easily testable prediction propose new theory neural processing spike train provide deterministic online representation ratio spike signal event eg ratio exceed predict previous spike form code inspire idea energy landscape code propose brown contrary theory use representation probability model require different model encode decode output spike provide new unpredictable temporally independent evidence use directly input bayesian neuron finally neuron use build block theory approximate bayesian inference recurrent spiking network connection neuron implement underlying bayesian network consist couple hide markov model propagation spike form belief propagation underlie graphical model theory provide computational explanation general physiological property cortical neuron spike frequency adaptation poisson statistic spike train existence strong local inhibition cortical column maintenance tight balance excitation inhibition finally discuss implication model temporal code spike log posterior odd synaptic integration inference hidden markov chain propose neuron code underlie hide binary variable state evolve time assume depend state previous time step conditionally independent past state state switch constant rate constant rate example transition rate represent motion preferred direction appear receptive field long likely stay neuron infer state hidden variable n noisy synaptic input consider observation hidden state initial version model assume input conditionally independent homogeneous poisson process emit spike time t sit constant probability constant probability synaptic spike assume independent previous synaptic spike previous state spike synapsis result generative model hide markov chain figure estimate state hidden variable communicate estimate neuron example emit spike sensory evidence threshold neuron report communicate certainty current state certainty form log ratio probability hidden state probability state synaptic input receive far log use short hand notation synaptic input receive present past refer log odd ratio thank conditional independency assume generative model compute log odd ratio iteratively limit follow differential equation l ron sit b dt s j t t e count log odd time figure generative model synaptic input schematic representation odd ratio encoding decode dash circle represent element place model neuron spike fire exceed c example trial state switch shade area plain dot gt black stripe correspond spike train mean log odd ratio dark line mean output firing rate clear line e output plot line trial isi distribution neuron c clear line isi distribution poisson neuron rate synaptic weight describe informative synapse state hidden variable log synaptic spike sit impulse log odd ratio positive synapse active hidden state ie increase neuron confidence state negative synapse active decrease neuron confidence state bias determine informative receive spike convention consider bias positive need simply invert status state generation output spike spike train convey sparse representation report new information state redundant report precede spike proposition base argument spike expensive minimum second spike convey redundant information require entire spike train independent spike account individually finally seek consistent model spike output similar semantic spiking input maximize independence spike condition propose neuron fire difference log odd ratio prediction gt log odd ratio base output spike emit far reach certain threshold suppose element predict good neuron need fire expect prediction inaccurate figure b practice happen neuron receive new evidence follow dynamic spike receive equation gt output output spike fire positive constant free parameter parameter constrain statistic synaptic input result figure c plot typical trial behavior l g presentation stimulus random synaptic input integrate l fluctuate eventually exceed lead output spike immediately spike jump g prevent rare case second spike immediately follow jump implement relative refractory period g decay tend converge stable level log eventually exceed lead new spike threshold crossing happen stimulation net synaptic input alter create high overall level certainty mean log odd ratio output firing rate t neuron presentation preferred stimulus mean firing rate o switch plot figure d t average trial surprisingly mean log posterior ratio ratio reflect integration synaptic evidence effective time constant depend transition probability ron state stable ron synaptic evidence integrate infinite time period mean log posterior ratio tend increase decrease linearly time example figure d state stable old synaptic evidence discount saturate t track state perfectly contrast mean output firing rate o p form predictive code output spike reflect new synaptic evidence log posterior ratio particular mean output firing rate linear function mean input p wi q analogy integrate interesting insight computation perform neuron linearize l g mean level trial reduce analysis statistically stable period state constant case output prediction constant time mean level certainty approximation post spike jump input fluctuation small compare mean level certainty rewrite gt membrane potential kl leak membrane potential depend overall level certainty positive monotonic increase function s t s t s t c t t t t t t t t t tn t t tn t s t s t t s t log odd feedback tiger stripe time figure bayesian causal network tiger network feedforward compute log posterior recurrent network compute log posterior odd variable d log odd ratio simulated trial net work c text thick line lxt thin line lxt lxt inhibition lxt average trial effect feedback linearize bayesian act stable regime integrate fire membrane potential integrate input leak neuron fire membrane potential reach constant threshold spike reset interestingly appropriately choose compression factor mean input linearize neuron mean membrane potential purely drive threshold input fluctuation random walk membrane potential consequence neuron fire close poisson process particular find factor close isi distribution figure e entire range parameter test neuron balanced input propose model reproduce statistic real cortical neuron balance implement model neuron effective synaptic input balanced decode previously element predict log odd ratio compute gt output spike fig b course require estimate transition probability ron learn observe spike train explicit decode necessary perform bayesian inference spike network intuitively quantity model neuron receive transmit new information exactly probabilistic propagate connected statistical element choose optimally influence drift usually negligible compare large fluctuation membrane potential bayesian inference cortical network model neuron input output semantic use build block implement complex generative model consist couple chain consider example example figure parent variable presence tiger cause state child variable represent presence motion child variable bayesian neuron identical describe previously result bayesian network consist couple hide markov chain inference architecture correspond compute log posterior odd ratio tiger log posterior observe stripe motion xkt synaptic input receive entire network far unfortunately learn network general couple chain require expensive computation perform simply propagate message time variable node particular state child variable depend state child previous time contrast network implement pairwise interaction connection spike neuron implement conditional probability link corresponding binary variable need assume additional conditional independency node generative model joint probability pairwise factorize word mean variable bias ability influence dynamic ie affect transition probability example tiger affect probability stripe appear disappear increase probability present naive implementation restrict case marginal posterior probability compute iteratively propagate belief time variable model propagate spike neural network probability variable xkt directly update conditional probability observe synaptic input connected marginalize hidden state course use use output code previously output directly represent new synaptic evidence receive result equation identical derive previously ex previously synaptic weight ron describe informative receive spike synapse p log p informative receive l p xkt p xkt model stage process input propose come bayesian neuron result implement update rule spike neural network figure b represent generative model figure possible child consider case feedback connection mean wk case network compute probability tiger time t integrate multiple sensory cue presence stripe motion visual scene example trial plot figure fix state tiger stripe present shaded temporal window absent sample state child ie motion correspond observe synaptic input generative model synaptic input generate use input network figure b plot log odd ratio tiger lxt stripe lxt function time stripe receive noisy synaptic input provide weak evidence present tiger neuron able combine input child high certainty unfortunately bayesian inference feedforward network incomplete presence tiger affect probability stripe way round implement need feedback connection network feedforward feedback processing fail activity illustrate figure d balanced failure presence loop spike increase certainty probability fire spike increase turn certainty loop result spike network ad report new information loopy belief propagation avoid evidence discount old evidence synaptic input implement use inhibitory neuron connect figure inhibitory neuron use predict redundant feedback bayesian neuron receive prediction new information account communicate neuron excitatory loop compensate inhibitory loop result balance excitation inhibition level neuron network result trial plot figure d tiger log odd ratio feedforward case plot stripe log odd ratio increase presentation tiger feedback word account synaptic input synaptic input child neuron evidence motion thank presence common source tiger trial find statistic output firing linear function input firing rate stable statistical regime discussion start interpretation synaptic integration single neuron form inference hidden markov chain derive model spike neuron interaction able compute marginal posterior probability sensory motor variable evidence receive entire network view brain implement underlying bayesian network interconnect neural architecture conditional probability represent synaptic weight model rich set prediction general property neuron synaptic dynamic time constant depend overall level input specific form frequency spike synaptic adaptation excitation inhibition restrict probabilistic computation involve binary variable relate work similar idea apply population encoding log probability distribution analog variable dayan submit nip nonlinear processing single neural level emerge picture relatively neuron act integrate fire neuron drive noise output firing rate weight sum input fire rate firing statistic poisson output spike train deterministic function input train spike report fluctuation level certainty predict stability stimulus contribution loop network contribution inhibitory neuron fire definition unpredictable observation lead suggest irregular firing poisson statistic observe cortical neuron arise direct consequence random fluctuation sensory input instability real word unreliable chaotic neural processing finally crucial biological model find adaptive dynamic synaptic plasticity able learn parameter internal model conditional probability currently explore issue fortunately learning rule local unsupervised accord preliminary work synaptic weight bias depend joint probability spike learn spike time dependent plasticity observe transition probability simply correspond neuron switch active inactive state acknowledgment thank helpful discussion comment work support fellowship royal society reference m analysis visual motion comparison neuronal psychophysical performance journal neuroscience brown spike boltzmann machine solla t muller editor neural information processing system volume page mit perception bayesian integration sensorimotor learn nature m synaptic efficacy neuron nature m m role neural integrator perceptual decision cortex r rao bayesian computation recurrent neural circuit neural computation m noise neural code cortical organization current opinion neurobiology weiss velocity likelihood biological machine vision r rao b olshausen m editor probabilistic model brain perception neural function page mit press cambridge weiss correctness belief propagation gaussian graphical model arbitrary topology neural computation
harmonet neural net harmonize chorale style harmonet system employ connectionist network music processing present train bach chorale use error backpropagation system capable produce chorale style melody system solve musical realworld problem performance level appropriate musical practice harmonet power base new code scheme capture relevant information integration backpropagation symbolic algorithm hierarchical system combine advantage introduction neural approach music processing previously propose implement mozer promise neural network offer shed light aspect human term symbol rule ultimately music lie eye ear great learn obey number rule eg famous parallel fifth rule suffice characterize personal style easy test generate music random use menzel chorale melody chorale harmonization figure beginning chorale melody harmonization rule constraint result error free overcome gap obey rule produce music accept standard propose harmonet integrate symbolic algorithm neural network compose chorale style melody neural net concentrate task responsible standard set bach nearly example original bach chorale use training datum conventional algorithm task observe pitch range prevent parallel fifth harmonet level performance approach applicable musical practice task definition process compose chorale melody chorale harmonization typically chorale melody plain melody harmonize sing voice chorale harmonization melody depict example chorale melody harmonization music student teach solve task harmonization theory rule develop task harmonet learn harmonize chorale example neural net use find characteristic harmonic sequence harmonet neural net harmonize chorale system overview set bach chorale goal find approximation complex function l map chorale melody harmonization demonstrate example follow section propose decomposition task decomposition learning task decompose dimension different level abstraction obtain eighth note view furthermore chord harmony certain attribute inversion characteristic chorale harmonic skeleton representation figure time divide small learn independently look local context window treat small independently certainly global consistency dependency lose current decision window additionally consider outcome external feedback figure consecutive window cut harmonic harmonize chorale harmonet start learn harmonic refine finally augment figure leave harmonic skeleton chorale rich harmonic structure mainly responsible musical appearance generate good harmonic skeleton important harmonet subtask harmonet create harmonic sequence sweep chorale melody determine harmony note consider local context previously find harmony input position t following information extract form training example nal target learn harmony t position t mark input consist harmonic context left external feedback t t t context pitch contain sure function relation legal melody simplicity view function menzel harmonic figure chord harmonic skeleton chorale figure information relative position t beginning end musical phrase boolean value indicate harmony t component importantly harmonic function relate key harmony key piece inversion indicate note harmony characteristic note directly belong harmony additional coding pitch recognize relevant regularity training example problem discuss place mozer develop new code scheme guide harmonic necessity music piece note represent set harmonic function contain t s sp c d e t d standard musical denote harmonic function result representation distribute respect pitch local respect harmonic function allow network anticipate future harmonic development harmony input unit pitch need harmonet neural net harmonize chorale r t t t figure harmonic skeleton break local window harmony determine position t input window position t input unit component harmony t t unit code phrase information unit stress net total input unit output unit use hidden layer unit advanced version figure right use net parallel train window different size harmonic function majority net vote pass subsequent net determine chord inversion characteristic harmony use window different size parallel employ statistical information solve problem appropriate window size task level find middle chorale melody harmony h determine neural net include information chord inversion pitch problem tackle generate test approach symbolic algorithm select good chord set chord consistent harmony h common chorale constraint subtask net teach add eighth output network set eighth note particular chord t augment network input describe local context c t term attribute interval voice lead characteristic presence eighth previous chord menzel melody determine harmony t t expand harmony chord ri r r j j harmonic function ill inversion eighth note r characteristic ui t harmonize chorale figure leave overall structure harmonet right specialized architecture parallel sequential net text performance harmonet train separately set bach chorale contain chorale major minor key respectively pass chorale window explain set training example net train error backpropagation algorithm need epoch achieve reasonable convergence figure harmonization produce harmonet melody training set music judge quality chorale produce harmonet level harmonet compare approach figure harmonet chorale melody use thesis demonstrate expert system harmonet neural net harmonize chorale t ri r ft r ri r r u r r r r ri lsi r r r r t figure chorale major key harmonize harmonet j j r r r d s t t t lj j r dd d figure harmonize harmonet l d menzel conclusion music processing system harmonet present paper clearly musical realworld application reach connectionist approach believe harmonet owe success clean task decomposition meaningful selection representation relevant feature use hybrid approach allow network concentrate musical essential instead structural constraint hard network learn easy code abstraction chord harmony reduce problem space resemble problem approach harmonic representation pitch harmonic character melody explicitly experiment replace neural net harmonet learn technique decision tree near neighbor classification report task et outperform harmonet general music processing system architecture design solve difficult specific task neural learn component small musical expert knowledge necessary design system easy build flexible pure rule base system acknowledgement thank fruitful discussion contribution research music lab reference fischer edition comparative study backpropagation proc seventh international conference machine learn expert system harmonization chorale style phd department understand music cognition connectionist music composition base constraint advance neural information processing nip r p approximation structure musical pitch psychological review vol m connectionist approach algorithmic composition computer music journal vol
mathematical model axon guidance diffusible factor geoffrey cognitive medical center abstract develop nervous system gradient diffusible factor play important role guide axon appropriate target paper shape gradient calculate function distance target time start factor production use estimate relevant parameter value experimental literature spatiotemporal domain growth cone detect gradient derive large time value maximum guidance range obtain value fit experimental datum small time analysis predict guidance long range possible prediction remain test introduction develop nervous system grow axon guide target distance away mechanism contribute review mechanism diffusion factor target extracellular space create gradient increase concentration axon sense follow central nervous system process occur case guidance ofaxon ganglion process mouse davy axon plate ofaxon axonal branch tract evidence come vivo experiment piece target tissue embed dimensional near piece tissue contain appropriate population neuron axon growth observe direct target diffusible signal vivo system describe target population ofaxon distance axon target readily varied guidance generally distance great limit explain term mathematic diffusion related constraint distribution diffusible factor satisfy provide effective guidance cue point firstly absolute concentration factor small large secondly fractional change concentration factor width generally assume growth cone sufficiently large constraint relate case problem overcome statistical noise low concentration noise exist fluctuation number molecule factor growth cone analyze high concentration limit source noise stochastic variation bind factor receptor distribute growth cone high concentration receptor saturate gradient apparent close concentration upper low limit high gradient need ensure detection zigmond limitation constraint impose guidance range diffusible factor investigate discussion mathematical model consider source factor diffusion constant cm sec rate infinite spatially uniform threedimensional volume initially decay factor assume symmetric diffusion dimension concentration distance r source time r d r r complementary error function percentage change concentration p small distance width growth cone r function surprising characteristic firstly fix decrease t large gradient distance occur immediately source start factor large secondly fix numerical result p r particular decrease distance reach minimum increase position minimum large distance t increase general characteristic constraint summarize follow small time start production factor distribute concentration c fall quickly away source gradient percentage change growth cone p large time proceed factor evenly distribute c increase p decrease large time tend inverse variation distance source r tend mathematical model ofaxon guidance diffusible factor rr independent parameter mean large time maximum distance guidance diffusible factor possible linearly growth cone diameter parameter value diffusion constant estimate diffusion constant molecule mass cm subsequently direct determination diffusion constant molecule mass mammalian cell yield value cm et fit particular solution diffusion equation datum determination gradient active calculate value cm tissue identify diffusible factor know involve axon guidance protein molecular mass scale roughly inversely radius molecule le cube root mass value cm scale yield cm paper consider cm cm rate production factor q hard estimate vivo insight gain consider experiment find turn response sensory axon nearby fill solution estimate rate find effect concentration low nm correspond q study growth cone induce gradient contain solution release rate order calculation q perform suggest appropriate value growth cone diameter r system mention diameter main body growth cone lm ignore increase effective width value lm lm consider minimum concentration gradient detection study suggest gradient detection limit dynamic receptor bind physical limit lack molecule factor optimal detection occur concentration growth cone equal dissociation constant receptor zigmond zigmond study suggest low concentration limit dissociation constant zigmond protein delete recently possess bind activity order estimate dissociation constant nm ai comparison dissociation constant p nm low concentration limit consider maximum concentration gradient detection theoretical consideration suggest sensitivity fix gradient fall plot log background concentration peak dissociation constant receptor zigmond raise time dissociation constant appear prevent axon guidance discuss concentration great dissociation constant number receptor reduce sensitivity dissociation constant constitute reasonable upper bind concentration minimum percentage change growth cone p establish gradient factor directly substrate measure response retinal axon estimate p study cell system suggest optimal value concentration far dissociation constant p expect large zigmond p p consider result order estimate bound rate production factor q biological tissue empirical observation use order day guidance generally distance m davy et assume constrain low concentration limit substitute parameter equation specify day nm q hand assume constraint high concentration limit ie day nm q reasonable assume roughly nm q nm result discuss use value q constraint arise equation plot figure case respectively picture constraint c nm plot constraint p growth cone diameter m graph change growth cone diameter m identical change diameter m constraint satisfied region left relevant line line c nm approximately vertical axis case parameter high concentration limit prevent gradient detection axon micron source assume important constraint expect large t gradient constraint p r m p r m p m growth cone gradient constraint satisfied time distance source m p m gradient constraint line end right early time p exceed critical value distance formula p r branch p curve graph right t increase guidance initially limit concentration constraint maximum distance guidance occur increase smoothly t reach instance m assume concentration limit hour hour d particular time gradient constraint start effect rapidly reduce maximum range guidance asymptotic value t increase time p hour cm day cm sec clear picture exact size mathematical model ofaxon guidance diffusible factor ci e cd e distance micron c p distance micron d ci c c e p distance micron c p distance micron figure graph gradient constraint solid line interact minimum concentration constraint line limit guidance range constraint evolve time row p row p left column d cm right column d cm constraint satisfied leave appropriate curve cm limit quickly dominant constraint maximum guidance range contrast d cm sec concentration limit dominant constraint time day gradient constraint start effect rapidly reduce maximum guidance range diffusion constant affect position constraint play important role constraint gradient evolve effect subtle d cm cm increase time nm limit reach decrease time nm limit reach discussion gradient constraint fractional change growth cone width yield asymptotic value maximum distance guidance occur gradient stabilize respectively fit datum fact system mention introduction grow axon target vivo concentration limit provide weak constraint gradient limit distance possible dependent value q roughly estimate q significantly low concentration limit provide restrictive constraint q different value different target tissue gradient constraint curve independent gradient constraint provide robust explanation observed guidance limit model prediction guidance long distance observe possible gradient stabilize early stage follow start factor production concentration fall provide effective guidance time guidance range maximum diffusion constant d rapidly diffuse molecule cm occur hour slowly diffuse molecule occur day easy investigate vivo molecule large time immediately follow start production source definite benefit gradient molecule nature optimize start production factor relative time guidance require order exploit evolve gradient extended range especially important large animal axon need guide long distance develop axon guidance gradient component physics journal diffusion mathematic diffusion second edition pn zigmond cell focus rev cell characterization concentration gradient active j cell mathematical model ofaxon guidance diffusible factor diffusion axon mathematical guidance axon axon high concentration nerve growth factor science target control extension directional growth mammalian brain science m delete cancer encode cell m diffusible factor axon asymmetric modulation activity induce growth cone turn davy early sensory nerve guide peripheral target nerve growth factor nature davy effect specific target develop mammalian nervous system nature ad diffusion small molecule mammalian cell nerve growth family m m target develop axon guide m molecular biology axon guidance science m m guidance develop axon mammalian central nervous system nature stochastic model movement math consequence modulation orientation cell
robust lasso missing corrupt observation tran paper study problem accurately recover sparse vector highly corrupt linear measurement e e sparse error vector nonzero entry unbounded bounded noise propose socalled extended lasso optimization consideration sparse prior information e result extend lasso faithfully recover regression corruption vector analysis rely notion extended restrict eigenvalue design matrix second set result apply general class gaussian design provide surprising phenomenon extended lasso recover exact sign support e log p log observation fraction corruption arbitrarily close analysis observation require achieve exact sign support optimal introduction central problem statistic linear regression goal accurately estimate regression vector noisy observation np r measurement design matrix r stochastic vector noise particular situation recently attract attention research community concern model number regression variable p large number observation p circumstance impose additional assumption model know problem illpose linear regression consistent accordingly line work high dimensional inference base impose different type structure constraint sparsity group sparsity popular model focus sparsity assumption regression vector estimate standard method lasso propose use l penalty surrogate function enforce sparsity constraint positive regularization parameter l norm define past year numerous study understand regularization sparse regression model work mainly characterize type loss function consider instance author seek obtain regression estimate deliver small prediction error author seek produce minimal parameter estimation error measure norm b line work consider variable selection goal obtain estimate correctly identify support true regression vector achieve low prediction parameter estimation loss know sufficient necessary impose certain low bound small singular value design matrix notion small mutual incoherence design matrix require achieve accurate variable selection notice previous work rely assumption observation noise bound energy assumption likely estimate reliable unable identify correct support observation mind paper extend linear model consider noise unbounded energy clear entry corrupt large error impossible faithfully recover regression vector practical application face acoustic recognition portion vector contaminate gross error formally mathematical model e e rn sparse error location nonzero entry unknown magnitude arbitrarily large noise vector bounded entry paper assume multivariate gaussian inn distribution model include particular case miss datum problem entry fully observe miss problem particularly important computer vision biology application entry miss nonzero entry e location associate miss entry observation vector value entry inverse sign problem recover datum gross error gain increase attention recently interesting practical application theoretical consideration recent line research recover datum corrupt measurement study context robust principal component analysis let consider example illustrate face recognition model originally propose context face recognition problem face test sample assume represent linear combination training face dictionary coefficient vector use classification case face occlude object glass occlusion occupy portion test face consider sparse error e model subspace cluster important problem high dimensional analysis cluster data point multiple subspace recent work problem solve express data point sparse linear combination datum point coefficient vector recover solve problem employ clustering datum point represent matrix wish find sparse coefficient matrix b datum miss contaminate outlier formulate problem e minimize sum norm respect b e sensor network model sensor collect measurement signal independently simply project row vector sense matrix hxi measurement send center analysis highly likely sensor fail send measurement correctly report totally irrelevant measurement accurate employ observation model model worth notice aforementioned application e play role sparse error application e contain meaningful information necessary recover example kind signal separation e distinct signal component video audio furthermore application classification cluster assumption test sample linear combination training sample dictionary design matrix violate sparse component e linear regression model observation model sparsity assumption regression vector error e propose follow estimate unknown parameter error e e e e positive regularization parameter optimization extended generalization lasso program set e return standard lasso additional regularization associate e encourage sparsity error parameter e control sparsity level paper focus follow question necessary sufficient condition ambient dimension p number observation sparsity index regression fraction corruption extended lasso able unable recover exact support set e extended able recover e small prediction error parameter error particularly interested understand asymptotic situation fraction error arbitrarily close previous work problem recover estimation vector error e originally propose analyze absence stochastic noise observation model author propose estimate e solve program e e result asymptotic nature class gaussian design entry optimization recover e precisely high probability fraction corruption arbitrarily close result hold stringent condition particularly require number observation grow ambient dimension p sparsity index k small portion condition course far optimal bind compressed sensing statistic recall log p sufficient conventional analysis line work focus optimization paper author establish gaussian design matrix log p s sparsity level e recovery exact follow fact combination matrix obey restrict isometry property wellknown property use guarantee exact recovery sparse vector minimization result allow fraction corruption close previous work closely related current paper recent result positive regularization parameter employ control sparsity e use different method set author deterministically select log p matrix solution follow optimization exact constant fraction observation corrupt establish similar result gaussian design matrix number observation order log p know optimal e e contribution paper consider general setting observation contaminate sparse dense error allow corruption linearly grow number observation arbitrarily large magnitude establish general scaling p k s extended lasso recover regression corruption vector particular interest follow equation scaling n p k extended lasso obtain unique solution small estimation error b second scaling p k extended lasso obtain exact sign support recovery observation corrupt scaling n p k solution extended lasso specify correct sign support answer question introduce notion extended restrict eigenvalue matrix identity matrix property satisfie general class matrix answer question require strict condition design matrix particular design matrix rely standard assumption invertibility mutual incoherence t denote z identity matrix vector reformulate z standard lasso model previous result apply design matrix irrelevant set long behave gaussian matrix establish theoretical analysis need study interaction gaussian identity matrix exploit fact matrix consist component component special structure analysis reveal interesting phenomenon extended lasso accurately recover corruption e fraction corruption measure recoverability variable criterion parameter accuracy feature selection accuracy analysis extend situation identity matrix replace tight frame d extend model group lasso matrix lasso sparse notation summarize standard notation use paper reserve s sparse support e respectively design matrix subset t use denote t submatrix obtain extract row index column index t use notation c c c c refer positive constant value change line line function g notation mean exist constant c notation gn mean notation gn mean n symbol mean main result section provide precise statement main result paper subsection establish parameter estimation provide deterministic result basis notion extended restrict eigenvalue far satisfy property high probability subsection consider feature estimation establish condition design matrix solution extended lasso exact sign support parameter estimation conventional lasso obtain low parameter estimation bind necessary impose condition design matrix paper introduce notion extended restrict eigenvalue extend condition let restrict set matrix satisfie extend assumption set exist l l khk k c restrict set c interest define e follow c k assumption natural extension restrict eigenvalue condition restrict strong convexity consider absent vector f equation set c condition return restrict eigenvalue define explain length restrict eigenvalue weak assumption design matrix solution lasso consistent assumption hand state optimization problem consider optimal solution parameter choose wk kwk assume design matrix obey extended error set e bound khk k e interesting observation theorem error bind naturally split component relate sparsity index addition error bind contain quantity sparsity index regularization parameter extend constant term relate corruption e omit obtain similar parameter estimation bind standard lasso choice regularization parameter e explicitly assume vector entry n design matrix column clear high probability p log p kwk log sufficient p select log p e log n glance parameter meaningful interpretation good selection small estimation error produce parameter actually control sparsity level regression vector respect fraction corruption relation restrict set follow lemma extend condition actually exist large class design matrix row mean covariance state let define quantity operate covariance matrix cmin min small eigenvalue cmax max big eigenvalue maximal entry diagonal consider random design matrix row assume cmax select p log log p probability great expc matrix satisfie extended parameter l provide c cmin k log p s min small constant c c like remark choice parameter special design matrix p independent easily wk log p probability log p selection follow proof lemma appendix control term restrict eigenvalue x k r khk k mutual incoherence column space matrix incoherent column space identity matrix exist m khk k incoherence column space sufficiently small m r conclude r m khk small mutual incoherence property especially important provide regression separate away sparse error simplify result consider special case uniform gaussian design situation cmin cmax following result corollary corollary standard gaussian design let standard gaussian design matrix consider b optimization problem regularization parameter choose optimal solution p p log p e log assume ck log p s small constant c c probability great expc error set h e bound p p khk log log corollary reveal interesting phenomenon set log fraction corruption linearly proportional number sample extended lasso capable recover coefficient vector corruption miss vector e bounded error dense noise observation model extended recover exact solution result impossible achieve standard lasso furthermore know prior number corrupted observation order log p select instead log minimize estimation error equation feature selection random application feature selection criterion preferred feature selection refer property recover parameter sign support true general good feature selection imply good parameter estimation reverse direction usually hold investigate condition design matrix scaling p k regression sparse error vector obtain criterion consider linear model design matrix row mean covariance matrix know lasso order obtain feature selection accuracy covariance matrix obey property invertibility small mutual coherence restrict set t property guarantee strictly lead unique solution program second property require separation component relate set t set t c sufficiently small invertibility guarantee uniqueness require t t invertible particularly let cmin min t t require cmin mutual incoherence t c t t refer operator norm worth note standard factor omit condition tight condition use establish feature estimation lasso constant factor fact quantity special set value close number sample increase simplicity proof end elaborate quantity operate restricted covariance matrix t cmax define maximum eigenvalue t cmax max t dmax dmax denote norm matrix t dmax t t dmax t k result involve quantity operate conditional covariance matrix define t t c t c t c t t t define l t t t t t c t end denote shorthand establish following result design covariance matrix obey assumption linear model random design covariance matrix satisfy invertibility incoherence property suppose solve extended lasso regularization parameter obey dmax n log p log let log sequence p s regularization parameter e satisfy dmax log max c c numerical constant addition suppose e r logp k log k c s r cmax c e ns follow property hold probability great c log b extended lasso unique exact sign support solution pair norm bound b e e interesting observation important observation extended lasso robust arbitrarily large sparse error observation sense extended lasso view generalization invertibility mutual incoherence assumption covariance matrix standard lasso extended lasso program recover regression vector error exact sign support observation contaminate arbitrarily large error unknown support sacrifice corruption robustness additional log factor number sample notice error fraction log logp k sample sufficient recover exact sign support regression sparse error vector consider special case design covariance matrix case entry n quantity cmin cmax dmax dmax l addition invertibility mutual incoherence property automatically satisfied imply number error close number sample need recover exact sign support satisfie n logp furthermore theorem guarantee norm estimate q p regression rate log p n choose log equivalent establish close error rate order log p know standard lasso corollary interesting able guarantee stable recovery fraction corruption converge fraction come arbitrarily close sacrifice factor log number sample theorem imply significant difference recovery obtain small parameter estimation error recovery obtain correct variable selection corrupted observation linearly proportional recover exact sign support require increase p corollary log p log sample behavior capture similarly standard lasso point corollary theorem number sample need recover accurate sign support optimal rescaled sample size satisfy regularization parameter e select solution extended lasso correctly identify sign support high probability theorem linear model random covariance satisfy invertibility incoherence property let logn sequence p k satisfie p dmax log min log cmin cmax e c small universal constant probability tend solution pair extended lasso correct sign support illustrative simulation section provide simulation illustrate possibility extended lasso recover exact regression sign support significant fraction observation corrupt large error simulation perform range parameter p k s design matrix uniform row n fix set p k generate sparse vector e location nonzero entry uniformly random magnitude distribute experiment consider vary problem size p type regression sparsity index sublinear sparsity p logp linear sparsity fractional power sparsity p case fix error support size mean half observation corrupt selection theorem suggest number sample logp log guarantee exact sign support recovery choose log n logp k parameter rescaled sample size parameter control extended algorithm select log p log e log suggest noise level fix report success solution pair sign support e fig point curve represent average trial demonstrate simulation extended lasso recover exact sign support observation contaminate furthermore unknown constant theorem match simulation result sample size n logp probability success start imply failure extended acknowledgment acknowledge support research office aro national science grant reference agarwal m wainwright noisy matrix decomposition relaxation optimal rate high dimension mach learn icml page sublinear sparsity p p p rescaled sample size probability success probability success probability success fractional power sparsity linear sparsity rescale sample size p rescale sample size figure probability success recover sign support p tsybakov simultaneous analysis statistic e robust principal component analysis submit publication e cande plan model selection minimization annal statistic e cande tao selector statistical estimation p large statistic e r sparse subspace cluster ieee conference computer vision pattern recognition cvpr page m r g baraniuk exact signal recovery corrupt measurement pursuit conference signal system computer page compressed sensing matrix completion constant proportion corruption preprint systematic measurement matrix compressed sensing presence gross error datum compression conference page p high dimensional graph variable selection statistic recovery sparse representation highdimensional annal statistic p ravikumar m unified framework analysis decomposable regularizer preprint h nguyen d tran exact recoverability dense corrupt observation minimization preprint m restrict eigenvalue property correlated gaussian design machine learn research r tibshirani regression shrinkage selection royal statistical society series s p condition use prove oracle result electronic journal statistic m wainwright sharp threshold highdimensional noisy sparsity recovery use l constrain quadratic programming lasso ieee tran information theory dense error correction minimization ieee transaction information theory face recognition sparse representation ieee transaction pattern analysis machine intelligence s robust pursuit ad neural nip page m yuan model selection estimation regression group variable royal statistical society series t sharp performance bound square regression regularization annal statistic p model selection consistency machine learn research
asymptotic qiearne research group artificial intelligence abstract paper discount mdps factor asymptotic rate convergence qiearne r r log tit provide stateaction pair sample fix probability distribution ratio minimum maximum stateaction frequency result extend convergent online learning provide pmax minimum maximum stateaction frequency correspond stationary distribution introduction qiearne popular reinforcement learning convergence demonstrate literature littman szepesvari littman aim paper provide upper bind convergence rate base qiearne algorithm upper bind strict computer experiment present form lemma underlie proof indicate obtain upper bind strict slightly complicated definition r result extend learn aggregated state relate algorithm admit certain form asynchronous approximation present address associative m asymptotic watkin introduce follow algorithm estimate value pair discount markovian decision process mdps watkin e x e state action respectively finite assume random sample mechanism eg simulation interaction real markovian environment generate random sample form probability yt fix denote rx immediate average reward receive execute action state assume independent history assume xt c c value learning rate associate stateaction pair time t value assume value actual state action reestimate step l l ax qiearne guarantee converge fix point q operator lr define rx convergence proof find littman szepesvari littman q identify learn agent act optimally underlie mdp simply choose action maximize q agent state main result condition learning rate require stateaction pair visit infinitely mild condition article strong assumption sequence independent random variable common underlying probability distribution assumption essential simplify presentation proof greatly relaxation discuss later far assume learning rate special form l number time stateaction pair visit process time step t ie szepesvari t assumption relax discuss later technical reason far assume absolute value random reinforcement signal admit common upper bind main result follow condition follow relation hold asymptotically probability theorem try q b t suitable constant b pmax px px sample probability note slow slow proof present step step extended littman main idea compare qt simple process note essential difference definition qt qt appearance q define equation qt firstly notice consequence change process qt clearly converge q convergence investigate component separately use standard technique use simple device difference process satisfie follow inequality t stand maximum norm task convergence rate reduce convergence rate step simplify notation introduce abstract process update equation e identify stateaction pair qt analyze process step consider process ft miss process follow lemma assume tit independent random variable common underlying distribution pi process define asymptotic satisfie probability mini pi maxi proof outline let ie tk small time time time component update equation maxi inequality hold time ith component update ll exploit time yield decrease iterate consider follow approximation c compute explicitly use large deviation theory kl sj hold define tk k hold monotonicity lk ly step assume fortunately know extension law iterate logarithm stochastic approximation process convergence szepesvari rate uniform boundedness ofthe random reinforcement signal exploit step major sufficient provide convergence rate estimate perturb process define c state convergence rate fast define process zti process clearly low bound perturb process obviously convergence rate slow convergence rate provide prove fast asymptotically lxt decrease large t argument similar use derivation ftk mini approximation argument similar step bind j follow theorem integral law integration r ly case treat similarly step piece apply qt theorem discussion conclusion restrictive condition assumption concern sample note fix learning policy process nonstationary markovian process learning policy converge sense xt stand history learning process process eventually sample distribution replace stationary distribution underlie stationary markovian process action asymptotically optimal course learn support stationary process exclude stateaction pair action suboptimal ie condition long satisfied notice proof convergence process follow similar line proof present paper expect convergence rate hold prove use nearly identical technique case step find explicit expression constant clearly depend heavily sampling transition probability reward underlie mdp choice harmonic learning rate arbitrary general sequence employ artificial time x use note harmonic sequence t note develop bound asymptotic present form proper usage large deviation theory enable bound asymptotic possible way extend result paper include qiearne learn aggregate state qiearne algorithm corresponding difference process satisfie inequality similar application estimate convergence proof average reward reinforcement learning algorithm idea algorithm follow kind discount sum converge average value discount rate converge scheme rely idea use method develop proof convergence corresponding possible like note related result obtain finally note application result immediately convergence rate modelbased rl transition reward estimate respective average clearly qiearne simple calculation law iterate hold learning process underlie modelbase rl exact expression convergence rate depend explicitly computational effort spend obtain estimate optimal value function effort spend fast convergence bind provide direct way control tradeoff computational effort rate acknowledgement research support grant provide contract like thank numerous helpful discussion reference scientific programming nd modify form iterative method dynamic programming annal statistic convergence stochastic iterative dynamic programming algorithm neural computation littman m markov game framework learn proc international conference machine learn page m c generalize reinforcement learn model convergence application int machine learn discount discount reinforcement learn case study compare r learning q learning proceeding international conference machine learn page average reward reinforcement learning foundation algorithm empirical result machine learn major p law iterate logarithm method training algorithm automation m markov decision process discrete stochastic dynamic programming apply probability model optimization application m reinforcement learn soft state aggregation proceeding neural information processing system littman m convergence learn littman m generalize markov decision process dynamic programming reinforcement learning machine learn preparation available tr asynchronous stochastic approximation machine learn stochastic approximation cambridge watkin learn delay reward thesis cambridge
predict emg datum neuron variational bayesian square kakei science neuroscience computational neuroscience laboratory abstract increase number project neuroscience require statistical analysis high dimensional datum set instance predict behavior neural ring operate articial device brain recording brainmachine interface linear analysis technique remain case classical linear regression approach numerically high dimension paper address question emg datum collect arm movement monkey faithfully reconstruct linear approach neural activity primary motor cortex m achieve robust datum analysis develop bayesian approach linear regression automatically detect exclude irrelevant feature datum overtte comparison ordinary square stepwise regression partial square regression force combinatorial search predictive input feature datum demonstrate new bayesian method superior mixture characteristic term regularization overtte computational ease use demonstrate potential replacement linear regression technique result analysis demonstrate emg datum predict neuron far open path possible realtime interface brain machine introduction recent year grow interest large scale analysis brain activity respect associate behavioral variable instance project find area brainmachine interface neural ring directly use control articial system robot control computer screen brain signal classify visual stimulus present subject project brain signal process typically high dimensional order thousand input large number redundant irrelevant signal linear modeling technique linear regression primary analysis tool datum computational problem datum analysis involve datum require model extract datum good generalization property crucial predict behavior future neural recording online interpretation brain activity control device study information processing brain surprisingly robust linear modeling high dimensional datum nontrivial noise encounter numerical problem high classical technique ridge stepwise regression partial square regression know prone overtte require careful human ensure useful result paper focus improve linear datum analysis high dimensional scenario describe view develop statistically robust box approach automatically detect relevant input dimension generalization exclude dimension statistically sound way purpose investigate bayesian treatment linear regression automatic relevance detection algorithm variational bayesian square vbls formulate closed form help variational bayesian approximation turn computationally highly apply vbls reconstruction emg datum motor ring use datum set collect datum analysis address important question term neuron directly predict emg trace m topological organization information m use predict behavior future brainmachine interface main focus paper robust statistical analysis kind datum comparison classical linear analysis technique force combinatorial model search cluster demonstrate vbls achieve black box quality robust statistical analysis technique parameter follow section rst sketch derivation variational bayesian square subsequently perform extensive comparative datum analysis technique context prediction emg datum neural ring high dimensional regression develop vbls algorithm let revisit classical linear regression technique standard model linear regression m regression vector compose component number input dimension additive noise input output ordinary square estimate regression vector main problem ols regression high dimensional input space rank assumption violate datum set ridge regression problem numerically introduce bias additionally input dimensionality exceed dimension matrix inversion prohibitively computationally expensive idea exist improve ols stepwise regression employ strongly potential overtte inconsistency presence input datum linear regression b d probabilistic zi vbls figure graphical model linear regression random variable circular node observe random variable double circle point estimate parameter square node deal directly dimensionality reduction technique principal component regression factor regression useful method retain component input space large variance regardless component prediction eliminate low variance input high predictive power output class linear regression method projection regression technique notably partial square regression perform computationally inexpensive univariate regression projection direction choose accord correlation input output slightly heuristic nature surprisingly successful algorithm highdimensional regression problem tendency overtte lasso absolute shrinkage selection operator regression shrink certain regression interpretable model sparse tuning parameter need set use crossvalidation manual finally method inversion assume regression problem degrade presence input follow section develop linear regression algorithm framework automatically regularize problem overtte iterative nature formulation expectationmaximization problem avoid computational cost numerical problem matrix inversion address major problem highdimensional ol simultaneously conceptually algorithm interpret bayesian version partial square regression variational bayesian square figure illustrate graphical model need order develop robust bayesian version linear regression figure depict standard linear regression model spirit know optimal projection direction input datum entire regression problem solve univariate regression project datum output optimal projection direction simply true gradient input output algorithm encode projection direction hidden variable figure b variable zim denote index datum set datum point result input multiply corresponding component projection vector zim sum form predict output formally linear regression model eq xim m probabilistic treatment standard normal assumption distribution form normal normal zim xim t model identical ols notice graphical model regression fanin output data xi view new regression model problem maximize incomplete log likelihood log pyx maximize expected complete log likelihood log log log m m denote n matrix zim result update require standard manipulation normal distribution result pn pn m m xim pn pn estep d m m t p dene dm important note update computationally complexity od number input dimension instead od associate ols regression come cost iterative solution instead oneshot solution b ols prove version square regression guarantee converge solution ol new appear replace matrix inversion ols iterative method alternative convergence guarantee improvement previous approach true power probabilistic formulation apparent add bayesian layer achieve desire robustness face datum automatic relevance determination bayesian point view parameter treat probabilistically integrate overtte purpose figure c introduce precision variable m regression p m m m m gammaa m m vector m order obtain tractable posterior distribution hidden variable b zim use factorial variational approximation true posterior q b z q note connection m correspond zim figure design graphical model marginal distribution student allow traditional hypothesis test minimal factorization posterior q possible special design result augment model follow distribution t m xim zim xim m m m m b mechanism infer dimension contribution observed output mean large m equivalent small variance suggest close contribution output use nd posterior update distribution omit update equation space constraint similar update focus posterior update xim p bm m xim zim m m p zim z m m m pn note update equation m rewrite m pn m pn bm m m sm pn pyi m demonstrate absence correlation current input dimension residual error rst term cause current regression decay result regression solution regularize number retain input nal regression vector perform functionality similar automatic relevance determination update equation algorithmic complexity remain far marginal distribution degree freedom t m m allow principle way determine regression exclude mean standard hypothesis testing variational bayesian square vbls regression bayesian treatment linear regression problem evaluation turn application evaluation vbls context predict emg datum neural datum record m monkey key question address application emg datum reconstruct accurately good generalization neuron contribute reconstruction muscle iii vbls algorithm compare analysis technique underlying assumption analysis relationship neural ring muscle activity approximately linear datum set investigate datum dierent experiment rst experiment monkey task dierent direction equally space horizontal planar circle radius variation experiment hold place apply force direction condition movement force feedback visual display monitor neural activity m neuron record condition datum point neuron emg output muscle second experiment kakei involve monkey train perform dierent combination wrist movement dierent arm datum set consist neural datum m neuron record step step nmse train nmse test datum nmse test kakei datum figure normalize mean squared error crossvalidation set fold fold vbls datum set kakei set step lasso table percentage neuron match baseline algorithm average muscle datum set wrist produce datum point neuron emg output contribute muscle experiment neural data represent average ring rate time align emg datum base analysis scope paper method datum set baseline comparison good emg reconstruction obtain limited combinatorial search possible regression model particular model characterize subset neuron use predict emg datum neuron theoretically possible model exist value large exhaustive search consider possible combination neuron require week computation node cluster computer optimal predictive subset neuron determine fold cross validation baseline study serve comparison stepwise regression lasso ols vbls ve algorithm use validation set employ baseline study number projection datum t find leaveoneout crossvalidation stepwise regression use matlab regression implement manually choose optimal tuning parameter crossvalidation set ols implement use small ridge regression parameter order avoid matrix inversion step step lasso neuron find neuron find muscle data vbls muscle kakei datum figure average number relevant neuron find crossvalidation set fold fold average number relevant neuron calculate crossvalidation set nal set relevant neuron reach algorithm common neuron find relevant crossvalidation set inference relevant neuron base subspace span projection relevant neuron infer regression parameter use stepwise regression lasso regression determine number relevant neuron input include nal model note retain input dimension algorithm omit relevant analogous rst datum set combinatorial analysis perform kakei datum set order determine optimal set neuron contribute muscle produce low possible prediction error fold crossvalidation stepwise regression lasso regression ols vbls apply use crossvalidation set employ procedure describe rst datum set result figure general emg trace predictable result generalization error comparable produce baseline study kakei algorithm perform similarly regression perform little rest stepwise regression perform far bad dataset ols regression attain bad error performance typical traditional linear regression method high dimensional datum motivate development vbls average number relevant neuron find vbls slightly high baseline study figure result surprising study consider possible combination neuron good generalization result vbls bayesian approach regularize participate neuron overtte occur note result muscle figure b datum consider outlier table demonstrate relevant neuron vbls coincide high percentage baseline result stepwise regression inferior outcome general vbls achieve comparable performance baseline study reconstruct emg datum neuron iterative statistical method perform slow classical oneshot linear square method ie order minute data set analysis achieve comparable result combinatorial model search week cluster computer discussion paper address problem analyze high dimensional datum linear regression technique encounter neuroscience new brainmachine interface achieve robust statistical result introduce novel bayesian technique regression analysis automatic feature detection variational bayesian square comparison classical linear regression method standard obtain force search possible linear model demonstrate vbls perform manual parameter tune quality black box statistical analysis technique point concern vbls variational approximation algorithm quality function approximation know approximation joint distribution create distribution potentially assume vbls tend case vbls distribution push regression parameter close vbls slightly function unlikely future evaluation comparison method reveal detail nature variational approximation regardless appear vbls useful replacement classical regression method incremental implementation need realtime analysis brain information acknowledgment research support national science foundation grant grant grant intelligent control dynamic brain project fund science computational neuroscience laboratory fund medical research service reference action thought nature si ab schwartz direct cortical control device science control twodimensional movement signal interface human proceeding tong decode visual content human brain nature neuroscience ree predict orientation stimulus activity primary visual cortex nature neuroscience optimize linear algorithm realtime robotic control use cortical ensemble recording monkey cognitive neuroscience h cognitive control signal neural science neal bayesian learn dept computer science change temporal pattern primary motor cortex activity directional force movement task neurophysiology kakei muscle movement representation primary motor cortex science kakei direction action represent ventral cortex nature neuroscience e direct cortical control muscle activation arm movement model nature neuroscience r apply regression analysis backward forward stepwise automate subset selection algorithm frequency obtain noise variable mathematical statistical psychology principal component regression exploratory statistical research journal cg local dimensionality reduction kearn sa solla editor advance neural information processing system mit statistical view regression tool h soft modeling latent variable nonlinear iterative partial square approach editor perspective probability statistic paper m r tibshirani regression shrinkage selection royal statistical society series optimal num t hastie r generalize additive model number statistic apply probability chapman maximum likelihood incomplete datum royal statistical society series s schaal bayesian relevance vector machine proceeding conference machine learn rubin bayesian datum chapman mj graphical model variational method d m opper editor advance mean field method theory practice mit press
perturbative msequence auditory system identification mark center theoretical abstract paper present new method study auditory system base msequence method allow study linear response system presence stimulus speech sinusoidal modulation allow construct linear kernel receptive field time stimulus present use method calculate modulation transfer function single unit inferior cat different operating point discuss nonlinearitie response introduction popular approach system identification ie identify accurate analytical model system behavior use wiener expansion model behavior functional orthogonal polynomial series respectively approach model response linear combination small power stimulus effective mild nonlinearitie derive linear combination numerically unstable highly nonlinear system problem biological system adaptive system behavior dependent stimulus ensemble instance find auditory nerve information rate depend white noise ensemble use approach handle difficulty expansion simply compute linear response small perturbative stimulus presence different ensemble operating point collect linear response email perturbative msequence auditory system identification different operating point fit nonlinear response fit nonlinear function piecewise linear approximation adaptive system procedure apply different operating point correspond different point time axis perturbative stimulus wide application physics use characterize linear response perturbative analysis use circuit analysis small signal model structural diagnostic analysis neurophysiology perturbative stimulus unknown effective stimulus calculate perturbative linear response system msequence msequence long history use engineering science application range system identification communication physiology msequence use primarily compute system kernel especially visual system work use perturbative msequence study linear response single unit inferior cat stimulus add small msequence signal carrier allow study linear behavior system particular operating point manner ie change operating point perturbative msequence allow calculate linear response particular stimulus study little extra effort allow characterize system wide range stimulus sinusoidal auditory system select study response single unit central inferior colliculus ic cat single unit response record action potential store discriminate offline use commercial unit record yield sufficiently stable response analyze msequence linear system binary msequence sequence s sequence length l order sequence typically binary msequence generate shift bit feedback connection derive polynomial multiplicative group linear system identification msequence important property msequence nearly mean second autocorrelation function form t l impulse stimulus function autocorrelation function context perturbative stimulus advantage msequence stimulus impulse stimulus signal noise ratio msequence perturbation stay close original signal square sense impulse perturbation perturb signal far operating point measurement linear response operating point accurate model ic response system scalar stimulus pass response m e schreiner purpose section functional linear functional dc component real experiment input output signal sample discrete sequence t integer indexing sequence system write discrete convolution l l t kernel determine assume system finite memory m time step delay m coefficient nonzero determine kernel add small msequence base stimulus response original msequence yield l l l l use sum formula sequence sum eq simplify use autocorrelation eq sum simplifie find l l l r value kernel set implicitly equation term right hand eq widely different size large land equation simplify auditory system assume response small estimate size term compute statistical estimate size look scaling parameter term l sum m element correlate uncorrelate conservative estimate size order term subtle rewrite ll r l time series ambient stimulus msequence assume uncorrelate central limit theorem sum average standard deviation l turn term correlate kernel l l mi perturbative msequence auditory system identification cycle msequence perform sort different cycle term scale double sum central limit argument double sum scale result eq solve kernel yield m c l ml constant c c depend neural firing rate statistic determine experiment term term contribute error determine kernel think noise error term vanish l procedure asymptotically exact arbitrary uncorrelated stimulus sort order crosscorrelation ram r yield good estimate inequality hold practice memory small sequence length second inequality strict bind second inequality represent tradeoff sequence length number trial size perturbation level systematic noise estimate instance noise perturbation large signal sort present term drop usual msequence crosscorrelation result recover msequence modulation response previous work ree cell inferior colliculus tune characteristic frequency tune good frequency modulation carrier highly simplify model unit response sound stimulus cascade filter linear circuit transfer function match frequency tuning curve nonlinear unit linear circuit transfer function match modulation transfer function detect modulation inherently nonlinear operation approximate linear kernel modulation response characterize ordinary msequence stimulus use method describe section approach ll step entirely concentrate measure l accomplish create modulation msequence ambient signal ie operating point mt msequence add amplitude b carrier frequency effective input stimulus note little physiological evidence purely linear fact work ree ree nonlinear modulation response modulation transfer e schreiner function imply assume modulation linear imply static nonlinearity use linearity use convenient assumption organize stimulus ask nonlinearitie exist msequence modulation stimulus neural response use compute crosscorrelation modulation transfer function l system alternatively b msequence perturbation underlie modulation envelope sort derivation linear modulation kernel calculate use crosscorrelation msequence modulation depth use ree ree calculate kernel use msequence different calculate property stimulus sort msequence use experiment length unit cycle msequence present determine characteristic frequency unit stimulus present differ characteristic frequency figure depict sinusoidal msequence component combine result stimulus present random order mitigate adaptation effect figure stimulus use experiment pure wave modulation modulation depth msequence modulation depth graph msequence modulation depth add sinusoidal modulation graph graph perturbative depth result figure spike rate pure combine msequence stimulus note rate nearly indicate perturbation large effect average response unit unit adaptation firing rate trial find perturbative msequence auditory system identification statistically significant change kernel different trial unit msequence q q q time figure plot unit fire rate pure msequence stimuli carrier frequency khz close characteristic frequency neuron sinusoidal modulation frequency msequence modulation frequency figure modulation response kernel different value modulation depth note system linear superposition cause kernel equivalent fact nonlinearitie magnitude linear response particular unit behavior small modulation depth way behavior high modulation depth kernel bandwidth modulation transfer function broaden increase depth discussion paper introduce new type stimulus perturbative msequence study auditory system derive property apply perturbative msequence analysis modulation response unit ie find linear response different operation point demonstrate nonlinear response presence sinusoidal modulation nearly large linear response description unit response incomplete believe perturbative stimulus effective tool analysis system unit phase stimulus main factor systematic noise discuss section possible trade duration measurement size perturbation achieve good result msequence stimulus possible derive high order information suitable noise possible derive secondorder kernel work support sloan foundation onr grant number m e schreiner response msequence e ns time spike millisecond figure plot temporal kernel derive perturbative msequence stimulus conjunction sinusoidal modulation modulation depth yaxis unit amplitude spike millisecond spike reference shift schreiner g e schreiner code inferior colliculus cat neuronal mechanism neurophysiology z analysis physiological system ree r ree dynamic property single neuron inferior colliculus rat hearing research ree ree r stimulus property influence response inferior colliculus neuron sound research increase rate efficiency information transmission primary auditory proceeding royal b e e practical approach nonlinear analysis editor advanced method physiological modeling vol page biomedical simulation resource
hierarchical mixture expert methodology apply continuous speech recognition system technology abstract paper incorporate hierarchical mixture expert hme method probability estimation develop continuous speech recognition system result system think hmm system instead use hme system employ large set hierarchically organize relatively small neural network perform probability density estimation hierarchical structure reminiscent decision tree important difference expert neural net perform soft decision hard decision ordinary decision tree parameter neural net hme automatically use report result arpa oooword use hme model recent research hmm system outperform constrained tiedmixture hmm system large training datum available work utility decision tree demonstrate classification problem use divide paradigm effectively problem divide hierarchical set simple problem present new system mit similar property possess advantage decision tree additional important advantage automatically soft decision boundary hierarchical mixture expert method hierarchical mixture expert hme develop recently break large scale task small partition input space nest set region build simple specific model local expert region idea method follow principle utilize certain approach classification problem decision tree decision tree approach level tree datum divide explicitly region contrast hme model use soft split datum ie instead datum explicitly divide region datum lie simultaneously multiple region certain probability effect lop distant datum decision tree furthermore hard boundary decision tree fix decision soft boundary hme parameterize generalize sigmoidal function adjust automatically use expectationmaximization describe apply hme methodology problem state hmm separate hme use estimate likelihood actual compute posterior probability s probability phoneme class input feature vector z state s probability divide priori probability class state hme perform follow computation c l indicate phoneme class ci represent local region input space c number region view gate network s view local expert classifier expert network region hme region ci divide turn c subregion term compute similar manner equation subregion datum available parent network technical detail paper use generalize sigmoidal function parameterize follow direct input neural net hide layer vector twolayer weight need train similarly local phoneme classifier region ci parameterize generalize mixture expert apply continuous speech recognition sigmoidal function ij l weight system consist set parameter vi vi parameter estimate use algorithm e iterative approach maximum likelihood estimation iteration compose step expectation e step maximization m step m step involve maximization likelihood function iteration e step use parameterization obtain follow iterative procedure compute parameter l estep iteration data pair compute represent probability data t lie region current parameter estimation use weight datum region mstep idea soft reflect weight probability instead mstep iterate vi converge maximization mean fit generalize sigmoidal model use label datum weight second mean fit generalize sigmoidal model use input zt output criterion fitting crossentropy typically fitting solve method expensive view type fitting multiclass classification task develop technique invert generalize sigmoidal function efficiently describe follow common method multiclass classification divide problem classification method result positive negative training usually avoid positive negative training follow technique use solve multiclass posterior probability simultaneously suppose label data set l label tth datum use generalize sigmoidal function model posterior probability l follow obviously probability sum l training sample class label let interpret let define pz log equation imply l expression generalize sigmoidal function mean train parameter satisfy equation datum use square criterion objective t log l denote data matrix square solution xi l zt l substitute l zt equation easy compute basically accumulate matrix x sum different class l obtain probability single inversion matrix pass training datum mixture expert apply continuous speech recognition relation work work report different previous work utilize neural net single neural network snn use model complete segment estimate probability density state hmm work similar cohen major difference single large neural net use perform probability density model training large network require use specialized parallel processing machine training reasonable time use hme method divide problem small problem able perform need training computation regular previous work work utilize mixture estimate probability density hmm multilayer network universal continuous function approximator decide explore use neural net alternative approach continuous density estimation experimental result hmm word error rate table error rate test trigram grammar hmm word error rate table error rate set trigram grammar initial application hme method use contextindependent hme estimate likelihood state state hmms implement hme input space divide region region far divide subregion initial division accomplish supervised training division train phoneme system gate local expert network hme identical structure twolayer generalize sigmoidal network hme system implement nb paradigm recognize sequence obtain result nb list obtain baseline system tiedmixture statistical trigram grammar build contextdependent hme system base structure contextindependent hme model describe state training datum divide accord left right context context separate hme model build context computationally feasible use hme experiment use test hme implementation arpa set report word error rate test set number different system table word error rate baseline hmm neural net system snn hybrid system hme system hme system combine vi hme hmm system modify prior table performance baseline tiedmixture performance snn system comparable hmm performance hme good hmm system snn system combine baseline hmm system hme snn improve performance hmm respectively find improvement large hybrid adjust contextdependent prior contextindependent prior smooth contextdependent model contextindependent model specifically contextdependent hme model usually estimate posterior probability phoneme s left right context c acoustic input z particular state sample sparse model necessary regularize smooth contextdependent model contextindependent model datum available model different prior contextdependent model contextindependent model simple interpolation model d s px context mean contextindependent model inconsistent scale contextdependent contextindependent prior weight input datum point z weight prior adjust modification contextdependent actually estimate combine contextindependent model experiment table word error drop prior modify oooword development set total word error reduction tiedmixture hmm system use neural network system switch experiment domain oooword oooword test set year system improve tiedmixture system continuous density system switch use new continuous density hybrid system language model use oooword trigram grammar result table table word error rate reduction continuous density hmm system combine contextdependent hme system compare improvement tiedmixture system oooword development set improvement continuous density system oooword mixture expert apply continuous speech recognition development big improvement hmm system conclusion method hierarchical mixture expert use density estimator speech recognition experimental result estimation approach consistent estimation hmm system neural net system use hierarchical mixture expert improve performance stateoftheart tie mixture hmm system continuous density hmm system hme system performance stateoftheart tie mixture hme system acknowledgment work fund advanced research project reference mixture expert computation press d b test arpa speak language program arpa human language technology workshop r schwartz j hybrid neural net system stateoftheart continuous speech recognition advance information processing system hanson l gile ed m rumelhart v contextdependent multiple distribution modeling mlp advance neural information processing system hanson d l gile ed publisher
handwritten word recognition use contextual hybrid radial basis function abstract hybrid contextual radial basis function model offline handwritten word recognition system present task assign radial basis function network estimation emission probability associate state model contextual estimation emission probability account left context current image segment represent sequence new system outperform previous system context act differently introduction hide markov model hmms commonly use offline recognition handwritten word ai gilloux et gilloux et approach gilloux word image transform sequence image segment explicit segmentation procedure segment pass module estimate probability segment appear corresponding hide state state state emission probability model probability generally optimize maximum likelihood estimation mle mle training know suboptimal respect discrimination ability underlie model true model datum estimate emission probability region example sparse difficult estimation accurate reduce risk training image segment consist bitmap replace feature vector reasonable length discrete symbol gilloux et ai handwritten word recognition use network previous paper gilloux et describe hybrid basis function system emission probability compute bitmap use radial basis function rbf neural network system demonstrate recognition rate previous base symbolic feature gilloux et b misclassification example simplify assumption hmms responsible significant error particular observe consider segment independently accuracy model example figure example letter segment obviously correlate figure example segment propose new variant hybrid model emission probability estimate account context current segment context represent precede image segment sequence model choose prove efficient model recognize isolate digit letter poggio lemarie interestingly bear close relationship use model emission probability markovian advantage lie fact directly estimate emission probability prone error estimation sparse region train mean square error discriminant idea use neural net particular rbf conjunction hmm new singer lippman apply speech recognition task use context improve emission probability propose use discrete set context event neural network use estimate relation state context event current segment point propose different method discrete context base adapt decomposition likelihood model apply offline handwritten organization paper follow section overview architecture hmm section describe justification use output contextual hide markov model section describe radial basis function network section report experiment contextual model apply recognition handwritten word find bank overview hide markov model hmm model recognition score associate word likelihood n pi term product encode probability model word generate image sequence image segment paradigm term decompose sum path ie sequence hide state product probability hidden path probability path generate image sequence lemarie m gilloux assume path contribute significantly term hmms sequence element assume depend corresponding state isi sn firstorder markov model assume path generate firstorder markov chain report previous paper gilloux et gilloux ai recognition system base hide markov architecture use system extensively present gilloux ai model word letter associate threestate model design account situation letter realize segment word model result corresponding letter model architecture depict figure use transition emission state emission e e figure outline model change previous formula replace state transition ie pair state system hybrid model radial basis function network use estimate emission probability p output introduce apply baye rule expression s ps isi p product priori image segment probability p depend word write formula term form p transition probability estimate term form p priori probability state note baye rule apply estimate consistently term form p independent statistical distribution state prove lippman system train tend approximate baye probability sense baye handwritten word recognition use network optimal practice way system come close baye optimum easily predictable bias train system initial parameter local optimum architecture net real output score generally equal baye probability exist different procedure act output system train mse close baye probability singer lippman provide use assume term estimate output recognition system p priori probability state set use train system system output hybrid handwritten word recognition system demonstrate performance previous system word image represent sequence symbolic feature instead bitmap gilloux et recognition error remain explain simplify assumption model particular fact emission probability depend state correspond current bitmap appear poor choice example figure fourth segment classify half letter letter figure image classify segment second half naturally correlate figure markov model architecture design half assume uncorrelated effect consecutive bitmap unique letter recognize figure emission probability second letter low consider estimate probability contextual model describe section design different assumption emission probability hybrid contextual model exact decomposition emission word likelihood follow x assume bitmap condition state previous image p ij l sl sn pi introduce apply baye rule following way ps l l ps p lsi p sl p term form p contribute discrimination word hypothesis write p n p iii lemarie m gilloux estimate term form term long compute radial basis function use estimate probability common architecture describe section radial basis function model radial basis function model describe lemarie network inspire theory theory study multivariate real function know finite set point approximate point family parametric function bias regularity bias tend select smooth function sense linear combination derivative minimum exist analytical solution linear combination gaussian centre point function know poggio straightforward transpose paradigm problem learn probability distribution set example practice theory tractable require gaussian example training set empirical method lemarie develop reduce number gaussian centre theory long applicable number centre reduce parameter model centre covariance matrix weight linear combination train method case gradient descent method mse finally result model look particular neural network layer input layer second layer completely connect input layer connection unit weight transfer function cell second layer gaussian apply distance corresponding centre input cell weight distance analogous parameter diagonal covariance matrix finally layer completely connect second weight connection cell layer output sum input experiment input rbf feature vector length compute bitmap word segment rbf estimate term form use vector input second rbf term p feed vector associate ij l vector inspire method encode proportion white bitmap border reach meet black pixel direction experiment model assess apply recognition word appear legal bank size vocabulary perplexity training test basis image word write unknown real use image training different image test image resolution dpi manually segment word automatic procedure use separate word line form training conduct use result hybrid system segmentation module unchanged segment training set test set assume base system correct align segment letter model use alignment label segment training set label desire output rbf use set different label letter appear vocabulary type segment possible letter output directly interpret baye handwritten word recognition use network ability far assess quality system evaluate ability recognize class segment value p ii compare previous hybrid system result report table test set demonstrate importance context potential interest table recognition confusion rate segment classifier recognition rate confusion rate mean square error rbf system context rbf system context word recognition system compare performance word recognition data base image word result table remark system table recognition confusion rate word recognition recognition rate confusion rate confusion rbf system context rbf system context present result contextual system difference system context figure explain contextual system remain low level performance word figure recognize system context badly identify contextual system respectively image word example segment segment letter word necessarily consider separate fifth sixth segment recognize half letter standard system contextual system avoid decomposition letter image contextual system propose second segment mainly absence information relative position segment hand figure example contextual system outperform system context case propose class half letter fifth sixth segment second case context clearly useful recognition letter word experiment try combine system benefit respective figure new confusion produce contextual system experiment reveal contextual system remain sensible numerical output value network estimate approach solve problem currently investigation obtain try approximate network estimate p sj network estimate p sj ij b lemarie m gilloux conclusion describe new application hybrid radial basis model architecture recognition offline handwritten word architecture estimation emission probability assign discriminant classifier estimation emission probability enhance account context represent previous bitmap sequence classify formula derive introduce context estimation likelihood word score ratio output value network use estimate likelihood report experiment reveal use context segment recognition level useful word recognition level new system act differently previous system context future application try exploit difference dynamic ratio network output value unstable solution stabilize test experience reference r maximum likelihood approach speech recognition ieee transaction pattern analysis machine intelligence lr brown maximum mutual information estimation hide markov model parameter speech recognition int acoustic speech signal processing continuous speech recognition connectionist statistical method ieee tran neural network vol pp offline handwritten word recognition use hide markov model type stochastic network ieee tran pattern analysis machine intelligence vol gilloux m m jm strategy handwritten word recognition use hide markov model proc nd int document analysis m m strategy script recognition use hide markov model machine vision application special issue recognition r accept publication gilloux m lemarie m lb hybrid radial basis function network hide markov model handwritten word recognition system proc rd int document analysis classification mixed font characteristic annual ieee practical implementation radial basis function network handwritten digit recognition proc nd int document analysis poggio network approximation learn proc neural network classifier estimate bayesian posteriori probability neural computation singer e speech recognizer use radial basis function network hmm framework proc int acoustic speech process
higherorder communication science laboratory abstract machine supervised learning approach use feature combination datum highdimensional unfortunately increase interest exist date efficient train algorithm higherorder hofms paper present generic efficient algorithm train arbitraryord hofms present new variant share parameter greatly reduce model size prediction time maintain similar accuracy demonstrate propose approach different link prediction task introduction factorization machine supervised learning approach use secondorder feature combination efficiently datum highdimensional key idea model weight feature combination use lowrank matrix main benefit achieve empirical accuracy par polynomial regression method small fast evaluate model second infer weight feature combination observe training set second property crucial instance system domain increasingly popular fail generalize unseen interaction unfortunately higherorder briefly mention original work exist date efficient algorithm train arbitraryord hofms fact compute prediction model parameter polynomial time number feature reason knowledge apply problem addition originally define model degree polynomial expansion different matrix require estimation large number parameter paper propose efficient algorithm train arbitraryord hofms rely link socalle propose dynamic programming algorithm evaluate compute gradient base propose stochastic gradient coordinate descent algorithm arbitraryord reduce number parameter prediction time introduce new kernel derive allow define new variant share parameter demonstrate propose approach different link prediction task factorization machine machine increasingly popular method efficiently use secondorder feature combination classification regression task datum highdimensional let rd p rank column d s hyperparameter denote row p conference neural information processing system nip predict output r vector xd p important characteristic consider combination distinct feature square feature ignore main advantage compare naive polynomial regression number parameter estimate instead addition compute prediction time use wt s indicate elementwise product training set p learn minimize follow objective convex loss function hyperparameter popular implement efficient stochastic gradient coordinate descent algorithm obtain stationary point algorithm runtime complexity training algorithm provide extend higherorder feature combination original work let p t order degree feature combination consider kt row p t hofms define hyperparameter let p m m jm t t t sum sum elementwise product define objective function express similar way m t hyperparameter avoid combinatorial hyperparameter combination search experiment simply set m km look recently express simple let define degree m d m later convenience define p p xi pm s s x s t column p perspective view type machine support vector learn directly datum intuitively think kind polynomial kernel use feature combination replacement distinct feature key property p xj denote dimensional vector similarly fix p affine function d training include constant factor fair late comparison arbitraryord come decomposition function provide base objective function row p p m separately interpretability hofms advantage interpretability case notice rewrite m define t t time t intuitively t rd lowrank tensor contain weight feature combination degree t instance weight similarly decomposition function consider combination distinct feature jm j paper unfortunately exist date efficient algorithm train compute odm polynomial time follow present algorithm hofms originally define require estimation m matrix p p m produce large model m large address issue propose new variant share parameter stochastic gradient algorithm hofms view present section allow focus main computational unit train hofms section develop dynamic programming algorithm evaluate compute gradient odm time evaluation main observation section use recursively remove feature compute kernel trivial let denote p similarly let introduce shorthand ajt ajt ajt ajt t convenience define p t exist feature t dimensional vector quantity want compute p instead use recursion lead redundant computation use bottomup approach organize computation dp table start topleft corner initialize recursion table arrive solution corner procedure summarize odm time memory table example ad gradient compute gradient p wrt p use differentiation backpropagation neural network context allow compute entire gradient single pass supplement variable ajt table socalled adjoint represent sensitivity p wrt ajt recursion edge case ajt influence ajt ajt use chain rule obtain t similarly introduce adjoint d influence ajt t m m ajt m t run recursion reverse order table start dm use m approach compute entire gradient p p pd wrt p time memory procedure summarize evaluate odm input p rd rd t m j d algorithm computing p odm input p rd rd ajt m j d dm t m d t end end d m output p p m t d ajt ajt end end output p stochastic gradient base easily learn arbitraryord hofms use gradientbased optimization algorithm focus discussion algorithm minimize wrt p p m associate degree form m p kp s r fix offset account contribution degree m prediction subproblem convex row p update wrt instance xi compute learning rate define xi evaluate p compute gradient odm cost ie visit instance cost implement sparse datum conclude section useful remark sparse datum let denote support vector xd d let define st easy gradient support p useful remark p provide m number nonzero element datum sparse need iterate nonzero feature consequently time memory cost cost epoch coordinate descent algorithm arbitraryord hofms describe coordinate descent cd solver arbitraryord good choice learn objective function coordinatewise thank algorithm generalization high order propose alternative recursion efficient cd implementation typically require maintain statistic training instance prediction current iteration coordinate update statistic need synchronize unfortunately recursion use previous section suitable algorithm require store synchronize dp table training instance coordinatewise update turn alternative recursion m t p xdt p m define p note recursion know context traditional kernel method cf section application novel know p p xi use compute p p overall evaluation cost arbitrary m coordinatewise derivative apply differentiation recursion order compute entire gradient appendix need derivative variable time simply use differentiation m p p m advantage need cache p t m memory complexity sample om instead odm use algorithm similarly assume loss function smooth update p element p cyclic order define p update guarantee objective value monotonically nonincrease exact coordinatewise minimizer squared loss overall total cost ie update coordinate m time compute assume previously cache t m compute m operation fix m loop need compute modern far reduce number operation need quadratic dependency m mean cd good small m typically m share parameter hofms originally define model degree separate matrix p p m assume use rank k matrix total model size use odm cost compute prediction tend produce large model reduce model size prediction time introduce new kernel allow share parameter degree inhomogeneous allsubset kernel derive share appeal property sparse gradient inhomogeneous wellknown sum kernel equivalent concatenate associated feature map section let m t combine different degree natural kernel x m t t p kernel use feature combination degree m inhomogeneous inhomogeneous polynomial contrast p homogeneous main difference kernel sum share parameter increase modeling power allow kernel different weight m evaluation recursive nature compute p p p free kernel available column table ie p t m cost evaluate time total cost compute instead learn certainly possible learn p directly minimize objective function propose easy solution work practice key observation easily turn add value feature vector let denote concatenation p scalar p similarly easily obtain p p p table dataset use experiment detailed description cf nip enzyme column author enzyme disease user column b nb gene movie similarly apply twice obtain p p x p p x apply m m obtain p p p p generally add feature p convert p learn mean automatically learn m weight convert m recursion simple experiment approach work favorably compare directly learn p main advantage approach use software simply need minimize augment datum cost compute entire gradient use augment datum m compare odm hofms separate parameter allsubset consider closely relate kernel allsubset definition sp d main difference traditional use kernel learn p interestingly sp x ad p p number nonzero feature kernel use combination distinct feature order uniform weight large kernel good choice training instance contain nonzero element learn parameter simply substitute cd algorithm entail substitute sp x compute sp easy verify sp d sp x sp x sp spd p p d xd main advantage allsubset kernel evaluate compute gradient od time total cost compute s experimental result application link prediction problem set demonstrate novel application predict presence absence link node graph formally assume set possibly disjoint node size respectively assume feature set node represent matrix b nb instance represent user feature movie feature denote column b ai bj respectively matrix nb element indicate presence positive sample absence negative sample link node ai bj denote number positive sample use datum goal predict new association dataset use experiment summarize table note nip enzyme dataset b supervise problem need convert information handle predict element simply form concatenation table comparison area curve auc measure test set hofm m hofm m hofm m hofm m hofmsharedaugmente m hofmsharedaugmente m hofmsharedaugmente m hofmsharedaugmente m m m m m polynomial network m polynomial network m polynomial network m polynomial network m bilinear regression nip enzyme bj feed hofm order compute prediction use feature combination learn weight feature combination bj training time need positive negative sample let denote set positive negative sample training set compose pair model compare define originally propose minimize alternate minimization degree learn p use simple augment datum approach describe section hofmsharedaugmente inspire report result learn p directly minimize ij subject h allsubset s xij explain section model equivalent model m pk polynomial network s m model think machine variant use polynomial kernel instead bilinear ai u t bj link prediction learn v minimize p work ij experimental setup evaluation experiment model use cd avoid tuning learning rate hyperparameter set square loss omit notation clarity fit bias term model evaluate compare model use area curve auc probability model correctly rank positive sample high negative sample split positive sample training testing sample number negative sample positive sample training use rest testing choose crossvalidation follow empirically set experiment initialize element p randomly result indicate table overall good model hofm hofmsharedaugmente achieve good score dataset model outperform regression dataset benefit use higherorder feature combination hofmsharedaugmente achieve similar accuracy hofm use small model surprisingly improve hofmsharedaugmente convergence m b convergence m c convergence m d scalability wrt degree m figure solver comparison minimize vary degree m nip dataset result dataset appendix b dataset conclude augment datum approach convenient work practice allsubset polynomial network perform bad hofm hofmsharedaugmente good finally observe hofm robust increase likely benefit model degree separate matrix comparison compare coordinate descent minimize vary degree m nip dataset construct datum way explain previous section add m feature result n sparse sample dimension m compute stochastic gradient use solver use initialization result indicate figure cd perform m start deteriorate case advantageous figure cost epoch scale linearly benefit compute gradient surprise find sensitive learning rate diverge large value work explain outperform cd low cost epoch future useful create cd algorithm dependency m conclusion future direction paper present training algorithm introduce new variant share parameter popular way deal large number negative sample use objective function directly maximize especially easy algorithm sample pair positive negative sample dataset update expect algorithm develop section especially useful setting recently propose distribute train secondorder straightforward extend algorithm base contribution section finally possible integrate deep learning framework order easily compose layer convolutional reference m learn heterogeneous system g b pearlmutter automatic differentiation machine learn survey preprint m factorization machine proceeding european conference machine learning principle practice knowledge discovery database ecml m m polynomial network new insight efficient training algorithm proceeding international conference machine learning icml e hazan singer adaptive subgradient method online learning stochastic optimization machine learn research m distribute factorization machine proceeding international conference web search datum mining r shalevshwartz computational efficiency training neural network advance neural information processing system page elkan link prediction matrix factorization machine learning knowledge database page s dhillon inductive matrix completion predict structure matrix polynomial unify learn research factorization machine proceeding international conference datum mining page ieee factorization machine acm transaction intelligent system technology z bayesian rank implicit feedback proceeding conference uncertainty artificial intelligence page l fast recommendation factorization machine sigir page roweis shawetaylor cristianini kernel method pattern cambridge vapnik statistical learning theory wahba spline model observational datum volume m supervise enzyme network inference integration genomic datum chemical information tensor machine learn polynomial feature preprint
pacbaye approach set cover machine site abstract design new learning algorithm set covering machine pacbaye perspective propose pacbaye risk bind minimize classifier achieve non trivial tradeoff introduction learn algorithm try produce classifier small prediction error try optimize function compute training set example classifier currently know exactly function optimize form propose end spectrum set cover machine propose shawetaylor try find sparse classifier training error end support vector machine svm propose try find maximum separate hyperplane training datum learn machine produce classifier small prediction error recently investigate classifier find learn algorithm try optimize nontrivial function depend sparsity classifier magnitude margin main result general risk bind apply produce classifier represent complementary source information subset training set compression set message string additional information addition propose new algorithm information string use encode radius value datadependent ball consequently location decision surface classifier small message string sufficient large region equally good radius value exist ball datum compression risk bind apply version exhibit indirectly nontrivial tradeoff version currently suffer fact radius value use final classifier depend choose distance scale r paper use new pacbaye approach apply set present new learning suffer scaling problem propose risk bind depend explicitly margin minimize classifier achieve nontrivial tradeoff definition consider binary classification problem input space consist arbitrary subset rn output space example z inputoutput pair y probably approximately correct pac setting assume example generate independently accord unknown distribution true risk classifier define probability misclassifie random draw accord d predicate true training set z m example task learning construct classifier small possible risk information d achieve goal learner compute empirical risk accord m m focus learn algorithm construct conjunction feature datadependent ball training set datadependent ball define center radius value center input example choose training set test example output ball radius center example def denote boolean complement denote distance point note metric use distance specify conjunction ball need list example participate center ball conjunction purpose use vector index m number index present number ball conjunction complete specification conjunction ball need vector radius value m input example output ci conjunction ball finally algorithm build conjunction use build role positive negative label example lack space describe case conjunction pacbaye risk bind pacbaye approach initiate mcallester aim provide guarantee bayesian learning algorithm algorithm specify term prior distribution p space classifier characterize prior belief good classifier observation datum posterior distribution q space classifier account additional information provide training datum remarkable result come line research know pacbaye provide tight upper bind risk stochastic classifier gibb input example label gq assign gibb define following process choose classifier accord posterior distribution q use assign label x pacbaye theorem propose mcallester b later improve langford survey version pacbaye theorem prior p define reference training datum consequently theorem apply set classifier partly describe subset training datum case sample compression set classifier describe subset si training datum compression set message string represent additional information need obtain classifier word set exist reconstruction function r output classifier r arbitrary compression set message string training set s compression set define vector index point individual example case conjunction ball point training example use ball center message string vector radius value define use ball si classifier obtain r conjunction ci define previously recently extend pacbaye set propose risk bind depend prior p datadependent posterior q define m denote set m possible index vector m denote case set possible radius vector posterior q use stochastic classifier gibb gq define follow training set s new testing input example gibb gq choose randomly accord q obtain classifier r use determine class label x paper focus case training set s learner return gibbs classifier define posterior distribution q weight single vector single compression set si use final classifier radius choose stochastically accord posterior consider posterior q qi vector index choose learner training set s true risk empirical risk define e rr def e r qi denote set index present m contrast posterior q prior p assign non weight vector let denote prior probability p assign vector let assume example si order kth radius value assign kth example denote probability density function associate prior p risk bind depend kullbackleibler divergence posterior prior p case class posterior q prior p pacbaye reduce follow simple previous definition prior p m m q q ln p p obtain bind need specify pi vector size equally good choose pi m pm p choose m d m complete size final classifier risk bind deteriorate large generally preferable choose pd slowly decrease function d specification pi assume radius value predefined interval r equally likely choose large distance specify margin interval bi r equally good radius value choose learner choose r r bi ai gibbs classifier return learner draw radius uniformly bi deterministic classifier specify fix radius value ai bi point choose ai bi middle interval shortly pacbaye offer guarantee type deterministic consequently choice pi kl divergence p r m bi ai notice kl divergence small small value small large margin value bi divergence term favor sparsity small large margin minimum occur sacrifice sparsity large margin find posterior q identify interval ai bi refer gibb classifier b vector form union s respectively obtain risk bind giab need find closedform expression task let b denote uniform distribution b let probability ball assign x class label radius draw accord b ab yi dx let giab denote probability ci draw accord bi ai bi consequently risk single example giab giab empirical risk gibbs classifier giab rs m expression rs small training point avoid pacbaye theorem provide risk bind gibb baye perform majority vote posterior distribution use giab iff definition note decision surface baye differ decision surface classifier ai bi fact exist classifier ci decision surface baye relation follow consequently follow pm previous definition p satisfy fix distance value r m m r m bi ai furthermore b recall kl divergence small small value small large margin value bi ai furthermore gibb empirical risk small training point locate far away baye decision surface giab consequently gibb classifier small guarantee risk perform non trivial tradeoff soft greedy learning suggest learner try find baye use small number ball small large separate margin bi ai empirical gibbs risk low value achieve goal adapt greedy algorithm set covering machine propose shawetaylor consist choose feature large utility ui define ui p pi set negative example cover classify feature set positive example misclassifie feature p learn parameter penalty p misclassifie positive example feature large ui find remove pi training set repeat remain example negative example present maximum number feature reach case need gibb risk low instead risk deterministic classifier gibbs risk soft measure use function instead hard indicator function need soft version utility function negative example fall linear region fact partly cover follow observation let vector index example use ball center far construction classifier let define covering value negative example assign class def define error positive example assign class want add ball center example index obtain new vector k contain new index addition present introduce covering contribution ball center xi def bi bi error contribution ball def bi bi typically covering contribution ball increase utility error decrease define utility add ball def parameter represent penalty positive example fix value p soft greedy algorithm simply consist add current gibb classifier ball maximum add utility maximum number possible feature ball reach negative example totally cover understand soft greedy remove example totally cover occur term ai present risk bind favor soft ball large margin ai introduce margin parameter use follow greedy step search ball bi ai ball maximum utility find try increase far utility search possible value ai center xi fix p choose validation training set conclude section analysis run time soft greedy learn fix p potential ball center sort m example respect distance center log m time center set ai value examine specify distance sort example example sort time compute covering contribution error m value ai k large number example fall margin use small value olog m result time om log m compute utility value different ball center time om log m compute utility possible m center ball large utility value choose try increase far utility search pair value ai bi remove example cover ball repeat algorithm remain example know greedy kind following guarantee exist r ball cover m example greedy algorithm find r ball r run time algorithm om log m empirical result natural datum compare new pacbaye learn old algorithm algorithm compare svm equip kernel variance soft margin parameter use l metric metric present argument contrast constrain use ball center class negative conjunction positive possible value bi define location training point recall value ai value bi set ai stage table svm result uci datum set datum set train credit glass heart test c svm result err err b err algorithm test uci data set table data set randomly split half example use training remain set example use test corresponding value number example train test column table learn parameter algorithm determine training set parameter c svm determine cross validation method perform training set parameter small fold error use train svm training set result classifier run testing set exactly method fold split use determine learning parameter result report table column refer number support vector present final classifier err column refer number classification error obtain testing set notation use result report table addition b column refer respectively number ball margin parameter divide average distance positive negative example result report refer baye classifier result gibb classifier similar observe heart generalization error small significant difference occur observe generally sacrifice sparsity compare obtain margin reference e m vapnik training algorithm optimal margin classifier proceeding th annual acm workshop theory page acm practical prediction theory classification machine learn research pacbaye risk bound gibb classifier proceeding nth international conference machine learning icml page tradeoff set cover machine proceeding th machine learn ecml lecture note artificial intelligence set cover machine journal learn theorem machine learn mcallester model average colt page b
infinite relational modeling functional connectivity rest state fmri section cognitive system centre magnetic resonance university centre magnetic resonance university centre magnetic resonance university lar section cognitive system abstract functional magnetic resonance imaging fmri apply study functional connectivity neural element form complex network brain level analysis functional rest state network base analysis correlation temporal dynamic region brain model identify behave group term correlation little insight group interact paper different view analysis functional rest state network start definition rest state functional coherent group search functional unit brain communicate brain coherent manner measure mutual information use infinite relational model irm quantify functional coherent group rest state network demonstrate extract component interaction use discriminate functional rest state activity multiple sclerosis normal subject introduction neuronal element brain constitute intriguing complex network functional magnetic resonance fmri apply study functional connectivity neural element form complex network brain level suggest fluctuation blood bold signal rest reflect neuronal activity brain correspond functionally relevant network analysis functional rest state network base analysis correlation temporal dynamic region brain assess voxel correlate signal predefined region socalle seed unsupervised multivariate approach independent component analysis figure propose framework pairwise mutual information calculate group voxel subject rest state fmri activity graph mutual information thresholde undirected link analyze infinite relational model irm assume functional unit subject interaction individual use extract interaction characterize individual model identify behave group term correlation limit group interact furthermore correlation optimal extract second order statistic easily fail establish high order interaction region brain paper different view analysis functional rest state network start definition rest state functional coherent group search functional unit brain communicate brain coherent manner consequently define functional unit way interact remain network consider functional connectivity region measure mutual information mutual information root information theory datum mi detect functional relation region regardless order interaction rest state represent mutual information graph pairwise relation voxel constitute complex network numerous study analyze graph idea study complex network common procedure extract summary statistic network compare random network analysis demonstrate fmri derive behave far random paper propose use relational modeling order quantify functional coherent group rest state network particular investigate line modeling use discriminate patient multiple sclerosis healthy individual multiple sclerosis disease result widespread subcortical matter axonal axonal result variable delay signal transmission connection addition characteristic lesion structural magnetic resonance imaging mri advanced lesion cortical appear normal structural mri finding brain affect brain functional connectivity structural mri information extent lesion provide information impact functional brain connectivity widespread brain ie affect brain anatomical functional represent disease state particular suited relational modeling relational modeling able provide global view communication functional network extract functional unit furthermore method facilitate brain network simultaneously completely datum drive manner illustration propose analysis figure method data stable patient rr multiple sclerosis rr female mean age year range year healthy individual female mean age year range year participate study patient examine assign score accord range median mean disease duration year range year perform subject rest eye close use gradient planar imaging sequence coverage repetition time isotropic voxel session min brain volume scan session cycle monitor use preprocesse volume remain volume mean volume use body transformation image normalize template order remove effect relate residual movement physiological effect linear filter comprise motion relate total physiological effect volume time construct filter mask divide voxel group consist voxel estimation pairwise mutual information graph mutual information group pij pij u pi mutual information hinge estimation joint density pij approach exist estimation mutual information range parametric nonparametric method near neighbor density estimator histogram method accuracy approach rely number observation present use histogram approach use base derive individual distribution group pij u count number cooccurrence observation voxel group corresponding voxel group v time total sample bin joint histogram generate mutual information graph subject total pairwise evaluate thresholde graph pairwise mi link graph graph size total direct link ie undirected link result graph link density total number link link count link direction infinite relational modeling irm importance model brain connectivity interaction widely recognize literature fmri approach dynamic causal model structural equation model dynamic baye net normally limit analysis interaction know brain region predefined region interest benefit current relational modeling approach region define completely datum drive manner method establish interaction low computational complexity admit analysis large scale brain network functional connectivity graph previously consider discrimination rest state network define base normalize graph cut order derive functional unit normalize cut suited separation voxel group disconnected component method lack ability consider coherent interaction group stochastic block model denote relational model rm propose identification coherent group node complex network node belong class ir ir denote ith row clustering assignment matrix probability link node determine class assignment ir z denote probability generate link node class node class use dirichlet process propose nonparametric generalization model potentially infinite number class infinite relational model irm inference irm jointly determine number latent class class assignment class link probability knowledge attempt explore irm model fmri datum follow follow generative model infinite relational model dp b b b beta b tendency participate relation determine solely assignment prior r element conjugate result integral p z p dn analytical solution p z b b m b beta b b m b m z b b m b n m b number link functional unit b b number functional unit b link node vector length entry number group assume graph independent subject p z b b m b beta b result posterior likelihood p p p z b b m b beta b d number express functional unit number group assign functional unit expect value bi m m mcmc sample irm model propose use gibbs sample scheme combination splitmerge sample clustering assignment matrix use splitmerge sample procedure propose restrict gibb sample sweep initialize restrict gibbs sampler sequential allocation procedure propose mcmc sample posterior likelihood node assignment assignment remain node need gibbs sampler calculate ratio p b beta ir q q size functional unit assignment node note posterior likelihood efficiently calculate consider computation m b m b evaluation beta function affect consider assignment change p score functional unit term stability sample obtain large potential solution visualization interpretation difficult average sample require extract group different sample run relate visualization instead select single extract sample r map estimate separate randomly initialize run iteration facilitate interpretation display extract functional unit separate run identify functional unit analyze nod cluster extract sample random start r accord p r r z use follow score c rr z c z r c cz r c count number time voxel group c voxel group total number time voxel group c voxel graph c indicate voxel group cluster sample indicate voxel sample result discussion follow calculate average short path length average cluster degree distribution large connected component component g subject specific graph mi threshold value use define link table derive graph far erdosrenyi random cluster coefficient degree distribution parameter component g differ significantly random graph significant difference normal group indicate global feature appear affect disease run initialize irm model randomly generate functional unit set prior b favor functional unit link density relative link density set log number voxel group model estimation treat link equivalent number miss random graph treat entry miss random ignore maintain count observe value estimate model stable average extract functional unit figure area curve auc score receiver operator characteristic predict link subject prediction link base average final sample auc score random subject high degree variability subject term model ability account link graph find significant difference normal group term table median threshold value average short path average cluster coefficient degree distribution exponent ie component large connected component graph relative complete graph normal group nonparametric test difference median group random erdosrenyi random graph density construct graph normal random p p figure auc score different run subject normal group group right distribution auc score group normal blue significant difference median value distribution find p model ability account network dynamic difference term irm model able account structure network normal subject finally link prediction surprisingly stable subject run link treat miss indicate high degree variability graph extract rest state fmri subject relative variability subject consider inference stochastic optimization procedure visualize sample high likelihood ie map estimate run figure display extract group functional unit run functional unit easily identify functionally relevant network select functional unit similar network previously identify fmri datum use sensorimotor network represent functional unit posterior network functional unit frontoparietal network functional unit visual system represent functional unit note similarity sensorimotor posterior default network frontoparietal network visual component contrary current approach able model interaction component consistent pattern reveal functional unit high connectivity strong connectivity furthermore functional unit appear symmetric connectivity profile functional unit strongly connect functional unit sensorimotor system strongly connect functional unit case network functional unit attribute noise unit appear connected remain functional unit panel figure test difference median connectivity extract functional unit connection significant p healthy individual strong connectivity select functional unit relative patient functional unit involve distribute brain comprise visual system functional unit sensorimotor network functional unit frontoparietal network functional unit expect affect brain globally change brain patient strong connectivity relative healthy individual select sensorimotor functional unit frontoparietal network functional unit interpretation finding communication increase frontoparietal sensorimotor network consequence disease beneficial mechanism maintain motor function figure panel visualization map model restart functional unit indicate red circle indicate median unit link density line median functional unit link density gray scale line width code link density functional unit use logarithmic scale panel select rest state component extract group independent component analysis temporal concatenation subject use identify spatially independent component subsequently individual component time series use regression model obtain subject specific component map display map base sample correct multiple comparison p use theory panel c auc score relation extract group thresholde significance level base test blue indicate link density large normal yellow large normal high resolution version figure find supplementary material table leave classification performance base support vector machine svm linear kernel linear discriminant analysis neighbor knn significance level estimate compare classification performance correspond classifier randomly class label bold indicate significant classification p raw datum pca p p degree irm p p p discriminate normal subject evaluate classification performance subject specific group link density base leave crossvalidation consider standard classifier soft margin support vector machine svm linear discriminant analysis lda base pooled estimate feature project principal component analysis dimensional feature space prior analysis knn compare classifier performance classify normalize raw subject specific time series ie matrix subject voxel time datum project dominant dimensional subspace denote include group analysis performance use node degree degree feature previously successful classification irm model use bayesian average prediction dominate map estimate figure classification analysis normalize feature table classification result group propose irm model significantly classify random irm model high classification rate significant classifier finally note contrary analysis base temporal correlation pca approach use classification benefit mutual information high order account necessarily reflect correlation brain region drive variance brain region capture mutual information necessarily capture correlation conclusion functional unit extract use irm model correspond previously describe conventional model assess functional connectivity datum aim divide brain network irm explicitly model relation functional unit enable visualization analysis interaction use classification model predict subject disease state reveal irm model high prediction rate discrimination base component extract conventional group approach irm readily extend direct graph network derive task relate functional activation believe propose method constitute promising framework analysis functionally derive brain network general reference s r b e human brain functional network highly connect association neuroscience sejnowski information maximization approach blind source separation blind deconvolution neural computation b z m functional connectivity motor cortex rest human brain use mri magnetic resonance medicine e complex brain network graph theoretical analysis structural functional system nature review neuroscience d method group inference functional mri datum use independent component analysis human brain mapping g b m ml discriminative network model advance neural information processing system b l explore functional connectivity human brain use multivariate information analysis advance neural information processing system b sampler conjugate dirichlet process mixture model p consistent network healthy subject proceeding science m p m blood level dependent contrast rest state network relevant functional activity sensorimotor system experimental brain research v m r g m v brain functional network physical review letter m m mri evidence multiple sclerosis diffuse disease central nervous system journal neurology dynamic causal modelling t q method retrospective correction physiological motion effect fmri magnetic resonance medicine jain r m neal splitmerge markov procedure dirichlet process mixture model computational graphical statistic c j b t l griffith n learning system concept infinite relational model artificial intelligence proceeding b estimation prediction stochastic d neuron reduce cortical multiple sclerosis lesion annal neurology m e m power default mode brain function proceeding science j storkey e learn equation model fmri advance neural information processing system b d m r l axonal lesion multiple sclerosis medicine s willsky analysis functional mri datum use mutual information proc medical image computing lecture note computer science b m automate anatomical labeling activation use anatomical mri brain m r h normalize cut group clustering plo estimation mutual information survey lecture note computer science axonal multiple sclerosis role channel review neuroscience z infinite hide relational model proceeding international conference uncertainty artificial intelligence l r modeling neuronal use dynamic bayesian network advance neural information processing system
transelliptical component analysis abstract propose high dimensional semiparametric scaleinvariant principle component analysis tca utilize natural connection elliptical distribution family principal component analysis elliptical distribution family include wellknown multivariate distribution multivariate logistic extend metaelliptical use technique paper extend metaelliptical distribution family large p family transelliptical prove tca obtain nearoptimal log dn estimation consistency rate recover lead eigenvector latent generalize correlation matrix transelliptical distribution family distribution heavytailed infinite second moment density possess arbitrarily continuous marginal distribution feature selection result explicit rate provide tca far implement numerical simulation largescale stock datum illustrate empirical usefulness theory experiment confirm tca achieve model flexibility estimation accuracy robustness cost introduction xn rd iid realization random vector rd population covariance matrix correlation matrix principal component analysis pca aim recover m lead eigenvector practice unknown m lead eigenvector sample covariance matrix obtain estimator pca wellknown mean change measurement scale variable estimator different pca conduct sample correlation matrix regular literature aim recover m lead eigenvector m use m lead eigenvector sample correlation matrix scaleinvariant pca aim recover eigenvector scaleinvariant pca high dimensional setting scale discuss d generally consistent estimator vector denote angle v v prove u b converge commonly assume t sparse mean result variety sparse procedure note elliptical distribution special interest principal component analysis study elliptical distribution extension statistic recently elliptical distribution characterize stochastic representation random z t follow elliptical distribution elliptically distribute parameter rank q admit stochastic representation au r u independent random variable uniformly distribute unit sphere fix matrix generate variable density necessarily exist elliptical distribution family include variety famous multivariate distribution multivariate student t logistic distribution refer detail introduce term metaelliptical distribution extend continuous elliptical distribution density exist wide class distribution density exist construction metaelliptical distribution base technique initially introduce particular latent elliptical distribution multivariate gaussian distribution introduce elliptical distribution special interest principal component analysis pca variety literature pca conduct elliptical distribution share number good property enjoy pca conduct gaussian distribution particular regard range hypothesis relevant pca test base multivariate gaussian assumption identical power elliptical distribution second moment utilize connection construct new model paper paper new high dimensional scaleinvariant principle component analysis approach propose transelliptical component analysis tca firstly achieve estimation accuracy model flexibility build model tca transelliptical distribution random vector t follow transelliptical distribution exist set univariate strictly monotone function fj dj follow continuous elliptical distribution parameter transelliptical distribution necessarily possess density strict extension metaelliptical distribution define tca aim recover m lead eigenvector secondly estimate robustly efficiently instead estimate transformation function dj realize preserve rank datum utilize nonparametric correlation coefficient estimator kendall estimate prove generate variable change marginal distribution continuous kendall correlation matrix approximate parametric rate op log key observation kendall estimator pearson sample correlation regard large distribution family term methodology theory analyze general case follow transelliptical distribution sparse lead eigenvector obtain tca estimator e utilize kendall correlation matrix prove tca obtain fast p convergence rate term parameter estimation rate sin op log dn estimator tca obtain feature selection consistency result explicit rate provide background start notation let rdd t rd let entry index denote m submatrix row index column index denote let mi mj submatrix m row column submatrix m column row q define q vector norm d q max define matrix max norm elementwise maximum value km kmax pn norm let m jth eigenvalue m special min m m max m m small large eigenvalue m vectorize matrix m denote define m let rd ddimensional unit sphere sign denote equality distribution vector b rd squared matrix rdd denote inner product b b b bi b elliptical transelliptical distribution section brief discussion elliptical transelliptical distribution sequel clear random vector t continuous marginal distribution function continuous elliptical distribution section firstly provide definition elliptical distribution follow definition rd rdd d random vector t elliptical distribution elliptically distribute parameter stochastic representation z d random variable independent u sq uniformly distribute unit sphere setting denote ecd random variable r continuous marginal distribution function necessarily possess density wellknown set example distribution support set set refer discussion phenomenon symmetric positive semidefinite necessarily positive definite proposition random vector z t stochastic representation ecd characteristic function t t characteristic function denote ecd absolutely continuous density z exist form g t denote ecd proof find page density exist uniquely determined relationship describe proposition state unique theorem au z z continuous exist constant c ecd ecd z continuous exist constant c proposition discuss case identifiable proposition ecd continuous rank q identifiable z constraint define generalized correlation matrix correlation matrix z second moment exist reflect rank infinite second moment transelliptical distribution extend elliptical distribution firstly define set symmetric matrix r d rdd t rd rdd t diag definition random vector xd continuous marginal distribution function density exist follow metaelliptical distribution exist continuous elliptically distribute random vector ecd marginal distribution function r d paper generalize metaelliptical distribution family broad class transelliptical distribution assume density exist x z strict extension metaelliptical distribution definition random vector t follow transelliptical distribution exist set strictly monotone function fj dj latent continuous elliptically distribute random vector ecd rd z t ed latent generalize correlation matrix proposition follow metaelliptical distribution word possess density continuous marginal distribution continuous random ecd t e clear transelliptical distribution family strictly large metaelliptical distribution family sense generate variable latent elliptical distribution necessarily absolute continuous transelliptical distribution parameter strictly enlarge r d rd iii marginal distribution necessarily possess density term introduce term metaelliptical introduce actually alternative definition metaelliptical distribution term elliptical introduce metaelliptical metaelliptical elliptical elliptical transelliptical elliptical represent elliptical distribution continuous possess density latent correlation matrix estimation transelliptical distribution firstly study correlation covariance matrix elliptical distribution z ecd explore relationship moment z proposition ecd q finite second moment generalize correlation matrix z random vector elliptically distribute second moment finite sample mean correlation matrix elementwise consistent estimator elliptical distribution generally heavytailed multivariate t cauchy distribution example pearson sample correlation matrix bad estimator distribution family extend transelliptical sample correlation matrix generally long elementwise consistent estimator similar idea work know case unknown idea tca section propose tca approach tca method estimate lead b secondly plug eigenvector firstly estimate kendall correlation matrix r sparse measure association main idea tca exploit kendall statistic estimate generalize correlation matrix efficiently robustly detail let xd t ddimensional random vector marginal distribution joint distribution xk population kendall correlation coefficient ek ek independent copy xk particular kendall follow theorem state explicit relationship t matter generate variable strict extension result metaelliptical distribution family theorem t ed distribute sin remark conclusion correct proof provide wrong ambiguous theorem build result sample statistic generalize statistic multiple sample kendall provide new clear version detailed proof find long version paper depend generate variable follow multivariate prove xk hand prove estimate xk let x xn n independent realization consider follow statistic bjk sign xi bjk k approximate xj xk measure association define kendall r bjk bjk sin bjk correlation matrix r method elliptical distribution special interest principal component analysis pca variety literature pca conduct elliptical distribution share number good property enjoy pca conduct gaussian distribution utilize connection construct new model paper tca model utilize natural relationship elliptical distribution pca propose model transelliptical component analysis tca idea transelliptical distribution family scaleinvariant exploit wish estimate lead eigenvector latent generalize correlation matrix particular follow model fj dj consider t ed lead eigenvector latent generalize correlation matrix interested estimate spectral decomposition write d d correspond eigenvector inspire model natural consider follow optimization problem b arg max subject b s b estimate kendall correlation matrix s rd s r correspond global optimum denote e tca algorithm b sparse list generally plug kendall correlation matrix r paper approximate consider use truncate power method propose main idea utilize power method truncate vector ball iteration detailed algorithm provide long version paper final estimator denote e section section kendall correlation matrix statistic estimate correlation matrix pearson sample correlation matrix sense enjoy rate large distribution family include distribution heavy tail robust estimator ie outlier use iterative method learn instead lead eigenvector vector rd follow discussion detail matrix b orthogonal v achieve new matrix way theoretical property section theoretical property tca estimator provide especially interested high dimensional case correlation matrix estimation b section concentration result kendall sample correlation matrix r b provide correlation matrix max convergence rate independent realization t ed b kendall correlation matrix probability let p proof sketch prove realize bjk unbiased estimator xk ustatistic size hoeffding inequality ustatistic apply obtain result detailed proof find long version paper tca estimator section statement main result upper bind estimate error tca global optimum e solver e assume model hold provide upper bind angle lead eigenvector e true lead eigenvector let e global solution equation model hold vector sd let q sin v p sin e r log d b proof sketch key idea proof utilize max norm convergence result r detailed proof find long version paper p generally scale d rate op log dn parametric rate obtain d infinity lead eigenvalue typically infinity away usually seemingly common rate log d corollary feature selection consistency tca let e global solution equation model hold let b far r log min s p proof sketch key proof construct theorem condition minimum value detailed proof find long version paper experiment section investigate empirical performance tca method utilize propose follow method consider pearson classic high dimensional scaleinvariant pca use pearson sample correlation matrix kendall tca use kendall correlation matrix classic high dimensional scaleinvariant pca use pearson sample correlation matrix datum draw latent elliptical distribution perfect data numerical simulation simulation study randomly sample datum point certain transelliptical distribution consider set d determine transelliptical distribution firstly derive follow way covariance matrix firstly synthesize eigenvalue decomposition eigenvalue eigenvector sparse detail let d lead eigenvector u sparse entry second entry nonzero remain eigenvector choose arbitrarily generalized correlation matrix generate lead eigenvector sparse secondly use consider follow generate scheme scheme t ed d d word degree freedom d equivalent example scheme t ed m independent m equivalent m m ie follow distribution degree freedom m mean covariance matrix example consider m scheme t ed d m dependent h h h h h h h h h r h x h h h equivalent distribute latent elliptical distribution m evaluate robustness different method let r represent proportion sample contaminate dimension randomly select entry replace equal probability final data matrix obtain pick r scheme scheme different level r repeatedly generate datum matrix time compute average false positive rate false negative rate use path tuning parameter feature selection performance different method evaluate plot corresponding curve present figure result long version paper observe kendall generally outlier compare successful match k b figure curve scheme scheme scheme middle different level r left right yaxis b successful match market trend proportion use stock bk represent tuning parameter scaling yaxis represent successful match curve denote kendall represent point curve denote represent point bk datum section apply tca stock price datum collect price j stock consistently sp index altogether point data point correspond vector price day let sttj denote price stock day wish evaluate ability use k stock represent trend stock market end run kendall obtain lead eigenvector use tuning parameter let ak bk let denote trend stock stock bk stock tth day compare t date sttj sttj sttj sttj sttj sttj indicator function way calculate proportion successful match market trend use stock bk t t bk p t visualize result plot bk d figure result present figure b observe figure b kendall summarize trend stock market average difference method p bk standard deviation difference significant acknowledgement research support nsf award reference statistical inference elliptically contour relate distribution principal component analysis sensory analysis covariance correlation matrix food quality preference s theory elliptically contour multivariate analysis metaelliptical distribution marginal multivariate analysis symmetric multivariate related distribution use frequency analysis multivariate datum water pr measure theory volume springer tca transelliptical principal component analysis high dimensional nongaussian report hoeffding probability inequality sum bounded random variable page h extreme aggregation dependence elliptical distribution advance apply probability structure principal component conditioning concentration principal component australian journal statistic h multivariate model dependence concept volume chapman m sparse principal component analysis m generalize power method sparse principal component analysis journal machine learn research r bivariate density use stochastic measure association association page d rm elliptical international conference page l semiparametric estimation high dimensional undirected machine learning research sparse principal component analysis iterative thresholding l method sparse advance neural information process system gp principal variable page m augment sparse principal component analysis high dimensional datum preprint gq principal component selection criterion minimum mean difference complexity multivariate analysis dimension et statist minimax rate estimation sparse high dimension cm principal component case test equality q multivariate analysis truncate power method sparse eigenvalue problem preprint relaxation algorithm application handbook conic polynomial optimization page
learn generative model visual attention computer abstract attention long propose important efficiently deal massive sensory stimulus inspire attention model visual neuroscience need datum generative model propose base generative framework use attention attentional mechanism propagate signal region interest scene canonical representation generative modeling ignore scene background clutter generative model concentrate resource object interest convolutional neural net employ provide good initialization posterior inference use hamiltonian learn image face model robustly attend face region novel test subject importantly model learn generative model new face novel dataset large image face location know introduction build rich generative model capable extract useful highlevel latent representation highdimensional sensory input lie core solve task include object recognition speech perception language understand model capture underlie structure datum define flexible probability distribution highdimensional datum complex partially observe system successful generative model able discover meaningful highlevel latent representation include family model restrict boltzmann machine deep belief net deep boltzmann machine mixture model mixture factor mixture use model natural image patch recently denoise autoencoder propose way model transition operator invariant distribution data generate distribution generative model advantage discriminative model image occlude miss occlusion common realistic setting largely ignore recent literature deep learning addition prior knowledge easily incorporate generative model form structured latent variable lighting content image generative learn difficult generative model find success learn model small patch natural image object weiss learn mixture image patch use center image toy object clear background use image center crop face fact model require training datum limit applicability use unlimited unlabeled data paper propose framework infer region interest big image generative modeling allow learn generative model face large dataset unlabeled image contain face framework able dynamically route relevant information generative model ignore background clutter need dynamically selectively route information present biological brain evidence point presence attention visual cortex recently visual neuroscience attention exist area way attention form routing originally propose extend dynamic routing hypothesize provide way achieve shift size invariance visual cortex propose model combine search attention selective tuning model larochelle propose way use machine combine information gather foveal model choose look find location informative object class propose hierarchical model certain aspect attention model deep boltzmann machine related model attempt learn look object video base tracking inspire use d similarity transformation implement scale rotation shift operation require route main motivation enable learning generative model big image location object interest unknown apriori gaussian restrict machine describe model briefly review restrict machine serve building block model type random field model bipartite structure realvalued visible variable rd connect binary stochastic hide variable h h energy configuration h define follow bi h vi h ij p model parameter marginal distribution visible vector v p correspond conditional distribution follow form wij cj h vi bi wij observe condition state hide variable visible unit model gaussian distribution mean shift weight combination hide unit activation direct model rbms conditional distribution hide node factorial easily compute add binary rbm learn treat infer h visible layer second hide layer h result layer gaussian belief network gdbn powerful model specifically gdbn model ph model energy function rbm eq efficient inference perform use greedy approach treat dbn layer separate rbm model apply task include image classification video action speech recognition model let high resolution image scene eg image want use attention propagate region interest canonical representation example order learn model face canonical representation align crop frontal face image let rd represent low resolution canonical image work focus deep belief network model illustrate diagram fig left panel display model right panel graphical diagram propose generative model attentional mechanism represent latent hide variable dbn model generative model use attention framework d similarity transformation model figure leave circuit wellknown neuroscience model visual attention right propose model use d similarity transformation geometry gaussian dbn model canonical face image associative memory correspond dbn frame correspond visible layer attentional mechanism model d similarity transformation position rotation scale parameter d similarity transformation similarity transformation use rotate scale translate canonical image denote let p pixel coordinate canonical image let p set coordinate v example p range let gaze variable r parameter similarity transformation order simplify derivation transformation linear wrt transformation parameter equivalently u x sin s detail far define function p transformation function warp point p p use notation ip denote bilinear interpolation coordinate p let extract image location intuitively patch extract accord shift rotation scale parameter fig right panel patch datum seek model generatively note dimensionality equal cardinality p p denote set coordinate canonical image standard generative learning task datum static change latent variable u v u model topdown generative process gaussian distribution diagonal covariance matrix xi u fact seek model rest design use d similarity transformation mimic attention discard complex background scene let generative model focus object interest propose generative model follow form px ui u use flat prior constant u define layer gaussian deep belief network conditional u gaussian distribution simplify inference procedure gdbn model v share noise parameter omit dependence u clarity presentation inference generative equation section straightforward intuitive inference model typically intractable complicated energy landscape posterior inference wish compute distribution gaze variable canonical object big image standard rbms simplify factorial assumption conditional distribution latent variable d similarity transformation reminiscent machine perform topdown multiplicative gate connection v know inference higherorder model complicated way perform inference model resort gibb sample compute set alternate conditional posterior conditional distribution canonical image v follow form p bi topdown influence dbn note know gaze variable layer hide variable h v simply define distribution mean average topdown influence bottomup information x conditional distribution v dbn inference equation conditional posterior gaze variable xi u log pux log log use baye rule unnormalized log probability pux v define eq stress equation random variable interest actually affect conditioning variable eq explore gaze variable use hamiltonian intuitively condition canonical object model hmc search entire image find region good match v goal find map estimate pux want use secondorder method optimize equivalent framework vision develop image alignment advantage proper mcmc sampler satisfie detailed balance fit probabilistic framework specify hamiltonian position variable u auxiliary momentum variable r r u u potential function define energy function ri system define h r wpi u observe decompose sum single coordinate position pi yt let denote p wpi u coordinate pi warp u term ip dimension wpi u denote sampling gradient image location pi second term rhs eq note rewrite h h x wpi h x hmc simulate discretized system perform update u r use eq additional hyperparameter need specify include step size number step mass variable detail approximate hmc essentially perform gradient descent momentum prone stick local optimum especially problem task find good transformation parameter posterior u optimum local minima exist away global optimum example fig big image enclose blue box canonical image v enclose current setting wrong eye hard left local optima create dark intensity eye resample momentum variable iteration help significantly model realvalue image use gaussian distribution residual lead quadratic cost difference eq energy barrier mode extremely high average b alleviate problem need find good initialization u use convolutional network convnet perform efficient approximate inference result good initial guess specifically u predict change figure hmc easily lead maximum log pux v stick local optima b importance word instead use gradient field update modeling learn convnet output vector field space use fairly standard convnet architecture standard stochastic learning procedure note standard feedforward face detector seek model completely ignore canonical face v contrast v account convnet use initialize u important proper generative model condition v appeal multiple face present scene fig b euclidean space black manifold represent canonical face blue manifold represent crop face blue manifold low intrinsic dimensionality span u b blue come close black manifold mean mode posterior conditioning v narrow posterior single mode depend want focus attention demonstrate exact capability demonstrate iterative process approximate inference work model specifically base convnet window patch input predict output s step u update accordingly follow step alternate gibbs update v h discuss process repeat detail convnet supplementary material learn inference framework localize object interest object detection main objective motivation compete stateoftheart object detector propose probabilistic generative framework capable generative modeling object unknown location big image label expensive obtain available image unconstrained environment learn generatively label propose simple monte base expectationmaximization algorithm unbiased estimator maximum likelihood gibb step convnet step convnet step step step figure inference process u step randomly initialize average v extract input convnet approximate inference new u new u use sample step step gibbs sample gdbn perform step repeat approximate use update v inference step hmc figure example inference step v approximate inference quickly find good initialization u provide adjustment intermediate inference step right subsample actual iteration estep use gibbs sample develop draw sample posterior latent gaze variable u canonical variable hide variable h gaussian dbn model mstep update weight use posterior sample training datum addition update parameter convnet perform approximate inference fact estep require good inference algorithm need convnet use label gaze datum bootstrap process obtain training datum initial phase problem create datum demonstrate ability learn good generative model face image cmu dataset experiment use face dataset experiment dataset frontal face dataset caltech face collect dataset face unique individual different lighting condition expression background downsample image factor dataset contain manually label eye coordinate serve gaze label use cmu dataset contain subject capture viewpoint illumination condition recording session total image demonstrate model ability perform approximate inference learn label perform attention image people approximate investigate critical inference caltech face dataset run step approximate inference detail diagram fig follow iteration step hmc initially know correct v initialize average face subject image v inference test subject initial gaze box color yellow left subsequent gaze update progress yellow blue approximate inference good initialization start step iteration step use sample posterior quantitative result intersection union iou ground truth face infer face box result inference robust initialization require accuracy approximate inference accuracy approximate inference accuracy improvement average iou improvement trial iou average iou accuracy accuracy accuracy trial iou average iou initial pixel offset inference step initial pixel offset figure accuracy function offset blue curve percentage success iou red curve average iou b accuracy function number approximate inference step initialize pixel away c accuracy improvement function initialization dbn train caltech b dbn update figure leave sample layer dbn train caltech right sample update dbn train cmu label sample highlight green similar face cmu step approximate inference converge clearly improve model performance result iou increase localization impressive test subject training background different background training set compare inference template match task face method template subject test iou subject rest training table face localization accuracy image width accuracy iou image height s image scale number inference e inference near stateoftheart face detection system step use obtain accuracy use cascade form adaboost normalize cross correlation obtain accuracy euclidean distance template matching achieve accuracy note algorithm look constant number window baseline base scan window generative learning label main advantage model learn large image face localization label information manual require demonstrate use cmu face table variational lowerbound estimate dataset cmu face high subset frontal face ground truth label use split caltech dataset training validation set cmu face image training case convnet approximate inference need completely different background caltech cmu dataset remain cmu face split training validation set train gdbn h hide unit caltech training set train cmu training cmu label cmu caltech train caltech valid cmu train cmu valid log z u randomly initialize scale range detection use pretraine model figure leave condition different v result different note initial u exactly trial right additional example difference panel condition v view color convnet approximate inference use caltech training set cmu train image table estimate variational lowerbound average gdbn model assign groundtruth crop face image set different scenario left column model train caltech face low probability cmu face achieve variational lowerbound test image middle column use approximate inference estimate location cmu training face far train gdbn newly localize face dramatic increase model performance cmu validation set achieve lowerbound test image right column good possible result train cmu manual localization label case achieve lowerbound use anneal importance sample estimate partition function rbm detail estimate variational lower bind supplementary material fig far sample draw caltech train dbn fig b sample train cmu dataset use estimate u observe sample diverse set face train use greedy algorithm layer use fast persistent contrastive divergence substantially improve generative performance supplementary material detail inference ambiguity attentional mechanism useful multiple present posterior pux v condition mean attend function canonical object model mind fig b explore generate dataset concatenate face caltech dataset train approximate inference convnet test heldout subject predict fig depend canonical image condition exact gaze initialization lead different gaze shift note phenomenon observe different scale location initial gaze example fig panel yellow box female face left condition canonical face v right male attention shift right conclusion paper propose probabilistic graphical model framework learn generative model use attention experiment face modeling convnet base inference combine sampling sufficient explore complicated posterior distribution importantly generatively learn object interest novel big image future work include experiment face object large scene currently train supervised manner reinforcement learning use instead acknowledgement author gratefully acknowledge support grant note use label come cmu training set order convnet label mean label cmu image reference teh fast learning algorithm deep belief net neural computation r salakhutdinov deep boltzmann machine aistat geoffrey model manifold image handwritten digit ieee transaction neural network weiss learn model natural image patch image restoration pascal generalize denoise autoencoder generative model advance neural information processing system h r r convolutional deep belief network scalable unsupervised learning hierarchical representation icml page deep generative model application recognition geoffrey e hinton deep mixture factor icml m attention primary visual cortex proc e p r h r backward attentional effect ventral stream e visual attention insight brain imaging nature review neuroscience h circuit computational strategy dynamic aspect visual process model visual attention invariant pattern recognition base dynamic routing information journal neuroscience official journal society neuroscience visual system achieve shift size invariance c poggio bayesian inference theory attention vision s m selective tuning artificial intelligence geoffrey e hinton learn combine foveal boltzmann machine nip page p p storkey hierarchical generative model recurrent attention visual cortex volume page springer b teh search object drive context nip learn look m larochelle freitas learning attend deep architecture image tracking neural computation e hinton r salakhutdinov reduce dimensionality datum neural network science krizhevsky learn multiple layer feature tiny image fergus convolutional learning spatiotemporal feature eccv springer modeling use deep belief network ieee transaction audio speech language processing computer vision algorithm application text computer science springer hybrid monte physics letter r m neal use hamiltonian dynamic handbook chapman iain matthew year unifying framework international computer vision gross iain image vision comput p fast normalize crosscorrelation t e hinton use fast weight improve persistent contrastive divergence icml volume page acm r salakhutdinov murray quantitative analysis deep belief network proceeding machine learning volume
statistical analysis couple time series crossspectral density operator mpi intelligent system mpi biological cybernetic biological cybernetic mpi intelligent system abstract application require analysis complex interaction time series interaction nonlinear involve vector value complex datum structure graph string provide general framework statistical analysis dependency random variable sample stationary timeserie arbitrary object achieve goal study property density kcsd operator induce positive definite kernel arbitrary input domain framework enable develop independence test time series similarity measure compare different type couple performance test compare hsic test use iid assumption improvement term detection error suitability approach test dependency complex dynamical system similarity measure enable identify different type interaction neural time series introduction complex dynamical system observe monitor time series variable find characterize dependency time series key understand underlie mechanism system problem address easily linear system nonlinear system challenging high order statistic provide helpful tool specific extensively use system identification causal inference blind source separation example difficult derive general approach solid theoretical result account broad range interaction especially study relationship time arbitrary object text graph general framework largely hand dependency independent identically distribute arbitrary object study framework positive definite rely define crosscovariance operator variable map implicitly reproduce space rkh use characteristic kernel mapping property rkh operator relate statistical independence input variable allow testing principled way hilbertschmidt hsic test suitability test rely heavily assumption iid sample random variable use assumption obviously violate nontrivial setting involve time series consequence try use hsic context lead incorrect conclusion establish framework context markov chain structured hsic test provide good asymptotic property absolutely regular process methodology assess extensively empirical time detection interaction important able characterize nature coupling time recently suggest generalize concept crossspectral density space rkh help formulate nonlinear dependency measure time series statistical assessment measure establish paper recall concept spectral density operator characterize statistical property particular define independence test base concept similarity measure compare different type coupling use test section compute statistical dependency simulate time series type object recording neural activity visual cortex primate technique reliably detect complex interaction provide characterization interaction frequency domain background notation random variable reproduce space let possibly non input domain let c c positive definite kernel associate separable hilbert space function h h respectively canonical mapping xi define h detail way mapping extend random variable random variable map random element statistical object extend classical mean covariance random variable rkh define follow mean element e xi crosscovariance operator cij cov use tensor product notation represent rank operator define follow consequence crosscovariance operator hilbert space linear hilbertschmidt operator hi interestingly link cij covariance input domain hilbertschmidt scalar product fi cov fi fi hilbertschmidt norm operator space prove measure independence random variable kernel characteristic extension result provide markov chain time series assume result classical hsic generalize structured hsic use universal kernel base state vector x x k statistical performance methodology study extensively particular sensitivity dimension state vector following section propose alternative methodology density operator consider bivariate discrete time random process t assume stationarity process use follow translation invariant notation mean element crosscovariance operator cov cij crossspectral density operator introduce stationary signal base second order cumulant mild assumption hilbertschmidt operator define normalize frequency ei object summarize crossspectral property family process h gx sense f g refer object density operator kcsd statistical property kcsd measure independence interesting characteristic kcsd follow assume characteristic process independent integer t t independent theorem state use test pairwise independence imply independence arbitrary set random variable general joint probability distribution time series encode direct graph follow independence broad sense achieve mild assumption proposition joint probability distribution time series encode dag markov property assumption pairwise independence time imply mutual independence relationship proof proof use fact markov property assumption provide equivalence independence set random variable dseparation corresponding set node start assume pairwise independence time arbitrary time assume dag contain arrow link node x t path link node dseparate consequence independent initial assumption exist arrow t hold t t path link node time series x accord markov property joint probability distribution node factorize term time consequence use kcsd test independence justify widely use markov assumption graphical model comparison structured hsic propose theoretically able capture dependency range sample assume fourth order operator statistical property require assumption regard high order statistic analogously covariance high order statistic generalize operator product important example setting joint th order skip general expression cumulant focus simplified form center scalar random variable ex x x ex x ex x ex x ex x ex x ex x object generalize case random variable map operator linear operator space h h x arbitrary element fi property operator useful section follow property tensor let center random element space h center random element h center random element define h h c tr c tr c c c case jointly stationary time series define translation invariant stationary time series estimation following address problem estimate property crossspectral density operator finite sample idea analytically select sample timeserie window function r support include scale window accord multiply time series t sample sequence select estimate kcsd operator t successive sample time series c c pt ft x t kwk z ei fourier transform delay time series rkh property fourier transform relate regularity window particular choose window bounded variation case follow lemma hold supplementary material proof property variation bound function bounded variation wt t use assumption estimate asymptotically unbiased follow let bounded function bound variation proof definition p p t kwk p p kwk t kwk z wt n use t use z t kwk z n p z t p z c t s square norm asymptotically biased estimator square norm accord follow theorem assumption proof base decomposition provide supplementary information estimate require specific bias estimation technique develop independence test biased estimate kcsd squared norm kcsd define hilbert space enable define similarity kcsd operator possible compare quantitatively different dynamical system similar coupling follow enable estimate scalar product kcsd operator reflect similarity theorem assume assumption hold independent sample bivariate time map couple reproduce kernel pt t proof similar provide supplemental information interestingly estimate scalar product operator unbiased come assumption bivariate series independent provide new opportunity estimate hilbertschmidt norm case independent sample bivariate series available corollary assume assumption hold bivariate time series t independent copy time assume t t respectively provide estimate p p t p experimental setting neuroscience possible measure time independent trial case corollary state estimate norm kcsd bias possible use trial estimate unbiased estimate square norm estimate compute efficiently t frequency sample use fast fouri transform center kernel matrix time series general choice kernel tradeoff capacity capture complex dependency characteristic kernel respect convergence rate estimate simple kernel relate low order statistic usually require sample relate theoretical analysis find state bandwidth parameter yk use characteristic kernel vector space let denote matrix ith jth time kxi matrix m center matrix m t define center kernel m define discrete fouri transform matrix matrix t estimate scalar product pt kwk efficiently compute use fast fouri transform product biased unbiased squared norm estimate trivially retrieve expression independence test accord theorem pairwise independence time require crossspectral density operator frequency test independence test hilbertschmidt norm operator vanishe frequency rely corollary compute biased unbiased estimate norm achieve generate distribution hilbertschmidt norm statistic null hypothesis cut time interval nonoverlapping block match block time series pair central limit theorem sufficiently large number time window empirical average statistic approach gaussian distribution test empirical mean differ null distribution use prevent false positive result multiple hypothesis testing control error rate test perform frequency follow estimate global maximum distribution family frequency null hypothesis use distribution assess significance original square norm time number dependency detect number dependency detect detection probability biased rbf number sample frequency detection probability unbiased frequency number sample type c bias unbiased error rate square norm estimate bias unbiased type block hsic hsic figure result couple system topleft example time course estimate kcsd square norm estimate square norm performance biased kcsd test function number sample performance unbiased kcsd test function number sample rate type type error independence test experiment follow validate performance test kcsd dataset biased unbiased case general time analysis tool literature compare approach dataset main source comparison hsic test independence assume datum enable compare approach use kernel vector datum compare performance approach linear dependency measure implement test use linear kernel instead linear finally use alternative approach structured hsic cut time series time window use approach independence test consider single multivariate sample block hsic bandwidth hsic method choose proportional median norm sample point vector pvalue independence test set phase amplitude couple simulate nonlinear dependency time series generate oscillation frequency introduce modulation amplitude second oscillation phase achieve use follow discrete time equation sin k normal simulation hz hz sample frequency plot figure topleft panel parameter use window length sample use gaussian compute nonlinear dependency time series divide standard deviation panel figure plot mean standard error estimate squared hilbertschmidt norm system c linear gaussian respectively bias estimate appear clearly case power pick signal biased estimate second unbiased estimate spectrum exhibit mean peak hz kernel correspond expect frequency nonlinear interaction observed negative value direct consequence unbiased property influence bandwidth parameter kernel study case weakly couple time series leave middle panel figure biased unbiased frequency time s error rate transition probability estimate state type type error block hsic hsic figure markov chain dynamical system upper leave markov transition probability fluctuate value indicate graph upper right example simulate time leave biased unbiased kcsd norm estimate frequency domain right type type error hsic kcsd test influence parameter number sample require actually reject null hypothesis detect dependency biased unbiased estimate respectively observe choose hyperparameter close standard deviation signal optimal strategy test rely unbiased estimate outperform biased estimate use unbiased estimate subsequent analysis coupling parameter varied test performance independence test case null hypothesis independence true c reject c weak coupling c strong couple setting enable quantify type type ii error test respectively panel figure report error independence test method especially type error particular method base hsic detect weak dependency time time vary markov chain illustrate use test hybrid setting generate symbolic time series use alphabet control scalar time series coupling achieve modulate time transition probability markov transition matrix generate symbolic time series use current value scalar time series model describe follow equation sin k px sj bound markov transition matrix fluctuate time model represent figure topleft panel model fluctuation m simulate measure type error time course hybrid system illustrate panel figure order measure dependency time series use kernel use use count occurrence single symbol efficient compute kernel word successive symbol time series use signal factor signal cut time window sample biased unbiased estimate kcsd norm represent figure clear peak frequency independence test result figure illustrate kcsd type error type error stay acceptable range figure leave experimental setup lfp recording monkey visual stimulation movie right proportion detect dependency unbiased kcsd test interaction gamma band wide band lfp different neural datum local field potential visual analyze dependency local field potential time series record primary visual cortex monkey visual stimulation commercial movie figure scheme experiment lfp activity reflect nonlinear large variety underlie mechanism investigate extract activity frequency band electrode quantify nonlinear interaction approach lfps filter frequency band wide band range contain rich variety high gamma band range play role processing visual information time series sample use nonoverlapping time window point compute hilbertschmidt norm kcsd operator gamma large band time series originate electrode perform statistical testing frequency use fouri transform point result test average record site plot figure observe highly reliable detection interaction gamma band use linear nonlinear kernel fact gamma band filter version wide band lfp signal highly correlate gamma band addition obvious linear dependency observe significant interaction low frequency explain linear interaction detect linear kernel characteristic illustrate nonlinear interaction high frequency gamma low frequency brain electrical activity report study interpretability approach test nonlinear dependency frequency domain conclusion independence test time series base concept cross spectral density estimation introduce paper generalize linear approach base fouri transform respect allow nonlinear interaction time live vector space measure dependency complex object include sequence arbitrary alphabet graph long appropriate positive definite kernel define space time series paper provide asymptotic property estimate efficient approach compute real datum space operator constitute general framework analyze dependency multivariate highly structure dynamical system follow independence test far combine recent development series prediction technique define general reliable technique acknowledgment mb grateful fruitful discussion advice reference hilbert space probability statistic m d scholkopf find dependency frequency crossspectral density ieee international conference acoustic speech process page o statistical property principal component analysis machine learn time datum analysis theory contrast independent component analysis neural computation statistical convergence advance information processing system page m reduction supervised learning reproduce space mach learn r b scholkopf rkh embedding probability distribution advance neural information processing system page sun scholkopf kernel measure conditional dependence advance neural information processing system page g b identification phase system use high order statistic speech signal processing ieee transaction teo l song b scholkopf kernel statistical test independence advance neural information processing system page m optimal kernel choice largescale twosample test advance neural information processing system page hyvarinen s p hoyer causal modelling combine instantaneous lag effect identifiable model base proceeding th international conference machine learn page acm c e spectrum kernel string kernel protein classification c higherorder spectra analysis nonlinear signal processing framework prenticehall d t r comparison random field theory permutation method statistical analysis datum causality model reasoning press cambridge scholkopf causal inference time series use structural equation model advance neural information processing system b scholkopf smola learning mit cambridge q scalable kernel learn nonlinear multivariate regression causality proceeding th conference uncertainty artificial intelligence surface eeg reflect spike activity visual kernel measure independence advance neural information processing system page
combinatorial multiarmed bandit paper study stochastic combinatorial multiarmed bandit framework allow general nonlinear reward function expect value depend mean input random variable possibly entire distribution variable framework enable large class reward function max function nonlinear utility function exist technique rely accurate estimation mean random variable upper confidence bind ucb technique work directly function propose new algorithm stochastically dominant confidence bind estimate distribution underlie random variable stochastically dominant confidence prove sdcb achieve olog distributiondependent regret distributionindependent regret time horizon apply result kmax problem expect utility maximization problem particular kmax provide approximation scheme pta bind approximation offline problem regret online problem introduction stochastic multiarmed bandit mab classical online learning problem typically specify player machine arm arm pull generate random reward follow unknown distribution task player select arm pull round base historical reward collect goal collect cumulative reward multiple round possible paper specify use mab refer stochastic problem demonstrate key tradeoff exploration exploitation player stick choice perform good far try alternative provide reward performance measure mab strategy cumulative regret define difference cumulative reward obtain play arm large expect reward cumulative reward achieve learn strategy variant extensively study literature classical result tight log t distributiondependent t distributionindependent upper low bound regret t round important extension classical mab problem combinatorial multiarmed bandit cmab player select arm round subset arm combinatorial research email author list order email email email email economic email conference neural information processing system nip object refer super arm collectively provide random reward player reward depend outcome select arm player observe partial feedback select arm help decision cmab wide application online advertising online recommendation wireless route dynamic channel allocation setting action unit combinatorial object set advertisement set recommend item route wireless network allocation channel user reward depend unknown stochastic behavior user click behavior wireless transmission quality cmab attract lot attention online learning research recent year study focus linear reward function expect reward play super arm linear combination expect outcome constituent base arm study generalize nonlinear reward function typically assume expected reward choose super arm function expect outcome constituent base arm super arm natural reward function satisfy property example function max group variable output maximum expectation depend distribution input random variable mean function max variant application illustrative example consider follow scenario auction repeatedly item m bidder round select bidder bid bidder independently draw bid private valuation distribution submit bid use auction determine winner collect large bid goal gain high cumulative possible refer problem kmax bandit problem effectively solve exist cmab framework kmax problem expect utility maximization eum problem study optimization literature problem formulate maximize xi feasible set s s independent random variable u utility function example random delay edge ei routing graph route path graph objective maximize utility obtain routing path typically short delay large utility utility function typically nonlinear model behavior user eg concave utility function use model behavior nonlinear utility function objective function complicate particular long function mean underlie random variable distribution unknown turn eum online learning problem distribution need learn time online feedback want maximize cumulative reward learning process cover exist cmab framework learn mean paper generalize exist cmab framework feedback handle general reward function expect reward play super arm depend mean base arm outcome distribution base arm arbitrary generalization nontrivial prior work cmab rely estimate expect outcome base arm accurate use standard tool bind case need estimation method analytical tool accurately estimate distribution mean readily available tackle problem step discrete random variable know finite support turn problem estimate cumulative distribution function value support point use stochastically dominant confidence bind obtain distribution stochastically dominate true distribution high probability algorithm able olog t distributionindependent regret bound t round second distributiondependent handle general bound discretization technique t distributionindependent regret bound regret bound distributiondependent tight respect dependency t logarithmic factor distributionindependent bound scheme work reasonable assumption include boundedness monotonicity reward function independence base arm apply sdcb kmax eum problem provide efficient solution concrete understand auction truthful example illustrative purpose max function olog distributiondependent bind general variable hold sufficiently large t regret bound way provide polynomial time approximation scheme pta offline kmax problem formulate maximize subject cardinality constraint k s independent nonnegative random variable summarize contribution include generalize cmab framework allow general reward function expectation depend entire distribution input random variable propose sdcb algorithm achieve efficient learning framework nearoptimal regret bound arbitrary outcome distribution c pta offline kmax problem general framework treat offline stochastic optimization oracle effectively integrate online learn relate work mention relevant work study cmab framework focus linear reward function look nonlinear reward function particular look general nonlinear reward function consider specific nonlinear reward function conjunctive form paper require expect reward play super arm determine expect outcome base arm work combinatorial bandit aware require assumption expect reward base general sample framework assume joint distribution base arm outcome know parametric family known likelihood function parameter unknown assume space finite contrast general case nonparametric allow arbitrary bounded distribution know finite support case distribution parametrize probability support point parameter space continuous unclear efficiently compute posterior algorithm regret bound depend complicated coefficient large combinatorial problem provide result kmax problem consider outcome arm simple case general distribution allow extensive study classical mab problem refer survey study adversarial combinatorial bandit bear similarity stochastic cmab technique use different expect utility maximization eum encompass large class stochastic optimization problem study good knowledge study online learn version problem provide general solution systematically address problem long available offline approximation kmax problem trace provide constant approximation generalize version objective choose subset cost maximize expectation certain knapsack setup notation problem formulation model combinatorial multiarmed bandit cmab problem tuple e d r e m m set m base arm e set subset d probability distribution m r reward function define m arm produce stochastic outcome draw distribution d outcome ith arm feasible subset arm super arm realization outcome player receive reward rx choose super arm s play loss generality assume reward value nonnegative let s maximum size super arm let iid sequence random vector draw outcome vector generate tth round tth round player choose super arm play outcome arm reveal player accord definition reward function reward value tth round st expect reward choose super arm s round denote rx assume fix super arm s reward rx depend reveal alternatively express rx s rs function define learning algorithm cmab problem select super arm play round base reveal outcome previous round let super arm select tth round goal maximize expect cumulative reward t round e rd note underlie distribution d t rx know optimal algorithm choose optimal super arm rd quality algorithm measure regret t round difference expect cumulative reward optimal algorithm t rd rd cmab problem instance optimal super arm s computationally hard find distribution d know efficient approximation algorithm exist approximate solution s satisfy rd rd efficiently find d input provide exact formulation requirement approximation computation oracle shortly case fair compare cmab optimal algorithm choose optimal super arm s instead define approximation regret algorithm t rd mention previous work cmab require expect reward rd super arm depend expectation vector m outcome strong restriction satisfied general nonlinear function general distribution main motivation work remove restriction assumption paper assumption outcome distribution d reward function assumption independent outcome arm outcome m arm mutually independent d mutually independent write d d d dm distribution remark independence assumption past study offline eum kmax problem extra assumption online learning case assumption bound reward value exist m rx m assumption monotone reward function vector xi m rx rx assumption reward function exist c m p x x rx rx xi computation oracle discrete distribution know finite support require exist approximation computation oracle maximize rd s m know finite support case suppose support m fully describe set cumulative distribution function value define d oracle d describe input output super arm s note random random outcome previous round possible randomness use sdcbfsd sdcb finitely support distribution parameter initialization m action play super arm si contain arm observe outcome arm find k si ti end t m action tth round m q end play super arm observe outcome arm find k ij ti ti ti ti end end discrete distribution know finite support k section consider special case outcome distribution di m know finite support let let finite upper bind support size s m assume m require assumption section assumption use general distribution case section present algorithm stochastically dominant confidence bind refer sdcbfsd sdcb finitely support distribution algorithm start initialization round arm play line maintain empirical probability m base observe outcome arm far denote store variable ti number time outcome arm observe far tth round t m consist step calculate low confidence bind line ensure fi fi m valid vector distribution d dm remark numerical low confidence bind distribution level di serve stochastically dominant upper confidence bind second step approximation oracle input line super arm st output oracle satisfy rd s finally choose super arm play observe outcome arm update accordingly line idea algorithm face uncertainty principle key principle algorithm algorithm ensure high probability loss generality assume arm m contain super arm firstorder stochastic dominance monotonicity property rx assumption know rd hold s high probability d provide optimistic estimation expect reward super arm note use parameter control confidence radius line section set constant eg general distribution case section choose carefully work discretization regret bound prove distributiondependent log t distributionindependent upper bound regret sdcbfsd constant parameter super arm s bad super arm s define max rd let s set bad super arm let m set arm contain bad super arm define imin min recall m upper bind reward value assumption suppose outcome distribution di arm m know finite support size s distributiondependent upper bind approximation regret sdcbfsd t round m m m distributionindependent upper bind p m m m proof supplementary material main idea analysis deal summation reward function rx xi able generalize reward function satisfie assumption apply algorithm previous cmab framework focus general reward function note sdcbfsd apply previous framework expect reward depend mean random variable achieve regret bound previous combinatorial upper confidence bind finite support case let xi arm mean outcome round calculate arm upper confidence bind essential property hold high probability sdcbfsd use di stochastically dominant confidence bind hold high probability interval length proof supplementary material analysis apply sdcbfsd result regret bound term contain t term contain t extra factor s union bind support point far remark case need assumption section particular independence assumption summation reward case work nonlinear reward case rely property monotonicity bound smoothness use general distribution section consider general case di arbitrary distribution perform discretization step distribution apply sdcbfsd note use condition assumption use finite support case section sdcb general distribution know t parameter input s e m s invoke sdcbfsd round follow change observe outcome arm find s regard outcome sdcb general distribution know parameter round invoke input q parameter q q invoke input parameter end algorithm describe time horizon t know advance sdcb general distribution know time horizon algorithm summarize perform discretization distribution d dm obtain d d m x d x m discrete distribution d mutually independent di support set value positive integer determine specifically partition interval define di pr pr x d know finite support sdcbfsd apply discretized r original cmab problem m d r problem m instead replace outcome ij outcome draw r set apply sdcbfsd problem m d e constant parameter choose discretization parameter depend time horizon t know t advance use double trick avoid dependency t present algorithm know refer sdcb general distribution regret bound achieve o log t distributionindependent regret bound achieve olog distributiondependent regret bind sufficiently large t proof supplementary material recall coefficient lipschitz condition suppose time horizon t know advance distributionindependent upper bind approximation regret parameter t round p m t m m t distributiondependent upper bind approximation regret parameter t round ck m m m imin proof sketch regret consist regret discretize cmab problem r error discretization analyze discrete m distribution case section second key step rd rd theorem time horizon t approximation regret parameter t round m m ln application describe kmax problem class expect utility maximization problem application general kmax problem problem player allow select k arm set m arm round reward maximum outcome select arm word set feasible super arm m k reward function rx xi easy verify reward function satisfy assumption m c consider correspond offline kmax problem select k arm m independent arm large expect reward imply result find exact optimal solution nphard resort approximation algorithm use simple greedy algorithm achieve furthermore pta problem pta generalize constraint cardinality constraint include simple path matching knapsack algorithm corresponding proof supplementary theorem exist pta offline kmax problem word constant factor approximation algorithm kmax problem apply algorithm kmax bandit problem obtain olog t distribution t distributionindependent regret bound accord theorem dependent study online submodular maximization problem oblivious adversary particular result cover stochastic kmax bandit problem special case log m upper bind technique bind regret obtain t bind approximation regret constant use pta o offline oracle use simple greedy algorithm oracle experiment sdcb perform significantly algorithm supplementary material expect utility maximization framework apply reward function form rx u increase utility function correspond offline problem maximize expect utility subject feasibility constraint note nonlinear expect utility function mean arm follow expect utility function extensively use capture behavior economic linear utility function correspond obtain pta expect utility maximization eum problem class utility function include example increase concave function typically indicate large class feasibility constraint include cardinality simple path matching knapsack similar result utility function feasibility constraint find online problem apply sdcb use offline oracle obtain tight regret bound approximation regret class online eum problem support national natural science foundation grant support national basic research program grant reference minimax policy adversarial stochastic bandit colt page fischer analysis multiarmed bandit problem machine learn multiarmed bandit compute utility equivalence theorem function page springer regret analysis stochastic multiarmed bandit problem foundation trend machine learn lugosi combinatorial bandit journal computer system science pure exploration multiarmed bandit nip bandit extension probabilistically trigger arm journal machine learn research m alexandre combinatorial bandit revisit nip p c foundation expect utility combinatorial network optimization unknown variable bandit linear reward individual observation transaction network ask right question optimization use probe page acm probe extreme value acm transaction sampling complex online problem icml page bandit fast combinatorial optimization learn page combinatorial cascade bandit nip tight regret bound combinatorial aistat page robbin asymptotically efficient adaptive allocation rule advance apply mathematic maximize expect utility stochastic optimization problem page stochastic combinatorial optimization poisson approximation stoc page monitoring game linear feedback application icml page online greedy learning feedback nip analysis approximation maximize submodular set function mathematical program matthew online algorithm maximize submodular function nip maximize expect utility knapsack constraint operation research letter
joint analysis timeevolve binary matrix associate document electrical computer abstract consider problem incomplete binary matrix evolve time vote legislator particular legislation year characterize different matrix objective analysis infer structure underlie matrix define feature associate axis matrix addition assume document available entity associate matrix axis jointly analyze matrix document use inform analysis model offer opportunity predict matrix value vote base associate document legislation research present merge area previously investigate separately analysis topic model analysis perform bayesian perspective efficient inference constitute gibb sample framework demonstrate consider voting datum available document legislation year lifetime representative introduction significant recent research analysis incomplete matrix analysis perform assumption matrix real interesting problem matrix binary example reflect link node graph analysis datum associate series binary question connect underlie real matrix binary generally integer observation probit logistic link function example analysis perform context analyze datum problem receive attention concern analysis timeevolve matrix specific motivation paper involve binary question setting interested analyze datum session legislator change time undesirable treat entire set vote single matrix piece legislation question unique desirable infer commonality time similar latent grouping relationship exist legislator general setting interest analysis social network distinct line research focus analysis document topic modeling constitute popular framework analysis matrix document perform independently problem document matrix couple example addition matrix link website email datum access associated document website email content analyze matrix document simultaneously infer example model matrix associate document use relate matrix factor provide insight document matrix vice versa author knowledge paper represent joint analysis timeevolve matrix associate document analysis perform use nonparametric bayesian tool truncated dirichlet process use jointly cluster latent topic matrix feature framework demonstrate analysis largescale datum set specifically consider binary vote matrix house representative present document legislation available recent year analyze jointly matrix datum quantitative predictive performance framework demonstrate power setting qualitative assessment largescale complex joint data modeling framework timeevolve binary matrix assume set binary matrix number row column respectively vary time example datum consider time index t correspond year number piece legislation legislator change time historical datum consider number state legislator change grow use modeling framework analogous binary matrix generative process latent real matrix define yi ij t denote vector inner product random effect draw gammaa b gammaa point measure infinity correspond associate random effect probability random effect control draw beta distribution random effect motivate example application index j denote specific piece legislation vote parameter reflect difficulty vote large people likely vote way easy vote small t t detail legislator define legislation define strongly impact vote previous political science bayesian analysis researcher simply set consider model setting infer relationship t t additionally previous bayesian analysis dimensionality set usually related probabilistic matrix factorization apply real matrix employ constrain dimensionality latent feature employ sparse binary vector set large integer set appropriately favor component b impose sparseness specifically integrate readily number nonzero component b random variable draw expect number b relate draw truncated betabernoulli process consider type matrix axis specifically assume row correspond present matrix assume column correspond question example piece legislation question unique column unique assume denote vector product associate row time t introduce time associate feature vector similarly draw b yi ik assume draw active prior time simple autoregressive model use draw prior set favor change feature individual consecutive year model constitute relatively direct extension exist technique real matrix specifically introduce probit link function simple construction impose statistical correlation trait consecutive time introduction random effect consider literature use standard political science bayesian model principal modeling contribution paper concern integrate timeevolve model associated document topic model manner topic modeling perform generalization latent dirichlet allocation lda assume document interest word draw vocabulary kth topic characterize distribution pk word bagofword assumption v generative model draw possible topic document characterize probability distribution topic t t correspond distribution t topic document generative process draw word document l draw cl document l word document l draw topic specific word draw multinomial probability vector procedure standard lda difference manifest handle dirichlet distribution v t t t dirichlet distribution draw constitute sethuraman construction allow place prior v t retain conjugacy permit analytic gibb sample posterior distribution model parameter implementation employ point estimate probability topic specifically follow hierarchical construction use draw v similarly pk h beta h nh probability mass associate component h probability vector infinite sum truncate analogous truncate stickbreaking representation dirichlet process joint analysis matrix document section discuss model timeevolve binary matrix section describe procedure implement topic model model specifically consider case document dj word associate column time example correspond jth piece legislation year possible document associate matrix row eg speech model development example document assume present column column j time t feature vector matrix distribution t topic cj document couple remainder matrix topic model unchanged define set atom cm m m atom m draw gamma prior place m draw gamma distribution cm draw t t use dirichlet tion construction couple cj draw indicator variable m m m cm ci cm beta m t gamma prior place cm cj define t t far cj set construction cluster column clustering mechanism define truncated stick break representation dirichlet process component m m define t m m figure graphical representation model hyperparameter omit simplicity plate indicate replication fill circle indicate observe mixture gmm feature space cm define set m probability vector topic vector associate aforementioned gmm mixture component truncated dirichlet process infer mixture component need represent datum construction matrix column associate distribution topic base mixture component draw provide powerful insight latent feature matrix model word associated document far topic matrix model constitute jointly topic define match characteristic matrix simply model document isolation yield topic necessarily connected matter matrix graphical representation model extension consider future work example simplicity column feature space assume consider separate gmm time year far explicitly impose topic model consider example present real datum simplification model perform computation posterior distribution model parameter compute use gibb sample detailed update equation provide supplemental material gibb iteration discard follow collection truncation level model t m number word vocabulary hyperparameter set b e d h parameter optimize reasonable relate setting yield similar result perform joint matrix text analysis consider vote record available document associate legislation consider house representative legislation document available session legislation stop word remove use common list stop word post stem use datum available library vote text metadata vote date binary matrix manifest map vote code present negative code eg present present vote piece legislation miss datum manifest naturally vary year year typically vote miss year implement propose model matlab computation perform pc ghz cpu memory total hour cpu time require analysis session hour house session case correspond joint analysis vote text legislation analyze vote hour cpu require session hour respectively number legislator house time large experiment joint analysis document vote consider joint analysis legislation document vote key aspect analysis clustering legislation legislation time map cluster mixture component mixture component characterize latent topic cj latent feature associate matrix analysis recall section dominant cluster infer datum run gibbs sampler cluster index change general consecutive iteration index illustrate nature cluster base gibbs iteration dimensionality feature infer average gibb collection dimension dominate legislation feature vector figure present infer distribution principal mixture component cluster index index feature arbitrary example number cluster illustrative simplicity figure depict distribution topic cm associate cluster figure list probable word associate topic examine topic characteristic figure distribution topic assign latent feature link associate matrix vote example cluster latent space row figure share similar support topic row figure cluster appear associate highly topic specifically topic topic consider word figure base voting datum party legislation sponsor author red appear represent viewpoint topic blue appear represent viewpoint distinction play important role predict vote legislation base document discuss section figure plot present estimate density function estimate gibbs collection iteration note p tightly concentrate p political science literature consider researcher simply set assume random effect legislation analysis appear confirm simplification reasonable matrix prediction base document significant recent interest analysis matrix particularly predict entry miss random research view subset individual movie example help inform prediction rating people movie fraction people movie problem consider previous model applicable prediction vote new legislation ln require relate vote previous legislation l absence prior vote correspond estimate entire column vote matrix joint analysis text legislation vote offer ability relate l connection underlie topic legislation document absence vote examine predictive potential perform joint analysis vote legislation document process yield model similar summarize figure use model predict vote new legislation base document associated legislation use vote information new legislation mixture topic learn datum assume fix topic characterize distribution word fix topic use analysis dimension latent dimension latent dimension latent dimension topic topic cluster topic topic topic figure characteristic principal mixture component cluster associate datum base joint analysis document associate vote matrix row principal dimension latent matrix feature ellipsis denote standard deviation mean cluster point reflect specific legislation result color ellipsis link color topic distribution row distribution topic cm cluster number index arbitrary t topic consider cluster characterize distribution topic cm row associate feature row topic annual product topic topic public public electrical annual topic topic cost annual waste topic topic topic annual child student organization list topic topic minor annual topic topic topic topic cost work build topic topic topic annual health parent topic cost public depend topic train annual annual parent male ve figure probable word associate topic document new legislation manner new document map distribution topic cm particular piece legislation map mapping base word assume matrix feature associate legislation associated mean m learn modeling datum mapping legislation matrix latent space achieve use senator latent feature vector readily compute m probit link function probability vote quantify new legislation model base figure plot approximation t t reasonable random effect expect important senator vote senator sort predict probability senator vote probability voting senator sort predict probability empirical predict voting frequency empirical predict voting frequency probability voting senator vote senator sort predict probability senator vote senator sort predict probability probability empirical predict voting frequency probability voting probability voting empirical predict voting frequency figure plot predict probability voting legislation text base model learn use datum dot color empirical voting frequency legislation cluster use model cluster utilize session infer model overall period plot estimate log p log p note p sharply peak t t legislation senator vote large positive large negative t test predictive quality model heldout year assume parameter infer model text vote jointly model document test model legislation senator agree legislation assume correspond small assume practice simple determine piece legislation likely modelbase prediction vote legislation deem interesting figure compare predict probability senator vote legislation cluster figure point figure represent empirical datum senator curve represent prediction probit link function result deem remarkably good figure senator horizontal axis order accord probability voting interesting issue arise prediction concern cluster figure associate prediction heldout year figure distribution cluster topic similar document distinguish cluster sponsor piece legislation base datum piece legislation map cluster base party sponsor viewpoint topic viewpoint base voting record time evolution legislation joint analysis text vote restrict document legislation available year dataset contain vote legislation present analyze vote datum figure snapshot time latent space legislation house representative similar result compute omit brevity supplemental material present movie legislation evolve time feature infer feature choose axis blue symbol denote legislator legislation sponsor red point correspond result interest political scientist allow degree time example year year year year year year year year year year figure legislation latent space session house representative separation usually sharp frequently information matter note rotation blue axis view additional quantitative test ask model address problem estimate value matrix datum miss uniformly random absence document examine question consider binary vote datum remove fraction vote uniformly random use propose timeevolve matrix model process observe datum compute probability vote miss datum probit link function probability large vote set set compare timeevolve model addition probit link function process year large matrix analyze timeevolve structure missingness propose model modify version perform identically average probability error binary vote approximately great missingness propose timeevolve model manifest phase transition probability error increase smoothly fraction miss datum rise contrast generalize model probit link continue yield probability error phase transition propose model likely manifest entire matrix partition year year manifest markov process legislator analyze datum contiguous large matrix phase transition expect base theory fraction miss datum large size contiguous matrix analyze timeevolve model small entire matrix phase transition expect missingness analysis entire matrix result interest deem encourage uniformly random missingness matrix datum motivation propose model traditional method predict vote new legislation base word figure model allow analysis timeevolve property element matrix figure conclusion new model develop joint analysis timeevolve matrix associate document author knowledge paper represent integration research perform separately topic model matrix model implement efficiently gibbs sample unique set result present use datum house representative demonstrate ability predict vote new legislation base associated document legislation datum consider readily available interesting right propose framework interest problem example model applicable analysis timeevolve relationship multiple entity augment presence document link website associate document content acknowledgement research report support research office grant office naval research grant reference vert new approach collaborative filter operator estimation spectral regularization machine learn research d m blei lafferty dynamic topic model proceeding conference machine learn page m blei d correlated topic model science annal apply statistic m blei m dirichlet allocation learn research cande tao power relaxation nearoptimal matrix completion ieee transaction information theory d statistical analysis datum political review ferguson bayesian analysis nonparametric problem annal statistic p d multiplicative latent factor model description prediction social network computational mathematical organization theory l gibb sample method stickbreake prior e r neal s roweis model datum binary latent factor advance nip page l l hierarchical bayesian model topic document ieee mach r salakhutdinov bayesian probabilistic matrix factorization mcmc advance nip r salakhutdinov probabilistic matrix factorization advance nip sethuraman constructive definition dirichlet prior srebro matrix factorization advance nip r hierarchical beta process indian buffet process international conference artificial intelligence statistic h m wallach topic modeling bag word proceeding conference machine learn largescale collaborative prediction use nonparametric random effect model int machine learning
differential entropic clustering dept computer learn model input single sample draw multivariate gaussian reallife setting input object describe multiple sample draw multivariate gaussian datum arise example movie review database movie rate user timeserie domain sensor network input naturally describe mean vector covariance matrix parameterize gaussian distribution paper consider problem cluster input object represent multivariate gaussian formulate problem use information theoretic approach draw interesting theoretical connection bregman divergence bregman matrix divergence evaluate method domain include synthetic datum sensor network statistical debugging application introduction walk life learn component analysis linear discriminant analysis input object single sample draw multivariate gaussian example assume input single sample draw unknown isotropic gaussian goal kmean view discovery mean gaussian recovery generate distribution input object reallife setting input object naturally represent multiple sample draw underlying distribution example student score read arithmetic measure school year alternately consider website movie rate basis plot act different user rate movie multiple sample timeserie datum sensor network sensor device continually monitor environmental condition temperature light clustering important data analysis task use application example clustering sensor network device use optimize route network discover trend sensor nod employ mean distribution cluster ignore second order covariance information clearly solution need paper consider problem cluster input object represent multivariate gaussian distribution distance gaussian quantify information theoretic manner particular differential relative entropy interestingly differential relative entropy multivariate gaussian express combination bregman mahalanobis distance mean vector burg matrix divergence covariance matrix develop style cluster optimal cluster parameter determine simple closedform solution algorithm clustering method cluster mean covariance distribution unified framework evaluate method domain present result synthetic datum experiment incorporate second order information dramatically increase cluster accuracy apply algorithm sensor network comprise sensor device measure temperature light voltage finally use statistical debugging tool cluster behavior function program set know software bug preliminary present essential background material multivariate gaussian distribution multivariate generalization standard univariate case probability density function pdf ddimensional multivariate gaussian parameterize positive definite covariance matrix t px d determinant bregman divergence respect define d x yt realvalued strictly function define convex set rd differentiable relative interior example result bregman divergence standard square euclidean distance similarly xt arbitrary matrix result divergence mahalanobis distance s parameterize covariance matrix s alternately result divergence unnormalized relative entropy bregman divergence generalize property square loss relative entropy bregman divergence naturally extend matrix follow try t matrix realvalued strictly function define matrix denote trace consider function corresponding bregman matrix divergence square norm burg matrix divergence generate function eigenvalue d positive definite matrix log log burg entropy eigenvalue result burg matrix divergence log later burg matrix divergence arise naturally application let eigenvalue corresponding eigenvector let eigenvalue eigenvector burg matrix divergence write wj log d bx term burg matrix divergence function eigenvalue eigenvector differential entropy continuous random variable probability density function define log quantization continuous random variable approximately equal continuous analog discrete relative entropy differential relative entropy random variable g differential relative entropy define x log cluster multivariate gaussian differential relative entropy set multivariate gaussian parameterize mean vector m covariance sn seek disjoint exhaustive partitioning gaussian different cluster cluster represent multivariate gaussian parameterize covariance use differential relative entropy distance measure problem clustering pose minimization clustering differential relative entropy multivariate gaussian differential entropy multivariate gaussian express convex combination mahalanobis distance mean burg matrix divergence covariance matrix consider multivariate gaussian parameterize mean vector m covariance respectively differential relative entropy express log log log term negative differential entropy logd consider second term log px d t log t d log e t logd e m m m m t logd m m t logd t tr m m logd expectation distribution s second line follow definition mx fact ex t d logd s tr logd m t m log m t m m m burg matrix divergence m m mahalanobis distance parameterize covariance matrix consider problem find optimal representative gaussian set mean m covariance nonnegative weight optimal representative minimize cumulative differential relative entropy px px m second term view minimize bregman information respect fix unknown bregman divergence mahalanobis distance parameterize covariance matrix consequently unique minimizer form mi note equation strictly convex derive optimal set gradient respect px set yield si si t figure illustrate optimal representative dimensional gaussian mean mark point b covariance outline solid line optimal gaussian representative denote dotted covariance representative left use weight b representative right use weight equation optimal representative mean weighted average mean constituent gaussian interestingly optimal covariance turn average constituent covariance update change account deviation individual mean representative mean present clustering algorithm case gaussian equal weight method work framework initially cluster assignment choose assign randomly algorithm proceed iteratively convergence mean covariance parameter cluster representative distribution optimally compute cluster assignment parameter update cluster update input gaussian assign cluster representative gaussian close differential relative entropy figure optimal gaussian representative dotted line gaussian center b different set weight optimal mean representative average individual mean optimal covariance average individual covariance correction step locally optimal convergence algorithm local optima note problem n p hard convergence global optima guarantee consider run time input gaussian ddimensional line compute optimal mean covariance cluster require ond total work respectively line compute differential relative entropy input representative gaussian arg min need reduce burg matrix divergence computation equation log inverse cluster covariance compute cost term compute time second term similarly compute cluster total cost compute mahalanobis distance od operation total cost line total running time algorithm iteration d differential entropic clustering multivariate gaussian m mean input gaussian s sn covariance matrix input gaussian initial cluster assignment converge update mean end update covariance mi t end n assign gaussian close representative mi b burg matrix divergence m mahalanobis distance parameterize end end experiment present experimental result algorithm different domain synthetic dataset sensor network datum statistical debugging application synthetic datum synthetic dataset consist set object consist sample draw randomly generate ddimensional multivariate gaussian normalize mutual information normalize mutual information multivariate multivariate number cluster number dimension figure cluster quality synthetic datum traditional kmean cluster use firstorder information ie mean gaussian clustering algorithm incorporate covariance information algorithm achieve high clustering quality dataset compose gaussian varied number cluster leave varied dimensionality input right generate choose mean vector uniformly random unit simplex randomly select covariance matrix set matrix eigenvalue figure compare algorithm cluster object solely mean sample accuracy quantify term normalize mutual information discover cluster true cluster standard technique determine quality cluster figure leave cluster quality function number cluster dimensionality input gaussian fix figure right cluster quality cluster vary number dimension result represent average value experiment figure multivariate gaussian yield significantly high value kmean experiment sensor network sensor network wireless network compose small sensor monitor surround environment open question sensor network research minimize communication cost sensor base wireless communication require relatively large power limited resource current sensor device usually power recently propose sensor network system reduce communication cost model sensor network datum sensor device use timevarye multivariate gaussian transmit model parameter base apply multivariate gaussian cluster sensor device lab clustering use sensor network application determine efficient routing scheme discover trend group sensor device sensor network consist work sensor monitor ambient temperature light level voltage second condition time sensor reading fit multivariate figure result multivariate gaussian apply sensor network datum device compute sample mean covariance sensor reading pm day total day account vary scale normalize variable unit variance second cluster denote figure large variance cluster sensor cluster locate high traffic area include large conference room lab small table lab measurement expect high traffic area interestingly cluster high voltage characterize high temperature surprising window left lab window face view finally moderate level total variation relatively low light level cluster primarily locate center right lab away outside window figure reduce communication cost sensor network sensor device model multivariate plot result apply algorithm cluster sensor group denote label statistical debugging leverage program runtime statistic purpose software debugging receive recent research attention apply algorithm cluster functional behavior pattern software bug latex document preparation program data system system use machine learn provide error message dataset contain software bug cause latex specify incorrect number column array environment ambiguous unclear error message provide latex error message document example message latex error s line end cause numerous problem source document function program source measure frequency software bug model distribution dimensional multivariate dimension bug distribution estimate set sample sample correspond single latex file draw set grant proposal submit computer science research paper file bug latex execute slightly modify version file change exhibit bug execution function count measure record detail find cluster function count yield important debugging insight software understand error dependent program behavior figure covariance matrix sample clustering cluster capture dependency bug normalize input mean unit variance cluster represent function highly error matrix high level pair error class conversely cluster b function highly error dependent cluster high dependency bug cluster exhibit high bug bug b figure covariance matrix cluster discover cluster functional behavior latex preparation program cluster correspond function cluster b c represent group function exhibit different type error dependent behavior relate work work differential relative entropy multivariate distribution express convex combination mahalanobis distance mean vector burg matrix divergence covariance contrast information theoretic cluster input probability distribution finite set parametric form assume divergence discrete relative entropy compute directly distribution differential entropy multivariate consider context solve mixture model algebraic expression differential entropy connection burg matrix divergence algorithm base standard expectationmaximization style cluster closedform update use algorithm similar employ bregman cluster note computation optimal covariance matrix equation involve optimal mean problem cluster gaussian respect symmetric differential relative entropy consider context learn parameter speech recognition result algorithm computationally expensive method optimal mean covariance parameter compute simple close form solution solution present iterative method instead employ problem find optimal gaussian respect argument note equation minimize respect second argument consider problem speaker interpolation source assume clustering need conclusion present new algorithm problem cluster multivariate gaussian algorithm derive information theoretic context lead interesting connection differential entropy multivariate gaussian bregman divergence exist cluster algorithm algorithm optimize second order information datum demonstrate use method sensor network datum statistical application reference banerjee dhillon s cluster bregman divergence international conference datum mining page l bregman relaxation method find common point convex set application solution problem programming comp mathematic mathematical physics volume page t m cover element information theory series modelbase approximate query sensor network international large datum basis dhillon r informationtheoretic feature cluster algorithm text classification machine learn research volume page r o p e pattern e automate software support classify program behavior technical report tr lab datum t divergence base clustering normal distribution application adaptation page singer m batch online parameter estimation base joint entropy neural information processing system t speaker speech synthesis european conference speech communication technology m statistical debugging sample program information processing system
segmentation conditional random field learn partially label image conditional random field crf effective tool variety different datum segmentation labeling task include visual scene interpretation seek partition image constituent region assign appropriate class label region accurate labeling important capture global context image local information introduce crf base scene labeling model incorporate local feature feature aggregate image large section secondly traditional crf learning require fully label dataset costly produce introduce method learn crf dataset unlabeled node marginalize unknown label loglikelihood know maximize belief propagation use approximate marginal need gradient loglikelihood calculation bethe approximation loglikelihood monitor control step size experimental result effective model learn labeling incorporate topdown aggregate feature significantly improve segmentation result segmentation compare stateoftheart different image dataset introduction visual scene interpretation goal assign image pixel semantic class scene element jointly perform segmentation recognition useful variety application range image retrieval use segmentation automatically index image autonomous random field approach popular way model spatial regularity image application range lowlevel noise reduction highlevel object category recognition paper object segmentation early work focus generative modeling use random field recently conditional random field model popular owe ability directly predict observe image ease arbitrary function observe feature incorporate training process crf model apply pixellevel coarser level patch paper label image level small patch use crf model incorporate purely local single patch feature function global context capture feature function depend aggregate observation image large region traditional crf training algorithm require training datum practice difficult label pixel image available image interpretation dataset contain unlabeled pixel work patch level problem patch contain different pixellevel label crf training algorithm handle allow partial mixed labeling optimize probability model segmentation consistent labeling constraint rest paper organize follow describe crf model section present train algorithm section provide experimental result section conclude section conditional random field use local global image feature represent image rectangular grid patch single scale associate hide class label patch crf model incorporate neighbor coupling patch label local image content patch encode use texture color position descriptor texture compute dimensional sift descriptor patch vector quantize word learn kmean clustering patch training dataset similarly color descriptor vector quantize word color dictionary learn training set position encode image m m grid cell m use index cell patch fall position feature patch code binary vector respectively kp m bit single bit set correspond observe visual word crf observation function simple linear function vector generatively modality model independent naive baye model image omit neighbor coupling assume depend observation function parameter estimation reduce trivially count observe visual word frequency label class feature type msrc class image dataset model return average classification rate section isolate appearance suffice reliable patch labeling recent year model base histogram visual word prove successful image categorization decide image belong category scene motivate model global image context account include observation function base imagewide histogram visual word patch hope help overcome ambiguity arise patch classify isolation end define conditional model patch label incorporate local patch level feature global aggregate feature let denote label patch denote dimensional concatenate binary indicator vector visual word p hh kp h denote histogram visual word image patch normalize sum conditional model p h c matrix coefficient learn think multiplicative combination local classifier base patchlevel observation global context bias base imagewide account correlation spatially neighbor patch label add coupling label neighboring patch single patch model let denote collection patch label image denote collect patch feature crf model couple patch label ij xj denote set adjacent neighbor pair patch write explicitly include h argument deterministic function explore form pairwise potential xj dij xi argument true dij similarity measure appearance patch form general symmetric weight matrix need learn second potential design favor label transition image location high contrast use dij zi ir denote value patch average l norm value image model use form potential denote crf use second denote crf crf fix graphical representation model figure figure graphical representation model single image wide aggregate feature function denote square denote feature function circle denote variable node connect neighbor grid cover image arrow denote single node potential feature function undirected edge represent pairwise potential dash line indicate aggregation single patch observation h h x x estimate conditional random field partially label image conditional model usually train maximize log likelihood correct training datum log pxn require completely label training datum collection pair completely know xn practice restrictive useful develop method learn partially label example image include completely unlabeled patch nontrivial set possible label formally assume incomplete labeling x know belong associate set admissible labeling maximise log likelihood model predict label l log px log log note log likelihood difference partition function restricted unrestricted labeling px pxy completely label training image reduce trivially standard label log likelihood partially label term log likelihood typically intractable set contain distinct labeling number unlabeled patch c number possible label similarly find maximum likelihood parameter estimate use gradient descent need calculate partial derivative respect parameter general term intractable l px situation actually bad p fully label case case need approximate partition function log p derivative method apply restrict sum contrast partition function base approximation use bethe energy approximation partition function l px bethe approximation variational method base approximate complete distribution pxy product pair marginal normalize single node marginal apply graph tree necessary marginal approximate use loopy belief propagation log likelihood gradient evaluate use run twice singleton marginal initialize single node potential estimate marginal pxy px use standard uniform initial message encounter convergence problem approximate gradient objective consistent allow parameter estimation use standard optimization adaptive step length base monitor comparison unlabeled node training procedure require run simple use alternative discard unlabeled patch node class frequency model build tree cow plane face car pixel table classification accuracy msrc class use different model class frequency ground truth labeling correspond unlabeled partially label patch graph leave random field completely label connected component loglikelihood px maximize directly use gradient base method equivalently use complete model set pairwise potential connect unlabeled node decouple label unlabeled node rest field result pxy px equivalent unlabeled node contribution loglikelihood gradient vanishe problem approach systematically overestimate spatial coupling strength look training labeling figure figure pixel class boundary remain unlabeled leave patch unlabeled contain unlabeled label transition training datum cause strength pairwise coupling greatly overestimate contrast crf model provide realistic estimate force include fully couple label transition unlabeled region experimental result section analyze performance segmentation model detail compare exist method set experiment use research cambridge dataset consist image pixel partial pixellevel labeling labeling assign pixel class build tree cow plane face car pixel unlabeled sample image labeling figure experiment divide dataset image training test report average result random partition use pixel patch center pixel interval patch size red disc obtain labeling patch pixel assign near patch center patch allow label pixel unlabeled pixel allow label learning inference place patch level map patchlevel segmentation pixel level assign pixel marginal patch near center figure segmentation apply gaussian filter marginal scale set patch space performance metric ignore unlabeled test pixel relative contribution different component model summarize table model incorporate neighbor spatial coupling denote crf incorporate local global patchlevel potential denote model include global aggregate feature denote locglo include local patchlevel feature denote available figure classification accuracy function aggregation c individual patch classifier use single training test set aggregate feature compute cell c c image partition result model solid line single dotted curve grid solid curve grid c dash curve accuracy c c c local benefit aggregate feature main conclusion include global aggregate feature help example improve average classification rate msrc dataset spatially model idea aggregation generalize scale small complete image experiment divide image c c grid range value c cell grid compute separate histogram visual word patch cell include energy term base histogram way imagewide histogram figure performance individual patch classifier depend use aggregate feature dotted curve figure use large cell aggregate feature generally informative fine contain patch cell provide significant performance increase furthermore include aggregate compute different scale help performance increment small compare gain obtain imagewide aggregate include imagewide aggregate subsequent experiment benefit include spatial couple second main conclusion table include spatial coupling pairwise crf potential help respectively increase accuracy locglo crf relative improvement particularly rare class global aggregate feature include case single node potential informative class tend favor large priori probability imagewide aggregate feature include locglo simple pairwise potential work general model crf local feature include pairwise potential work performance increment global feature small crf model include local contextual information overall influence local label transition preference express crf appear similar global contextual information provide imagewide aggregate feature benefit training marginalize partial labeling main conclusion table marginalization base training method handle miss label superior common heuristic delete unlabeled patch learn crf locglo model remove patch unlabeled table lead estimate maximum estimate lead particular delete unlabeled training accuracy model drop significantly class plane relatively small area relative boundary partially label patch interesting note severely overestimate model crf improve individual patch classification obtain locglo class plane recognition function labeling consider performance drop fraction label pixel decrease apply operator manual annotation vary size element accuracy disc disc disc locglo percentage pixel label figure recognition performance learn increasingly label image leave example image original annotation thereof disk size right way obtain series annotation resemble increasingly manual annotation figure figure recognition performance crf locglo function fraction label pixel addition superior performance train label image crf maintain performance labelling note crf locglo learn label image disc radius pixel label outperform locglo learn original labeling pixel label crf actually perform pixel original labeling presumably ambiguity relate training patch mixed pixel label reduce comparison relate work table compare recognition result dataset report crf model clearly outperform approach use aggregate feature optimize scale lack spatial coupling random field performance similar locglo model crf model perform slightly generative approach base feature set differ implementation imagewide contextual information use partition use dataset subset corel dataset compare model crf model operate pixellevel dataset consist image pixel scene label different class vegetation road mark surface building object car subset corel dataset contain image pixel natural scene label class polar bear water vegetation ground use pixel patch respectively pixel corel dataset parameter table compare recognition accuracy average pixel crf independent patch model result report dataset model table stand result obtain single node potential use respective model spatial random field coupling total training time test time image list crf model result dataset model perform comparably pixellevel approach fast train test operate patchlevel use standard feature oppose boost procedure conclusion present crf model semantic image labeling incorporate local patchlevel observation global contextual feature base aggregate observation scale partially label training image handle maximize total likelihood image segmentation partial labeling use loopy approximation calculation allow learn effective crf model image small fraction pixel label class transition observe experiment msrc include image gibb min s gibb test h gibb min s gibb table recognition accuracy speed corel dataset wide aggregate feature helpful include additional aggregate fine scale relatively little improvement comparative experiment patchlevel comparable performance stateoftheart pixellevel model efficient number patch small number pixel reference p base environment terrain type classification proceeding ieee conference intelligent system page geman geman stochastic relaxation gibb distribution bayesian restoration image ieee transaction pattern analysis machine intelligence blake interactive extraction use iterate graph cut acm transaction graphic lafferty mccallum pereira conditional random field probabilistic model labeling sequence datum proceeding international conference machine learn volume page r conditional random field image labelling proceeding ieee conference computer vision pattern recognition page m hierarchical field framework unified classification proceeding ieee international conference computer vision page joint appearance shape context model multiclass object recognition segmentation proceeding computer vision page m collin darrell conditional random field object recognition advance neural information processing system volume page p schmid freitas semisupervised learning approach object recognition spatial integration local feature segmentation cue object recognition page region classification field aspect model proceeding ieee conference computer vision pattern distinctive image feature scaleinvariant keypoint international vision c schmid color local feature extraction proceeding conference computer vision page pascal visual object class challenge editor machine learning challenge evaluate predictive uncertainty visual object classification recognize textual pascal machine learning challenge workshop weiss understanding belief propagation generalization report laboratory zisserman class model image segmentation proceeding indian conference computer vision graphic image processing labeling label labeling figure sample msrc corel dataset segmentation labeling
reinforcement learning use approximate belief state andre artificial computer abstract problem develop good policy partially observable markov decision problem pomdps remain challenging area research stochastic planning line research area involve use reinforcement learning belief state probability distribution underlie model state promising method small problem application limit compute represent belief state large problem recent work setting maintain approximate belief state fairly close true belief state particular great success approximate belief state marginalize correlation state variable paper investigate method belief state reinforcement learning novel method reinforcement learning use factor approximate belief state compare performance algorithm wellknown problem literature result demonstrate importance approximate belief state representation large problem introduction markov decision process good way mathematically formalize large class sequential decision problem involve agent interact environment generally mdp define way agent complete knowledge underlie state environment formulation pose challenging research problem optimistic modeling assumption rarely realize real world time agent face uncertainty information available extension formalism generalize mdps deal uncertainty partially observable markov decision process pomdps focus paper solve pomdp mean find optimal behavior policy l map agent available knowledge environment belief state action usually function assign value belief state fully observable work present paper author reinforcement learning use approximate belief state case value function compute efficiently reasonably sized domain situation somewhat different pomdps find optimal policy number underlie state date know exact algorithm solve pomdps problem state general approach approximate pomdp value function use reinforcement learning method space permit review approach focus use belief state probability distribution underlie model state contrast method manipulate augment state description memory method work directly observation main advantage probability distribution summarize information necessary optimal decision main disadvantage model require compute belief state task represent update belief state large problem difficult paper address problem obtain model focus effective way use model know model reinforcement learning technique competitive exact method solve pomdps focus extend modelbased reinforcement learning approach large problem use approximate belief state risk approach inaccuracy introduce belief state approximation agent inaccurate perception relationship recent work present approximate tracking approach provide theoretical guarantee result process far exact belief state approach maintain exact belief state infeasible large problem maintain approximate belief state usually restricted class distribution approximate belief state update action observation continuously project restrict class specifically use decompose belief state certain correlation variable ignore paper present empirical result compare approach belief state reinforcement learn direct approach use neural network input element belief state second method use function approximator design pomdps use neural network approximate belief state input present result wellknown problem pomdp literature belief state approximation problem effective mean attack large problem basic framework algorithm pomdp define tuple s t r set function set state set action set observation transition function ii s action affect state world view si s p probability agent reach state j currently state si action reward function r s r determine immediate reward receive agent observation model determine agent perceive depend environment state action p probability agent observe s action rodriguez r pomdp belief state b define probability distribution state e represent probability environment state action observe belief state update use baye rule b s p s b size exact belief state equal number state model large problem maintain manipulate exact belief state problematic transition model compact representation example suppose state space describe set random variable xn xi value finite domain particular define value e variable xi belief state representation exponential n use approximation method analyze variable partition set disjoint cluster ck belief function bk maintain variable cluster time step compute exact belief state compute individual belief function marginalize correlation assignment ci variable c obtain approximation original belief state reconstruct nl bi ci represent belief state product marginal probability project belief state reduce space belief state representation state variable exponential size decomposed belief state representation exponential size large cluster additive number cluster process mix rapidly error introduce approximation stay bounded time discuss type decompose belief state particularly suitable process factor represent dynamic bayesian network case avoid represent exponentially sized belief state approach fully general apply set state define assignment value set state variable value function policy pomdps think define belief state wellknown fix point equation hold specifically vb l l oed discount factor define belief state optimal policy determine maximize action belief state principle use qiearne value iteration directly solve main difficulty lie fact belief state representation value function impossible exact method use fact finite horizon value function convex ensure finite representation representation grow exponentially horizon exact approach impractical setting function approximation attractive alternative exact method implement function approximation use set parameterized qfunction action belief state b value function reconstruct qfunction vb update rule wa transition reinforcement learning use approximate belief state state b b action reward r function approximation architecture consider type function approximator twolayer sigmoidal internal unit linear layer use network q function belief state reinforcement learning use network hide node lsi input component belief state approximate belief state reinforcement learning use network input assignment variable cluster cluster example binary variable q network input number hide node network square root number input second function approximator soft max function design exploit structure pomdp value function maintain set weight vector evaluate practice small value usually adopt start learn function smooth increase learning closely approximate function usually maintain q function vector function parameter action assign belief state neural network empirical result present result problem pomdp literature present extension know machine problem design highlight effect approximate belief state result present form performance graph value current policy obtain snapshot value function measure discount sum reward obtain result policy simulation use refer neural network train reinforcement learner train belief state term decompose refer neural network train approximate belief decompose product marginal use simple exploration strategy start probability act randomly decrease linearly space limitation able describe model detail use publicly available model description file table running time different method generally low require solve problem use exact method grid world begin consider grid world world state world world contain state natural decomposition state variable compare belief state note file specify starting distribution problem result report respect starting distribution rodriguez r o c figure grid world b state maze experimental result average training run simulation policy snapshot present figure learn fast network network eventually state robot navigation problem amenable decomposed belief state approximation underlie state space come product robot position robot orientation decompose belief state cluster contain position state variable contain orientation state variable figure b result dominate decomposed problem effect position orientation value function easily decouple ie effect orientation value highly mean decomposed nn force learn complicated function input function learn network use belief state aircraft identification aircraft identification problem study thesis include sense action identify incoming aircraft action attack aircraft attack aircraft penalize failure intercept aircraft challenging problem decide sensor sensor tend base visible aircraft sensor accurate sensor information aircraft type distance base state space problem comprise main component aircraft type aircraft distance far aircraft currently base discretize number distinct distance measure visible base approach aircraft discretize level choose problem state problem natural decomposition state variable aircraft type distance base result algorithm figure problem start advantage decompose belief state decomposed use separate cluster variable mean network input simple network learn fast learn policy overall believe illustrate important point belief network expressive decomposed decomposed able search space function represent efficiently reduce number parameter reinforcement learning use approximate figure aircraft identification b machine maintenance machine maintenance problem machine maintenance problem database problem assume machine certain number component quality produce machine determine condition component component condition good component good condition fair component benefit maintenance bad use break break replace status component observable machine completely figure b performance result problem component version problem state maximum size belief state approach belief state problem decompose naturally cluster describe status machine create decomposed belief state component graph dominance simple decomposition approach believe problem clearly demonstrate advantage belief state decomposition decomposed learn function input fraction time net learn function input run time table running time different problem present generally require solve problem exactly roughly comparable decompose neural network considerably fast exploit problem structure approximate belief state computation time spend compute belief state actually large decomposed saving come reduction number parameter use reduce number partial derivative compute expect saving significantly substantial process represent way approximate belief state propagation advantage additional structure conclude remark propose new approach belief state reinforcement learning use approximate belief state use wellknown example pomdp literature compare approximate belief state reinforcement learning method rodriguez r problem aircraft min min min min decompose min min min table run time second minute hour different use exact belief state result demonstrate approximate belief state ideal tightly couple problem feature position orientation robot natural effective mean address large problem problem approximate belief state reinforcement learning outperform belief state reinforcement learning use trial cpu time problem exact belief state method simply impractical approximate belief state provide tractable alternative acknowledgement work support aro program integrate approach intelligent system onr contract darpa program foundation sloan foundation reference optimal control markov decision process incomplete math anal press s decision theoretic planning structural assumption computational leverage journal research tractable complex stochastic process exact approximate algorithm partially observable markov decision problem phd thesis computer science dept brown m littman algorithm sequential decision phd thesis dept brown m littman lp learn policy partially observable environment scale proc icml page use eligibility trace find good memory policy partially observable markov decision process proc icml r mccallum overcome incomplete perception distinction memory page approximate optimal policy partially observable stochastic domain r e optimal control partially observable markov process finite horizon operation research m schmidhuber discover markovian learn
method deep learning computer science drive code abstract introduce new family positivedefinite function mimic computation large multilayer neural net kernel function use shallow architecture support vector machine svms deep kernelbase architecture evaluate svms mkms kernel function problem design illustrate advantage deep architecture problem obtain result previous lead benchmark svms deep belief net introduction recent work machine learning highlight circumstance appear favor deep architecture multilayer neural net shallow architecture support machine svms deep architecture learn complex mapping transform input multiple layer nonlinear processing researcher advance motivation deep architecture wide range function parameterize compose weakly nonlinear transformation appeal hierarchical distribute representation potential combine unsupervised supervised method experiment benefit deep learning interesting application issue surround ongoing deep shallow architecture deep architecture generally difficult train shallow involve difficult nonlinear optimization heuristic challenge deep learning explain early continue appeal svms learn nonlinear classifier trick deep architecture svms train solve simple problem quadratic programming svms seemingly benefit advantage deep learning success deep architecture draw kernel method paper explore possibility deep learning machine share similar motivation previous author approach different paper main contribution develop new family function mimic computation large neural net second use kernel function train mkms benefit advantage deep learning organization paper follow section describe new family function experiment use svms result svms interesting right certain trend observe certain choice mkms introduce section section describe kernelbase architecture multiple layer nonlinear transformation different layer train use simple combination supervise unsupervised method finally conclude section evaluate strength weakness approach arccosine kernel section develop new family function compute similarity input d shorthand let z step function define nth order function integral representation e integral representation straightforward kernel function function eq interesting connection neural computation explore far section begin basic property basic property evaluate integral analytically appendix final result easily express term angle input integral simple trivial dependence magnitude input complex interesting dependence angle particular write angular dependence capture family function evaluate integral appendix angular dependence sin sin expression reduce supplement angle input angular dependence complicated expression sin sin describe eq arccosine kernel simple form fact zeroth order kernel family strongly motivate previous work neural computation explore connection section arccosine intriguing property magnitude dependence observe follow map input unit feature space preserve norm input kxk iii high order expand dynamic range input x property share respectively radial basis function rbf linear polynomial kernel interestingly highly nonlinear satisfy input practical matter note arccosine kernel continuous tuning parameter width kernel set crossvalidation computation singlelayer threshold network consider singlelayer network fig leave weight wij connect input unit ith output unit network map input output f apply elementwise nonlinearity matrixvector product input weight matrix nonlinearity describe network socalle activation function consider family polynomial activation function n illustrate right panel fig step ramp figure single layer network activation function activation function step function network array perceptron activation function ramp function nonlinearity mapping piecewise linear generally nonlinear behavior network induce thresholde weight sum refer network activation function singlelayer threshold network degree computation network closely connect computation function connection consider inner product transform mapping singlelayer threshold network notation let vector row weight matrix express inner product different output network m m number output unit connection function emerge limit large network imagine network infinite number output unit weight distribute mean unit variance limit reduce eq trivial multiplicative factor m function eq view inner product feature vector derive mapping infinite singlelayer threshold network researcher note general connection machine neural network layer hide unit derive early result obtain context gaussian process previous theoretical empirical work general family kernel degree arccosine kernel differ polynomial rbf kernel especially interesting respect highlight integral representation arccosine kernel induce feature space mimic sparse nonnegative distribute representation singlelayer threshold network kernel encode input way particular feature vector induce polynomial kernel sparse nonnegative feature vector induce resemble localized output soft vector implication difference explore section computation multilayer threshold network kernel function view induce nonlinear mapping input feature vector kernel compute inner product induce feature section consider compose nonlinear mapping induce function specifically derive new function z time time compute inner product successive application nonlinear mapping motivation follow intuitively base kernel function mimic computation singlelayer network iterate mapping eq mimic computation multilayer network test set svms cosine error rate result s kernel vary degree n level recursion good previous result svms kernel deep belief net text detail test error rate test error rate step figure leave example datum set right classification error rate svms cosine error rate result vary degree level recursion good previous result step net text detail deep belief figure leave example datum set right classification error rate test set svms arccosine kernel error rate result train validation set margin penalty parameter vary degree n level example result crossvalidation retrain svm use training svms kernel deep net text detail reference report good result obtain previously layer deep belief svms kernel reference representative deep shallow architecture datum set examine result procedure widely use kernel find iterate right panel figure test set error rate o mapping yield particularly interesting result consider recursion experiment map level degree correspond single layer threshold network identity map homogeneous ramp function experiment describe section c composition yield level recursion overall figure datum different previously report svms good result kernel deep belief net detail experiment h note arc cosine straightforward train result especially interesting imply require tune high degree d construct parameter deep b require solve difficult nonlinear optimization search possible rbf e composition yield experiment quickly discover multilayer kernel perform kernel use high level recursion fig s set result particular group bar test error rate represent particularly recall degree n interesting use layer successive layer formal mimic computation effect r n arc norm input large compare kernel width hard iterate unit space higherorder kernel generate qualitatively different representation original mapping space severely distort dynamic range hypothesize consider fold composition eq state input work preserve information result form recursion case kernel depth degree inductive step finally result data set reveal interesting trend layer counterpart svms shallow perform angle image feature space induce composition particular write h n x recursion eq simple compute practice result kernel mimic computation large multilayer threshold network simplicity assume degree level layer recursion use kernel different degree different layer section experiment svms kernel function construct way experiment binary classification evaluate svms arccosine kernel challenge datum set image datum set specifically construct compare deep architecture machine data set know image contain occlude rectangle task determine width rectangle exceed height example fig leave second data set know image contain white region task determine white region example test set svms cosine error rate result s kernel vary degree n level recursion good previous result svms kernel deep belief net text detail test error rate test error rate step dbn figure leave example datum set right classification error rate th cosine error rate result vary degree level recursion good previous result step net text detail deep belief figure leave example datum set right classification error rate test set svms arccosine kernel error rate result kernel example validation choose margin penalty parameter vary degree n level recursion training good previous set svms crossvalidation deep belief text detail retrain svm use training reference report good result obtain previously layer deep belief svms kernel reference representative deep shallow architecture datum set fig leave datum set training example figure test set rate o set training example example level recursion binary experiment kernel degree extensively author experiment classification correspond benchmark single layer threshold step ramp focus datum set previously report exhibit experiment describe section c performance gap deep deep net traditional level recursion overall figure datum different outperform good result previously report svms follow experimental methodology previous svms train use deep belief net version publicly available use experiment h svms straightforward train train example validation set choose penalty choose require parameter deep parameter crossvalidation retrain svm train example require solve difficult reference report good result obtain previously deep belief search possible dbn svms reference appear representative kernel perform experiment quickly discover architecture kernel use high level recursion fig s current stateoftheart deep shallow set set result particular group bar test error rate figure test set error rate degree vary nonlinearity recursion experiment successive degree correspond effect r use layer old network step ramp function activation preserve norm input map multilayer kernel describe section level kernel distort dynamic range hypothesize overall figure outperform traditional preserve sufficient certain number outperform deep belief net addition solid performance input work note svms arccosine straightforward train svms kernel require tune kernel deep interesting perform possible single layer counterpart svms shallow require solve difficult nonlinear optimization search architecture experiment multilayer kernel reveal svms perform cosine degree use high level recursion fig set result particular group bar test error rate particular kernel degree use layer nonlinearity kernel use successive layer hypothesize preserve sufficient information magnitude input work effectively composition kernel recall preserve norm input kernel map input unit feature space higherorder kernel induce feature space different dynamic range finally result data set reveal interesting trend perform singlelayer counterpart svms inherently shallow architecture trend suggest problem binary classification arccosine yield advantage typically associate deep architecture deep learning section explore use kernel method deep architecture train deep kernelbase architecture simple combination supervised unsupervised method use arccosine kernel previous section perform multiclass datum set design shallow architecture explore train mkms stage involve kernel pca feature selection intermediate hide layer near neighbor classification final output layer specifically layer mkms consider follow training procedure prune uninformative feature input space repeat time compute principal component feature space induce nonlinear b prune uninformative component feature space learn mahalanobis distance metric near neighbor individual step procedure method combination new approach worth investigate positive result procedure provide discuss step great detail pca deep learning mkms achieve iterative application pca use suggest decade recently inspire deep belief net unsupervised method mkms output feature pca layer input layer strictly transmit layer principal component layer component discard deem uninformative nonlinear kernel use pca natural choice mimic computation large neural net feature selection layer mkms train interleave supervised method selection unsupervised method pca feature selection use prune away uninformative feature layer include zeroth layer store raw input intuitively feature selection help focus unsupervised learning mkms statistic input actually contain information class label prune feature layer simple twostep procedure rank estimate mutual information truncate use crossvalidation specifically step discretize realvalue feature construct classconditional marginal histogram discretized value use histogram estimate feature mutual information class label sort feature order estimate second step consider feature ordering compute error rate basic knn classifier use euclidean distance feature space compute error rate heldout set validation example value record optimal value layer optimal determine number informative feature pass layer essentially width layer practice vary exhaustive crossvalidation quickly efficiently careful note procedure determine architecture network greedy fashion distance metric learning test example mkms classify variant knn classification output final layer specifically use large margin near neighbor lmnn classification learn mahalanobis distance metric output method equally use lmnn inspire supervised weight training deep architecture mkms supervise training occur final layer importance feature selection early layer learn distance metric solve problem program advantage optimization convex test example classify decision rule inspire early work multilayer neural net experiment classification evaluate mkms multiclass datum set previous benchmark exhibit large performance gap deep shallow architecture datum set create mnist datum set grayscale handwritten digit datum set generate fill image background random pixel value datum set generate fill image background random image patch example fig datum set contain training test example respectively test set svms cosine error rate vary degree n level recursion good previous result svms kernel deep belief net text detail test error rate error test error rate step dbn right rate figure leave example svms cosine error rate result leave classification error example figure vary degree level previous result set svms error rate result step deep belief net text detail vary degree n level recursion good previous result deep belief figure leave example net text detail test set mkms different kernel number layer mkms train validation choose margin penalty parameter error rate good previous test error rate parameter crossvalidation retrain svm use deep belief net reference report good result obtain previously layer deep svms reference stateoftheart shallow architecture datum set rate rate deep test set error rate right panel figure level recursion experiment kernel degree network step ramp correspond single layer threshold step function experiment describe section figure leave example datum set level outperform error rate result different good result previously report degree deep belief net vary level good previous result detail experiment rbf step net text detail deep belief note svms straightforward set require tune figure leave example right parameter d solve difficult nonlinear optimization search test set mkms different kernel number require training example validation set choose margin error rate good previous result rbf experiment quickly discover train parameter use retrain level recursion deep belief net good result obtain layer deep belief result particular bar svms reference representative degree kernel use layer nonlinearity stateoftheart deep shallow architecture datum effect use successive layer formal explanation train mkms panel preserve set error rate initially training example validation set performance validation degree level experiment degree set use determine mkms describe previous section severely hypothesize correspond single layer network set width follow preserve methodology early sufficient input function experiment describe section parameter set crossvalidation datum level recursion example overall figure different good result previously report training set use training example learn result net datum set interesting trend experiment limit memory requirement process perform layer counterpart note training example choose example repeat straightforward train require tune parameter deep time obtain measure average performance report require solve difficult nonlinear optimization search possible average performance run experiment quickly discover multilayer right panel fig rate use different kernel high level recursion fig number layer reference previously result use result particular bar test error rate traditional svms perform nonlinearity n degree layer use layer successive layer explanation effect shallow architecture svms belief preserve norm input report case compare obtain unit space higherorder kernel slightly low error rate datum set slightly error space severely distort dynamic range hypothesize describe architecture number feature layer input work e input layer number feature essentially correspond number unit layer neural net good use finally interesting trend feature layer set svms shallow single layer datum counterpart good use feature layer mkms work arccosine kernel degree kernel degree perform mkms multiple iteration distort dynamic range input turn complicate training kernel difficult train sensitive dependence width parameter extremely kernel width layer obtain meaningful result twolayer mkms briefly summarize result lack space report experiment multiclass datum set use svms single multilayer arccosine describe section multiclass problem svms compare poorly deep architecture mkms presumably unsupervised training share information example different class experiment mkms attempt evaluate individual contribution performance feature selection lmnn classification feature selection help significantly datum set slightly datum set finally lmnn classification output layer yield consistent improvement basic knn classification provide use decision rule discussion paper develop new family function mimic computation large multilayer neural net challenge datum set obtain result outperform previous svms compare favorably deep belief net significantly experiment validate basic intuition deep learning altogether different context kernelbase architecture similar validation provide recent work kernel method semisupervised embed hope result inspire work kernel method deep learning possible direction future work svms currently experiment function fractional negative degree mkms hope explore scheme feature selection selection desirable incorporate prior knowledge invariance model convolutional net obvious issue leave future work derivation function appendix evaluate multidimensional integral eq let denote angle input loss generality x lie axis lie plane integrate orthogonal coordinate weight vector obtain result eq remain integral z e sin wn sin change variable u w w sin simplify domain integration sin sin sin eq jacobian simplify integral far adopt polar coordinate r r sin integrate radius coordinate r obtain evaluate consider special case following result derive contour integration complex plane sin substitute expression angular kernel function eq recover early claim relate integral special case find early work case integral perform method differentiate integral sign particular note substitute eq appeal previous result eq recover expression eq reference bengio lecun scaling learn algorithm teh fast learning algorithm deep belief net neural computation r salakhutdinov reduce dimensionality datum neural network science unsupervised learning invariant feature hierarchy application object recognition proceeding ieee conference computer vision pattern recognition cvpr page r collobert unified architecture natural language process deep neural network multitask learn proceeding th international conference machine learn icml page bengio learn deep architecture foundation trend machine learn appear r collobert deep learning semisupervise embed proceeding th international conference machine learning icml page computation infinite neural network neural computation permit set symmetric network neural computation neal bayesian learn neural larochelle bengio empirical evaluation deep architecture problem factor variation proceeding th machine learning icml page library support vector machine software available b scholkopf nonlinear component analysis kernel eigenvalue problem neural introduction variable feature selection journal machine learn weinberger lk metric learning large margin near neighbor machine learn research b scholkopf muller nonlinear component analysis problem technical report r salakhutdinov component analysis l bottou editor advance neural information processing system page mit bengio larochelle greedy training deep network hoffman editor advance neural information processing system page mit press s r learn similarity metric discriminatively application face verification proceeding ieee conference computer vision pattern recognition cvpr page lecun c mnist database handwritten digit m tip sparse principal component analysis advance neural information process system mit scholkopf sparse feature analysis cristianini p learn matrix programming machine learn research lecun ld backpropagation apply handwritten code recognition neural computation gf carrier m pearson function complex variable theory technique society industrial mathematic
exploit easy datum online sequel team abstract consider problem online optimization learner choose decision decision set suffer loss associate decision state environment learner objective minimize cumulative regret good xe decision hindsight past decade numerous variant consider algorithm design achieve sublinear regret bad case level come cost propose algorithm fail adapt actual complexity loss sequence far bad case paper introduce general algorithm provide safe learn benchmark effectively combine good worstcase guarantee improve performance easy datum derive general theoretical bound regret propose algorithm discuss implementation wide range application notably problem learn shift expert recent colt open problem finally provide numerical simulation setting prediction expert advice comparison state art introduction consider general class online decisionmake problem learner sequentially decide action decision set suffer loss associate decision state environment learner goal minimize cumulative loss interaction learner environment repeat performance usually measure regard regret difference cumulative loss good single decision horizon decision set objective learning guarantee regret converge time progress general set include wide range application online linear pattern sequential time numerous variant problem consider decade mainly differ shape decision set overview popular variant problem prediction expert advice decision set n dimensional simplex loss linear function learner decision set number algorithm know guarantee regret order t t repetition game set online convex optimization decision set subset rd loss function smooth number simple algorithm know guarantee worstcase regret order t set result hold possibly adversarial assignment loss sequence algorithm guarantee achieve decrease regret approach performance good xe decision hindsight bad case furthermore guarantee sense exist sequence loss function learner suffer t regret matter algorithm learner use robustness come cost algorithm fail adapt actual complexity loss sequence practice far bad possible fact know assumption loss generating mechanism improve regret guarantee instance simple strategy follow know play game theory chapter round pick single decision minimize total loss far guarantee olog regret expert set assume loss vector strategy guarantee regret setting assume loss function strongly convex hand risk use strategy know suffer t regret bad case paper focus distinguish easy hard problem instance achieve good possible guarantee type loss sequence problem recently receive attention variety setting propose solution require development specic scenario easy problem obvious solution theoretical analysis complicated difcult generalize complex problem current paper set dene algorithm provide general structure instantiate wide range setting simply plug appropriate choice algorithm learn easy hard problem exploit easy datum method potential application example sensitive application want complete risk high fact work build directly result evendar point learn algorithm expert set fail satisfy natural requirement perform strictly trivial algorithm merely decide expert follow uniform coin ip evendar method achieve goal leave open obvious open question possible strictly improve performance exist possibly solution mean principle online learning method problem polar opposite fail exploit easy datum paper push idea step far construct learn algorithm regret bound guarantee cumulative loss constant factor strategy refer benchmark stress property strong simply guarantee regret respect xe distribution d evendar allow comparison xe strategy allow learn method guarantee replace exist solution negligible price term output performance additional strong guarantee worstcase performance follow regard aspect result interesting consequence emphasize ability algorithm exploit easy datum general structure refer rod receive learn benchmark b input depend online optimization set set learning algorithm performance guarantee hard problem b strategy exploit structure easy problem rod smoothly mix decision b achieve good possible guarantee online optimization benchmark parameter set decision number round t t repeat environment choose loss function learner choose decision environment reveal possibly choose depend past history loss decision forecaster suffer loss figure protocol online optimization present formal setting algorithm online optimization benchmark interaction protocol learner environment formally describe figure online optimization problem characterize decision set class loss function utilize environment performance learner usually measure term regret dene algorithm learn decision let online optimization algorithm map observation history decision possibly randomize fashion formal x time index t dene observation history short history end round t h dene set furthermore dene random variable draw standard uniform distribution independently learn algorithm b formally dene mapping respective decision finally dene hedge strategy c produce decision base history decision propose b possible help external randomness represent uniform b h t history consist b ft c base decision past loss incur b use information loss function total t c et ft expectation integrate expect loss c dene possible realization internal randomization b c total expect loss b xe decision s similarly dene goal dene hedge strategy low regret benchmark strategy b enjoy nearoptimal guarantee worstcase regret good decision hindsight expect regret c xe decision s benchmark dene e e t strategy rod base input learning rate initial classic p rod weight wa num round build variant p rod t repeat p rod propose evendar et let properly tune achieve constant regret expert guarantee o t log t regret good ex observe predict hindsight variant rod probability figure base observation use xe distribution d benchmark actually learn observe suffer loss use baseline rod maintain feed ft weight balance advice learn benchmark b benchmark weight set ne unchanged t entire learning process initial weight assign remain round figure t update bs wa difference loss b use output set probability set follow state performance guarantee assignment loss sequence total expect loss rod initialize weight wa simultaneously log wa t rod rod l t log decision set s loss family directly set expense proof directly follow p rod analysis suggest parameter set rod guarantee constant regret benchmark b t log t regret learning bad case t b set corollary let c upper bind total benchmark loss l log wa simultaneously guarantee rod c log c s assignment loss sequence notice s previous bound write min c log c log state rod achieve minimum regret benchmark b online learn additional regret c log c consider t previous bind optimization setting worstcase regret learn cost additional factor o log t bad case rod perform benchmark useful small suggest set learning algorithm worstcase guarantee difcult problem algorithm good performance easy problem successfully adapt problem nde suitable mixture furthermore discuss evendar note case p rod update rule crucial achieve result algorithm base decision solely cumulative difference bind suffer additional regret t b edge fall category easily case p rod similar observation discuss possibility combine robust learning ftl h edge conclude approach goal sect finally note parameter propose corollary compute practice t b rarely available fortunately upperbound loss benchmark adapt improve version p rod adaptive learning rate recently propose obtain anytime version rod result algorithm correspond bound report application follow section apply result special case online optimization note theorem direct consequence corollary proof omit prediction expert advice rst consider basic online optimization problem prediction expert advice dimensional simplex r loss function linear loss decision round t inner product t loss vector round t accordingly family loss function equivalently set algorithm know achieve optimal regret guarantee log n setting include edge freund schapire work littlestone warmuth prediction method later note algorithm usually conservative exploit easily learnable loss sequence outperform simple strategy know ftl predict arg instance ftl know optimal case loss achieve regret direct consequence corollary use general structure match performance ftl easy datum time obtain worstcase guarantee standard algorithm prediction expert advice particular set ftl benchmark da h edge learning obtain follow theorem let run rod h edge ftl parameter setting suggest corollary simultaneously guarantee log rod da h edge c log c s x assignment loss sequence recover worstcase guarantee log additional regret o log t hard loss sequence easy problem inherit good performance comparison lip lop lip propose address problem construct algorithm perform nearly ftl easy problem retain optimal guarantee possible loss sequence precisely f lip lop h edge learning rate alternate correspond ftl value suggest da h edge depend cumulative gap regime result guarantee achieve regret guarantee lop ftl x t log olog time notice guarantee similar nature concern lip lop result slightly different rst difference worstcase bound inferior factor order log t positive guarantee strong outperform h edge observe regret bind rewrite lop inf x result replace term log advantage result directly bind total loss algorithm term total loss da h edge contrast result upper bind regret term regret bind da h edge tight bad practice actual performance da h edge advantage approach stem fact smoothly mix prediction h edge ftl lip lop explicitly follow policy extended period time potentially accumulate unnecessary loss switch late early finally note lip lop sophisticated algorithm design balance performance h edge ftl expert set reasonably hope performance respect use generalpurpose notice analysis lip lop difcult generalize learn setting discuss section comparison dp rod expert set use straightforward rod originally propose evendar variant p rod include benchmark b additional expert perform p rod update base expert use expert benchmark loss worstcase regret c log c log asymptotically inferior guarantee instance situation dp rod merit special case total loss ftl regret da h edge dp rod guarantee regret rod guarantee remain t fact bad case bound realize t precisely case da h edge excellent performance sect parametrize lip lop decrease gap bound bind lip lop linear lop track good expert turn problem track good expert goal learner control regret good xe strategy allow change prediction time entire decision process regret algorithm produce prediction arbitrary sequence decision yt t dene t ft regret bound setting typically depend complexity sequence yt measured number decision switch yt example properly tune version algorithm herbster warmuth guarantee t log upper bind tighten log learner know upper bind complexity bind general possible achieve performance loss sequence easy precise question pose recently colt open problem generality approach allow solve open problem use rod master algorithm combine strategy principled learning follow state performance let sequence know complexity run rod appropriately tune instance parameter setting suggest corollary simultaneously guarantee rod log c log log c s assignment loss sequence remain problem nd benchmark work easy problem notably loss unknown segment round strategy suggest analyze variant refer base decision time loss observe time window t t pick expert bt arg proposition prove appendix performance guarantee optimal parameter set proposition assume exist partition t interval loss generate interval furthermore assume expectation loss good expert interval away expect loss expert set regret upper bound yt e yt t expectation respect distribution loss online optimization consider problem online convex optimization convex closed subset rd family function setting assume loss function smooth appropriately tune version online gradient descent know achieve regret hazan additionally assume environment play strongly convex loss function tune parameter accordingly algorithm use guarantee improved regret furthermore ftl enjoy essentially guarantee question guarantee combine study present online gradient descent algorithm guarantee olog regret aggregated loss function f strongly convex t retain o bound case replace complicated analysis general argument essentially guarantee theorem let convex close subset rd family smooth function run rod appropriately tune instance b ftl parameter setting suggest corollary simultaneously guarantee rod c log c t log c s assignment loss sequence particular imply rod olog t loss function strongly convex similar previous setting cost additional regret o log t bad case rod successfully adapt easy loss sequence case correspond strongly convex function achieve olog regret learn feedback consider multiarmed bandit problem feedback assume round learner pick arm decision set possibility choose observe loss arm learner suffer loss ft setting consider previous section learner observe loss function arm special case game recently study similar model study version online convex optimization partial feedback setting entirely assumption concern b observe hedge strategy c dene b require access loss suffer algorithm entire loss function formally b access decision set c hedge strategy c select pair base arm suggest b probability probability regret bind probability deterministic function p rod directly apply case easy problem correspond gap expected loss ucb algorithm auer guarantee olog regret problem rely suffer regret t bad case performance guarantee rod combine ucb consider multiarmed bandit problem arm feedback run rod appropriately tune instance e ucb parameter setting suggest corollary simultaneously guarantee rod e c log c t log log c arm assignment loss sequence particular loss generate iid fashion exist unique good arm s e rod olog t expectation respect distribution loss result multiarmed bandit set achieve nearly good performance hard easy problem allow pull arm time result contrast slivkin later improve slivkin consider standard feedback set algorithm slivkin variant algorithm simultaneously guarantee regret stochastic environment retain regret bind log adversarial setting result hold strong assumption rod restrict work setting note result obtain simply combine prediction ucb e generic learn edge empirical result set set pr od b od b regret regret time set pr od b od b set pr od b od b regret time pr od b od b regret time time figure hand tune loss sequence study performance rod expert set verify theoretical result importance rod weight update rule compare lip lop report performance ftl da h edge lip lop ftl h edge anytime version rod rod bh edge variant exponential weighting scheme use consider setting dene deterministic loss sequence step design obtain different refer detailed setting result report figure rst remark performance rod comparable good algorithm b set ftl suffer linear regret rod rapidly adjust weight h edge achieve order performance setting situation reverse ftl constant regret da h edge regret order t case short initial phase increase regret stabilize performance ftl set da h edge ftl constant regret attain performance result match behavior predict bound guarantee regret rod roughly minimum ftl da h edge discuss sect p rod update rule use rod play crucial role obtain constant regret benchmark rule exponential update use bh edge fail nde suitable mix b illustrate setting bh edge suffer regret similar h edge fail advantage good performance ftl constant regret set bh edge perform rod ftl bad h edge corresponding weight decrease quickly set ftl da h edge achieve constant regret bh edge finally compare rod lip lop discuss sect algorithm share similar theoretical guarantee potential advantage depend specic set particular lop perform slightly setting rod obtain small regret set constant lip lop bind possible clearly rank algorithm rod clearly avoid behavior exhibit lop set finally note anytime version slightly rod consistent difference observe conclusion introduce rod generalpurpose receive learning benchmark strategy b input guarantee good regret learning algorithm worstcase performance guarantee strategy exploit specic structure loss sequence obtain smoothly adapt easy hard problem apply principle number different setting online optimization match performance exist solution optimization solve open problem learn easy loss sequence tracking good expert set propose point general structure rod instantiate setting scenario online optimization learn switch cost generally problem objective improve benchmark strategy main open problem extension technique work bandit feedback acknowledgement work support high education research european seventh framework programme grant agreement project project reference agarwal dekel l optimal algorithm online optimization bandit feedback m editor proceeding rd annual conference learn theory colt page auer p n fischer p finitetime analysis multiarmed bandit problem learn auer p freund schapire r e b problem bartlett p l hazan e adaptive online gradient descent singer roweis t editor advance neural information processing system page associate slivkin good world stochastic adversarial bandit colt page lugosi g prediction learning improve secondorder bound prediction expert advice machine learn p follow hedge accept machine learn evendar e kearn m regret good regret average machine learn freund schapire r e decisiontheoretic generalization online learning application boost journal computer system science secondorder bind excess loss editor proceeding th conference learn theory volume jmlr proceeding page b m regret minimization online problem use weight majority algorithm colt page editor nip workshop learn fast easy datum t lugosi g efcient tracking large class expert ieee transaction information theory nearoptimal rate universal lossy source code submit ieee transaction information theory approximation baye risk repeat play contribution theory game e agarwal logarithmic regret algorithm online optimization machine learn herbster m m track good expert machine learn efcient algorithm online decision problem science littlestone m weight majority algorithm information computation p prediction limited advice multiarmed bandit pay observation proceeding th international conference machine learn icml page slivkin practical algorithm stochastic adversarial bandit proceeding th international conference machine learning icml page aggregate strategy proceeding annual workshop learn theory colt page m shift expert easy datum colt open problem m online convex programming generalize innitesimal gradient ascent proceeding international conference machine learning icml
sparse online learning truncate computer abstract propose general method truncated gradient induce sparsity weight onlinelearne algorithm loss method essential property degree sparsity parameter control rate sparsification sparsification total sparsification second approach theoretically instance regard online counterpart popular l regularization method batch setting prove small rate sparsification result small additional regret respect typical onlinelearne guarantee finally approach work empirically apply dataset find dataset large number feature substantial sparsity introduction concerned machine learn large dataset example large dataset use paper sparse example feature use set common approach fail simply load dataset memory sufficiently efficient roughly approach work parallelize batch learn machine eg stream example onlinelearne paper focus second approach typical onlinelearne algorithm weight feature expensive application couple reason space constraint state onlinelearne ram efficiently run similar problem occur state l cache second constraint reduce number feature significantly reduce computational time evaluate new sample paper address problem induce sparsity learn weight use onlinelearne algorithm natural solution work problem example simply add l regularization gradient online weight update simply round small weight problematic idea closely related algorithm propose detailed discussion find section solution approach eliminate feature test impact approach typically run algorithm time particularly undesirable large dataset similar problem consider setting commonly use achieve l regularization linear regression algorithm work automatically online fashion formulation l regularization consider loss function convex xi inputoutput pair formulation arg min subject kwk s parameter parameter arg min appropriately choose formulation equivalent formulation simple online version use projection idea require projection weight l ball online step operation difficult implement efficiently datum feature feature sparse important progress recently complexity logarithmic number feature contrast formulation efficient batch set pursue online set complexity independent number feature addition l regularization formulation family onlinelearne algorithm consider include sparsification technique onlinelearne algorithm memory use operate decay weight previous example round weight small state online algorithm concerned simple linear setting apply linear kernel computationally space competitive approach operate directly feature weight high level approach weight decay default value simple method enjoy strong performance guarantee section instance algorithm perform bad standard onlinelearne algorithm additional loss sparsification control continuously single realvalue parameter theory family algorithm loss function induce onlinelearne instantiate square loss section algorithm implement efficiently largescale problem sparse feature problem truncate gradient enjoy follow property computationally efficient number operation online step linear number nonzero feature independent total number feature memory efficient maintain list active feature insert corresponding weight nonzero delete corresponding weight feature dynamically theoretical result state sparsity achieve use method generally require additional assumption meet practice consequently rely experiment section truncated gradient achieve good sparsity practice compare truncate gradient small dataset include lasso online coefficient l regularize subgradient descent detail algorithm section online learn stochastic gradient descent interested standard sequential prediction problem unlabeled example arrive prediction yi base current weight wi wi rd observe let xi yi incur know loss zi convex update weight accord rule want update rule allow bind sum loss zi achieve sparsity purpose start standard stochastic gradient descent sgd rule form zi subgradient b respect variable parameter refer learning rate analysis consider constant learning rate simplicity theory desirable decay learning rate small increase socalled bind know advance t know advance select constant accordingly regret vanishe t focus present paper weight sparsity choose learning rate use constant learning rate analysis lead simple bound method widely use online learning argue efficient solve batch problem repeatedly run online algorithm training datum multiple time example idea successfully apply solve standard svm formulation scenario outline introduction onlinelearne method suitable traditional batch learning method learning rule achieve sparsity weight address paper note exist literature gradient descent focus sparsity sgd consider modification simplicity sparse online learn section examine method achieve sparsity online learning include novel algorithm truncated gradient idea closely relate provide theoretical justification algorithm include general regret bind fundamental connection simple coefficient round order achieve sparsity natural method round small coefficient magnitude threshold online step ik integer use standard sgd rule integer modify rule zi threshold t t vector function word perform standard stochastic gradient descent round coefficient effect remove nonzero small weight general especially small step modify small coefficient remain small online update round operation pull consequently round k step reasonably large k case nonzero coefficient sufficient time threshold large training stage nonzero feature intermediate step round extreme case simply round coefficient end solve storage problem training phase sensitivity choose appropriate k main drawback method drawback lack theoretical guarantee online performance issue motivate consider principled solution l regularize subgradient experiment combine simple online subgradient method l regularization regularization parameter vector vj experiment online method round end use simple baseline notice method produce sparse weight online simply rare case add feasible largescale problem feature memory truncate gradient order obtain online version simple round rule observe direct aggressive aggressive version shrink coefficient small idea truncate gradient shrinkage control parameter vector scalar max min vj t truncation perform online step ik integer let gi integer let gi kg parameter reason instead constant g perform aggressive truncation parameter step potentially lead sparsity note truncate gradient coincide practice verify theory adopt small g new learning rule expect differ general large parameter sparsity expect extra truncation t method lead sparse solution confirm empirically section special case use experiment let case use parameter control sparsity small truncation operation aggressive procedure appear way fix establish regret bind subsection method theoretically sound regard principled variant round important special case set weight component shrink online step method modification regularize subgradient descent rule parameter control sparsity achieve algorithm set exactly standard sgd rule section special case truncated regard online counterpart l regularization approximately solve regularization problem limit prediction performance truncated gradient measure total loss comparable standard stochastic gradient descent introduce sparse weight vector regret analysis paper use k norm k norm reference follow assumption regard loss function assumption assume convex exist nonnegative constant b b rd rd linear prediction problem general loss function form x following common loss function corresponding choice parameter b unique assumption kxk c use binary classification use regression r logistic p ln c svm hinge loss p c square square loss p p c main result theorem parameterize b proof provide long theorem sparse online regret consider sparse online update rule assumption hold w rd x zi b k pd j vector state constant learning rate mention early possible obtain result variable learning rate decay increase lead bind know advance introduce extra complexity presentation main idea focus sparsity optimize learn rate include result bind simply clarity know advance regret order t theorem righthand involve term k depend easily estimate remove dependency trivial upper bind use lead penalty general case remove dependency effective regularization condition penalty solve formulation hard online batch setting general know efficiently discover local minimum difficult characterize good characterization local minimum possible replace k righthand formulation imply efficiently solve problem simple online update rule naturally expect righthand penalty k small corresponding l penalty especially wj component close situation potentially yield performance datum theorem imply tradeoff sparsity regret performance simply consider case constant small sparsity term righthand small large able achieve sparsity regret k righthand large tradeoff sparsity prediction accuracy empirically study section achieve significant sparsity small g small decrease performance consider case g let t d t follow word let l z l regularize loss regularize regret small t imply truncate gradient regard online counterpart l regularization method stochastic setting example draw underlying distribution sparse online gradient method propose paper solve l regularization problem stochastic setting onlinelearne method use solve largescale batch optimization problem setting training example online fashion repeat multiple time training datum simplify analysis instead assume example assume additional example draw training datum randomly equal probability correspond standard stochastic optimization set observe sample underlie distribution following result simple consequence theorem simplicity consider case constant g expectation e sequence index theorem stochastic setting consider set training datum xi let l regularize loss training datum let define recursively t t draw uniformly random assumption hold t rd e kwk x g e b g t observe t g h p bind e rw word average t approximately solve inf rw batch regularization problem inf large choose random stopping time t inequality average t solve l regularization problem approximately use solution instead aggregated solution t experiment regularization use achieve sparsity batch learning set connection truncated gradient regularization regard alternative justification sparsity ability algorithm efficient implementation truncated gradient square loss truncated descent update rule apply leastsquare regression use square loss p lead yi prediction alter efficient sgd implementation leastsquare accord truncated gradient program operate entirely online fashion feature hash instead store explicitly weight easily insert delete table dynamically memory essentially number nonzero weight total number datum feature large onlinelearne situation web application small subset feature nonzero value example desirable deal sparsity small subset feature simultaneously induce sparsity feature weight approach store feature initialize index example feature nonzero time online learning step nonzero feature example calculate shrinkage current time weight update time reset idea delay shrinkage calculation need key efficient implementation truncated gradient implementation satisfie efficiency requirement outline end introduction section similar trick apply algorithm section empirical result apply algorithm efficiently implement option describe previous section selection dataset include dataset uci repository large private largescale dataset relate ad interest prediction uci dataset useful benchmark purpose interesting embody realworld dataset large number feature informative prediction uci dataset use feature large fraction feature useful prediction purpose demonstrate behavior algorithm add random binary feature dataset feature value set experiment interested reduction number feature possible affect learn performance significantly specifically require accuracy reduce classification task total square loss increase regression task common practice allow algorithm run training datum set multiple pass decay learning rate dataset perform fold cross validation training set identify good set parameter include learn rate g number pass training set decay learn rate pass set parameter use training set finally learn evaluate test set fix set experiment effect include extended version paper figure fraction reduce feature sparsification apply dataset uci dataset randomly add feature truncate gradient able reduce number feature fraction ad dataset reduction observe result improve extensive parameter search cross validation base datum extra fraction feature leave ad dataset ratio ad ratio auc fraction leave base datum extra dataset figure leave feature leave sparsification dataset performance loss right ratio auc sparsification decrease accuracy instead cross validation truncate able achieve reduction indicate large reduction possible tiny additional accuracy loss original uci dataset add feature useful feature remove level performance maintain classification task study truncate gradient affect auc area curve standard metric classification use auc insensitive threshold accuracy use set parameter fold cross validation describe find criterion affect significantly sparsification case actually improve removal irrelevant feature ratio auc sparsification classification task plot figure ratio previous result exercise power approach present apply dataset standard lasso computationally apply approach large dataset goal predict ad click context information content ad query information accept increase classification error allow reduce number feature factor decrease number feature set experiment compare truncate gradient algorithm regard ability tradeoff feature sparsification performance focus auc metric classification task algorithm comparison include truncated gradient truncated gradient round regularize subgradient algorithm batch l regularization publicly available implementation use choose work choice especially important coefficient round algorithm parameter identify use cross validation note attempt compare algorithm simply size large lasso subgradient descent figure result dataset ad result dataset qualitatively similar dataset truncate gradient consistently competitive online algorithm significantly outperform problem imply truncated gradient generally effective truncate gradient behave similarly round expect truncate gradient regard principled variant round valid theoretical justification interesting observe qualitative behavior gradient similar lasso especially sparse weight vector allow left graph consistent relation algorithm usually perform bad allow number nonzero weight large right graph case lasso overfit truncate gradient robust overfitte robustness online learning attribute early stopping extensively study finally worth emphasize comparison experiment shed light relative strength algorithm term feature sparsification consider efficiently implement large dataset sparse feature truncate gradient coefficient round algorithm applicable ad round subgradient number feature auc auc round subgradient number feature figure comparison algorithm sample uci dataset conclusion paper cover efficient sparsification technique online learn strong theoretical guarantee truncate gradient natural extension regression onlinelearne set prove technique sound performance compare standard stochastic gradient descent adversarial situation furthermore asymptotic solution instance algorithm essentially equivalent lasso regression justify ability produce sparse weight vector number feature large theorem verify experimentally number problem case especially problem irrelevant feature approach achieve order magnitude reduction number feature reference asuncion machine learn repository pm long m quadratic loss bound prediction use linear function gradient descent ieee transaction neural network machine learn multicore advance neural information processing system page o dekel s singer kernelbase perceptron fix budget advance neural information processing system page duchi shalevshwartz singer efficient projection ball learn high dimension proceeding icml page gradient gradient descent linear predictor information computation langford l fast online learn efficient sparse code algorithm advance neural information processing system nip dd rise new benchmark collection text research machine learn research shalevshwartz singer srebro primal estimate subgradient solver proceeding icml page implementation lasso lar elastic net version tibshirani regression shrinkage selection royal statistical society t solve large scale linear prediction problem use stochastic gradient descent algorithm proceeding icml page m online programming generalize gradient ascent proceeding icml page
global regularization inverse kinematic redundant manipulator dept computer science dept electrical computer institute neural abstract inverse kinematic problem redundant manipulator illpose nonlinear fundamentally different issue result need form regularization existence multiple solution branch global existence excess degree freedom local certain class manipulator learn method apply inputoutput datum generate forward function use globally regularize problem partition domain forward mapping finite set region inverse problem wellpose local regularization accomplish appropriate parameterization redundancy consistently region result problem transform finite set wellpose problem solve separately construct approximate direct inverse function introduction robot forward kinematic function map vector joint variable configuration space workspace assume euclidean denote mapping m e input space joint space e wm workspace m manipulator redundant inverse kinematic problem follow desire workspace location find joint variable forward kinematic know demer inverse kinematic manipulator solvable closed form problem illpose l separate phenomenon multiple solution branch exist redundant manipulator second source arise redundant dof inverse solution branch consist submanifold dimensionality equal number redundant dof inverse solution require regularization global regularization select solution branch local regularization resolve redundancy paper existence solution assume inverse seek point reachable workspace ie desire image inputoutput datum generate kinematic mapping pair consist joint variable value corresponding endeffector location inverse mapping learn priori assumption restriction answer approach solution base use learn method partition datum group inverse kinematic problem restrict group wellpose direct inverse function approximate group direct inverse function instance direct inverse computable quickly implement feedforward network use function evaluation equivalent single forward propagation importantly theoretical result track cyclic path workspace produce cyclic trajectory joint angle equivalent direct inverse function inverse function necessary ensure follow closed loop ann configuration result endeffector location unfortunately topological result single global inverse function exist generic robot manipulator global topological analysis function nature manifold induce input space workspace certain robot mapping express union finite set local region case redundancy form submanifold parameterize locally consistently example use topology preserve neural network topology kinematic know certain robot input space partition disjoint region property inverse solution branch lie region assume following manipulator question geometry joint redundancy manifold case topology n m torii compact manifold dimensionality compact manifold dimensionality m smooth map en wm let differential map space e en tangent space wm set point arise constraint result unique valid solution system illpose solution system typically solve find leastsquare minimum cost solution system multiple possibly infinite solution inverse kinematic problem redundant manipulator global regularization inverse kinematic redundant manipulator map x e wm preimage denote differential natural representation m jacobian matrix element consist partial derivative wrt basis define s set critical point set e rank dimensionality wm element image critical value regular value e e e point kinematic certain class manipulator geometry assume decompose base surface divide finite number disjoint connected region ci image ci connected region workspace denote kinematic mapping restrict locally inverse solution branch region workspace structure product space conjecture wi ci form locally trivial bundle ci form region inverse unique point workspace configuration seek global regularization require choose multiple preimage torii local regularization redundancy resolution require find location choose torus like effectively redundancy manifold construct index invertible mapping preimage manifold point m obtain consistent representation manifold construct invertible mapping set m location parameter global regularization existence multiple solution manipulator pose difficult problem u plausible reason manipulator allowable configuration task space effectively constrain exist single inverse solution martinetz approach regularize problem allow existence datum seek generalize case learn possible solution manipulator typically multiple preimage torii particular point workspace topology preimage solution branch generally know type geometry number obvious inspection usually different different region workspace upper bind number inverse solution know consequently determination number search good fit possibility sample clustering approach describe demer use partition inputoutput datum disjoint preimage set approach use sample forward behavior identify set input space map bundle consist base space t m total space projection p mapping total space base space certain property projection equivalent fi restrict total space locally trivial bundle consistent parameterization possible demer u r t u t r u t figure preimage position endeffector r planar manipulator specific location small distance specific location workspace preimage point lie disjoint preimage manifold manifold typically separable cluster figure view redundancy manifold particular endeffector location r planar arm input space point position arm endeffector location note input space torus t order visualize space torus slice dimension opposite face cube identify local regularization inverse kinematic problem manipulator redundant dof usually solve use differential method attempt exploit redundancy optimize objective function use learn method regularize training time add constraint equal number redundant dof computationally inefficient iterative solution derivative matrix inversion require realtime control eliminate runtime available redundancy impose prior constraint use extra dof practice numerical differential method use redundancy resolution recently simple recurrent neural network resolve redundancy optimization certain linden differential method number property general possible iterate order achieve solution arbitrary accuracy tend capable handle flexible constraint global regularization discuss use augment method example choice solution branch initial starting location away select differential method use achieve accurate solution branch work construction approximation endeffector location value preimage manifold tend merge phenomenon identify method global regularization inverse kinematic redundant manipulator inverse achievable mapping workspace augment parameterization redundancy input space approximate local regularization accomplish parameterize solution branch torii sample j point pair x image parameterization discover branch method use exploit fact neighborhood ci map neighborhood neighborhood preimage finite number solution branch point sample image initial point find input space access j component x datum pair find point preimage set neighborhood topology set know torus selforganize map appropriate topology fit point order parameterize manifold neighboring torii similar parameterization repeat process point use parameterization initial condition parameterization preimage qualitatively similar construct efficiently step query point xi set parameterization obtain r redundant planar arm approach use provide global local regularization manipulator perform task plane manipulator r map restrict connected region input space bound surface define t r x r preimage point workspace consist torii actual number number inverse solution manipulator type torus preimage restrict mapping goal identify torii parameterize figure input space workspace arm surface partition workspace disjoint input space disjoint region circle indicate workspace location reach singular configuration inverse image circle form separate surface input space link length use single manifold workspace location region c manifold workspace location b figure view datum point manifold end location region parameterization selforganize map use elastic net algorithm parameterization location workspace inverse kinematic compute locate near parameterization network workspace position choose configuration manifold runtime kinematic map locally smooth interpolation network node network great accuracy convenience node torus choose canonical remain node assign value base normalize distance point parameter value scale interval endeffector location preimage manifold need identify global partitioning individual manifold parameterize demer workspace input space l t o joint figure forward kinematic generic planar manipulator sep surface input space depend value joint angle input space project joint joint space endeffector location region c workspace single preimage manifold respectively input space endeffector location region b workspace preimage manifold b resp d input manifold belong finite set homotopy class live ambient space torus dimension torus euclidean space possible onedimensional manifold multiple distinct type class closed loop serve manifold fortunately physical robot rarely joint unlimited range motion practice manifold usually like able parameterize possibility appropriate choice topology net result effective parameterization figure view parameterization manifold figure preimage endeffector location region b figure discussion global regularization accomplish method describe partition original inputoutput datum set distinct ci region redundancy parameter t obtain local regularization use augment datum result transformation datum let t function compute parameter value input space let construction regularized mapping fi t onetoone example t directly approximate onetoone mapping inverse map feedforward neural network global regularization inverse kinematic redundant manipulator joint joint angle l angle figure datum point preimage manifold point workspace r planar arm close d elastic network adaptation manifold smoothly point dimension method require datum sample size exponential number degree freedom manipulator adequate large manipulator practical industrial robot dof amenable technique especially common design separable wrist dof redundant dof work use augment differential method redundancy resolution approximate solution find extremely rapidly use initialize gradientbased method iterate achieve highly accurate solution global decision choose multiple manifold identify criterion choose location manifold runtime computation approximate direct inverse constant time acknowledgement work support nsf young award fellowship cognitive neuroscience author like thank nip foundation provide student travel grant reference topological problem robotic mathematical vol kinematic design redundant robot manipulator dept mechanical classification regional manipulator geometry robotic automation demer joint joint joint figure view data point preimage manifold point region b elastic net adaptation belong different homotopy class fig point introduction robotic learn global direct inverse kinematic hanson sj advance information processing system forward model supervise learn teacher cognitive science alexander linden inversion neural network parallel computing neural controller control neural network vol martinetz schulten threedimensional network learn coordination robot arm network vol topology geometry kinematic manipulator general application feasibility analysis continuous trajectory m ed robotic recent trend research education application
learn profile human activity use nonparametric bayesian model alexander information computer science abstract datum set characterize human activity time collection event count increase interest application area interaction video web datum analysis propose nonparametric bayesian framework model collection datum particular use dirichlet process framework learn set intensity function correspond different category form basis set represent individual eg day depend category assign allow model learn fashion factor generate observation particular day include example weekday weekend effect effect correspond unique occurrence unusual behavior share information appropriate obtain improved estimate behavior associate category application datum set count datum involve people use illustrate technique introduction sensor storage technology continue improve term cost performance increasingly rich data set available characterize human activity time example include log frequency identification tag traffic time datum statistic email web access log datum use support variety different application classification human animal activity detection unusual event support broad understanding behavior particular context temporal pattern web usage ground discussion consider datum consist collection individual aggregate event single sensor eg log recording entry exit building timing number highway traffic accident example figure day worth datum building log smooth similarity pattern readily visible interest modeling underlie intensity process generate datum intensity refer rate event occur process typically inhomogeneous time figure arise aggregated behavior individual exhibit temporal dependence link underlie human activity complexity temporal dependence generally unknown observe datum suggest non semiparametric method method complexity capable grow number observation increase particularly appropriate formulate underlie event generation inhomogeneous poisson process common step allow application classic density estimation technique estimate intensity function normalize version rate function tion technique use context include kernel density estimation wavelet analysis discretization nonparametric bayesian model nonparametric bayesian approach number appeal tag allow represent reason uncertainty intensity function provide single estimate distribution function second bayesian framework provide natural method model selection allow datum naturally plain set intensity function use complex similar effect achieve use penalize likelihood function fi bayesian method generalize multiple hierarchical model allow tion share related differ set observation multiple day figure count datum building entry log datum second point crucial observe monday smooth use problem rarely obtain enable visual comparison tion exactly process exactly condition instead observe multiple instance think similar fact represent number slightly differ circumstance example behavior dependent time day day week type day weekend weekday factor weather unusual circumstance share information allow improve model appropriate indicate similarity datum bayesian remain agnostic datum share reason uncertainty structure follow propose nonparametric bayesian framework model intensity function event datum time particular describe dirichlet process framework learn unknown rate function learn set function correspond different category individual eg individual day represent additive combination intensity function depend category assign allow model learn fashion factor generate observation particular day include example weekday weekend effect effect correspond unusual behavior present single day application realworld datum set building access log accident statistic use illustrate technique discuss detail section follow propose approach relate prior work similar topic broadly speak viewpoint modeling inhomogeneous timeserie count work extend work allow sharing information multiple relate process eg different day approach view alternative hierarchical dirichlet process hdp problem pattern different group constrained expect hdp model poisson process common model continuoustime event data poisson process discrete poisson distribution characterize rate parameter poisson process characterize rate function t property time interval t number event occur time poisson rate t use bayesian semiparametric model t describe let suppose single collection event time arise poisson process rate function t p t use term poisson process inhomogeneous poisson process mean rate function time t r t define t write t t intensity function normalize version rate function bayesian model place prior distribution quantity select parametric prior nonparametric prior t obtain semiparametric prior t specifically choose z gamma distribution kernel function example gaussian dirichlet process parameter base distribution g dirichlet process provide nonparametric prior probability form mixture model infinitely component wj desire place prior distribution quantity eg b parameter dirichlet process variation gain recent attention ability provide representation consist arbitrarily large mixture model particular subject recent work model intensity function poisson process define time poisson process model describe likelihood datum time t ie observe complete data set rightmost term term involve form likelihood mixture model distribution define mixture model application helpful create auxiliary assignment variable indicate mixture component sample associate complete datum likelihood zi inference typically accomplish use markov sample specifically posterior simple close form b sample complicated sample draw variety way common method socalled chinese restaurant process crp relative weight wj marginalize draw assignment variable zi exact sampling approach work exploit fact finite number mixture component occupy datum treat cluster single group infinite number potential association treat finite number operation involve sample value collection associate event time easy certain choice example use gaussian distribution necessary quantity convenient close form way issue infinitely mixture component perform approximate sampling use truncate dirichlet process representation describe datum set size tolerance compute maximum number component m necessary approximate dirichlet process dirichlet distribution use relation manner work finite number mixture component representation prove useful section approximation helpful primarily allow sample complete function t compare crp formulation set assignment zi occupy arbitrarily cluster sample weight wj sample occupy mixture weight wj total weight w wj draw independent random variable accord respectively normalize sum value weight wj cluster sample use stickbreake note approximation highlight importance sample hope representation act nonparametric sense grow complex datum increase fix number component m insensitive detail sample hyperparameter finite time domain description nonparametric bayesian technique poisson process far implicit use fact domain t infinite domain finite example minor complication arise example function properly define positive interval possible solution issue use alternate function beta distribution mean posterior sampling parameter long possible closed form method metropolishasting use highly dependent choice proposal density slightly different approach draw truncate gaussian parameter sample truncate distribution specifically define r t n w distribution sample model turn relatively simple efficient use rejection method restriction impose quantity z great sample posterior simply draw original close form posterior distribution discard resample probability categorical model mention introduction collection d observation arise d instance similar process process know identical independent sharing information relatively obtain observation nd estimate collectively use estimate t process necessarily identical sharing information difficult situation typical consider figure event datum different monday clearly great deal consistency size shape day exactly stand different look example sunday tuesday section appear different little share information appear relatively similar similarity probably use improve rate estimate day example reasonably assume category membership know example day weekday weekend relax assumption later section structure potential relationship reasonable model share information category course possible choice use simple additive model describe section additive model intuition additive model datum arise superposition underlie cause present period interest initially assume category membership know category associate particular day activity profile associate category observe additional activity arise category present let associate rate function c t c t category model define rate function day sum rate function category belong denote membership indicator ie category c present day p d model restrictive match intuition datum generate stem presence absence particular behavioral pattern associate underlying cause work day fact want model flexible linear combination pattern meaningful example day monday learn profile associate cause eg thing happen day weekday monday sense model pattern present suggest method couple dirichlet process hierarchical dirichlet process flexible hdp couple parameter component level relate actual shape profile allow component large small disappear completely desirable quality application use additive model allow consistent size shape emerge category associate deviation profile category far inference system significantly difficult single rate function case section define association indicate category generate event easy sample accord sample membership course frequently case membership collection datum know precisely extreme case idea collection similar group wish find profile unsupervised manner commonly prior knowledge interpretation profile wish strictly enforce known membership example create category assign meaning weekday weekend sunday monday day monday happen closure unusual circumstance completely different profile similarly day unusual extra activity behavior unique particular circumstance additional category represent accommodate possibility sample value membership indicator variable sdc binary indicator day d behavior category c end let assume prior knowledge membership probability sdc resample posterior distribution iteration sample step difficult truncated representation point easily use example crp formulation sample variable tightly couple membership c instead sample sdc condition truncate rate function c t truncation depth m choose provide arbitrarily high precision likelihood datum rate function value sdc compute directly practice propose change value membership variable individually complex apply follow sequence mcmc sample truncated representation sample membership variable c t sdc sample association association sample sunday monday tuesday figure posterior mean estimate rate function build entry log datum estimate individually day dot learn share information multiple day solid sunday b monday tuesday share information similar day greatly improve estimate rate function resolve feature decrease increase subsequent category magnitude c truncated representation consist weight wj parameter j experiment section consider application model datum set mention previously entry log people enter large building produce optical sensor log traffic accident design datum set contain week worth observation case plausible prior structure interpretation category ie similar day similar profile end create category day weekend weekday sunday category high probability membership day account possibility unusual increase activity add category unique day low prior probability membership allow day add new category evidence unusual activity build entry datum improvement estimate rate function information share similar day figure result different day week panel estimate profile day estimate individually use day observation dirichlet process mixture model dot line superimpose panel single black curve correspond total profile day week estimate use categorical model sum rate function day weekend sunday b sum day weekday monday use prior distribution individual estimate share estimate feature worth note share day worth observation model produce accurate estimate profile case single day contain observation confident detail rate function rate function appear relatively smooth information day include rate function begin resolve clearly bimodal shape weekday bimodal rate behavior real correspond arrival mode large narrow second peak return second profile appear similar distinct behavior increase activity late behavior basis reality correspond regular meeting hold week question particular day component category figure expect little consistency weekday weekend bit similarity weekday tuesday particular day little set apart tuesday check category membership sdc use effectively monday datum set fall individual profile average probability compute estimate posterior probability sdc day b weekday c tuesday unique figure posterior mean estimate rate function category datum belong comparison total rate sum category dotted line day category small indicate little consistency datum weekday weekend weekday category large contain component appear correspond return category mode capture regular meeting class unique category category unique particular day little activity figure profile associate category entry log datum day know event period dash vertical line model successfully learn day significant unusual activity associate reasonable profile activity note increase count rate typically occur shortly beginning event time particular day find probability belong weekday category use unique category examine day high probability require category indicate unusual activity datum set partial ground truth consist number date time activity schedule place building figure day correspond rate profile associate category day estimate additional activity period time activity correspond actual start end time schedule dash vertical line accident datum second datum set consist database accident time record department expect drive pattern activity weekend far observe building log share information allow figure posterior mean uncertainty decrease posterior uncertainty rate single day accident datum estimate individually particular day figure quantify red datum share black sharing datum idea posterior mean point considerably reduce posterior uncertainty confidence interval profile shape rate function estimate day datum set use day datum red curve use additive model black additive model leverage additional datum produce tight estimate rate profile previous example additional datum help resolve detailed feature day profile figure example weekday profile shape mode correspond small mode large mode sunday monday figure posterior mean estimate rate function accident estimate individually day dot sharing multiple day solid sunday monday c figure share information help resolve feature individual day datum reliably estimate help pattern deviation clear expect increase activity conclusion increase availability log human activity datum provide interesting opportunity application statistical learning technique paper propose nonparametric bayesian approach learn profile activity datum base inhomogeneous poisson process framework propose approach allow collection observation day group category day week turn leverage datum different collection yield high quality profile estimate categorization day priori certain day fall day unusual additional activity model infer appropriate categorization allow example automate detection unusual event large realworld datum set model able infer interpretable activity profile correspond direction work area include rich model allow incorporation observe covariate weather exogenous phenomenon modeling multiple sensor loop sensor datum traffic reference scott p smyth markov modulate poisson process poisson cascade application web traffic datum bayesian statistic r r consistent estimation intensity function cyclic poisson anal r r intensity density estimation submit bayesian nonparametric mixture model intensity function nonhomogeneous poisson process technical report apply math statistic bayesian mixture model spatial poisson process application extreme value analysis technical report dept apply math statistic density estimation statistic datum analysis chapman dm blei hierarchical dirichlet process nip statistical method connect series event r stat soc neal markov chain sample method dirichlet process mixture model graph stat m density estimation inference use amer functional dirichlet process identity process ann h gibb sample method stickbreake prior amer stat dirichlet process compute finite normal mixture smooth prior information comp graph statist sethuraman constructive definition dirichlet prior
structured learning gaussian model abstract consider estimation multiple highdimensional gaussian graphical model correspond single set node distinct condition assume aspect network share structured difference specifically network difference generate node perturbation node perturb network edge stem node differ network correspond simple model mechanism underlie cancer gene regulatory network activity specific gene propose solve problem use joint graphical lasso convex optimization problem base use rowcolumn overlap norm penalty solve convex problem use alternate direction method multiplier proposal illustrate synthetic datum application brain cancer gene expression data introduction probabilistic graphical model widely use variety application computer vision natural language processing computational biology modeling framework use increasingly complex domain problem select exponentially large space possible network structure importance problem especially highdimensional setting number variable node graphical model large number observation available estimate example suppose access gene expression measurement lung cancer patient brain cancer patient like estimate gene regulatory network underlie type cancer consider estimate single network basis patient approach unlikely successful fundamental difference true lung cancer brain cancer gene regulatory network stem tissue gene expression differ disease alternative simply estimate gene regulatory network use lung cancer patient separate gene regulatory network use n brain cancer patient approach fail exploit fact underlie gene regulatory network likely substantial commonality pathway order effectively use available datum need principled approach jointly estimate lung cancer brain cancer network way network estimate encourage similar allow certain structured difference fact difference scientific interest paper propose general framework jointly learn structure network assumption network similar overall certain structured difference electrical engineering computer science engineering computer science engineering science specifically assume network difference result perturbation certain node perturb condition edge associate node differ network detect difference use rowcolumn overlap norm penalty figure illustrate toy example pair network identical single perturb node detect use proposal problem estimate multiple network differ node perturbation arise number application instance gene regulatory network cancer patient normal individual likely similar specific node perturbation arise small set gene mutation example arise analysis conditional independence relationship p stock distinct point time interested detect stock differential connectivity edge time point likely correspond undergo significant change example find field neuroscience interested learn connectivity neuron human brain change time figure example network differ node perturbation network adjacency matrix b network adjacency matrix leave edge differ network right shaded cell indicate edge differ network proposal estimate multiple network presence perturbation formulate optimization problem solve use efficient direction method multiplier admm significantly outperform generalpurpose optimization tool test method synthetic datum generate know graphical model realworld task involve infer gene regulatory network experimental datum rest paper organize follow section present recent work estimation graphical model ggm section present proposal structured learning multiple use rowcolumn overlap norm penalty section present admm solve propose convex optimization problem application synthetic real datum section discussion section background graphical lasso suppose wish estimate ggm basis observation independent identically distribute know learn sparsity structure p estimate maximum likelihood p possible empirical covariance matrix singular consequently number author consider maximize penalize trace maximize p empirical covariance matrix base n observation positive tuning denote set positive definite matrix size p norm solve serve estimate estimate positive definite sparse sufficiently large penalty refer graphical lasso formulation formulation convex efficient algorithm solve available fuse graphical lasso recent literature convex formulation propose extend graphical lasso setting access number observation distinct condition goal formulation estimate graphical model condition assumption network share certain characteristic suppose independent identically distribute n distribution let denote empirical covariance matrix kth class maximize penalize log likelihood p maximize ij p ij l trace nonnegative tune parameter p ij ij penalty apply element solve order encourage similarity serve estimate particular consider use p ij lasso penalty difference pair network edge large network estimate sparse large pair network estimate identical edge refer penalty graphical lasso formulation solve formulation allow accurate network inference simply learn network separately strength available observation estimate network implicitly assume difference network arise edge perturbation approach advantage structure learning problem difference network drive node differ network difference individual edge joint graphical lasso detect node perturbation challenge glance problem detect node perturbation simple case simply modify follow p maximize l p column matrix apply group lasso penalty column group lasso penalty simultaneously shrink element apply appear desire node perturbation structure refer naive group unfortunately problem arise fact optimization problem perform subject symmetry constraint symmetry constraint effectively impose overlap element p group lasso penalty element ith row jth column group presence overlap group group penalty yield estimate support complement union group figure simple example case node perturbation estimate obtain use figure reveal use detect node perturbation task require penalty yield estimate support union group propose approach ggm equivalently represent perturbation entry row column corresponding precision matrix figure word figure toy example p variable perturb red panel estimate display network adjacency matrix shade edge network element adjacency matrix indicate nonzero element result pnjgl q correct sparsity pattern bc naive group lasso naive group lasso unable detect pattern detect single node perturbation look row correspond column nonzero element define rowcolumn group group consist row correspond column matrix note p p matrix exist p group overlap node ggm perturb correspond union correspond rowcolumn group order detect node perturbation ggm figure construct regularizer promote estimate support rowcolumn group task propose rowcolumn overlap norm penalty definition rowcolumn overlap norm rcon induce matrix norm define v rcon satisfie follow property easy check norm consequently symmetric argument ie paper interested particular class rcon penalty p vj q norm know q norm interpret norm q norm column matrix little abuse notation let denote q norm form note q closely relate group fact derive case q definition naturally handle group structure induce overlap row column accommodate q norm q generally norm discuss apply penalty q encourage support union set row column matrix consider task jointly estimate precision matrix solve maximize p s s refer optimization problem joint graphical lasso pnjgl formulation nonnegative tuning parameter q note satisfie property rcon penalty follow remark formulation special case pnjgl formulation q optimal solution note formulation approach let set set q q promote entry edge encourage union approach support row correspond column node perturb clearly detect use pnjgl q example sparsity structure detect panel figure note formulation penalty easily extend estimation ggm include term pair model restrict case paper admm algorithm pnjgl formulation optimization problem convex directly solve model environment conic solver general approach fully exploit structure problem scale largescale instance algorithm propose overlap group lasso penalty apply setting formulation combination loss instead square error loss rcon penalty positivedefinite constraint note firstorder method easily apply solve formulation subgradient rcon easy compute addition proximal operator rcon nontrivial compute section present fast scalable alternate direction method multiplier admm solve problem reformulate introduce new variable decouple term objective function difficult optimize jointly result simple algorithm closedform update reformulation follow p minimize z z z subject z admm obtain standard fashion defer detail long version paper complete algorithm operator expand nk nk log det d p eigenvalue decomposition mention early nk number observation kth class operator p q know proximal operator correspond q norm q simple form omit space constraint description operator find section interpret approximate dual gradient ascent method approximation fact gradient dual augment lagrangian iteration compute coordinate descent primal variable typically admm iterate group primal variable algorithm convergence property wellknown case cycle group investigation convergence property admm algorithm arbitrary number group ongoing research area optimization literature specific convergence result algorithm know empirically observe good convergence behavior study issue direction future work initialize primal variable identity matrix dual variable matrix set implementation stop criterion difference consecutive iterate small tolerance admm order magnitude fast interior point method comparable accuracy note complexity admm op complexity compute hand complexity interior point method op p interior point method use minute run admm second p time hour minute respectively observe average error admm solution average random generation datum o admm optimization problem input t converge q expand expand z q t gt zi experiment describe experiment report result generate datum real datum synthetic experiment synthetic data generation generate network follow network share individual edge node node node perturb node differ network create p p symmetric matrix diagonal element equal set probability set equal aij randomly select node set element corresponding row column distribution step result background pattern structure common network copy matrix randomly select m perturb node differ set element corresponding row column choose random iid draw distribution finally compute min small eigenvalue set equal ci set equal ci step perform order ensure positive definiteness generate independent observation distribution use compute empirical covariance matrix s s compare performance graphical lasso p m result result average iteration figure increase yield accurate result pnjgl graphical lasso furthermore pnjgl q identify nonzero edge differ edge accurately turn accurate graphical lasso pnjgl lead accurate estimate extent pnjgl outperform apparent small infer biological network apply pnjgl method cancer gene expression datum set expression measurement gene patient brain cancer patient distinct clinical subtype classical select subtype patient figure simulation study result pnjgl graphical lasso p panel line correspond fix value pnjgl q plot denote number edge estimate nonzero follow leave number edge correctly estimate nonzero center number edge correctly estimate differ network divide number edge estimate differ network right frobenius norm error estimate precision matrix ie ij ij ij ij ij ij patient analysis experiment aim reconstruct gene regulatory network subtype identify gene interaction gene vary significantly subtype gene likely mutation understand molecular basis subtype lead understanding brain cancer eventually improve patient treatment select gene high variance gene know frequently mutate subtype gene initial list gene select base variance lead total gene apply pnjgl result gene expression dataset gene variance tuning parameter select approach result estimate approximately nonzero edge approximately edge differ network estimate result follow wide range tune parameter value figure pnjgl perform brain cancer datum set correspond gene patient subtype plot gene base estimate pnjgl estimate cd c value white nonzero value black quantify extent node perturbation network estimate follow pnjgl formulation use threshold jth gene edge weight condition figure plot result value gene network estimate result pnjgl approximately number edge differ cancer subtype pnjgl result estimate gene appear node perturbation result estimate gene appear node perturbation pnjgl display clearly figure nonzero element pattern network difference result pnjgl far structured gene know frequently mutate subtype somewhat appear perturb accord estimate mutate gene detect total gene detect pvalue contrast detect gene node perturbation figure gene high value accord small belong receptor control organization tissue gene identify frequently mutate gene recent evidence play critical role drive pathway tissue result suggest possibility previously unknown role brain cancer discussion future work propose joint graphical lasso new approach jointly learn model assumption network difference result node perturbation impose structure use novel rcon penalty encourage difference estimate network union row column solve result optimization problem use admm efficient scalable standard point method propose approach lead far performance synthetic datum alternative approach learn graphical model assume edge perturbation simply learn model separately future work involve form structured sparsity simply node perturbation instance certain subnetwork know priori relate condition study rcon penalty modify order encourage subnetwork perturb condition addition describe paper require computation decomposition p p matrix iteration plan develop computational improvement mirror recent result related problem order reduce computation involve solve optimization problem acknowledgment support mf support nsf grant reference jm multivariate analysis academic press graphical model science publication m yuan model selection estimation gaussian graphical model biometrika friedman t hastie r sparse inverse covariance estimation graphical banerjee l model selection sparse maximum estimation binary datum jmlr dm new insight fast computation graphical computational graphical statistic sparse inverse covariance selection alternate method advance neural information processing system ravikumar model selection gaussian model highdimensional consistency mle advance nip cj dhillon p ravikumar sparse inverse covariance estimation use quadratic approximation advance neural information processing system r tibshirani regression shrinkage selection royal statistical society series o banerjee firstorder method sparse matrix analysis application joint estimation multiple graphical model biometrika p p joint graphical lasso inverse covariance estimation multiple class httparxivorgab m sparsity smoothness fuse royal statistical society series yuan model selection estimation regression group variable royal statistical society series vert group lasso overlap graph proceeding conference machine learn g group lasso overlap latent group httparxivorgab m grant m efficient order method linear composite regularizer smooth proximal gradient method structure sparse learn proceeding conference uncertainty artificial intelligence s l algorithm group sparse regularization overlap group neural information processing system page sp b distribute optimization statistical learning alternate direction method multiplier foundation trend ml m z linear convergence alternate direction method multiplier available m tao yuan alternate direction method separable optimization page duchi singer efficient online batch learning use forward backward machine learn research page integrate genomic analysis identify relevant subtype characterize cancer cell tissue peripheral blood cancer patient cancer interaction support cancer cell fashion journal
learn state recurrence m abstract research investigate new technique unsupervised learning nonlinear control problem approach apply michie chamber box extension aseace system significantly improve convergence rate stochastically base recurrence learning new nonlinear exploit information find learn trial reinforce decision result recurrence state recurrence learn apply positive reinforcement exploration search space box ase negative weight reinforcement apply failure result add information recurrence learning increase learn empirical result recurrence learning fast basic failure drive learning failure prediction method recurrence learning test failure drive experiment goal direct learn application detection recur oscillation provide useful information reduce learning time apply negative instead positive reinforcement detection cycle provide heuristic improve balance evidence gathering goal direct search research investigate new technique unsupervised learning nonlinear problem delay feedback approach compare michie chamber box extension ai ase search element aseace adaptive element system improved learning time stochastically base learn failure drive task consider adaptively control behavior system pass sequence state internal dynamic assume know priori choice action visit state controller refer learning decision deterministic accord stochastic rule learning discover action good circumstance produce action observe result information paper motivate previous work investigate adaptive element affect learn environment inspire current work recent attention neural network connectionist system choose use cartpole control problem enable comparison result address cartpole problem work consider learn system compose interact environment problem require balance pole act invert pendulum hinge cart cart travel leave right bounded dimensional track pole left right cart learn pole balance cart cart bound track parameter cartpole system cart velocity pole angle angular velocity action available application fix force cart right leave direction action extremely difficult problem priori knowledge system dynamic dynamic change time controller imitate smith controller assume priori knowledge dynamic controller anticipate system able deal change dynamic numerical simulation cartpole solution recurrence learning substantial improvement result ai michie chamber figure algorithm use result figure discuss detail fair trial figure aseace constant recurrence general problem assignment cartpole problem class problem know credit assignment particular temporal credit assignment recurrence learn approach general temporal credit assignment problem characterize seek improve learn decision early action goal find action responsible improved degrade late example bucket design assign credit rule system accord overall usefulness attain goal adjust strength value weight rule problem modify strength permit rule activate early sequence result successful action later consider credit assignment problem play program note easy credit rule combine produce jump point game hard decide rule active early responsible change later jump possible state recurrence learning assign strength individual rule action action strength system accumulate experience basis action overall usefulness situation invoke follow bucket previous work problem learn control dynamical system study past michie chamber different approach achieve vary degree success use cartpole problem basis empirically measure algorithm work michie chamber l build box program learn balance pole cart box algorithm choose action high average time failure trial trial run end failure time limit program able balance pole time step figure describe box learn state penalize system failure accord active state immediately precede system failure use adaptive element solve control problem aseace choose action high probability pole balance region able balance pole time step completion trial figure b box aseace associative search element figure box ase learn enter failure state c state traverse shaded rectangle state b state failure state edge diagram figure b describe aseace learn system failure occur state expect failure time state penalize system failure occur expect failure time state reward state penalize failure occur soon expect state expect failure time time traverse state failure point c leave state weight update new state expect failure time differ state use connectionist system learn balance pole previous experiment system provide state priori average trial necessary learn balance pole time step develop approach depend partition state space discrete region use function l interpolate degree cartpole state system learn control task trial system use knowledge representation information system relate propose class differential learning mechanism correlate early change input later change output adaptation formula use change output weighted sum absolute value previous input weight wj previous difference input previous time coefficient temporal difference approach class adaptive prediction method element class use sum previously predict output value multiply gradient exponentially decay coefficient modify weight use temporal difference underlie learning procedure classical conditioning definition state set value range parameter sufficient specify instantaneous condition system input decoder group environmental state equivalence class element class identical system response environmental input map n input state reference state assume input value fall discrete range determine decoder specify state return visit alternate state recur action cause modification system parameter change system state change state need occur alter parameter value decode range weight associate action state probability allow action dependent current value weight rule determine allowable action rule deterministic choose action stochastically base weight weight change wt reduce likelihood choose action cause failure change base idea previous action element present input influence cause similar pattern occur weight change increase likelihood element produce action pattern similar occur future example consider classic problem balance pole cart state specify position velocity cart pole allowable action fix velocity increment right left rule determine action base current weight recurrence learn present nonlinear method empirical result successful stationary environment contrast method applicable nonstationary environment effort develop algorithm reward decision choice lead quasistable cycle avoid failure limit cycle converge oscillation point technique exploit recurrence information obtain learn trial system reward return previous state weight change permit state transition occur system return state avoid failure recur state reward sequence recur state view evidence possibly unstable cycle form temporal cause effect association optimize performance dynamic search technique balance choose search path know solution cost explore new area search space find cheap solution know armed bandit problem l ie slot machine arm observe reward probability high exclude play arm aseace system recurrence learning learn search contrast box ase algorithm learn failure range decode work real value input parameter analyze member range reduce compute resource demand limited number range allow parameter possible range overlap aspect range decode discuss paper range consider nonoverlapping parameter value fall range range active specification state consist active range parameter range overlap set parameter value specify unique state set parameter value specify state parameter value time determine active state si set possible state value environmental parameter fall number range different different parameter state specify active range parameter set input parameter value decode n range problem boolean value use describe activity level state activity value state state active inactive decision model box aseace system input state active time state nonoverlapping mutually exclusive reason preclude overlap consistency previous model aseace system output decision rule controller base weighted sum input stochastic noise action output decision noise real randomly gaussian distribute value mean variance output controller interpret push leave right goal recurrence learn avoid failure state cycle state exist quasistable oscillation concept compare long ball judge success reward consideration ball high low left right controller try stable cycle optimum performance demand recurrence learn heuristic devise fundamental basis reward state repeatedly visit repeatedly activate heuristic reward state revisit cycle failure occur second heuristic augment reward state short cycle heuristic discuss heuristic state visit trial reward reinforce weight heuristic assume state visit cycle failure occur action previous visit assume influence recurrence reinforce weight state assume increase likelihood cycle occur assumption state responsible cycle action immediately cause environment change different state delay transition small change parameter decode input range state incorporate heuristic state appear twice succession weight reinforce assume action directly cause state immediate recurrence recurrence equation recurrence learn equation stem weight formula use ase system weight state sum previous weight product learning rate reward r state eligibility e e eligibility index exponentially decay trace function output value output decision p determine decay rate reward function system fail time reinforcement cycle equation describe basic ase system algorithm extend weight procedure follow term provide failure reinforcement term provide reinforcement success state virtue weight change multiply reward value current eligibility simplicity reward value positive constant need environmental feedback yield reinforcement value function time use instead second eligibility function yield constant value hi p p accord formula time state active state previously active t formula state previously visit state transition occur time step direction increase decrease weight change action final term heuristic constant recurrence learn eligibility function design reinforce cycle heuristic reward short cycle long heuristic short recurrence learn eligibility function design reinforce short cycle long cycle reinforcement short cycle basis second heuristic conjecture short cycle converge easily point long long cycle diverge easily short cycle grow diverge large cycle follow extension basic heuristic propose formula recurrence eligibility function current eligibility function similar previous failure eligibility function e reinforce short cycle eligibility decay time inversely proportional period cycle t reinforce converge oscillation term ensure weight reinforcement return visit state figure b constant recurrence algorithm short recurrence figure constant recurrence state reward transition state example state reward constant regardless weather cycle traverse state c figure b describe short recurrence algorithm m state reward accord difference current time activation time small difference reward large difference example state reward cycle state c traverse state dark heavy line cycle state b traverse light line state recur soon traverse dark simulation result simulate algorithm ase aseace recurrence algorithm experiment consist run cartpole balance task consist trial trial time step system fail ie pole fall cart track boundary effort conserve cpu time simulation terminate system achieve consecutive trial time step remain trial assume time step assumption reasonable result weight space cause controller deterministic regardless influence stochastic noise long time require run simulation attempt optimize parameter trial begin cart center pole assumption state space configuration initial state continuity state experiment consist failure recurrence reward learn ase failure learning run average time step failure trial predictive aseace system run comparative metric find method cause controller average time step failure result comparable describe experiment short recurrence learning system add system trial learning session execute average short recurrence learn run time step th trial aseace system final experiment constant recurrence learn ase system simulate constant recurrence learn eliminate failure time figure constant recurrence learn short recurrence learn failure rate average simulation run discussion detection cycle provide heuristic armed bandit problem decide evidence gathering goal direct search algorithm allow search cycle state state high probability search space rate exploration proportional recurrence learn parameter decrease influence cycle govern decision process decrease explore search space cycle oscillation path relatively large degree variance final trial average simulation range time step failure future experiment study effect prediction length manner similar prediction failure use aseace system effort minimize difference predict time cycle order predict period result experiment future report hope recurrence prediction system generally superior predictive system short recurrence system operate conclusion paper present extension failure drive learn base reinforce decision cause enter environmental state controller learn synthesize good value reinforce area search space produce recur state cycle state normal failure drive learn algorithm learn achieve weight success simulation recurrence reward improve overall learn cartpole task substantial decrease learn time reference michie r chamber machine intelligence e r coin tech b smith computer information science ed m genetic p m p seattle p coin tech r mathematical software l approximation theory academic h int tech tech
learn decision list linear threshold function distribution view computer abstract consider problem learn decision list use example irrelevant feature present smooth boost algorithm madaboost efficiently learn decision list length boolean variable use log n example provide marginal distribution relevant variable l norm sense use recent result extend analysis obtain similar quantitatively weak result learn arbitrary linear threshold function coefficient experimental result indicate use smooth boost algorithm play crucial role analysis impact actual performance introduction decision list boolean function define n boolean input follow form bk define n boolean variable bk boolean value work decision list widely study learn theory machine learn question receive attention possible learn decision list ie learn decision list length n variable use log n example question ask blum repose numerous briefly describe range partial result obtain different line author note littlestone winnow learn decision list use ok log example time log valiant sharpen analysis winnow special case decision list bounded number sequence output b know learn decision list use ok log example run time algorithm servedio use polynomial threshold function winnow obtain tradeoff run time number example require algorithm run time use ok log example work different approach relax requirement work distribution example model relaxation fact allow handle decision list arbitrary linear threshold function coefficient recall linear threshold function function real number function output numerical sign argument approach result analyze smooth boost section weak learner consider possible x xi weak hypothesis algorithm describe detail section performance bound term l norm distribution recall l norm distribution d finite set x kdk l norm use evaluate spread probability distribution probability concentrate constant number element domain l norm constant probability mass spread uniformly domain size l norm main result follow let distribution suppose target function relevant variable let rel denote marginal distribution k induce relevant variable ie relevant variable value rel input z let uniform distribution suppose rel note d minimal l norm distribution k suppose target function arbitrary decision list setting describe log example run log time probability construct hypothesis h accurate respect suppose target function arbitrary linear threshold function setting log o log example run log time probability construct hypothesis h accurate respect d relation previous work consider similar approach use boolean weak hypothesis boost algorithm case adaboost prove distribution example result algorithm require example learn linear threshold function ie function form boolean variable weight integer pn clearly imply relevant variable know general decision list length express linear threshold function weight result attribute efficient learning decision list recently consider essentially algorithm analyze work specifically study smooth boost algorithm weak consider general linear threshold learning problem assumption relevant variable distribution satisfy margin condition level noise analysis paper different good knowledge analysis smoothness property boost exploit attribute efficient learning boost smooth boost fix target function distribution d hypothesis function weak hypothesis respect d h refer advantage respect reader boost algorithm algorithm operate sequence stage stage t maintain distribution stage t boost algorithm weak hypothesis respect d boost algorithm use construct distribution t stage boost construct final hypothesis base weak hypothesis guarantee high accuracy respect initial distribution detail let d distribution d smooth respect d xd x follow boost algorithm b smooth initial distribution d distribution generate start d use boost accuracy weak hypothesis stage smooth wrt d know smooth dependence rest paper denote smooth boost easy distribution d smooth wrt uniform distribution u satisfie hand distribution d highly nonsmooth relative u kdk small instance distribution d k weight single point distribute remain weight uniformly point smooth nonsmooth satisfie kdk norm condition consider paper weak condition smoothness respect uniform distribution total variation distance l norm distribution total variation distance probability distribution d finite set x d s d d easy total variation distance distribution equal support distribution disjoint following immediate distribution d finite domain v mind d x bind total variation distance distribution d uniform distribution term ratio kdk l norm follow distribution d finite domain uniform distribution u proof let m d u m inequality m pr dx xd d u x m m m m second inequality use fact m inequality use use definition m solve v d u complete proof weak hypothesis decision list let decision list depend variable output output output bk following prove easy induction proof essentially equivalent claim lemma decision list represent linear threshold function form ci integer range easy fix c lemma xk vary linear form c assume odd integer value range k exactly prove let decision list length boolean variable let d distribution let denote marginal distribution induce relevant variable suppose weak hypothesis satisfy edrel proof observe wellknown distribution weak hypothesis k immediately establish lemma suppose assume decision list concern second concern let lx denote linear form c draw uniformly k lx distribute uniformly odd integer interval c uniform c let denote set satisfy note element correspond great integer certainly s follow consequently follow simple argument use prove edrel ef recall ci follow satisfy edrel proof complete weak hypothesis linear threshold function consider general setting arbitrary linear threshold function additional technical complication basic idea previous section use following fact fact theorem let linear threshold tion depend k variable x representation assume weight wk order decrease magnitude wk main result section follow lemma proof use idea proof theorem let linear threshold function depend variable let distribution let denote marginal distribution induce relevant variable suppose weak hypothesis satisfy edrel k proof sketch assume wk wk describe fact let specify detail later suppose k wellknown result linear threshold function depend variable represent use integer weight magnitude ok log k imply distribution p h log k log case hold henceforth assume remain edrel follow edrel edrel ef ef ef wi imply h satisfie edrel k o desire similar consider case slightly different case case wi wj r pk ln recall follow version hoeffding bind let rk kwk e write pk kwk denote bind directly wk e ln argument establish equation yield current set change need argument adjust constant factor definition equation yield wk v bind yield fact establish case value wj let fix setting z j variable inequality theorem wj wj wj qp wj immediately yield wj turn wj usual argument follow use fact rel work run smooth use madaboost algorithm domingo tth stage boost madaboost simulate distribution weak learning work follow log example draw simulated distribution example use obtain empirical estimate h upper bind advantage h weak hypothesis use stage discuss weak hypothesis use stage high observe empirical estimate algorithm run t stage boost consider fix stage t execution o draw original distribution d require madaboost simulate draw distribution direct consequence fact madaboost smooth distribution simulate use rejection sample standard tail bound good hypothesis h probability hypothesis select h madaboost accurate weak hypothesis stage t stage algorithm construct hypothesis error suffice overall number example use log establish theorem remain initial distribution d distribution arise course boost good weak hypothesis sufficiently large advantage suppose target function depend set variable consider happen run smooth boost initial distribution d satisfie stage consequently distribution satisfy imply case decision list good weak hypothesis require advantage experiment smoothness property enable analysis paper smoothness helpful learn decision list respect diffuse distribution critical section aim address question experimentally compare accuracy classifier output number smooth booster literature adaboost know smooth booster general section synthetic datum example distribute uniformly class determine apply randomly generate decision list number relevant variable fix decision list determine pick b independently uniformly possibility evaluate follow algorithm adaboost b madaboost c smooth booster propose gavinsky space constraint describe booster use training datum round minimize weighted training error choose algorithm choose number round roughly speak adaboost datum assign weight example previously choose base classifier classify incorrectly output weighted vote output base classifier voting weight determine function base classifier perform madaboost adaboost place weight prior normalization weight learning progress datum base classifier manner depend perform form manner update weight significantly different adaboost reminiscent gavinsky table average test set error rate m gavinsky table average smoothness boost function desire accuracy instead run algorithm round booster datum normalize function assign weight example base previously choose base classifier classify correctly propose gavinsky set weight case terminate choice number example m number feature repeat follow step generate random target generate random example split training set example test set remain d apply algorithm training set e apply result classifier test set repeat step time total size test set repeat average testset error report parameter analysis use set try algorithm set test set error rate table madaboost improve accuracy adaboost result consistent possibility adaboost learn decision list respect uniform distribution motivate theoretical study true possible route prove source adaboost high probability smooth boost algorithm average smoothness table fairly robust choice good choice perform good motivate research adaptive booster line reference query concept learn machine learn j r prediction general recursive function mathematic blum learn boolean function infinite attribute space proceeding annual symposium theory compute page learn available blum l n littlestone learn presence finitely infinitely irrelevant attribute computer system science blum p selection relevant feature example machine learn d gavinsky boost optimal distribution learn research domingo madaboost modify version adaboost proceeding annual conference computational learning theory page freund r schapire decisiontheoretic generalization online learning application boost journal computer system science gavinsky adaptive application agnostic learning machine learn research p m szegedy threshold circuit bounded depth computer system science linear function neuron structure training biological cybernetic r distribution somewhat hard problem proceeding annual symposium foundation computer science page m learn sparse perceptron nip page r servedio set machine learn preliminary version foc r servedio attribute efficient learning decision list parity proceeding th annual conference learn theory page littlestone learn quickly irrelevant attribute new machine learn m perceptron introduction computational geometry mit cambridge mitchell generalization search artificial intelligence s theory majority switching element r online learning decision list machine learn research limit theorem probability theory science publication r learning decision list machine learn r schapire theoretical view boost page r servedio pac learning use winnow perceptron algorithm proceeding twelfth annual conference computational learning theory page r servedio smooth learn noise journal machine learn research preliminary version proc colt r servedio linear threshold function approximator proceeding conference computational complexity page valiant projection learning machine learn
grow neural gas network learn topology incremental network model introduce able learn important topological relation set input vector mean simple learn rule contrast previous approach neural gas method martinetz schulten model parameter change time able continue learn add unit connection performance criterion meet application model include vector quantization clustering interpolation introduction unsupervised learning setting input datum available information desire output goal learn situation possible objective dimensionality reduction find lowdimensional subspace input vector space contain input datum linear subspace property compute directly principal component analysis iteratively number network model oja feature map grow cell structure fritzke allow projection nonlinear sample subspace dimensionality choose depend relation inherent datum dimensionality dimensionality target space information topological arrangement input datum lose process fritzke mapping highdimensional datum space structure exist general ask structure look allow mapping directly lead possible objective unsupervised learning describe topology learning highdimensional datum find topological structure closely reflect topology data distribution elegant method construct structure competitive martinetz require use vector quantization method martinetz schulten propose neural gas ng method purpose martinetz schulten briefly introduce discuss approach martinetz schulten propose new network model use contrast combination model incremental constant parameter lead number advantage previous approach competitive hebbian learning neural martinetz assume number center r n successively insert topological connection evaluate input signal draw data distribution principle method input signal connect close center measure euclidean distance edge result graph subgraph delaunay triangulation correspond set center subgraph fig b induce delaunay triangulation limit area input space r pe induce delaunay triangulation optimally preserve topology general sense martinetz center lie input datum submanifold actually develop edge purpose topology learning unit use center place region r p e differ vector quantization procedure martinetz schulten propose particular kind method mention method martinetz schulten main principle follow input signal adapt near center decrease large initial small final value large initial value cause adaptation movement input signal large fraction center adaptation range decrease finally near center input signal adapt adaptation strength underlie similar decay schedule realize parameter decay define total number adaptation step method advance grow neural gas network learn topology delaunay triangulation induce delaunay triangulation figure way define closeness set point delaunay triangulation thick line connect point neighbor thin line basically reduce point small euclidean distance wrt set point induce delaunay triangulation thick line obtain mask original delaunay triangulation data distribution p shade center connect common border lie partially region p closely adapt martinetz schulten data distribution run algorithm distribute certain number center use generate topology possible apply technique concurrently martinetz schulten case method remove edge require motion center edge generate early martinetz schulten use edge age scheme purpose note influence outcome method way adaptation base distance input space network topology hand influence topology generate center combination describe effective method topology learn problem practical application determine suitable number center depend complexity datum distribution want model different number center appropriate nature require decision advance result satisfy new simulation perform scratch following propose method overcome problem offer number advantage flexible scheme insertion grow neural gas algorithm following consider network consist set unit nod unit e associate reference vector e reference vector regard position input space corresponding unit set n connection edge pair unit connection weight purpose definition topological structure possibly infinite number ndimensional input signal obey unknown probability density function main idea method successively add new unit initially small network evaluate local statistical measure gather previous adaptation step approach use grow cell structure model fritzke b topology fix dimensionality approach describe network topology generate incrementally dimensionality depend input datum vary locally complete algorithm model grow neural gas follow start unit b random position wa generate input signal accord p find near unit unit increment age edge add square distance input signal near unit input space local counter variable direct topological neighbor fraction en respectively total distance direct neighbor connect edge set age edge edge exist create remove edge age large result point edge remove paper term neighbor denote unit topological neighbor graph oppose unit small euclidean distance input space step hebbian spirit correlated activity use decide insertion grow neural gas network learn topology number input signal generate far integer multiple parameter insert new unit follow determine unit q maximum accumulate error insert new unit r q neighbor large error variable edge connect new unit r unit q remove original edge q decrease error variable multiply constant initialize error variable r new value error variable q decrease error variable multiply constant d stop criterion net size performance measure step describe method work adaptation step input signal lead general movement unit area input space signal come p insertion edge near unit respect input signal generate single connection induce delaunay triangulation fig b respect current position unit removal edge necessary edge long induce delaunay triangulation end point achieve local edge age near unit combine age reset edge exist near unit insertion removal edge model try construct track induce delaunay triangulation slowly target adaptation reference vector squared distance adaptation help identify unit lie area input space mapping signal unit cause error reduce error new unit insert region simulation result simulation result demonstrate general behavior model probability distribution fig propose martinetz schulten demonstrate neural gas model model quickly learn important topological relation complicated distribution form structure different dimensionality second example fig illustrate difference propose model original network final topology similar model intermediate stage different model able identify cluster distribution grow neural gas model fritzke figure grow neural gas network adapt signal distribution different dimensionality different area input space initial network consist randomly place unit network input signal apply network necessarily final growth process principle continue parameter simulation continue grow discover small cluster present particular example discussion grow neural gas network present able explicit important topological relation distribution input signal advantage method martinetz schulten incremental character model eliminate need network size instead growth process continue performance criterion network size meet parameter constant time contrast model heavily rely decay parameter method map note topology generate feature grow neural gas network learn topology neural gas competitive grow neural gas use competitive hebbian learning ooo j co figure network martinetz schulten author grow neural gas model adapt cluster probability distribution respective initial state row number intermediate stage number unit model final number unit grow neural gas model row distribution center adaptation step edge previous row center distribution similar model intermediate stage differ significantly fritzke method ng model essential component use direct completely local adaptation insertion center probably proper initialization new unit interpolation exist possible constant parameter local adaptation possible application model cluster vector quantization network perform particularly situation neighborhood information edge use implement interpolation scheme neighboring unit use error occur early phase determine insert new unit generate topological lookup table different density different dimensionality particular area input data space promising direction research combination supervised learning early grow cell structure fritzke recently grow neural gas describe paper fritzke crucial property kind application possibility choose arbitrary insertion criterion feature present original grow neural gas result new supervise network model incremental radial basis function network promising far investigate currently reference fast learning incremental rbf network neural process letter b grow cell structure selforganize network unsupervised supervise learn supervised learn grow cell structure editor advance neural information processing system page selforganize formation correct feature map biological cybernetic martinetz m competitive hebbian learning rule form perfectly topology preserve map international conference artificial neural network page martinetz m schulten network learn topology editor artificial neural network page martinetz m schulten topology represent network oja simplified neuron model principal component biology optimality principle unsupervised learning editor advance neural information processing system page
high order recurrent network grammatical l gile sun advanced computer study college park way high order single layer recursive network easily learn simulate deterministic finite state machine recognize regular grammar version neural net state connect common error term external analog memory combination interpret neural neural finite state machine primitive push pop able read stack gradient descent learning rule derive common error function hybrid network learn effectively use action manipulate stack memory learn simple contextfree grammar introduction biological network readily easily process temporal information artificial neural network recurrent neural network model permit encoding learning temporal sequence recurrent neural net model example nearly encode current state representation model activity neuron state determine current state input automata perspective dynamical structure state machine formal model sequence machine generate recognize formal grammar respective automata model formalize foundation computer science hierarchy formal grammar simple level complexity state machine regular grammar machine high order recurrent network grammatical inference grammar describe deterministic level complexity describe automata associate contextfree grammar pushdown fmite state machine add power use stack memory network able perform type computation solve learn problem grammatical inference simple grammatical inference define problem find learn grammar fmite set string teaching sample recall grammar define tuple p s v terminal vocabulary p finite set production rule s start symbol grammatical inference define learning machine recognize teaching testing sample potential application grammatical inference include area pattern programming language design translation graphic language great deal interest teach net recognize grammar simulate pollack important extension work discuss particular construct recurrent high order net state machine hide layer powerful net state machine discuss far example learning time training sample size significantly reduce addition integrate neural net fmite state machine external stack memory inform network common objective function symbol stack operation primitive push pop devise common error function integrate stack net state machine hybrid structure learn effectively use stack recognize contextfree grammar interesting work recurrent net learn state machine machine associate read write operation input string know training set model present learn manipulate push pop read primitive external stack memory learn additional necessary state operation structure high order recurrent network recurrent neural network utilize consider high order modification network model develop recall recurrent net activation state neuron time define state machine w map state input time state weight matrix form mapping usually learn use high order form mapping gile range number state neuron number input neuron define order use net grammatical inference learn rule devise learn mapping weight matrix sample set p string grammar construct follow error function e e r tr l sum p sample error function evaluate end present sequence length activity output neuron recurrent net output neuron member state neuron target value pattern legal string u sing gradient descent procedure minimize error e function pattern weight update rule learning rate use easily calculate use recursion relationship choice initial value sit int note require update element string present know initial value adequate network topology neural net state machine capable learn regular grammar arbitrary string length complex grammar length state machine order net perform train net regular grammar dual grammar arbitrary length string dual parity string contain number number network architecture input neuron state neuron fully connect second order weight string vocabulary end symbol use unary representation initial training set consist positive negative string increase length length include training string length result string neural net state machine perfectly recognize string length total training time usually epoch look closely dynamic learn discover different input state network tend cluster value initial state state consider possible state actual fmite state machine movement state function input interpret state transition state machine construct state machine yield perfect state machine recognize dual parity grammar use procedure state transition reduce minimal high order recurrent network grammatical inference state machine extract state machine fig complicated grammar different initial condition difficult extract fmite state machine different initial weight choose different transition diagram state result interesting state machine learn simple grammar perfectly order net learn problem high order net learn fast easy prove fmite machine represent order single layer recurrent net discussion high order state machine figure learn state machine state start final state order easily learn complex deterministic grammar neural net develop andor learn use type memory simple memory approach easily come mind teach additional weight structure multilayer neural network serve memory pollack teach neural use external memory source appeal know formal language theory finite stack machine require significantly resource fmite state machine bounded problem recognize length contextfree grammar teach neural net use stack memory pose problem construct stack memory couple stack memory neural net state machine formulate objective function optimization yield effective learning rule formulate objective function stack couple neural net state machine stringent condition pushdown automata accept contextfree grammar automata final state stack error function eq modify include final state stack length term gile ly final stack length time ie time symbol string present legal string e automata final state stack consider stack connect neural net state machine recall pushdown automata state transition mapping eq include additional argument symbol read stack additional stack action mapping obvious approach connect stack neural net let activity level certain neuron represent symbol stack represent action stack pushdown automata additional action reading write stack base current state input stack symbol interpretation mapping extension figure single layer high order recursive neural network connect stack memory represent action neuron connect stack r represent memory neuron read stack activation proceed state input stack time state action time recursion replace state layer state layer output neuron control action stack input value connected memory neuron value dependent index current value fully connect nd order weight hidden neuron mapping eq define recursive network implement concurrently parallel let neuron state value range continuously action value range neural network architecture high order recurrent network grammatical inference depict fig number read neuron equal code representation stack application action neuron suffice order use gradient descent learning rule describe stack continuous value type algorithm require continuous stack explain continuous stack use connect action read neuron interpret stack action follow push pop action simplicity current input symbol push number input stack memory neuron equal input symbol ad value push stack t stack consist summation analog symbol definition symbol unit depth read neuron r time pop depth symbol depth remove stack time step remain r unit length current stack attempt pop stack occur remain stack pop depth description operation example find action operation remove add stack length time recursion relation stack construction error function define algorithm derive eq derivative term derive recurrent relation length equation change contain information past change ar ar depend symbol read assume read change effect action occur recent past change action respect weight define recursion derive form eq case pop stack weight change increase stack length legal string happen appear derivative necessary integrate neural net continuous stack memory automata simulation test theoretical development train neural net pushdown gile contextfree grammar non parenthesis grammar balanced string parenthesis parenthesis grammar net architecture consist nd order fully interconnect single layer net state neuron input neuron action neuron push pop epoch positive negative training sample increase length length network learn perfect pushdown conclude test string length similar analysis value use similar clustering analysis heuristic reduction approach minimal emerge note pushdown machine little easily learn fig pushdown emerge tuple represent input symbol stack symbol action push non pop successfully train small training set epoch learn compare computationally learning layer network minimal pushdown derive detail learning ei figure learn neural network parenthesis balance numerical result state state state state legal end state feed end symbol legal string end state stack conclusion work present different approach incorporate use memory neural network recurrent high order net learn effectively employ external stack high order recurrent network grammatical inference memory learn simple contextfree grammar require continuous stack structure possible reduce neural network ideal pushdown neural network perfectly learn simple grammar simulation appear promising question remain extend simulation complex grammar question architecture scale real problem evident power high order network demonstrate sp learning sparseness training set true complex problem question work reference training connectionist state theory method acm computing survey vol p syntactic pattern recognition application introduction theory language computation read dynamic parallelism connectionist sequential machine proceeding conference cognitive science society grammatical inference neural network state machine proceeding international joint conference neural network m vol p ml computation finite infinite machine generalization backpropagation recurrent neural network phy p pollack implication recursive distribute representation advance neural information system d encode sequential structure simple recurrent network advance neural information system p automata learn contextfree grammar proceeding international joint conference neural network m vol ri d learning algorithm continually run fully recurrent neural network cognitive science report u
learn informative statistic nonparametric approach alexander abstract discuss information theoretic approach categorize model dynamic process approach learn compact informative statistic summarize past state predict future observation furthermore uncertainty prediction characterize joint density learn statistic present observation discuss application technique noise drive dynamical system random process sample density condition past case result dynamic random walk statistic noise capture second case present result summarize statistic learn noisy random telegraph wave differ dependency past state case yield principled approach discriminate process differ dynamic andor dependency method ground idea information theory nonparametric dynamical process world human speech frequency stock market common example process difficult model categorize current observation dependent past complex way classical model come sort assume dynamic linear noise assume dynamic discrete hmms approach popular tractable understand unfortunately process underlie theoretical assumption model false example wish analyze system linear dynamic nongaussian noise wish model system unknown number discrete state present informationtheoretic approach analyze stochastic dynamic process model simple process mention retain flexibility model wide range complex process key insight learn simplify informative statistic past sample use estimate entropy mutual information framework predict future state equal importance characterize uncertainty accompany learn informative statistic nonparametric approach prediction nonparametric model flexible describe uncertainty complex secondorder statistic contrast technique use square prediction error drive learning focus mode distribution example financial forecasting likely sequence price event interest like know accompany distribution price value ie likely outcome price knowledge low probability valuable end describe approach allow simultaneously learn dependency process past uncertainty future state approach novel fold concept information theory nonparametric statistic learn type stochastic process consider challenge summarize past efficient way absence know dynamical probabilistic model learn informative statistic ideally sufficient statistic past minimize uncertainty future state classical linear statespace approach uncertainty characterize mean square error mse implicitly assume linear system interesting behavior nongaussian statistic violate assumption underlie mse nonlinear system purely probabilistic process exhibit complex behavior poorly characterize mean square error andor assumption approach applicable type process base nonparametric statistic characterize uncertainty prediction general way density possible future state consequently result system capture dynamic system parameterization statistic noise parametric modeling model use classify new signal prediction future learn stationary process paper consider related type stochastic process depict figure process differ current observation relate past type process describe follow set equation discrete time dynamical possibly nonlinear system l state process time function previous state present value general sequence stationary strict sense fairly mild condition rjk rjk sequence variable assume true sequence xk stationary term innovation sequence purpose stationarity suffice lead prediction framework estimate dynamical parameter system nonparametric characterization uncertainty second type process consider describe conditional probability density case conditional statistic xk concern definition constant learn informative statistic nonparametric estimator propose determine system parameter minimize entropy error residual system type parametric entropy optimization approach t p viola r l figure related system dynamical system drive stationary noise probabilistic system dependent finite past dotted box indicate source stochastic process solid box indicate learn propose novelty approach estimate entropy differential entropy integral approximate use function density estimator experiment use minimize entropy error residual equivalent maximize likelihood light propose criterion seek maximum likelihood estimate system parameter use nonparametric description noise density consequently solve system parameter noise density jointly explicit dynamical system second system type assume conditional statistic observed sequence constant bad slowly change online learn algorithm case desire minimize uncertainty prediction future sample summarize information past challenge efficiently function recent sample ideally like find sufficient statistic past explicit description density opt instead informative statistic informative statistic simply mean reduce conditional entropy future sample statistic sufficient mutual information reach maximum previous case propose find statistic maximize nonparametric mutual information define wj equation equivalent optimize joint marginal entropy practice equation minimize conditional entropy previously present related method incorporate kernel base density estimator information theoretic learning framework choose method provide exact gradient approximation entropy importantly convert implicit error function reduce computation cost learn informative statistic nonparametric approach distinguish random walk example random walk feedback function noise assume independent identically distribute nonstationary increment stationary context estimate statistic residual allow discrimination random walk process differ noise density furthermore demonstrate process drive implicit assumption criterion knowledge sufficient distinguish process figure random walk realization associate noise density solid line drive o l drive bimodal mixture note density unit variance learn process model autoregressive ar sample draw realization type ar parameter estimate use standard approach approach describe regard parameter estimation method expect yield essentially parameter coefficient unity remain coefficient interested ability distinguish process mention current approach jointly estimate parameter system density noise nonparametric estimate figure dot line estimate use compute accumulate average residual sequence rk known learn density figure surprising l ek bimodal mixture model dash line differ significantly drive increment process solid line explanation follow fact true density bimodal p density test gaussian divergence case relatively small true ll pe c entropy gaussian fix variance maximum entropy consequence likelihood test assumption reliably distinguish process likelihood test bimodal density nonparametric estimate figure distinguish method describe limit linear dynamic model certainly use nonlinear model long dynamic approximate differentiable function example multilayer perceptron describe learn structure noisy random telegraph noisy random telegraph wave describe figure b goal demonstrate analyze random telegraph wave robustly learn informative statistic past process define noisy random telegraph wave sequence distribute e process interesting parameter random function nonlinear combination set depend value observe different switching dynamic figure example signal t p iii figure random walk example leave comparison known learn density right figure lk know model leave compare learn model right leave right rapid switching dynamic possible signal period long duration figure noisy random telegraph leave right experiment learn sufficient statistic form q hyperbolic tangent function ie p layer perceptron note multilayer perceptron use experiment train sample noisy rtwn learn statistic type process use test situation depth perfectly learn informative statistic nonparametric approach figure comparison wiener filter non parametric approach synthesis p figure informative statistic noisy random telegraph wave m train equal leave right specify denote statistic train rtwn process memory depth m implicitly learn joint density m synthesis possible sample density figure compare synthesis use describe method wiener filter estimate datum result use information theoretic approach preserve structure wiener filter result achieve collapse information past sample single statistic avoid high dimension density estimation figure joint density m n m estimate density separable virtue fact learn statistic convey information future figure result monte trial case depth statistic match process plot accumulate conditional log likelihood e m learn statistic error bar figure similar result vary memory depth m statistic figure illustrate robustness choice memory depth m memory depth matter information exploit empirical result indicate useful information extract conclusion describe approach find informative statistic approach novel learning derive estimator entropy mutual information allow mean efficiently summarize predict future characterize uncertainty prediction secondorder statistic accomplish strong assumption accompany parametric approach t p viola figure conditional lk solid line indicate rtwn dash line indicate thick line indicate average monte run thin line indicate standard deviation left plot use statistic train rtwn right plot use statistic train rtwn rtwn z figure repeat figure case m obvious break indicate new set trial present empirical result illustrate utility approach example random walk serve simple illustration learn dynamic system ar model importantly demonstrate ability learn dynamic statistic underlie noise process information later use distinguish realization non parametric density possible use error prediction result experiment noisy random telegraph wave demonstrate ability learn compact statistic efficiently summarize past process identification method exhibit robustness number parameter learn statistic example dependence memory case useful statistic find conversely memory statistic experiment useful information available past extract opinion method provide alternative traditional connectionist approach timeserie analysis use nonparametric estimator add flexibility class density model place constraint exact form statistic reference cover element information theory son p viola et entropy manipulation real world problem mozer editor advance neural information processing system page methodology information theoretic feature extraction editor proc ieee int neural network page entropy optimization principle application e estimation probability density function mode math stat
efficient recovery jointly sparse vector school compute decision system abstract consider reconstruction sparse signal multiple measurement vector mmv model signal represent matrix consist set jointly sparse vector mmv extension single measurement smv model employ standard compressive sense recent theoretical study focus relaxation mmv problem base norm minimization extension wellknown norm minimization employ smv result optimization problem mmv significantly difficult solve smv exist algorithm reformulate secondorder cone programming socp semidefinite programming sdp problem computationally expensive solve problem moderate size paper propose new dual reformulation optimization problem mmv develop efficient algorithm base proxmethod interestingly theoretical analysis reveal close connection propose reformulation multiple kernel learn simulation study demonstrate scalability propose know compressive sampling recently receive increase attention area science engineering unknown sparse signal reconstruct single measurement vector recent theoretical study recover certain sparse signal far sample measurement traditional method paper consider problem reconstruct sparse signal multiple vector mmv model signal represent matrix consist set jointly sparse vector mmv extension single measurement vector smv model employ standard mmv model motivate need solve inverse problem arise modality image brain arise variety application dna microarray sparse communication channel cancellation compute sparse solution linear inverse problem source localization sensor network smv signal mmv model represent set jointly sparse vector share common occur set location additional structure lead improved performance signal recovery algorithm propose mmv model past sparse representation problem combinatorial optimization problem general algorithm employ greedy strategy recover signal use iterative scheme alternative relax optimization problem global optimal solution obtain widely study approach base norm minimization similar relaxation technique norm employ smv model recent study theoretical result relaxation smv model extend mmv model theoretical investigation need smv model norm minimization solve efficiently result optimization problem mmv difficult solve exist algorithm formulate secondorder cone programming socp programming sdp problem solve use standard software package problem moderate size solve socp sdp computationally expensive limit use paper derive dual reformulation norm minimization problem mmv especially norm minimization problem reformulate minmax problem solve efficiently proxmethod nearly convergence rate compare exist algorithm algorithm scale large problem achieve high accuracy interestingly theoretical analysis reveal close relationship result minmax problem multiple kernel learn perform simulation study result demonstrate scalability propose algorithm comparison exist notation matrix boldface vector boldface set space denote letter pnorm vector t define p d p p inner product define matrix denote ai ith row ith column respectively r pnorm define m multiple measurement vector model smv model aim recover sparse signal measurement vector b matrix smv model extend multiple measurement mmv model signal represent set jointly sparse vector share common set mmv model aim recover sparse representation simultaneously mmv model provably improve standard exploit structure specifically mmv model consider reconstruction signal represent matrix dictionary measurement matrix multiple measurement column associate atom set atom dictionary sparse representation mean matrix small number row contain nonzero entry usually d similar smv use measure number row contain nonzero entry problem find sparse representation signal mmv equivalent solve following problem sparse representation problem p b typical choice p include p p solve p require enumerate subset set d essentially combinatorial optimization problem general nphard similar use norm minimization smv model natural alternative use instead result follow optimization problem p p min b relationship p p mmv model study p optimal solve follow optimization problem min b exist algorithm formulate secondorder cone programming socp problem semidefinite programming sdp problem recall problem eq equivalent follow problem remove square objective min b introduce auxiliary variable ti problem reformulate standard cone programming socp d min ti ti ti b base socp formulation transform standard semidefinite programming d min wt ti ti b interior point method bundle method apply solve socp scale problem moderate size limit use practice propose dual formulation section present dual reformulation optimization problem eq preliminary result summarize let matrix following hold equality hold kxk proof follow definition r pnorm eq loss generality assume m m t ai m m m clearly inequality equality kxk let define following hold max kxk m kxi proof denote set q k m let pm clearly inequality proof equality construct matrix follow maximum achieve construct base result establish derive dual formulation optimization problem eq follow construct kwk bi kwk bi dual problem formulate follow max min kwk follow kwk min kwk note equality hold optimal w represent u d ui kat dual problem simplify following form max kat bi follow definition norm reformulate dual problem eq minmax problem summarize follow optimization problem eq formulate equivalently bj pd matrix define ai d ai ith column proof note reformulate follow max max ai max d d u substitute eq obtain follow problem max kat bi max p min condition satisfied minimization maximization eq result minmax problem let u optimal solution eq kat base solution dual problem eq construct optimal solution primal problem eq follow let optimal solution eq follow construct base u recall satisfy equality constraint main result summarize follow kat b u optimal solution problem proof assume u optimal solution problem eq follow partial derivative objective function respect b b prove reverse direction assume b b b d u define function u d u bi bj consider function u fix note function concave respect u maximum achieve partial derivative respect follow u u d u fix u u u linear combination d d u bi kat assumption kat d satisfy pd d d u d u imply u saddle point minmax problem eq optimal solution problem reconstruct solution primal problem base solution dual problem eq way efficient implementation base formulation eq paper proxmethod discuss detail section employ solve dual problem interesting observation result minmax problem eq closely relate optimization problem multiple kernel learn mkl minmax problem reformulate bj positive semidefinite constrain linear combination set gi base kernel ai formulation eq connect mmv problem mkl efficient algorithm develop past mkl apply solve extended level set method propose solve mkl outperform base linear programming formulation extended level set method involve linear programming iteration theoretical convergence rate denote number iteration slow propose algorithm present section main algorithm propose employ proxmethod solve minmax formulation eq differentiable objective function algorithm proxmethod firstorder method specialize solve saddle point problem nearly convergence rate denote number iteration iteration low computational cost scale problem key idea convert minmax problem associated variational inequality problem iteratively solve series problem let u problem equivalent follow associate problem find z z z s y operator constitute gradient solve vi problem key building block follow projection problem h arg min s denote u z easy verify u u follow present pseudocode propose mmvprox iteration compute projection sufficiently close control parameter l denote continuous constant operator inner iteration converge iteration wt hold global convergence rate input b output step set zt find small set final step set t t ui time complexity cost evaluate operator point involve euclidean projection simplex solve linear time ie od u eq analytically compute time recall inner iteration time complexity outer iteration analysis scale problem comparison secondorder method socp high complexity iteration accord socp cost iteration mmv d typically large m case propose small cost iteration socp explain scale socp experiment section table average recovery result experiment m datum set mean e e e e experiment section conduct simulation evaluate propose mmvprox term recovery quality scalability experiment setup generate set synthetic datum set vary value m n d experiment entry independently generate standard normal distribution ground truth recovery problem generate step randomly select row nonzero entry randomly generate entry row denote solution obtain propose ideally close experiment perform pc cpu employ optimization package solve socp formulation code implement matlab experiment terminate change consecutive approximate solution e recovery quality experiment evaluate recovery quality propose apply data set size report average experimental random repetition measure p recovery quality term mean square error kf dn report measure violation constraint experimental result present table observe table recover sparse signal successfully case study recovery error change sparsity w specifically apply mmvprox data set size m number row vary d d use kf dn recovery quality measure average experimental result random repetition present figure observe figure work case large k sparse tend result large recovery error p dn figure increase recovery error sparsity level decrease scalability experiment study scalability propose mmvprox generate collection datum set vary m step size set m m accordingly apply socp mmvprox data set record computation time experimental result present figure correspond value m yaxis correspond logt t denote computation time second observe figure computation time increase m increase socp fast small problem m m outperform socp value m large m socp formulation solve apply experimental result demonstrate good scalability propose mmvprox comparison socp formulation socp mmvprox logt logt m m b figure scalability comparison socp computation time algorithm problem size vary average computation time iteration algorithm problem size vary denote value m yaxis denote computation time second scale far examine scalability algorithm compare execution time iteration socp propose algorithm use setting experiment n m m m range step size time present figure observe significantly low cost socp iteration note socp applicable m consistent complexity analysis section observe figure m small computation time socp comparable fast iteration firstorder method slow convergence rate secondorder method socp tradeoff scalability convergence rate experiment advantage problem conclusion paper consider norm minimization reconstruction sparse signal multiple measurement vector mmv model signal consist set jointly sparse vector exist algorithm formulate secondorder cone programming programming computationally expensive solve problem moderate size paper propose equivalent dual formulation norm minimization model develop solve dual formulation base proxmethod addition theoretical analysis reveal close connection propose dual formulation multiple kernel learn simulation study demonstrate effectiveness propose algorithm term recovery quality scalability future plan compare exist solver multiple learn propose addition plan examine efficiency proxmethod solve mkl formulation acknowledgement work support nsf iis reference nemirovski restrict memory level method optimization mathematical programming l cambridge press cambridge e cande compressive sampling international mathematic number page e cande robust uncertainty principle exact signal reconstruction highly incomplete frequency information ieee transaction information theory theoretical result sparse representation vector ieee transaction signal process rao sparse channel estimation match pursuit application ieee transaction communication sf rao sparse solution linear inverse problem multiple measurement vector ieee transaction signal process donoho compress sense ieee transaction information theory normalize adaptation ieee transaction speech audio process m robust recovery signal structured union subspace appear ieee transaction information theory empirical baye estimation sparse vector gene expression change statistical application genetic molecular biology rao source image recursive weight minimum norm clinical neurophysiology c convex analysis minimization algorithm fundamental cristianini p learn programming machine learn research m application secondorder cone program application b reconstruction signal optimal number measurement m willsky source localization enforce sparsity laplacian ieee workshop statistical signal processing page m reduce boost recover arbitrary set jointly sparse vector ieee transaction signal process nemirovski proxmethod rate convergence variational inequality continuous monotone operator smooth saddle point problem optimization polynomial algorithm m baraniuk modelbase compressive sensing submit ieee transaction information theory s scholkopf large scale multiple machine learn research use matlab toolbox optimization symmetric cone method software algorithm simultaneous sparse approximation convex relaxation signal process algorithm simultaneous sparse approximation greedy pursuit signal process e m p recovery multiple measurement computer science jin extended level method efficient multiple kernel learning advance neural information processing system page
power identify statistical abstract paper study tradeoff different kind pure exploration depth focus biased coin problem ask total coin flip require identify heavy coin infinite bag contain heavy coin mean light coin mean heavy coin draw bag proportion unknown key difficulty problem lie distinguish kind coin similar mean heavy coin extremely rare exist solution problem require prior knowledge parameter propose adaptive require knowledge obtain nearoptimal sample complexity guarantee contrast provide lower bind strategy require quadratically sample characterize gap adaptive strategy connection detection prove low bound sample complexity differentiate single parametric distribution mixture distribution introduction tradeoff exploration exploitation online learn literature contrast paper study tradeoff different kind pure exploration depth consider bag contain infinite number kind biased coin heavy coin mean light coin mean player pick coin bag probability coin heavy probability coin light player flip coin pick bag time want goal identify heavy coin use total flip possible unknown key difficulty problem lie distinguish kind coin similar mean heavy coin extremely rare balance flip individual coin time estimate mean consider new coin maximize probability observe heavy previous work propose solution rely knowledge limit applicability work propose algorithm require knowledge guarantee return heavy coin probability flip total number coin expectation nearly match know low bound fully adaptive algorithm support general subgaussian source addition coin coin bag time constraint practical importance application addition connect biased coin problem detection prove novel low bound difficulty detect presence mixture single component known family distribution g detect presence mixture distribution difference difficulty conference neural information processing system nip underlying distribution parameter know biased coin problem view online adaptive mixture detection problem source distribution arrive time probability probability eg null player adaptively choose sample distribution increase signaltonoise ratio goal identify distribution use total number sample possible consequence work draw contrast power adaptive number sample time approach problem specifically unknown motivation related work biased coin problem biased coin problem characterize inherent difficulty realworld problem include detection discovery frequency spectrum interest problem stem automate crowd worker datum label machine learning application perform human recent work accelerate label organize worker pool pay wait incoming datum worker amazon mechanical vary widely skill identify worker quickly possible important challenge model worker performance accuracy speed random variable select good worker equivalent identify worker high mean observe worker expect performance directly task estimate repeatedly flip biased coin biased coin problem propose chandrasekaran work know exist algorithm base sequential probability ratio test optimal minimize expect number total flip find heavy coin posterior probability heavy expected sample complexity algorithm propose algorithm severely limited rely critically know exactly addition return coin previously flip require coin bag time rule application address shortcoming preprint consider alternative procedure sequential thresholding procedure propose algorithm coin bag time require knowledge relevant parameter require knowledge result present asymptotic case biased coin problem view multiarmed bandit problem player access distribution arm sample pull random variable mean observe objective identify arm associate high mean probability use pull possible short survey infinite armed bandit problem player confine arm infinite reservoir arm draw reservoir result arm mean draw distribution objective identify high mean possible total pull probability biased coin problem instance game arm reservoir distribution mean define p previous work focus alternative arm distribution reservoir satisfie e p e e e constant know arm distribution reservoir write term work note apply design infinite armed bandit problem finite bandit problem define arm reservoir place uniform distribution arm appeal large wish guarantee nontrivial performance number pull biased problem special case reservoir distribution arm mean arm mean k algorithm bandit problem know author begin sample arm number pull exceed k performance random selection provably optimal algorithm biased coin problem knowledge natural consider procedure estimate unknown parameter use estimate algorithm parameterized arm reservoir setting discuss exactly propose suggest particular estimator low bound b estimator sufficient obtain sample complexity result log factor know upper low bound biased coin problem estimatethenexplore approach require quadratically flip propose adapt unknown parameter specifically sufficiently small use static estimation step determine number sample quadratic optimal sample complexity contribution biased coin problem include novel algorithm coin bag time knowledge distribution parameter support distribution coin come log factor know informationtheoretic lower bind equation achieve algorithm know parameter table overview upper low bound prove work problem believe algorithm solution biased coin problem require prior knowledge problem parameter approach solve general instance bandit problem include parameterized reservoir case describe finally algorithm desire arbitrary arm reservoir distribution work rule estimatethenexplore approach problem statement let index family probability density function g fix assume know procedure note biased coin problem general arbitrary consider sequence variable pi let sequence random variable draw let m represent sample history generate procedure m valid procedure behave accordingly biased coin problem definition distribution draw sample declare heavy enforce rule coin bag time initialize history m repeat heavy distribution declare choose draw sample distribution mn draw sample st distribution declare distribution heavy definition strategy biased coin problem probably correct identify heavy g distribution probability definition strategy biased coin problem estimatethenexplore strategy strategy fix m begin sample successive coin exactly m time number coin minimum necessary test determine probability optionally continue sample arbitrary strategy declare heavy coin adaptive strategy strategy estimatethenexplore strategy study estimatethenexplore strategy exist optimal algorithm biased coin problem know natural consider estimate quantity use algorithm note algorithm parameterized infinite armed bandit problem discuss consider estimatethenexplore strategy estimate sample fix number sample set arm use estimate draw fix number arm apply algorithm arm contribution work strategy infeasible biased coin problem strategy probably correct follow interface goal pn provide low upper bound quantity et mi denote final number coin consider identify coin detect mixture distribution address biased coin problem analyze natural strategy fix m flip successive coin exactly m time relevant question large m order guarantee correctness probability m long wait declare heavy coin author partially answer question improve section lead study difficulty detect presence mixture distribution example kind low bound work observe sequence random variable consider follow hypothesis test h xn r p henceforth refer problem p p know sufficient observe max log sample determine correct hypothesis probability unknown necessary observe max log sample expectation max log appendix c recognize divergence gaussian observe consequence detection parameter underlie distribution unknown anomalous distribution separate null distribution detect anomalous component hard observe anomalous sample ie multiply inverse kl divergence null anomalous distribution distribution separate necessary sample complexity quantity square section investigate adaptive method dramatically decrease sample complexity low bound base detection presence mixture distribution exponential family single distribution family extensive work estimation mixture distribution literature assume mixture coefficient bound away ensure sufficient number sample distribution contrast highlight regime arbitrarily small case statistical detection property testing eg relevant lack strength favor generality consider exponential family allow interpretable statement relevant problem parameter different regime preliminary let p q probability distribution density p q respectively simplicity assume p support define divergence p log define divergence p q p q px note ep log pq log pq log p q p q example p e q p q bernoulli log p q p q log proof appear low bound present low bound sample complexity probably correct strategy biased coin problem follow interface low bound state adaptive strategy section strategy knowledge parameter sample distribution number time section estimatethenexplore strategy prior knowledge parameter section low bound exception adaptive strategy base difficulty detect presence mixture distribution reduction explain section adaptive strategy follow reproduce describe sample complexity probably correct algorithm biased coin identification problem note lower bind hold procedure return previously distribution draw additional sample know theorem fix let t total number sample procedure probably correct identify heavy distribution max c absolute constant directly applicable special case distribution imply lower bind max min biased coin problem upper bound propose procedure biased coin problem present later compare benchmark detection mixture distribution biased coin problem observe identify specific distribution heavy ie determine strictly great hard detect distribution distribution heavy lower bind total expect number sample consider distribution strictly easy detection problem lower bind estimatethenexplore strategy biased coin identification problem estimatethenexplore strategy fix m prior start game sample distribution exactly m time m simplify notation let denote distribution sufficient statistic m sample general product distribution bernoulli distribution biased coin problem distribution parameter m problem describe e xi p close small difficult decide know priori note parameter know e parameter unknown prove lower bind sample complexity estimatethenexplore algorithm task decide sample come mixture distribution single distribution family low bound sample complexity parameter know unknown follow analyze simple binary composite hypothesis test respectively follow event let ei denote probability expectation hypothesis specific value clear context claim instrumental ability prove low bound difficulty hypothesis test claim procedure probably correct satisfy p sample complexity parameter know fix consider hypothesis test problem p fix let random number distribution consider stop declare hypothesis procedure satisfie p max max p p particular log max corollary relate theorem biased coin problem relate theorem consider limit assume m sufficiently large specifically large apply contrast result hold m corollary fix consider probably correct strategy flip coin exactly m time number coin consider declare coin heavy log log min existence strategy nearly match upperbound know appendix b note log large sample complexity achieve adaptive algorithm parameter know sample complexity parameter unknown unknown test mixture instead general composite test individual distribution mixture e particular worstcase setting hard hypothesis test problem p specific form difficult pick bad case produce tight bind consequently section consider single parameter exponential family define formally provide class distribution reason different possible value exponential family include distribution follow general useful wide variety setting constant refer absolute constant certain condition outline follow remark corollary explicit form proof suppose r single parameter exponential family scalar function h strictly increase stop time procedure satisfy p o log max constant depend follow remark corollary apply theorem special case detection biased coin problem respectively e problem p remark unknown procedure knowledge consequently rule define known constant absolute constant consequently log conversely know simply need determine sample come sufficient log sample c corollary fix assume bound sufficiently far min m let number coin probably correct estimatethenexplore strategy flip coin m time exploration step log m c absolute constant remark unknown estimatethenexplore strategy strategy describe corollary unable choose m depend parameter treat constant case bound away corollary state fix m sufficiently small number sample necessary strategy identify heavy coin scale log example difference parameter know effectively rule estimatethenexplore strategy practical purpose set upper bind fix know log adaptive know unknown adaptive unknown log lower bind log log table upper low bound expect sample complexity different probably correct strategy fix refer strategy corollary table assume lower bound constant sufficiently small note upperbound apply distribution support coin result citation unknown prior work lower bind estimatethenexplore strategy propose algorithm optimal adaptive algorithm section propose algorithm prior knowledge parameter yield upper bind match lower bind logarithmic factor assume sample heavy light distribution support draw sample independent unbiased estimator mean result easily extend subgaussian distribution consider procedure find heavy distribution low bound improve prior work support arbitrary distribution require bound adaptive strategy heavy distribution identification input initialize n d log e m logn e log k logk min m draw distribution sample estimate b repeat draw distribution repeat sample distribution observe declare distribution heavy output distribution break output theorem run expect number total sample algorithm log c log absolute constant c follow hold probability light distribution return probability heavy distribution return procedure total sample second claim hold constant probability probability probability observe heavy distribution n log e distribution occur constant probability outer loop allow run n define heavy coin return probability expect number sample bound tight lower bind know parameter unknown double trick use identify heavy coin sample respectively appendix b consider assume prior knowledge result set aware remark search space appear elementary hindsight surprising cover dimensional space balance exploration believe similar similar approach generalize generic infinite armed bandit problem adaptive strategy heavy distribution identification unknown parameter initialize heavy distribution h null repeat null set repeat set run set output null break set output theorem unknown fix run probability heavy distribution return expect number total sample bound log log log absolute constant conclusion prior work require partial knowledge solve biased coin problem algorithm require knowledge parameter obtain nearoptimal sample complexity addition prove low bound sample complexity detect presence mixture distribution parameter know unknown consequence estimatethenexplore strategy approach previously propose infinite armed bandit problem extend adaptive algorithm arbitrary arm reservoir distribution significant interest believe successful algorithm significant impact researcher think sequential decision process finite action space acknowledgment generously support onr award research support nsf award sn amazon web service blue state reference c optimal testing property distribution advance neural information processing system page agarwal detect stream bayesian approach knowledge information system crowd second enable realtime interface bandit problem infinitely arm ann statist alexandre algorithm bandit reward burge bottou m weinberger editor advance neural information processing system page regret infinitely armed bandit find biased coin flip proceeding th conference learn theory page chandrasekaran m find biased coin flip url httparxivorgab detection noisy datum use learn probability distribution proceeding international conference machine learning icml page freund estimate mixture product distribution proceeding twelfth annual conference computational learning theory page acm speed crowd datum label proc price sharp bound learn mixture eprint algorithm multiarmed bandit fix confidence set information science system page ieee quick search rare distribution information science system page ieee progress available sequential analysis test confidence interval springer science business medium gamma function formula mathematic computation page parametric method detection aggregate traffic algorithm infinitely bandit d bengio l bottou editor advance neural information process system page
sparse nonnegative deconvolution compressive calcium image algorithm phase transition center theoretical statistic abstract propose compressed sensing cs calcium imaging framework monitor large neuronal population image randomized projection spatial calcium concentration timestep instead measure concentration individual location develop scalable nonnegative deconvolution method extract neuronal time series observation address problem spatial location neuron use matrix factorization method exploit sparsity neural spike demonstrate number measurement need timestep significantly small total number neuron result potentially enable imaging large population considerably fast rate compare traditional technique traditional cs setup problem involve blockdiagonal sense matrix nonorthogonal sparse basis timestep provide tight approximation number measurement need perfect deconvolution certain class spiking process number undergo phase transition characterize use modern tool relate conic geometry compress sensing introduction calcium imaging method datum acquisition experimental neuroscience record large neural population study structure function neural circuit multiple location dendritic tree examine detailed computation perform level traditional calcium imaging technique involve protocol scan image fashion predetermine pattern random access ramp number measurement timestep equal number voxel interest protocol produce measurement introduce tradeoff size field imaging frame rate large neural population image relatively low temporal resolution situation potentially overcome notice acquire measurement redundant voxel sense neuron locate active voxel nearby location timestep highly correlate neural activity typically sparse neuron spike timestep recent year image develop specialized technique leverage redundancy example describe use spatial light allow simultaneous imaging different predefined image region broadly compress sensing find application imaging mri imaging microscopy et imaging available hardware implementation recently present fluorescence base cs framework measurement obtain projection image random pattern framework lead significant undersampling ratio biological fluorescence image paper propose application imaging framework case neural population calcium image address problem image large neural population high temporal resolution basic idea measure calcium location individually small number mixed measurement base randomized projection datum use convex optimization method exploit sparse structure datum order simultaneously information randomized projection observation effect slow calcium indicator recover spike result indicate number require randomized measurement scale merely number expect spike ambient dimension signal number allow fast monitoring large neural population address problem estimate potentially overlap spatial location neuron location use method nuclear nonnegative matrix factorization method scale linearly experiment length largely ensure computational tractability result indicate calcium imaging potentially scale considerably large neuron population high imaging rate compressive traditional static compressive imaging paradigm sensing matrix dense observation come projection image voxel random underlie image directly sparse voxel sparse orthogonal basis fouri wavelet case sensing matrix blockdiagonal form observe activity specific time measurement sparse basis correspond inverse matrix implement convolution spike calcium indicator nonorthogonal multiple analyze effect distinctive feature noiseless set number measurement increase probability successful recovery undergo phase transition study result phase transition curve ptc ie number measurement timestep require accurate deconvolution function number spike analysis use recent result connect conic geometry statistical dimension sd descent cone demonstrate case interest sd provide good estimate ptc model description approximate inference vogelstein background statistical model calcium imaging datum assume timestep image light field threedimensional observe duration t timestep observe field contain total number d voxel vectorize single column vector activity describe t matrix assume field contain total number neuron general unknown spike cause rapid increase calcium concentration decay time constant depend chemical property calcium indicator neuron assume calcium activity ci describe stable autoregressive process ar process filter neuron spike si t accord procedure describe ci t t discrete time constant satisfy approximate t length timestep continuous time constant calcium indicator general assume si t binary small length timestep propose compressive imaging setting use iid prior let ai rd nonnegative location vector b rd nonnegative vector baseline concentration voxel spatial calcium concentration profile time t describe ai b generalization general process straightforward p simplicity choice merely simplicity general prior distribution incorporate framework conventional experiment timestep observe noisy version ddimensional image t typically large acquisition vector significant time lead timestep t low temporal resolution instead propose observe projection t random matrix entry choose probability yt bt t t t t denote measurement noise gaussian diagonal covariance t simplicity n satisfie d represent compression t potentially obtain quickly f t use statistical method recover t equivalently location ai spike si neuron compress measurement yt total imaging increase factor proportional undersampling ratio assumption random projection matrix construct quickly recent innovation enable fast construction use digital device enable spatial light modulation construct different excitation pattern high frequency order khz total fluorescence detect single detail refer discuss statistical recovery problem future reference note eq write matrix form vec denote operator approximate map inference interior point method assume know general map inference difficult discrete nature follow vogelstein relax continuous value interval remember assume binary spike appropriately modify prior t log si t si t choose prior mean exploit band structure g seek map estimate c instead bt b solve follow convex quadratic problem let act log pc subject c use prior relation c write log solve efficiently use interior point method vogelstein contribution likelihood term hessian blockdiagonal matrix contribute matrix nonzero block diagonal result newton search direction compute efficiently time use block version standard method system linear equation note large inefficient case use augment lagrangian derive fully order method n complexity iteration refer supplementary material additional detail example consider simple setup parameter assume know consider neuron observe t timestep assume b know correspond nonoverlapping point neuron neuron voxel b respectively case know point neuron think compressive ramp microscopy neuron location predetermine image serial manner treat case unknown possibly overlap neuron location section neuron assume fire iid fashion probability timestep p measurement obtain project spatial fluorescence time t random row iid normalize vector entry follow fair bernoulli distribution set measurement assume t true trace estimate spike d true e estimate trace timestep relative error estimate trace timestep measurement timestep figure performance propose algorithm different noise level true trace estimate trace undersampling snr estimate trace undersampling snr true estimate spike trace panel b c randomly select neuron e relative error true estimate trace different number measurement timestep different noise level error decrease number observation reconstruction stable respect noise signaltonoise ratio snr define tn quick calculation reveal log p fig examine solution number measurement timestep varied n different snr value noiseless case noiseless trace neuron panel b reconstructed trace respectively fig d estimate spike case randomly pick neuron small number measurement undersampling infer calcium trace fig b closely resemble true trace infer map value spike compute essentially difference operation lie interior result directly interpretable high temporal resolution increase red estimate spike lie close simple thresholding procedure recover true spike time fig e relative error k kf denote frobenius norm plot general true trace error decrease number observation reconstruction robust noise finally observe noiseless case dash curve error practically indicate fully compress acquisition calcium trace roughly undersampling factor undersampling factor inversely proportional firing rate recover highly sparse spike signal s use measurement estimation spatial matrix algorithm assume underlie neuron know location ie know case estimate apriori run conventional experiment high spatial resolution locate active voxel approach expensive challenge noise possible spatial overlap different neuron estimate compressive framework note spatiotemporal calcium matrix eq write ac number underlie neuron typically general unknown estimate solve nuclear norm penalize problem ft log yt subject t denote nuclear norm matrix ie sum singular value approximation function fazel prior choose similar fashion log pc log gt t rd complex solve efficiently use method solution estimate appropriately thresholde singular value use appropriately constrain nonnegative matrix factorization method alternately estimate note step baseline vector b estimate jointly method nonconvex prone local optima informative initialization important use solution initialize spatial component use clustering method similar method typically use neuronal extracellular spike sort detail supplement discussion estimation parameter problem refer detail true concentration estimate b estimate voxel timestep timestep singular value scaling e baseline estimation true location estimate location true voxel figure estimate location calcium concentration compressive calcium imaging measurement true spatiotemporal concentration b estimate solve estimate use method logarithmic plot singular value solution e estimation baseline vector true spatial location estimate spatial location method estimate number neuron recover spatial temporal component high accuracy fig present application method example spatially overlap neuron simplicity consider neuron onedimensional field total number voxel spatial position fig e timestep obtain noisy measurement use random projection binary mask solution problem fig b threshold singular value fig d estimate number underlie neuron note logarithmic gap large singular value enable separation use approach obtain final estimate spatial location fig baseline vector fig e spatiotemporal concentration fig estimate match true value note compressive image significant undersampling factor possible case classical scan protocol spatial location unknown estimation phase transition curve noiseless case result present indicate reconstruction spike possible significant undersampling section study problem compressed sensing case measurement noiseless simplicity assume similar ramp setup traditional cs setup sparse signal basis sense dense fully support random matrix case sensing matrix b blockdiagonal form standard justification approach proceed establish sense matrix satisfie restrict isometry property rip certain class sparse signal reduce potential shrinkage promote lowrank solution global penalty replace series local penalty spatially overlap patch high probability property turn guarantee correct recovery parameter interest cande tao et signal sparse orthogonal basis rip hold random blockdiagonal matrix number sufficient measurement scale square coherence sparse basis elementary identity basis nonorthogonal basis rip property establish fully dense sense matrix cande signal sparse establish perfect stable recovery condition assumption sensing matrix timestep satisfy certain rip sparsity level timestep know upper bound rip valuable tool study relaxation approach compress sense problem estimate usually constant relatively loose alternative viewpoint offer conic geometric argument chandrasekaran et examine measurement require relax program unique solution coincide true sparse solution use approach study theoretical property propose compressed calcium framework noiseless set noise absent quadratic program approximate map estimate converge linear program minimize c subject c t gt t plp denote kronecker product use identity d examine property plp follow approach fully dense random rotation matrix linear program plp succeed reconstruct true solution c total number measurement satisfie c o descent cone c induce set nonincrease direction c ie c statistical dimension sd cone c rm define expected square length standard normal project m analysis state probability plp succeed find true solution undergo phase transition phase transition curve ptc ie number measurement require perfect reconstruction normalize ambient dimension t donoho coincide normalize case b blockdiagonal matrix gaussian matrix sd provide estimate ptc estimate tight case interest compute statistical dimension use result statistical dimension express expect square distance standard normal vector cone induce true solution c min t general solve closed form easily estimate numerically supplementary material subdifferential form intersection linear half space result distance vector g find solve simple quadratic program statistical dimension estimate simple simulation detail present supplement characterization explain effect sparsity pattern sd case sparse basis illustrate generality approach allow arbitrary nonnegative spike value analysis discuss binary case direct interest compressive calcium framework identity cone induce subdifferential decompose union respective subdifferential cone induce coordinate follow sd invariant coordinate permutation depend sparsity level ie number nonzero element result general true nonorthogonal sparse basis indicate precise location spike sparsity pattern number effect sd case calcium signal sparse nonorthogonal basis describe matrix relation phase transition curve section examine relation sd ptc compressive calcium problem let denote set spike induce calcium trace argue statistical dimension descent cone depend cardinality spike set sparsity level location spike sparsity pattern examine effect sparsity level pattern define normalize expect statistical dimension respect certain distribution spike s draw e df fig examine relation phase transition curve noiseless problem plp consider setup n point neuron d observe t timestep choose discrete time constant neuron come distribution consider different distribution bernoulli spike ie spike probability kt periodic spiking neuron fire deterministically spike discrete frequency timestep random phase consider form spike nonnegative value si t binary value t consider form sense matrix timevarye matrix constant fully support matrix entry draw distribution different condition vary expect number spike neuron t number observe measurement fig empirical probability program plp succeed reconstruct true solution average repetition success declare reconstructed spike signal satisfie plot empirical ptc dash line ie empirical success probability line solid blue line approximate simulation use sample distinct case note sd depend structure sensing matrix b case problem undergo sharp phase transition number measurement timestep vary white region fig recover essentially perfectly transition high probability error black region note phase transition define function sparsity index signal sparsity set datum addition case timevarye provide surprisingly good estimate ptc especially binary case spiking signal actually sparse result justify overall approach use timevarye sense matrix lead result compression possible constant b important result implementation purpose change sensing matrix costly slow operation technical observe follow interesting property periodic spiking require measurement accurate deconvolution property predict sd come fact sparse basis orthogonal fix sparsity level kt sparsity pattern affect number require measurement difference depend time constant problem equivalent standard nonnegative cs problem spike pattern irrelevant spike nonnegative case sd numerically close ptc standard nonnegative cs problem add grow body evidence universal behavior relaxation approach cs donoho binary case result exhibit symmetry axis fact symmetry exact supplement prove result predict sd calculate error exclude timestep spike filter process effect multiple future optimal encoder sense multiple number depend length behavior negligible t spike undersampling index nonnegative spike timevarye periodic spike undersampling index binary spike constant b timevarye statistical dimension empirical ptc sparsity index constant b sparsity index figure relation statistical dimension phase transition curve different spike distribution periodic different spike value nonnegative binary class sensing matrix timevarye constant panel normalize sparsity undersampling index panel empirical success probability empirical success line dash line sd blue solid line b timevarye sd provide good estimate empirical ptc mention analysis approximate blockdiagonal fully dense approximation tight case possible construct adversarial counterexample sd approach fail provide good estimate ptc example neuron fire completely synchronize manner require number measurement grow rate predict present example supplement note research need understand extreme case conclusion propose framework compressive calcium imaging use relaxation tool compressed sensing low rank matrix factorization develop efficient method extract neuron spatial location temporal location spike limited number measurement enable imaging large neural population potentially high imaging rate currently available study noiseless version problem compressed sensing point view use newly introduce tool involve statistical dimension cone analysis certain case capture number measurement need perfect deconvolution help explain effect different spike pattern reconstruction performance approach suggest potential improvement standard scan protocol unknown location efficient ramp protocol know location analysis neglect issue arise practice result fig suggest tradeoff effective compression snr level compressive framework cycle length relax easily parallel nature imaging location target cycle sum activity collect introduce noise nature addition examine practice expect observed snr allow significant compression important issue motion correction movement especially vivo condition new approach derive problem novel approach setting hope work inspire leverage propose advanced signal processing method develop efficient imaging protocol acknowledgement support nsf career award work support aro reference m m functional imaging resolution use microscopy nature method d m m live edge geometric theory phase transition p e brown exact stable recovery sequence signal sparse increment differential l minimization advance neural information processing system compress sense sharp restrict isometry b distribute optimization statistical learning r machine learn alternate direction method multiplier foundation trend b m dendritic discrimination temporal input sequence cortical neuron science cande e p compress sense coherent redundant dictionary apply computational harmonic analysis cande e tao decode linear programming information theory ieee transaction chandrasekaran b p s willsky geometry linear inverse problem foundation computational mathematic r j e p p s threedimensional mapping correlation structure frontier neural circuit donoho j observe phase transition highdimensional geometry implication modern datum analysis signal process transaction royal society mathematical physical engineering science m m r g baraniuk image compressive sample signal processing magazine ieee m matrix rank minimization application d thesis m r compressive spectral imaging architecture opt compressive image apply physics letter m review method spike sort detection classification neural potential network computation neural system rr m donoho j m sparse mri application compressed sensing magnetic resonance medicine r d r microscopy imaging use spatial light frontier neural circuit l j vogelstein l nonnegative spatiotemporal deconvolution calcium imaging datum computational system neuroscience meeting paper preparation m fazel p guarantee solution linear matrix equation r p threedimensional random access functional imaging neuronal activity nature neuroscience convex analysis image stochastic optical reconstruction microscopy nature method m e cande m compressive fluorescence microscopy biological imaging proceeding vogelstein b r l fast nonnegative deconvolution train inference population calcium imaging neurophysiology h l restrict isometry property diagonal matrix information science system annual conference pp
active learning drift distribution abstract study problem active learning setting allow distribution example change time prove upper bound number prediction mistake number label request active learning algorithm realizable case tsybakov noise far prove minimax low bound problem introduction exist analysis active learning base iid assumption datum work assume datum independent allow distribution datum draw shift time target concept remain fix consider problem selective sampling model interested quantity number mistake t example stream number label request example stream particular study scenario distribution drift fix totally bounded family distribution previous model distribution drift bar minimax number mistake excess number mistake noisy case sublinear number sample specifically study classic cal active learning strategy cal context bind number mistake label request realizable case condition concept space family possible distribution exhibit low bound quantity match upper bound certain case far study variant cal analyze number mistake number label request noisy scenario noise distribution remain fix time marginal distribution shift particular upper bind quantity noise condition prove minimax low bound condition gap upper low bound definition notation usual statistical learning problem standard space instance space set c measurable classifier concept space additionally space d distribution distribution space suppose dimension c denote d d let denote total variation set sup range measurable subset let denote minimal cover meaning d d minimal possible size subset d property learning problem sequence distribution d d regular conditional distribution represent function base quantity let t denote infinite sequence independent random variable conditional distribution satisfie pyt joint distribution yt specify pair distribution z specify denote yt label example note conditional distribution restrict discuss drift marginal distribution drift concept drift important interesting topic scope present discussion active learning protocol time algorithm present value require predict label prediction optionally request observe true label value yt means algorithm request label round t define qt qt t pt yt yt cumulative primarily interested quantity m t total number mistake time second quantity interest number label request study expectation time particular e q e m t particularly interested asymptotic q quantity m t m m t m refer dependence t expect number label request m m expect excess number mistake distribution p define probability mistake p conditional probability equal note h m t m q t ie sublinear consider desirable scenario m represent case learn proper way predict label asymptotically use far label passive learn establish condition possible far explore tradeoff quantity additionally use follow notion c let diamt gx h p let c s s finally distribution p r define h r g p r assumption addition assumption independence variable result state additional assumption weak assumption d totally bounded following sense let d denote minimal subset d d minimal cover d d totally bound satisfy follow assumption assumption result interested derive specific rate convergence require strong assumption total boundedness specifically consider follow condition c m constant assumption d c m example class d satisfy total boundedness assumption consider let collection distribution uniformly continuous density function respect measure continuity value value fix realvalued function concrete example l l correspond family lipschitz continuous density function lipschitz constant l case d satisfy assumption relate work discuss active learning distribution drift fix target concept branch literature highly relevant include domain adaptation learning light learn concept drift empirical process independent identically distribute datum active learning fix distribution certain modify active learning achieve mistake bind logt query logt learn linear separator uniform distribution unit sphere bind realizable case analyze problem learn linear separator query form distribution allow tsybakov noise find qt d t m d possible achieve expect excess number mistake m time know work study number mistake query achievable active learn setting distribution change time passive learning drift distribution work learn drift distribution fixed target context passive learning bar study problem learn subset domain randomly choose example probability distribution example change slowly continually learning process upper low bound good achievable probability misclassification number example consider learn problem change environment model slowly change distribution product space allowable drift restrict ensure consecutive probability distribution close total variation distance assumption allow certain choice distribution sequence shift probability mass small small region algorithm uncertain target behavior number mistake grow linearly number sample bad case recently investigate learn distribution change linear function time present algorithm estimate error function use knowledge linear drift active learning realizable case section suppose c fix concept space c fix target function family scenario true collectively refer realizable case begin analysis study realizable case greatly simplify analysis core idea plain form discuss general scenario later section find essentially principle apply initial analysis particularly interested performance follow simple algorithm typically refer cal version present specify term passive learning subroutine mapping sequence label example classifier use notation g use t q let t t t predict max min qt request yt let qt qt yt let yt argmin min qt let qt qt yt let let denote oneinclusion graph prediction strategy specifically passive learning specify follow sequence datum point t oneinclusion graph graph vertex represent distinct labeling u realize classifier vertex adjacent correspond labeling u differ exactly label use oneinclusion graph define classifier base t training point follow t label datum point l yt test point ask predict label construct oneinclusion graph graph edge unique direction way minimize maximum break tie way invariant permutation order point u orient graph way examine subset vertex correspond labeling u consistent l vertex predict correspond label vertex vertex adjacent oneinclusion graph choose edge direct use label corresponding labeling u prediction label subsequent work detailed study oneinclusion graph prediction strategy learn fix distribution begin discussion simple case definition define disagreement coefficient distribution p p sup r r r theorem distribution p p run cal t d logt expect query bind achieve expect mistake bind m p t d log t t logt t completeness proof include supplemental material learn drift distribution generalize result sequence distribution totally bounded space section let d p prove basic result state cal achieve sublinear number mistake condition disagreement coefficient sublinear number query theorem totally bounded assumption cal empirical risk t achieve expect mistake bind m expect number query qt proof mention step equal invariant maintain t induction particular imply qt zt t fix enumerate element p p pd t let break tie arbitrarily let l d ln infinitely t let denote small value t kt l finitely time let denote large index t kt ti index t exist let ti czt t diamt diamkt let sequence l pair pi h let t e diamkt e diamkt e diamkt l classic result choice theory e diamkt combine argument t e e diamkt e diamt t t t t t e diamkt let t nonincrease sequence t t note t e t t t t czt t t follow note h result m t x e m t similarly r e e r e d r max diamt czt d r r d r e diamt czt let t e t diamt t equal t rt e t t state specific result case detailed information size finite cover theorem assumption satisfied cal empirical risk minimization expect number m achieve mistake bind query qt m d t t m m log t m m m log t proof fix enumerate p p pd t let let sequence independent sample e e t t e t t diamt czt t e czt t t classic convergence rate result learn imply x d log t e log log t d t d dd log t m e diamt czt dd d m t m m t m log t e m sup t e t m m t m log m similarly let t t max diamt czt t t e e t t t max czt t m t diamt t t o d t d m t m log additionally construct lower bind scenario follow suppose c contain infinite binary tree classifier tree agree point set point b b h bj bj instance case linear separator natural geometric concept space theorem c active learning set d assumption target function c distribution achieve m m m m m m mt t q proof analogous omit brevity learn noise section extend analysis allow type noise condition commonly study literature need study variant cal refer agnostic cal acal prove upper bound achieve acal minimax low bound noise condition following assumption refer strictly benign noise condition essentially model specify correctly c label stochastic completely random slightly biased assumption sign c x x particularly interesting special case assumption noise condition essentially control common value close formally assumption satisfie assumption t p t c t setting shift distribution interested condition assumption simultaneously distribution d formalize follow assumption assumption satisfied d c value agnostic following algorithm essentially adapt set base subroutine argmin q min q acal t element t let predict h let qt t e qt request yt let qt qt yt l qt let t power qt t l q define follow let algorithm express term function e nonincrease sequence value let denote sequence independent uniform random variable independent datum h v let m h r t h finite set let cl d l q h cl c l c l l q j define l q ut ct l q let z et inf z min ut q learn fix distribution following result essentially follow adapt set theorem strictly benign p acal achieve expect t m p excess number mistake m expect number query qt theorem p satisfy assumption p acal achieve expected p logt excess number mistake mt d log logt p logt p t d expect number query q t log t corollary p satisfy assumption p acal t expect number query t achieve expect mistake m p t m d q t t m learn drift distribution state result concern acal analogous theorem prove early cal realizable case theorem totally bound assumption satisfie assumption acal t m additionally achieve excess expect mistake bind m t d expect number query proof essentially follow combination reasoning theorem proof omit theorem assumption expect excess num achieve m m expect mistake mt t p m d t t log number query q m proof result way similar realizable case include supplemental material immediately follow corollary specific sequence corollary acal achieve expect number mistake m expect number query qt t t m m d t t m m t m m realizable case state minimax lower bind noisy set theorem c active learning set d satisfy assumption conditional distribution assumption satisfied sequence q achieve m distribution t t d t m m m t m t m m t m t m t t m m proof include supplemental material discussion query predict interesting alternative framework allow learner label request label prediction practical perspective desirable case realistic theoretical perspective analysis alternative framework essentially separate mistake mistake recognize uncertainty sense relate model learn analyze procedure alternative model yield interesting detail specifically natural modification cal produce method realizable case number label request mistake request label uncertainty label hand analysis natural modification acal far subtle noise particular version space guarantee contain good classifier high confidence small probability prediction disagree good classifier h round request label control number mistake setting come control probability remove version space confidence parameter appear analysis number query natural tradeoff number mistake number label request particular assumption procedure achieve expect t m expect number query excess number mistake m m logt t d t t q t m logt particular sequence set sequence maintain t m t m open problem imply result sort tradeoff number mistake number query intuitively tradeoff exist cal lack parameter adjust behavior respect tradeoff need different approach address question batch set analogous question tradeoff number label request number unlabeled example need realizable case tradeoff tightly characterize dasgupta index analysis interesting determine index tightly characterize tradeoff setting batch setting unlabeled example consider free performance measure function number label request find important distinction label complexity label complexity particular passive learn provide improvement thing performance measure set concrete follow open problem method class achieve mistake query realizable case reference m network learn theoretical university press bar p l bartlett learn slowly change distribution proceeding fifth annual workshop computational learning theory colt page true sample complexity active learn machine learn r p m long complexity learn drift distribution inf comput cal r improve generalization active learning learn e evendar minimization concept drift colt page s dasgupta coarse sample complexity bound active learning advance information processing system dekel c robust selective sampling single multiple teacher conference learn theory s dasgupta hsu general agnostic active learning computer science s dasgupta analysis active learning machine learn research learn persistent drift proceeding conference computational learning theory page s bind label complexity agnostic active learning proceeding conference machine learn s rate convergence active learn annal statistic littlestone m warmuth predict function randomly draw point information computation light littlestone learn quickly irrelevant attribute new algorithm machine learning l m l littman know know framework learn international conference machine learning m domain adaptation multiple source advance neural information processing system nip page m domain adaptation learn bound algorithm colt mt discrimination analysis annal statistic vapnik estimation dependency base empirical datum s empirical process cambridge series statistical probabilistic mathematic cambridge university press
dataflow architecture flexible platform simulation abstract dataflow architecture general computation engine optimize execution parallel algorithm neural network simulate system certain advantage paper review dataflow architecture examine network performance new generation dataflow machine compare performance simulation alternative discuss benefit drawback dataflow approach dataflow architecture dataflow research conduct mit ai number year dataflow architecture general computation engine treat instruction program separate task schedule asynchronous fashion dataflow program graph explicitly describe datum dependency computation directly execute machine computation link path graph execute parallel machine large number process element hardware optimize reduce task switch overhead minimum computation execute produce result cause follow computation require result schedule manner fine parallel computation achieve limit possible parallelism determine problem number processing element machine dataflow architecture flexible platform neural network simulation figure network dataflow graph neural network dataflow powerful hardware platform neural network simulation enumerate study supercomputer offer programming sequential language great cost array warp massively parallel machine connection machine offer power increasingly reasonable cost require specialized lowlevel programming map algorithm hardware vlsi optical device offer fast implementation fix algorithm dataflow architecture include list good reason use neural network simulation natural mapping neural network dataflow graph use encode dataflow program figure express neural network simulation dataflow program gain datum synchronization parallel execution efficiency architecture provide appropriate fine abstraction close mapping allow simple neural network specification program second ease programming approach extremely flexible good performance new algorithm time run spend additional time determine good way map hardware dataflow simulation particularly appropriate develop new learning algorithm architecture high level language develop machine provide environment neural net combine standard calculation specialized neural network way optimize dataflow architecture neural network simulation device wait h instruction form tag form token network oj ij structure oj memory figure schematic taggedtoken dataflow processor taggedtoken dataflow taggedtoken dataflow approach represent computation product token pass follow computation schematic view taggedtoken processor figure execution proceed cycle achieve synchronization instruction execute wait token operand match occur incoming token contain operand thing happen operation instruction execute result pass operation check operand second arrive location represent instruction tag operand store instruction continue wait second le instruction tag instruction execute token contain result send computation require result schematic view execution network figure taggedtoken dataflow machine illustrate figure transaction parallel computation distribute number physical device large number network transaction represent potential bottleneck taggedtoken dataflow architecture mitigate problem way enhance overall parallel execution time network transaction split phase process request external data value sleep token bear request value return process computation proceed standard approach processor wait result approach allow computation proceed value mask memory network independent thread computation cycle allow maximum parallel execution cycle long parallelism task length processor task large network latency processor consequently massively parallel application neural simulation benefit transaction approach dataflow architecture flexible platform neural network network dataflow simulation illustrate neural network execution dataflow processor network figure code dataflow language run taggedtoken architecture simulator figure operation profile vertical axis represent number processor simultaneously ie parallelism task particular instance horizontal axis represent computation cycle addition figure ideal simulation communication latency time infinite number processor available time ideal profile width represent absolute minimum time dataflow calculation possibly perform term critical path figure execution profile single threshold neuron process input initial peak activity correspond initialization activity late peak actual computation step complexity profile attribute dataflow synchronization mechanism figure ideal execution profile note initialization peak similar appear single neuron profile correspond neuron initialization occur simultaneously illustrate ability dataflow approach automatically expose inherent parallelism overall computation note critical path substantially time single neuron critical path possible dataflow approach perform computation parallel critical path attribute computation delay prior computation available figure represent execution net realistic condition token operation subject finite network delay regular profile correspond effect network delay interesting thing observe overall critical path length increase slightly average parallelism available token come net high dataflow ability interleave computation compensate network latency effect ss figure ideal parallelism profile dataflow execution single threshold neuron unit figure execution network figure taggedtoken dataflow processor black dot represent active token represent wait token shaded box represent enable operation execute dataflow architecture flexible platform neural network simulation figure ideal parallelism profile dataflow execution network la figure parallelism profile dataflow execution constant communication latency cost dataflow approach taggedtoken dataflow machine execute d program perform time instruction execute equivalent program overhead dataflow program mechanism asynchronous parallel execution similar overhead probably exist specialized neural network simulator write dataflow machine overhead justify maximum parallelism computation expose straightforward manner require additional programming effort conventional parallelism selectively problem parallelism increase associate cost increase eventually surpass cost dataflow parallel performance dataflow machine surpass alternative platform overhead early dataflow implementation use tag token approach number practical barrier useful result achieve cost expansion limit associative memory use token matching impractical system prove utility tag token approach recently architecture develop problem encounter tag token architecture problem solve treat token descriptor address global memory space partition processor system matching simple ram operation initial prototype construct processor machine schedule build processor element machine implementation fabricate processor board cycle time process rate total memory processor machine interconnect provide switch network processor machine estimate translate connection second processor connection second machine performance supercomputer class project cost significantly use standard process technology processor machine vlsi processor estimate performance processor machine aggregate neural simulation performance estimate connection second assume interconnect network suitable performance conclusion dataflow architecture cost effective flexible platform neural network simulation widely available general architecture performance exceed specialized neural network architecture maximum parallelism attain simply use dataflow approach machine tuning need dataflow excellent tool empirical simulation excellent performance obtain cost effective hardware special effort require performance improvement dataflow architecture optimize performance possible reference b performance stochastic learning neural information processing system dataflow architecture flexible platform neural network simulation d dataflow architecture mit technical d e k price asynchronous analysis dataflow architecture mit laboratory computer science computation structure group study laboratory nh architecture methodology selforganization stochastic learning analog neural net proceeding ieee international conference neural network ill connection machine cambridge system architecture dataflow supercomputer report computer system division laboratory reference manual computational structure group laboratory computer science s simulation warp speed connection second proceeding ieee international conference m implementation general purpose dataflow electrical engineering computer structure scale proceeding connectionist model history neural network
probabilistic relational dept comp crucial assumption principal component analysis pca probabilistic instance independent identically distribute common iid assumption unreasonable relational datum paper explicitly model covariance instance derive relational information propose novel probabilistic dimensionality reduction method probabilistic relational pca relational datum analysis assumption long adopt prpca learn algorithm prpca devise easily ppca explicit use assumption experiment realworld datum set prpca effectively utilize relational information dramatically outperform pca achieve stateoftheart performance introduction use lowdimensional embed summarize highdimensional datum set widely use explore structure datum method discover lowdimensional embed refer dimensionality reduction method principal component analysis pca popular method great success application recent development probabilistic pca provide probabilistic formulation base gaussian latent variable model compare original derivation ppca possess number practical advantage example ppca naturally deal missing value datum expectationmaximization use learn parameter ppca efficient highdimensional datum easy generalize single model ppca mixture model case furthermore ppca probabilistic model naturally exploit bayesian method exist method pca ppca base assumption datum assumption datum represent feature vector dimensionality datum represent form refer flat data socalled assumption mean instance assume independent identically distribute datum realworld application web page research paper contain relation link instance datum addition textual content information represent form feature vector datum sort refer relational datum find diverse application area web mining bioinformatic social network analysis hand link structure instance paper use document classification running example relational datum analysis convenience illustration specific term textual content information use paper refer feature vector describe instance algorithm derive paper apply relational datum instance feature vector represent attribute information exploit easily traditional method apply relational datum useful relational information simply discard example relation paper provide strong evidence belong topic bear low similarity content sparse nature bagofword representation relational information exploit apply pca ppca possible use relational information ppca convert link structure flat datum extract additional feature link argue approach fail capture important structural information datum hand assumption underlie pca ppca unreasonable relational datum relational datum attribute connected link instance correlate class label instance influence link instance example biology interact protein likely biological function interaction pca ppca generally exist method base assumption suitable relational datum analysis paper novel probabilistic method probabilistic relational pca propose relational datum analysis explicitly model covariance instance derive relational information prpca integrate relational information textual content information unified probabilistic framework learn base closedform solution base propose learn parameter prpca assumption long adopt learn prpca devise easily ppca explicit use assumption extensive experiment realworld datum set prpca effectively utilize relational information dramatically outperform pca achieve stateoftheart performance notation use boldface letter k denote matrix letter z denote vector ith row jth column matrix denote kj respectively denote element ith row jth column denote element inverse mean positive semidefinite mean positive definite pd tr denote trace matrix p q denote kronecker product p q denote determinant matrix identity matrix size e vector dimensionality depend context n multivariate normal distribution matrix variate normal distribution denote expectation operation cov denote covariance note relational datum exist content link observation denote set observe ddimensional datum content vector q matrix w denote principal axis factor loading denote datum sample mean denote correspond q principal component latent variable far use matrix t denote content matrix tn q matrix denote latent variable t relational datum matrix denote adjacency link matrix instance paper assume link undirected datum direct link convert direct link undirected link original physical meaning link describe detail section example section aij exist relation instance aij assume exist pca set stage section introduce prpca model briefly present derivation ppca originally base multivariate normal distribution perspective matrix variate normal distribution use denote process assume latent variable matrix follow distribution express generative model follow t et base property matrix variate normal distribution following result t et t let c correspond loglikelihood observation matrix nh c l s sample covariance matrix content observation easy loglikelihood form use matrix notation graphical model ppca base matrix normal distribution figure p t t model ppca b model prpca figure graphical model ppca prpca t observation matrix latent variable matrix parameter learn quantity constant probabilistic relational pca ppca assume observation independent identically distribute iid assumption modeling process simple achieve great success traditional application assumption unreasonable relational datum relational datum attribute connect link instance correlate section probabilistic relational pca model prpca propose integrate relational information content information unified framework eliminate assumption base reformulation ppca use matrix variate notation present previous section obtain prpca introduce relatively simple effective modification promising property computation need simple ppca eliminate restrictive assumption model formulation assume latent variable matrix follow distribution accord corollary mean actually reflect covariance instance ppca coincide assumption ppca eliminate assumption relational datum direct way use covariance matrix distribution reflect physical meaning semantic relation instance discuss detail later similarly change eliminate assumption noise process relational covariance construction covariance matrix prpca construct relational information datum refer relational covariance goal ppca find principal axis retain variance projection maximal specific retain variance rewrite follow observation px observation ppca large retain variance approach point low probability density prior point refer point goal ppca achieve ie retain variance maximal use retain variance measure define gap different point small gap retain variance point approach design principle prpca similar ppca hypothesis observation guide design relational covariance prpca effectiveness empirically verify section assume attribute link instance positively correlate assumption ideal goal prpca latent representation instance close possible exist relation link measure define gap point refer closeness link instance ie summation euclidean distance link instance base observation approach point low probability density prior latent space representation close link instance low probability density prior prove set typically small positive number appropriate prior note exist relation instance aij express ain denote diagonal matrix diagonal element p aij easy prove let d j mean let b aa d d number path path length instance instance original adjacency graph attribute link instance positively correlated actually reflect degree correlation instance instance let paper citation graph example illustrate existence citation relation paper imply topic paper cite paper cite paper highly likely paper paper topic exist paper k link paper paper confidence paper paper topic increase large bij strong correlation pn instance instance j similarity link vector instance instance weight matrix correspond weight graph derive original adjacency matrix b consistent physical meaning underlie let b find actually combine original graph reflect derive graph reflect b new graph weight aij bij edge instance instance new graph new weight graph reflect consistent physical meaning underlie let d g diagonal p matrix diagonal element j l laplacian matrix g dd define diagonal matrix l kxi kxi link physical meaning direct link web graph transform link satisfy assumption prpca preprocessing strategy strategy webkb datum set example section mean ratio ratio obtain set ain preliminary result sensitive long large omit detailed result scope paper let px q q pn term kxi treat measure weighted variance weight instance latent space large d instance reasonable mainly reflect degree instance graph easy latent representation fix value weight pn kxi close latent representation link entity large contribution subsequently contribution px mean latent space representation close link instance low probability density prior appropriate prior set model construct relational covariance generative model prpca define follow t et far obtain follow result t t graphical model prpca illustrate figure b difference ppca lie solely difference compare find observation ppca sample independently sample correlation fact ppca degenerate case prpca detailed remark assumption hold ie aij prpca degenerate ppca set note role play implementation set small positive value actually set pd t follow singular matrix variate normal distribution derivation prpca correct experiment find performance scope paper ppca set c loglikelihood observation matrix nh c c l pt ln constant independent parameter interesting compare find learn parameter difference prpca ppca lie difference s learn technique derive previously ppca potentially applicable simply substitute h learn set gradient l respect maximumlikelihood estimator mle follow ppca devise method learn base closedform solution base closedform solution theorem loglikelihood maximize l q m m l d eigenvalue q q q diagonal matrix contain q large eigenvalue q matrix q column vector principal eigenvector h correspond q r arbitrary q orthogonal rotation matrix proof use technique similar appendix omit learn process treat parameter missing datum t x complete datum operate alternate estep mstep briefly describe update rule derivation find long version download estep expectation loglikelihood respect distribution miss datum compute compute expectation loglikelihood need compute follow sufficient statistic hxi m et m m note statistic compute base parameter value obtain previous iteration mstep maximize expectation loglikelihood parameter update follow m e note use denote old value w update new value complexity analysis suppose nonzero element computation cost d application typically constant multiple time complexity compute h closedform solution invert d matrix computation cost typically large q computation cost d qt t number iteration datum high dimensionality efficient closedform solution experiment possess additional advantage compare original formulation similar result exist miss value datum task classify instance lowdimensional embed classifier base embed result ppca expect achieve comparable result paper adopt pca baseline study performance prpca use pca initialize initialize closedform solution achieve similar result report result prpca follow experiment datum set evaluation scheme briefly describe datum set evaluation scheme space save detailed information find long version use datum set evaluate datum set webkb adopt strategy datum set data set politicalbook datum set use webkb accord semantic page page link structure datum set follow web page link common web page add link page original link remove preprocesse direct link convert undirected link set contain subset ds pl webkb datum set contain subset adopt strategy evaluate prpca webkb datum set politicalbook data set use testing procedure latent process model evaluation convergence speed use ds datum set illustrate convergence speed learning procedure performance datum set similar characteristic omit average classification accuracy base fold cross validation number iteration t figure achieve promising stable performance small number iteration set t follow experiment visualization use politicalbook datum set visualize result prpca sake set result depict figure easy separate class latent space class separate latent space prpca clustering classification performance expect example cluster classify latent space accuracy t figure convergence speed figure visualization data point latent space learning procedure prpca politicalbook datum set positive negative example red blue circle respectively performance dimensionality webkb high dimensionality politicalbook high evaluate prpca different kind datum verify effectiveness general setting performance webkb average classification accuracy standard deviation base fold cross validation dimensionality latent space q figure find dramatically outperform pca datum set dimensionality confirm relational information informative prpca utilize effectively perform comparison prpca method evaluate method include svm content ignore link structure datum apply svm content information original bagofword representation svm link ignore content information treat link feature ith feature svm linkcontent content feature link feature method combine feature representation direct graph regularization introduce describe linkcontent joint linkcontent matrix factorization method note linkcontent sup adopt comparison procedure linkcontent sup employ additional label ml q pca accuracy accuracy accuracy accuracy pca pca accuracy accuracy accuracy figure comparison pca information employ method directly compare method linkcontent mf method set q prpca result figure prpca linkcontent achieve good performance evaluate method compare linkcontent perform slightly perform slightly bad ml achieve comparable performance datum set conclude overall performance prpca comparable linkcontent mf linkcontent nature naturally support inductive inference specifically apply learn transformation matrix perform unseen test datum linkcontent perform datum available training phase recently method propose regularize matrix factorization achieve performance prpca set similar linkcontent use inductive inference accuracy d ml accuracy svm link figure comparison prpca method performance politicalbook mixed graph gaussian process randomly choose half datum training rest test process repeat round average area curve auc standard deviation report table gaussian process train original feature representation relational gaussian process method use perform gaussian process classifier train base lowdimensional representation set q prpca datum set prpca dramatically outperform pca achieve performance comparable state art note learn lowdimensional embed instance learn lowdimensional embed instance computation cost obtain lowdimensional embed test instance invert matrix define training datum table performance politicalbook datum set result standard deviation report pca acknowledgment support general fund research support program project thank useful comment reference m latent variable model factor analysis kendall library statistic edition c m bishop nip m blei relational topic model document network aistat m blei connection line augment social network text page relational learning process nip theory number regional conference series cohn missing link probabilistic model document content connectivity nip m mccallum learn extract symbolic knowledge world wide web page maximum likelihood incomplete datum royal statistical society l taskar introduction statistical relational learn mit matrix variate distribution chapman h analysis complex statistical variable principal component educational psychology t principal component analysis springer second edition wj dy regularize matrix factorization dy process relational learn aistat page mccallum automate construction internet portal machine learn information r cohen joint latent topic model text citation page c e rasmussen process machine learn ghahramani hide common cause relation relational learn nip b taskar p abbeel discriminative probabilistic model relational datum uai page m e m bishop probabilistic principal component analysis journal royal statistical society series vert reconstruction biological network supervise machine learning approach element computational system biology s bayesian framework community detection integrate content link combine link content community detection discriminative approach page b scholkopf semisupervise learn direct graph nip combine content link classification use factorization sigir
hash hyperplane query near point application active learning computer computer abstract consider problem retrieve database point near hyperplane query scan database propose solution approach map datum twobit binary key localitysensitive angle hyperplane normal database point second approach embed datum vector space euclidean norm reflect desire distance original point hyperplane query use hashing retrieve point sublinear time method preprocesse stage efficient second strong accuracy guarantee apply active learning current hyperplane classifier query algorithm identify point approximately satisfy wellknown minimal selection criterion empirically demonstrate method tradeoff practical perform active selection million unlabeled point introduction efficient similarity search large database central application interest learning image audio retrieval datum compression search problem consider domain point datum database vector list attribute datum object point near novel query vector exist algorithm provide efficient data structure task useful distance function produce exact approximate neighbor force scan database item comparison work consider efficiently handle instance complex point particular little previous work address search problem database point near novel hyperplane query problem critical active learning goal request label point appear informative widely use selection criterion seek point near current support vector machine hyperplane decision boundary substantially reduce total human annotation effort active learn impractical apply classifier unlabeled point round learn exploit massive unlabeled pool fast sublinear time hyperplane search method need end propose solution approximate search introduce randomized hash function offer query time sublinear size database provide bound approximation error neighbor retrieve approach devise twobit hash function localitysensitive angle hyperplane normal database point second approach embed input euclidean distance reflect hyperplane distance exist approximate near neighbor vector datum preprocessing method efficient second method strong accuracy guarantee demonstrate algorithm significant practical impact active learning svm classifier result method help active learning realistic problem massive unlabeled pool order million example relate work briefly review relate work approximate similarity search subspace search method active learn approximate search lowdimensional point spatial decomposition treebased search algorithm provide exact neighbor sublinear time method break highdimensional datum number approximate neighbor method propose work highdimensional input localitysensitive hash lsh method devise randomized hash function map similar point hash bucket subset database search hash novel query related family method design hamming space embedding index efficiently contrast approach technique intend datum researcher recently examine approximate search task involve subspace euclidean embed develop norm embed space directly reflect principal distance original subspace mapping apply exist approximate method design point lsh provide related embed find point near hyperplane contrast provide bound embedding compact propose sampling strategy method find near subspace point query limit relatively low dimensional datum preprocesse requirement d log query time log n number database point dimensionality datum far approach restrict point query finally sublinear time method map line query near point derive contrast work propose specialized method hyperplane search problem handle highdimensional datum large database efficiently active learning exist active classifier learning method selection generally scan database instance select label wellknown effective active selection criterion support vector machine svms choose point near current separate hyperplane simple criterion intuitive theoretical basis term rapidly reduce version space widely use practice unfortunately inexpensive selection function large unlabeled dataset cost search pool impractical researcher previously attempt cope issue clustering randomly downsample pool strategy provide guarantee potential loss active selection quality contrast apply approach task consider order magnitude point active label request guarantee selection known error traditional exhaustive technique form approximate svm training avoid potential confusion note problem set differ consider computational geometry insight combine qp formulation efficient core vector svm training consider subset label datum point select online training consider specific hyperplane criterion paper active learning survey approach consider follow retrieval problem database point rd goal retrieve point database close hyperplane query normal rd near neighbor query problem loss generality assume hyperplane pass origin unit norm later section assumption affect solution euclidean distance point hyperplane parameterize kxt w goal nnqh problem identify point xi d minimize contrast traditional proximity problem near goal maximize respectively exist approach directly applicable problem formulate algorithm approach map datum binary key localitysensitive angle hyperplane normal database point permit sublinear hash second approach compute sparse euclidean embed query hyperplane map desire search task handle exist approximate method following provide necessary background localitysensitive hashing lsh subsequent section describe approach turn review tradeoff finally explain method apply active learning background localitysensitive hashing lsh lsh require randomized hash function guarantee probability vector inversely proportional distance distance define accord task hand similar point fall hash need search database item novel query hash table formally let distance function item set s item p let r denote set example radius r p definition let denote random choice hash function family family r r p p sensitive q p s p r q hh p p p r q hh p p family function useful satisfy p p lsh function compute hash key concatenate bit return random sampling gp h p hh p hh p note probability close point pk point preprocessing stage database point map series l hash table index independently construct function query q exhaustive search carry example union l bucket q hash candidate contain r near neighbor q meaning neighbor radius r high probability example radius r find lsh scheme use projection single coordinate localitysensitive log p hamming distance vector hash p use hash table approximate solution retrieve time relate formulation lsh function distance explore contribution define localitysensitive hash function nnqh problem hyperplane hashing base angle distance hhash recall want retrieve database vector minimize vector unit norm mean good close database vector perpendicular let denote angle w define distance definition reflect far perpendicular w d consider follow twobit function map input vector d hv t b return ut u v sample independently standard ddimensional gaussian define hyperplane hash hhash function family h database point query hyperplane vector prove family hash function localitysensitive definition claim family r r r r sensitive distance r proof vector u v use hash function sample independently vector database point vector hh hv w w hv use following fact prove sample define ac denote angle vector c use hh r hh r p similarly hh r p note traditional lsh function asymmetric hash database point use x hash query hyperplane use purpose twobit hash constrain angle respect w simply retrieve example know away function hand form hash key concatenate twobit pair hash function h store database point hash table query novel hyperplane retrieve close point approximation guarantee correctness scheme obtain adapt proof supplementary file particular high probability lsh scheme return point distance r d xi time log p p p approach sublinear time value r furthermore p r log log r r p r bound note bind dependent r efficient large value r supplementary material discussion bind embed hyperplane hashing base euclidean distance second approach nnqh problem rely euclidean embed hyperplane point offer strong bound expense preprocesse ddimensional vector compute embedding inspire yield dimensional vector vectorize corresponding rank matrix ad ad ai denote element b unit vector euclidean embedding b b b minimize distance embedding equivalent minimize intend function define hash function family e database point z query hyperplane vector hash function parameterize u claim function e define p r sin r sin r sensitive r proof use result vector rd x pr sign ut sign ut wk rd sample standard variate gaussian distribution note unit vector rd b b use definition hyperplane query database point r sin r p p obtain similarly observe p behave similarly r r p return value close return p plot illustrate supplementary file p factor log p improve previous section remain low low value lead approximation guarantee supplementary material detailed comparison bound hand hash function significantly expensive compute specifically require time hhash require od alleviate problem use form randomized sampling compute hash bit query reduce time method rely follow lemma state sample vector v accord weight element lead good approximation t vector constant probability similar sample scheme use variety approximation problem let rd define pi vi construct rd element vi probability pi select t element use sample replacement rd pr t c defer proof supplementary material imply query time hash function compute incur small additive error time sample embed v w accordingly cycle nonzero index compute note substantially reduce error hash function t embed sample o element vector use w case computational requirement increase o alternatively use reduce dimensionality embedding random projection major difficulty dimensionality subspace represent hyperplane imply random projection dimensionality large hold second projection dimension dependent sum number database point query hyperplane problematic field arbitrary number query time store grow database property intrinsic target active learning application contrast sample method incur little overhead compute hash comparison et define embedding find near subspace particular define euclidean embedding affine subspace query database point use nnqh specifically apply search work embedding tie lsh bound term distance function finally propose sample strategy offer compact representation advantage discuss hashing approach summarize present localitysensitive hashing approach nnqh problem hhash approach define context provide suitable twobit hash function bind time second consist dimensional euclidean embed vector dimension d turn reduce euclidean space near neighbor problem efficient search structure include lsh available bound hhash hash function expensive mitigate expense highdimensional datum use heuristic randomly sample query embed reduce query time linear d note approach attempt minimize retrieve distance dependent angle scaling vector effect method treat provide vector unit norm application active learning search algorithm introduce apply task fit specification especially interested relevance active learning scalable practical active learning intend reduce learning time choose informative example label real expense apply large unlabeled dataset generally method today test somewhat scenario sized label dataset simply label learner point select point oracle reveal label reality like deploy active learner massive truly unlabeled pool document web let instance appear valuable target classification task problem scan million point expensive compute purpose improve overall learn algorithm possible benefit massive unlabeled collection actively choose label request consider simple margin selection criterion classifier hyperplane classifier unlabeled pool vector datum point minimize distance current decision boundary select label nnqh solution supply exactly function need rapidly identify point label hash unlabeled database table active learning loop hash current classifier w query bias term handle point note approach assume linear kernel h m h b e e selection iteration r ex distance h time sec log scale improvement selection time exhaustive h h learn curve figure newsgroup result improvement prediction accuracy relative initial classifier average category run time require perform selection c value x select example lower approximate method hhash significantly outperform passive baseline nearly accurate ideal exhaustive active selection require order magnitude time select example view learn curve class selection time distance e ex h h h m h e h r selection iteration h exhaustive time sec log scale improvement figure cifar result ac plot figure method compare significantly expensive exhaustive baseline provide accurate selection hhash c require query time result demonstrate approach apply active learning task compare method hhash baseline passive learn label request randomly select exhaustive active selection margin criterion compute unlabeled example order find true minimum main goal algorithm retrieve example nearly exhaustive approach substantially great efficiency dataset implementation detail use publicly available dataset newsgroup consist document newsgroup category use provide d bagofword feature test set cifar consist image category manually label subset tiny image dataset form search web english noun lack ground truth label use provide train test split image respectively consist unlabeled image cifar use provide d descriptor feature dataset train linear svm setting use randomly select label set example class run active selection iteration average result run fix newsgroup document result figure result newsgroup start learn curve approach active learner exact approximate steepest curve indicate learn effectively choose label compare random baseline hashing method perform similarly exhaustive selection require scan order magnitude example b note random require time actual value select example iteration category run line method guarantee select point close find exhaustive search observe expect tradeoff hhash efficient provide result slightly small dataset cifar tiny image result figure set result cifar trend similar learning task difficult datum narrow hhash exhaustive random category newsgroup exhaustive selection labeling time sec improvement improvement category exhaustive selection labeling time sec figure example select method learn cifar class b improvement prediction accuracy function total time include selection label time minimize selection labeling time method provide good accuracy unit selection time distance hyperplane e h e h r m h time sec log scale b figure result error example select b time require c example select m candidate iteration learn class margin active random average class happen outperform exhaustive selection fig happen guarantee good active choice help test accuracy reflect wide variation result directly hashing method behave expect b illustrate tradeoff strong guarantee hhash retrieve low wt x value expensive figure example image selection result exhaustive search hashing method choose image useful learn figure b prediction accuracy plot total time iteration include selection labeling time dataset set labeling time instance second newsgroup tiny image dataset respectively note vary practice depend difficulty instance result advantage approximate method account type cost inherent train classifier outperform exhaustive random selection term accuracy gain unit time exhaustive active selection suffer large selection time random selection suffer waste expensive labeling time irrelevant example provide good accuracy gain minimize selection labeling time result finally demonstrate practical capability hyperplane hashing approach perform active selection tiny image set initialize example cifar m set lack label live test active learn annotate method select use method offer strong performance massive collection method selection similar quality exhaustive method fig require order magnitude time b image selection large pool live labeling test unlabeled example nearly likely belong class method retrieve seemingly relevant instance knowledge experiment exceed previous active selection result literature term scale unlabeled conclusion introduce method nnqh search problem permit efficient largescale search point hyperplane experiment dataset clearly demonstrate practical value active learning massive unlabeled pool future work plan far explore accurate hhash scheme investigate sublinear time method nonlinear base active learning work support foundation reference algorithm find good match expect time acm transaction mathematical software satisfy general proximity similarity query metric tree information process letter p r similarity search high dimension hashing proceeding th large datum basis p nearoptimal hashing algorithm near neighbor problem high dimension foc m similarity estimation technique round algorithm stoc weiss r fergus spectral hashing nip localitysensitive hashing scalable image search proceeding ieee international conference computer vision iccv s tong vector machine active learn application text classification international conference machine learn cohn active learning support vector machine international conference machine learn cristianini smola query learn large margin classifier international conference machine learn p viola darrell fast pose estimation hashing proceeding ieee international conference computer vision iccv r salakhutdinov semantic hashing proceeding sigir workshop information application graphical model r t l approximate near subspace search pami dimensionality reduction preserve volume distance affine space algorithmic application randomization approximation technique computer science p r l nguyen approximate line near neighbor high dimension settle active learn literature survey e active learn image retrieval ieee transaction m m c active learning support vector machine drug discovery process inf comput l bottou fast classifier online active learning machine learn research jmlr active learning large image database tool application special issue computer vision meet database long e scalable algorithm active learning frontier algorithmic r t w fast uncertainty sample label large email conference email machine fast svm training large datum set machine learn research p fast image retrieval embedding stat comp theory vision m williamson improve approximation algorithm maximum cut problem use semidefinite programming r s spectral algorithm foundation trend theoretical computer science krizhevsky learn multiple layer feature tiny image technical report r fergus freeman tiny image large dataset object scene recognition pami
learn deep compact image representation visual tracking computer science abstract paper study challenging problem track trajectory object video possibly complex background contrast exist tracker learn appearance track object online different approach inspire recent advance deep learning architecture emphasis unsupervised feature learn problem specifically use auxiliary natural image train stack denoise autoencoder offline learn generic image feature robust variation follow knowledge transfer offline training online tracking process online tracking involve classification network construct encoder train autoencoder feature additional classification layer feature classifier far tune adapt appearance change object comparison stateoftheart tracker challenge benchmark sequence deep learning tracker accurate maintain low computational cost realtime performance matlab implementation tracker use modest graphic processing unit introduction visual tracking object tracking refer automatic estimation trajectory object video numerous application domain include video interaction sport video analysis certain application require multiple object track typical setting treat object separately object track identify manually automatically video frame goal visual tracking automatically track trajectory object subsequent frame exist computer vision technique offer satisfactory solution problem environment problem challenge practical application factor partial occlusion background fast abrupt motion dramatic illumination change large variation viewpoint pose exist tracker adopt generative discriminative approach generative tracker generative model machine learning assume object track describe generative process track correspond find probable candidate possibly infinitely motivation generative tracker develop image representation facilitate robust tracking inspire recent advance fast algorithm robust estimation sparse code alternate direction method multiplier admm accelerate gradient method popular generative tracker include incremental visual tracking represent track object base principal component analysis pca l tracker assume track object represent sparse combination overcomplete basis vector extension propose hand discriminative approach treat track binary classification problem learn explicitly distinguish object track background representative tracker category online adaboost tracker multiple instance learn mil tracker structure output tracker generative tracker usually produce accurate result complex environment rich image representation use discriminative tracker robust strong occlusion variation explicitly background consideration refer reader recent paper empirically compare exist tracker base common benchmark learning perspective visual tracking challenge label instance form identify object video frame subsequent frame tracker learn variation track object unlabeled datum available prior knowledge object track easy tracker drift away target address problem tracker semisupervise learning approach propose alternative approach learn dictionary image feature sift local descriptor auxiliary datum transfer knowledge learn online tracking issue exist tracker use image representation good robust tracking complex environment especially case discriminative tracker usually emphasis improve classifier image feature use tracker simply use raw pixel feature attempt use informative feature feature histogram feature local binary pattern feature handcraft offline track object recently deep learning architecture use successfully promising result complicated task include image classification speech recognition key success use deep architecture learn rich invariant feature multiple nonlinear transformation believe visual tracking benefit deep learning reason paper propose novel deep learning tracker robust visual tracking attempt combine generative discriminative tracker develop robust discriminative tracker use effective image representation learn automatically key feature distinguish exist tracker use stack denoise autoencoder learn generic image feature large image dataset auxiliary datum transfer feature learn online tracking task second previous method learn feature auxiliary datum learn feature far tune adapt specific object online tracking process use multiple nonlinear transformation image representation obtain expressive previous method base represent object require solve optimization problem previous tracker base sparse code significantly efficient suitable realtime application particle filter approach visual tracking particle filter approach commonly use visual tracking statistical perspective sequential monte importance sampling method estimate latent state variable dynamical system base sequence observation state observation variable respectively time mathematically object track correspond problem find probable state time step t base observation previous time step argmax pst yt z argmax pst st pst yt new observation yt arrive posterior distribution state variable update accord baye st pst yt pst yt yt specific particle filter approach approximate true posterior state distribution pst yt set sample particle corresponding importance weight wit sum particle draw importance distribution yt weight update follow wit wit qs choice importance distribution yt simplify firstorder markov process state transition independent observation consequently weight update wit wit pyt note sum weight long equal weight update step case small threshold resample apply draw particle current particle set proportion weight reset weight weight sum threshold linear normalization apply ensure weight sum object track state variable usually represent affine transformation parameter correspond translation scale aspect ratio rotation skewness particular dimension model independently normal distribution frame tracking result simply particle large weight tracker adopt particle filter approach main difference lie formulation observation model apparently good model able distinguish track object background robust type object variation discriminative tracker formulation set probability exponentially relate confidence classifier output particle filter framework dominant approach visual tracking reason general kalman filter approach gaussian distribution approximate posterior state distribution set particle instead single point mode visual tracking property easy tracker recover incorrect tracking result use particle filter visual tracking find recent work far improve particle filter framework visual tracking tracker present tracker offline training stage unsupervised feature learning carry train sdae auxiliary image datum learn generic natural image feature apply sdae finetune online tracking process additional classification layer add encoder train result classification neural network detail provide rest section offline training auxiliary datum dataset preprocesse use tiny image dataset auxiliary datum offline training dataset collect web provide noun search engine cover object scene find real world tiny image size randomly sample image offline training stateoftheart tracker include empirical comparison use grayscale image convert sample image grayscale method use color image directly necessary consequently image represent vector dimension correspond pixel feature value dimension linearly scale range far preprocessing apply learn generic image feature stack autoencoder basic building block sdae neural network denoise recent variant conventional autoencoder learn recover data sample corrupted version robust feature learn neural network contain bottleneck hidden layer unit input unit architecture let total k training sample sample let denote original data sample corrupted version corruption mask corruption additive noise network weight let w denote weight encoder decoder respectively tie necessary similarly b b refer bias term dae learn solve following regularize optimization b parameter balance reconstruction loss weight penalty term denote norm nonlinear activation function typically logistic sigmoid function hyperbolic tangent function reconstruct input corrupted version dae effective conventional autoencoder discover robust feature prevent autoencoder simply learn identity mapping far enhance learn meaningful feature sparsity constraint impose activation value hide unit logistic sigmoid activation function use output unit regard probability active let denote target sparsity level unit average empirical activation rate crossentropy introduce additional penalty term k m h log j m number hide unit pretraine phase sdae form feedforward neural network network finetune use classical backpropagation algorithm increase convergence rate simple momentum method advanced optimization technique conjugate gradient method apply network architecture use overcomplete filter layer choice find overcomplete basis usually capture image structure line mechanism v visual cortex number unit reduce half new layer add hide unit serve bottleneck autoencoder structure sdae depict fig b far speed pretraine layer learn localized feature divide tiny image patch upper leave upper right lower leave low right center overlap train dae hide unit initialize large dae weight small dae train large dae normally randomly select filter layer expect filter play role highly localize edge detector online tracking process object track specify location bounding box frame negative example collect background short distance object sigmoid classification layer add encoder sdae obtain offline training overall network architecture fig c new video frame arrive draw particle accord particle filter approach confidence pi particle determine simple forward pass network characteristic approach computational cost step low high accuracy b c figure key component network architecture denoise autoencoder b stack denoise autoencoder c network online tracking figure filter layer learn sdae maximum confidence particle frame predefined threshold indicate significant appearance change object track address issue network tune case happen note threshold set maintain tradeoff small tracker adapt appearance change hand large object background track object lead drift target experiment empirically compare stateoftheart tracker section use challenge benchmark sequence tracker mtt ct mil late variant tld use original implementation tracker provide author case tracker deal grayscale video function provide image processing toolbox use convert color grayscale accelerate computation utilize computation provide parallel computing toolbox offline training online track code supplemental material provide project page implementation detail use gradient method momentum optimization momentum parameter set offline training sdae inject variance generate corrupted input set minibatch size online tuning use large value avoid overfitting small minibatch size threshold set particle filter use particle parameter affine parameter particle filter search window size method perform grid search determine good value setting apply method compare applicable quantitative comparison use common performance metric quantitative comparison success rate error let t denote bounding box produce tracker groundtruth bound box video frame tracker consider successful overlap percentage error define euclidean distance pixel center t quantitative comparison result summarize table row correspond video sequence good result red second blue report error frame video sequence tld report track object miss frame exclude error comparison average good accord performance metric video sequence good method list run time sequence detail table thank advance technology tracker achieve average frame rate frame second sufficient realtime application car car woman animal shake singer surfer bird average table comparison tracker video sequence number denote success rate percentage number parenthesis denote error car car woman animal shake singer surfer average table comparison run time video sequence car frame number animal center error center error frame number shake frame number singer frame number surfer center error center error woman center error center error center error center error center error center error car frame number bird frame number frame number frame number frame number frame number figure comparison tracker video sequence term error pixel qualitative comparison fig key frame bounding box report tracker video sequence detailed result complete video sequence find supplemental car car sequence track object car open road car challenge illumination change greatly car environment dark illumination background car track object shape change generative tracker mtt generally perform sequence track car accurately sequence tracker track face outdoor environment respectively sequence challenge illumination pose vary drastically video rotation occur frame consequence tracker drift fail different degree generally speak mtt yield good result follow woman sequence track woman walk woman severely occlude time car tld fail frame pose change tracker compare fail woman walk close car frame follow target accurately animal sequence target fast animal motion blur method merely track target end mil tld fail frame tld similar object background singer sequence recording stage illumination change shake pose head track change tld totally fail frame mtt mil drift effect satisfactory result follow compare shake singer sequence easy track tracker mtt track object ct mil support scale change result satisfactory surfer sequence goal track head surfer pose change video sequence tracker merely track tld incorrect scale drift slightly bird sequence challenging pose bird change drastically tracker fail drift frame exception tld bird turn tld totally fail ct mil recover degree track bird accurately entire sequence discussion propose method similar spirit key difference worth note learn generic image feature large general dataset small set choose image category second learn image feature raw image automatically instead rely sift feature far learning allow online tracking process method adapt specific object track choice deep network architecture note potential candidate popular convolutional network model result tracker similar previous patch fragment base method robust partial occlusion current research focus learn feature task image classification object detection nature object tracking different learn feature overcome drift problem little relevant work possible exception try improve pool step sparse code literature address issue interesting future research direction pursue remark paper successfully deep learning new challenging application note key success deep learning architecture learning useful feature train stack denoise autoencoder use auxiliary natural image learn generic image feature alleviate problem label datum visual tracking application offline train encoder sdae use feature car car singer shake surfer bird figure comparison tracker video sequence term bounding box report online tracking process train classification neural network distinguish object background regard knowledge transfer offline training use auxiliary datum online tracking tuning allow online tracking process feature classifier adapt appearance change object quantitative qualitative comparison stateoftheart tracker challenge benchmark sequence demonstrate deep learning tracker encourage result low computational cost work apply deep neural network visual tracking opportunity remain open research discuss interesting direction investigate classification layer current tracker linear classifier simplicity extend powerful classifier discriminative tracker provide room performance improvement research support general fund research reference robust tracking use integral histogram cvpr page m particle filter online bayesian tracking ieee transaction signal processing m robust object tracking online multiple instance learn ieee transaction pattern analysis machine intelligence c real time robust l tracker use accelerate proximal gradient approach cvpr page doucet freitas sequential method h m realtime tracking online page c semisupervise online boost robust tracking page p structured output track kernel iccv page practical guide train restricted boltzmann machine neural network trick trade page v deep neural network acoustic modeling speech processing magazine visual tracking adaptive structural local sparse appearance model cvpr page z learn bootstrappe binary classifier structural constraint cvpr page z ieee transaction pattern analysis machine intelligence krizhevsky classification deep convolutional network nip page visual tracking decomposition cvpr page robust visual tracking use minimization iccv page b olshausen field sparse code overcomplete basis set strategy employ vision incremental learning robust visual tracking computer vision r fergus freeman tiny image large datum set nonparametric scene recognition ieee transaction pattern analysis machine intelligence larochelle stack denoise autoencoder learn useful representation deep network local denoise criterion learn research object track sparse prototype ieee transaction image process m transfer visual prior online object track ieee transaction image process object track benchmark tracking eccv page t sparse learn robust visual tracking eccv page t robust visual tracking sparse learning cvpr page
fast algorithm invariant independent component analysis computer science engineering computer science computer science abstract performance standard algorithm independent component analysis quickly deteriorate addition gaussian partially common step typically consist whiten ie apply principal component analysis pca rescale component identity covariance invariant paper develop practical algorithm independent component analysis provably invariant gaussian noise main contribution work follow develop implement efficient gaussian noise invariant decorrelation quasiorthogonalization algorithm use hessian cumulant function propose simple efficient fixedpoint compatible quasiorthogonalization usual whitening noiseless case algorithm base special form gradient iteration different gradient descent provide analysis algorithm demonstrate fast convergence follow basic property cumulant present number experimental comparison exist method superior result noisy datum competitive performance noiseless case introduction related work blind signal separation set assume observe datum draw unknown distribution goal recover latent signal appropriate structural assumption prototypical setting socalle party problem room people speak simultaneously microphone microphone capture superposition voice objective recover speech individual speaker simple modeling assumption consider speaker produce signal random variable independent superposition linear transformation independent time lead follow formalization observe sample random vector x distribute accord equation b linear mix matrix rd constant vector latent random vector independent coordinate unknown random noise independent s simplicity assume rdd square rank latent component view contain information describe observe signal voice individual speaker party set goal independent component analysis approximate matrix order recover latent signal practice method ignore noise term leave simple problem recover mixing matrix observe arguably widely use algorithm base step process data center whiten identity covariance matrix typically use principal component analysis pca rescale appropriate component noiseless case procedure orthogonalize rescale independent component recover unknown orthogonal matrix r recover orthogonal matrix r practical ica differ second step objective function use perform projection pursuit style algorithm recover column r time use base technique simultaneously recover column r step affect addition gaussian noise noise white scalar identity covariance matrix whiten procedure long guarantee whitening underlie independent component second step process long justify failure significant noise white likely case practical situation recent theoretical development consider case noise arbitrary necessarily white additive variable draw independently observe certain technique apply second step underlie signal latent signal quasiorthogonalization significantly restrictive condition force underlie signal identity covariance whiten noiseless case noisy set usual pca achieve quasiorthogonalization whiten mixed signal underlie component quasiorthogonalization achieve way method base fourthorder tensor direct implementation method require estimate fourthorder cumulant tensor computationally challenge relatively low dimension paper derive practical version base directional hessian fourth univariate cumulant reduce complexity dependence data dimensionality d allow fully vectorize implementation develop fast simple gradient iteration compatible quasiorthogonalization step convergence order r implement use univariate cumulant order r cumulant order commonly use practical application obtain cubic convergence convergence rate follow directly property cumulant shed light somewhat surprising cubic convergence fourthorder base method update step complexity d number sample total algorithmic complexity d step t step t number iteration convergence interestingly technique different gradient iteration turn closely relate fast noiseless setting case datum whitened cumulant order use view generalization simplification fast general datum present experimental result superior performance case datum contaminate competitive performance clean datum note algorithm fast practice allow process decorrelate detect independent process orthogonalize latent signal later conflict definition require latent signal whitened avoid confusion use term quasiorthogonalization process orthogonalize latent signal component point dimension second standard computer matlab implementation available download finally observe method partially compatible robust cumulant introduce briefly discuss extend use technique reduce impact sparse paper organize follow section discuss relevant property cumulant discuss result prior work allow quasiorthogonalization signal fourth cumulant section discuss connection fourthorder cumulant tensor method quasiorthogonalization discuss section technique use connection create computationally efficient practically version quasiorthogonalization algorithm discuss section section discuss new fast style algorithm second step compatible quasiorthogonalization order simplify presentation state abstract form exact knowledge require distribution parameter section discuss estimator require distribution parameter use practice section discuss numerical experiment demonstrate applicability technique relate work independent component analysis refer broad range address blind signal separation problem variant extension extensive literature signal processing machine learn community applicability variety important practical situation comprehensive introduction book paper develop technique deal noisy datum introduce new efficient technique quasiorthogonalization subsequent component recovery quasiorthogonalization step introduce author propose case fourth cumulant independent component sign general complete theoretical analysis provide require estimate fourthorder note hessian base technique use use hessian fourthorder cumulant paper propose interesting randomized step base cumulant generating function fourth cumulant respectively primarily theoretical setting gradient iteration propose closely relate work provide gradientbased algorithm derive fourth moment cubic convergence learn unknown setting special case fourth cumulant idea gradient iteration appear context different justification equation theorem note work develop method assumption noise parameter know finally paper consider problem perform pca noisy framework provably robust algorithm sparse noise model perform pca robust perform pca robust sparse noise use cumulant orthogonalize independent component property cumulant cumulant similar moment express term certain polynomial moment cumulant additional property allow independent random variable separate interested fourth order multivariate cumulant univariate cumulant arbitrary order denote fourth order cumulant tensor random vector variable xk alternatively denote xk cumulant tensor symmetric ie invariant permutation index multivariate cumulant follow property write case fourth order cumulant xk xk random scalar random variable xk xk independence independent random variable xk independent vanish gaussian cumulant order variable order cumulant mean second order multivariate cumulant covariance matrix denote r univariate cumulant equivalent r time r appear r time univariate additive independent random variable ie r r r y homogeneous degree r r r r x quasiorthogonalization use cumulant tensor recall original notation b generative ica model define operation fourthorder tensor matrix m rdd qm matrix d x d ij use operation orthogonalize latent random signal definition matrix quasiorthogonalization matrix exist orthogonal matrix r diagonal matrix rd need follow result use aq denote th column let rdd arbitrary matrix m diagonal matrix entry m suppose component nonzero fourth cumulant let m let c diagonal matrix entry particular c positive definite factorization t b quasiorthogonalization matrix quasiorthogonalization use cumulant hessian method use observe datum method require estimation term connection cumulant technique use technique quasiorthogonalization describe allow rewrite use series hessian operation connection precise hessian version require term estimate datum simplifie computation consist matrix vector operation let denote hessian operator respect vector u rd follow lemma connect hessian method operation special case discuss section aq diagonal entry rewrite aq compare apply symmetric rank matrix rewrite term hessian operation ut formula extend arbitrary symmetric matrix follow let m symmetric matrix decomposition u pd u diag d m ui matrix m symmetric method quasiorthogonalization rewrite use hessian operation algorithm gradient iteration precede section discuss technique datum section assume quasiorthogonalization accomplish discuss approach quickly recover direction independent component let quasiorthogonalization matrix define w note noise exist rotation matrix r diagonal matrix rd let ds coordinate independent random variable recover scaling matrix impossible aim recover rotation matrix r algorithm generate quasiorthogonalization matrix function m pd t let m equation estimator let u let c equation estimator factorize c return end function recovery impossible note independent component impossible distinguish case signal ie case additive gaussian s draw independently noisefree set latent signal typically assume identity covariance place scaling information column presence additive gaussian noise recovery scale information impossible latent signal follow idea discuss technique recover column r time fast recovery single independent component approach function fix act directional vector u rd base criterion typically maximization minimization iterative optimization step perform convergence technique consider fast follow reason approximate newton require computation estimate iterative step estimate computation run time number sample iterative step local quadratic order convergence use arbitrary function global convergence use fourth cumulant note cubic convergence rate unique use gradient descent correct stepsize choose fourth moment propose comparable term computational complexity iterative step conceptually simple form rely r provide derivation fast convergence rate rely entirely property cumulant cumulant invariant respect additive gaussian noise propose method admissible standard noisy cumulant essentially unique homogeneity property restriction probability space preprocessing step additional structure orthogonality center provide additional admissible function particular design robust cumulant effect sparse noise robust cumulant version homogeneity property consistent update step reason state result great generality let function univariate random variable satisfy r homogeneity noisy case vanish gaussian property cumulant generic choice input vector demonstrate order r convergence particular obtain quadratic convergence obtain cubic convergence help explain true r ri consider happen basis column r multiplicative constant coordinate raise power renormalize step ultimately lead order r theorem unit vector input arg maxi ri r unique answer v order r convergence sign particular follow condition meet exist coordinate random variable choose uniformly random unit sphere s d converge column r sign surely convergence order r fast algorithm recover single column r v draw unit sphere equation provide base estimate use practical choice real datum function repeat convergence return end function algorithm presence recover column order scale matrix observe random vector function robust m r column draw v column uniformly r column r column end construct matrix r use element r column column return end function convergence sign include possibility alternate step occur r odd space limitation proof omit recover independent component corollary theorem corollary suppose r r rk know d suppose exist k draw uniformly random rk d denote unit sphere rd input converge new column r surely indexing r arbitrary corollary solution practice require theory enforce orthogonality column r orthogonalize v previously find column r end step expect fourth cumulant function typically choose g time complexity analysis estimation cumulant implement algorithm require estimation function datum limit discussion estimation fourth cumulant low order cumulant statistically stable estimate high order cumulant useful distribution si symmetric distribution plausible recover column r symmetric use g alternatively fall detect denote z n observe sample random variable sample cumulant estimate unbiased fashion denote z sample estimate r let z sample central moment n m n m z m k z fourth interested estimate gradient hessian cumulant cumulant follow lemma obtain unbiased estimate let ddimensional random vector finite moment order r let sample let nd u unbiased estimate r u z sample mean observe random variable result estimate n u xi u u u n use estimate datum implement algorithm result quasiorthogonalization algorithm run time use estimate choose respectively implement update step run d time t bound number iteration convergence step require recover column r quasiorthogonalization achieve simulation result figure compare algorithm baseline version use code available author choice contrast function baseline run use default setting test use generate datum implement algorithm available opt enforce orthogonality update step previously find column r figure comparison distribution indicate independent coordinate generate distinct distribution laplace distribution distribution parameter degree freedom exponential distribution continuous uniform distribution distribution symmetric generate datum generate random mixing matrix condition number minimum singular value maximum singular value intermediate singular value choose uniformly random noise magnitude indicate strength additive define noise magnitude mean variance noise noise indicate variance respectively performance measure use amari index denote approximate matrix return introduce let b pn pn m let m amari index e j ij pn pn amari index value dimensionality d roughly view distance m near scale permutation matrix p d p permutation matrix diagonal matrix datum quasiorthogonalization require datum whiten order provide accurate result sufficient data provide fourth order method perform comparably difference comparison distribution d datum comparison distribution noise magnitude white number sample comparison distribution data amari index white white index amari index white number sample number sample comparison distribution noise magnitude white number sample comparison distribution noise comparison distribution noise magnitude amari index index number sample number sample figure comparison algorithm level noise white refer choice step baseline algorithm use whiten report index denote mean amari index run different draw data d data dimensionality copy distribution use statistically significant run sample note whitening update step slightly different choice estimator differ allow quasiorthogonalization provide error bar confidence interval mean amari index case error bar algorithm provide error bar baseline algorithm provide clear algorithm degrade addition quasiorthogonalization degrade far sufficient sample reason outperform algorithm sufficient sample include log perform noiseless case contrast performance whiten quasiorthogonalization clear quasiorthogonalization necessary robust run time reasonably fast sample varied distribution include step average run time second use pca whitening second quasiorthogonalization correspond average number iteration convergence independent component error follow table report mean number step convergence independent component run noise distribution d note sufficiently sample number step convergence remarkably small number datum pt mean step mean num step acknowledgment work support nsf grant iis use standard ghz cpu gb ram reference h new learning blind advance neural information processing system page r provable unknown gaussian implication autoencoder nip page m l rademacher blind signal separation presence jmlr volume colt page m bishop variational principal component proc int articial network e robust principal component analysis ab blind nongaussian signal radar signal proceeding volume page cardoso matlab realvalued datum http online access p editor handbook blind source separation academic l bayesian robust principal component analysis image processing ieee transaction h online access hsu s m kakade learning mixture spherical gaussian moment method spectral decomposition page hyvarinen independent component analysis presence maximize joint likelihood hyvarinen fast robust fixedpoint algorithm independent component analysis ieee transaction neural network hyvarinen e oja independent component analysis algorithm application neural network mathematic statistic class complex base cost function ieee transaction neural network l p q nguyen learn l rademacher online m robust high order statistic tenth international workshop artificial intelligence statistic page blind source separation second characteristic function signal processing p fast
use mapreduce apply effective parameter estimation method machine learn s increase demand deal massive instance variable important scale parallelize effectively distribute system paper study problem parallelize large cluster thousand machine naive implementation use require significant memory large number mapreduce step negative performance impact second propose new avoid expensive dot product operation loop recursion greatly improve computation efficiency great degree parallelism scale enable variety machine learn algorithm handle massive number variable large dataset prove mathematical equivalence new demonstrate excellent performance scalability use machine learn problem billion variable production cluster introduction big data application require solve optimization problem billion variable huge training datum problem scale common ad prediction deep neural network trend wide mapreduce environment build hardware optimization problem expect solve mapreduce environment big datum store problem huge number variable solve efficiently storage computation cost maintain effectively diverse collection method frequently use optimization method practice paper study lbfgs implementation scale problem mapreduce environment original update procedure propose lot popular optimization software package implement fundamental building block approach apply problem million variable study implement optimization package study scale billion variable early stage massive scale parameter gradient associate historical state large store memory single computation node create huge computation complexity processor multicore reasonable time critical explore effective decomposition example model distribute learning knowledge limited work explore scale lbfgs directly lead consequence little work scale machine learn scale use mapreduce paper start carefully study implementation mapreduce environment examine typical lbfgs implementation mapreduce present scale obstacle particularly problem d variable m historical state approximate traditional implementation need store variable memory need perform m mapreduce step iteration clearly create huge overhead problem billion variable prevent scalable implementation limitation original propose new lbfgs update procedure vlbfgs specifically devise distribute learning huge number variable particular replace original update procedure depend vector operation know twoloop recursion new procedure rely scalar operation new twoloop recursion vlbfgs mathematically equivalent original algorithm independent number variable reduce memory requirement om m alternatively require mapreduce step compare m mapreduce step naive implementation new algorithm enable implementation collection machine learn algorithm scale variable mapreduce environment demonstrate scalability advantage approach design large scale problem billion variable share experience deploy industrial cluster thousand machine relate work method base update procedure maintain compact approximation hessian modest storage requirement traditional implementation follow use compact twoloop recursion update procedure apply industry solve optimization problem decade recent work continue demonstrate reliability effectiveness optimization method contrast work implement single machine focus lbfgs implementation distribute environment context distribute learning recently extensive research graphlab build parallel distribute framework graph computation introduce framework parallelize machine learn algorithm multicore environment apply admm technique distribute learning propose delay version distribute online learn general distribute learn technique close work approach base parallel gradient calculation follow centralized algorithm different work build fully connect environment mpi focus environment loose connection centralized algorithm bottleneck procedure limit scalability example clearly state impractical run large dataset huge memory consumption centralized algorithm excellent candidate problem close work lie apply lbfgs environment solve largescale problem mapreduce adapt environment use run lbfgs mapreduce environment demonstrate power learn mapreduce scale data instance entry datum matrix number variable problem constraint centralized computation use solve deep learning problem introduce parameter server split global model multiple partition store partition separately success algorithmic point view twoloop recursion update procedure highly dependent number variable compare work propose twoloop recursion update procedure independent number variable parallelism furthermore propose algorithm run pure mapreduce environment previous work require special component parameter server addition straightforward previous work leverage proposal scale problem order magnitude term number variable optimization problem d variable require store dense d matrix approximate inverse hessian need store vector length approximate hessian implicitly let denote objective function g gradient dot product vector maintain historical state previous m generally m update current position gradient historical state represent m update form xk yk represent position difference yk represent gradient difference iteration vector length m original gradient use calculate new direction line input start point integer history size m output position minimal objective function converge calculate gradient position compute direction pk use algorithm compute xk pk choose satisfy condition m discard memory storage end update xk xk end input m k output new direction p p m p p end p m p p end core update procedure line calculate new direction use current gradient f common approach calculation recursion initialize direction p gradient continue update use historical state s information twoloop recursion find mapreduce implementation main procedure lie line calculation gradient straightforwardly parallelize divide datum multiple partition environment use map step calculate partial gradient partial datum reduce aggregate global gradient vector verification condition depend calculation objective function follow line search procedure line easily parallelize follow approach line challenge line difficulty come calculation twoloop recursion algorithm centralized update simple implementation run single processor easily perform singleton reduce challenge require m vector length d feasible scale scale storage computation cost significant challenge impractical implement ad task example feature set m linear model produce variable compactly use point represent variable require gb memory store historical state gradient mapreduce cluster build hardware share application generally example cluster deploy maximal memory limitation mapreduce step gb distribute update storage limitation centralized update alternative store multiple partition overlap use mapreduce step calculate dot product p yi line algorithm dot product require mapreduce step perform calculation result m mapreduce step twoloop recursion n lead mapreduce step example produce mapreduce step mapreduce job unfortunately mapreduce step bring significant overhead cost application cost job thousand mapreduce step cost dominate overall run time useful computational time spend algorithmic vector operation negligible current production cluster example job huge number mapreduce step large execution trigger error complicated execution engine execute reason mention feasible twoloop recursion procedure limit memory consumption number mapreduce step iteration strictly limit memory consumption store vector length d memory scale allowable mapreduce step iteration practical perform mapreduce step assumption motivate carefully lead propose algorithm section basic idea illustrate new procedure let describe follow observation guide design new procedure input operation apply p linear respect input word p formalize linear combination input coefficient unknown core operation dot product vector observation motivate formalize input m base vector b bm yk bm xi represent p linear combination bi assume scalar coefficient linear combination write p m p input invariant twoloop recursion calculate coefficient proceed calculate direction follow observation classify dot product operation category term p involve calculation category involve dot product input straightforward intuition dot product produce scalar replace dot product scalar twoloop recursion second category dot product involve p follow procedure direction p loop dot product involve p settle fortunately thank linear decomposition p observation eqn decompose dot product involve p summation dot product base vector correspond coefficient new elegant mathematical procedure happen formalize p linear combination base vector vlbfgs present algorithmic procedure let denote result dot product base vector scalar matrix m scalar propose input similar original twoloop recursion operation dependent scalar operation line assign initial value equivalent line use opposite direction gradient initial direction original calculation line rely direction vector p worth note p variable loop update dot product involve p mention early accord observation eqn formalize bj p summation list dot product base vector corresponding coefficient line algorithm base vector dot product replace multiply l scalar operation extremely efficient line continue update scalar coefficient mj equivalent update direction p respect base vector correspond procedure apply line new formalization p eqn input m m dot product matrix output coefficient m m end m m b p bj end end m m p pm pm b l bj l l end si line update equivalent mathematically equivalent line line line line algorithm easy infer equivalence consideration eqn mathematically equivalent algorithm complexity analysis comparison use dot product matrix scalar input calculation substantially efficient calculation base scalar altogether require m multiplication scalar tiny compare vector operation involve variable necessary parallelize implementation integrate core step extra step need perform calculate dot product matrix vector base vector dimension d partition use way use mapreduce step calculate dot product matrix computation greatly suitable mapreduce consideration glance tell require m dot product unchanged new iteration save tiny dot product matrix reuse entry iteration consideration law multiplication si new iteration need calculate new dot product involve new yk complexity calculation fully parallel mapreduce partition calculate small portion multiplication final step calculate new direction p base base vector complexity multiplication mean overall complexity multiplication overall tiny vector dimension base vector use approach dot product calculation produce final direction p use eqn single mapreduce step sufficient final step altogether consider gradient calculation require mapreduce step iteration update centralized update approach section require multiplication loop recursion addition centralized approach analyze require m memory storage clearly limit application largescale problem hand vlbfgs require m memory storage independent d distribute approach section require m mapreduce step twoloop recursion number iteration n generally total number step mn fortunately vlbfgs require mapreduce step summary vlbfgs enjoy similar overall complexity bear massive degree parallelism problem scale variable mapreduce implementation different approach experiment discussion demonstrate clear vlbfgs scalability property original lbfgs desirable exact algorithm mathematically prove obtain scalability property beneficial demonstrate value large number variable industrial application hand problem billion variable exist practical approach reduce small number variable solve traditional approach design centralized algorithm section justify value learn large scale variable simultaneously compare hashing approach finally demonstrate scalability advantage dataset experimental setting dataset use ad rate ctr prediction problem collect industrial search engine click event click use label instance feature include term query ad keyword contextual information ad position information time collect day datum split training test set datum day use training set rest day use test set total training datum billion instance testing datum feature number nonzero feature instance average altogether entry data matrix table relative auc performance different number variable relative auc performance million million million million table relative auc performance different number hash relative auc performance bit million bit million bit million bit run logistic regression training feature correspond variable model evaluate base testing datum use area curve denote auc set historical state length m enforce l regularizer avoid overfitte achieve sparsity regularizer parameter tune follow approach run experiment share cluster thousand machine machine concurrent vertex vertex generally map reduce step allocation core memory different job run simultaneously number vary significantly split training datum partition allocate token job mean job use vertex time partition vector calculate dot product strategy allocate entry partial vector example variable split partition evenly use model train original feature baseline experiment compare allow exhibit exact auc number consideration report relative change compare baseline scale relative auc change produce pvalue value large number variable reduce number variable original problem sort feature base frequency training datum plan reduce problem variable feature baseline filter equivalent choose different k value report relative auc number table table reduce number variable result consistently significantly number variable drop considerably significant problem increase number variable obvious significant demonstrate large number variable need learn good model value learn variable comparison hash follow approach calculate new hash value original feature value base hash function number hash bit range experimental result compare baseline term relative auc performance present table consistently previous result hashing experiment result degradation experiment bit degradation substantial problem increase number bit gap small demonstrate hash approach sacrifice performance beneficial train largescale number raw feature training time comparison compare section propose vlbfgs enable large number variable support reduce m parameter conduct experiment vary number feature number report corresponding running time use original datum hash feature baseline compare experiment report relative training time number iteration run experiment time report mean cope variance run result respect different hash bit range original b feature figure number feature m original lbfgs small advantage vlbfgs continue increase feature number run time grow quickly vlbfgs increase slowly hand increase feature number m fail exception vlbfgs easily scale clearly scalability advantage vlbfgs traditional figure training time feature number conclusion present new exact lbfgs procedure vlbfgs oppose original core twoloop recursion vlbfgs independent number variable enable easily parallelize scale billion variable present mathematical equivalence original scalability advantage traditional great degree parallelism perform experiment demonstrate value largescale learn billion variable use vlbfgs emphasis implementation mapreduce paper straightforwardly utilize distribute framework avoid centralized problem scale algorithm short vlbfgs highly beneficial machine learn algorithm rely scale order magnitude reference r bayesian rate prediction sponsor search advertising search engine international conference machine learn page large scale distribute deep network advance neural information processing system page simplify datum processing large cluster communication acm nocedal limited memory method large scale optimization mathematical programming nocedal s volume springer series operation research springer nocedal subroutine optimization acm transaction mathematical software page method parallel optimization mathematical programming nocedal update matrix limited storage method optimization theory application stochastic method online optimization machine learn research page note cg optimization logistic regression low graphlab new framework parallel machine learn uncertainty artificial intelligence sing mapreduce machine learn multicore advance information processing system page mit e distribute optimization statistical learning alternate direction method multiplier foundation trend machine learn langford m slow learner fast advance neural information processing system page c teo scalable modular convex solver regularize minimization conference knowledge discovery datum mining distribute training largescale logistic model proceeding conference machine learn agarwal langford reliable effective learning system machine cx le statistically consistent discriminate measure accuracy ijcai page scalable training model proceeding th international conference machine learn page weinberger dasgupta feature hash large scale multitask learn international conference machine learning
structural risk minimization series prediction ron electrical problem time series prediction study uniform convergence framework vapnik dependence inherent temporal structure incorporate analysis generalize available theory process finite sample bound calculate term cover number approximate class tradeoff approximation estimation discuss complexity regularization approach outline base vapnik method structural risk minimization applicable context mix stochastic process series prediction mixing process great deal effort recent year problem derive robust distributionfree error bound learn mainly context memory process hand extensive work study parametric linear model time series dependence inherent sample preclude straightforward application standard result form theory process work propose extension framework vapnik problem time prediction elementary proof sketch main technical result prove detail version consider stationary stochastic process random variable define compact domain r b probability positive constant b problem onestep prediction sense phrase find function infinite past e minimal use notation ti work support grant science foundation structural risk nonparametric time series prediction know optimal predictor case conditional solution principle settle issue optimal prediction settle issue actually compute optimal predictor note compute conditional mean probabilistic law generate stochastic process know furthermore requirement know past course stringent work consider practical situation finite subsequence observe optimal prediction need condition datum finite sample size allow predictor base finite lag vector size ultimately order achieve generality let d order obtain optimal predictor consider problem select empirical estimator class function fdn rd r complexity index class example number computational node feedforward neural network single hidden layer b fdn consider empirical predictor xi base finite datum set xi depend ddimensional fdn possible split error incur predictor term possess intuitive meaning competition term determine optimal solution fix datum define loss functional predictor rd r e let optimal function fdn minimize loss furthermore denote optimal predictor associate loss able split loss empirical predictor basic component term relate error incur use finite memory model lag size predict process potentially infinite memory present useful upper bound term relate rate convergence martingale convergence theorem good knowledge unknown type mix process study work second term related socalled approximation error immediately relate inequality p max term measure excess error incur select function class limited complexity fdn optimal lag predictor arbitrarily complex course order bind term regularity assumption function finally term estimation error term depend data similarly problem regression datum expect approximation estimation term lead conflict demand choice complexity functional class fdn clearly order minimize approximation error complexity large possible cause estimation error increase large freedom choose specific function fdn fit datum case series additional complication result fact error minimize choose d large possible effect increase approximation estimation error expect optimal value d n exist sample size point specify select empirical estimator work follow idea vapnik study extensively context observation restrict selection hypothesis minimize empirical error function easy establish example main distinction case course random variable appear r empirical error long independent clear point assumption need regard stochastic process order law large number establish event obvious standard approach use randomization case work circumvent problem approach propose use socalled method extension inequality dependent datum second approach pursue base map problem characterize process standard result case order control estimation error discuss restrict work class socalled mixing process process future depend weakly past sense precise follow definition notation utilize sequel let t m event generate random variable m respectively define m coefficient absolute regularity m expectation xd stochastic process mix m t m t note exist definition mix detail motivation use mix coefficient weak form mix uniform law large number establish work consider type process coefficient decay decay process m m r r exponentially mix process m b note markov process mix imply exponential mixing case loss generality assume process exponentially mix note usual iid process obtain exponentially mix process limit t r t respectively section follow approach derive uniform law large number mix process extend mainly asymptotic result finite behavior somewhat broaden class process consider basic idea relate approach involve construction sequence close original process welldefine probabilistic sense briefly construction slightly modify notation fit present paper divide sequence block size assume simplicity block number accord order j define denote random variable correspond index xi e xi e sequence denote construct sequence independent identically distribute block e sequence independent block distribution block original sequence process stationary block independent identically distribute basic idea construction independent block sequence close welldefine sense original block sequence appropriately select number block depend mix nature sequence relate property original sequence independent block sequence let class bounded function b e order structural risk nonparametric series prediction relate uniform deviation respect original sequence xi sequence use lemma utilize modify hold sample size consider sequence define e fi sequence independent random variable remainder paper use variable denote quantity relate transform block sequence finally use symbol denote empirical average respect original sequence d ed following result prove simple extension suppose class b p p sup main merit transformation problem domain dependent process implicit quantity characterize independent process implicit term correspond independent block price pay transformation extra term appear inequality appear lemma error bound development section concern scalar stochastic process order use result context time series define new x sequence coefficient obey inequality let space function mapping rd r e let loss function loss space know theory empirical process example order obtain upper bound uniform deviation use socalled covering number function class respect empirical norm e similarly denote empirical norm respect independent block sequence similarly g follow common practice denote number functional space use metric p p definition let l class realvalued function r d d xi e r d let define e l rand r order obtain result term covering number space l correspond transform sequence need follow lemma hard prove lemma t t r proof result follow sequence simple inequality present main result section upper bind deviation mix process turn yield upper bound error incur empirically optimal predictor theorem let x o bounded stationary mix stochastic process b let class function rd b sample size let function minimize empirical error function minimize true error proof establish use basic result theory uniform convergence iid process relate covering number space covering number easily relate use p point specify result general order obtain weak consistency require rhs converge c immediately yield follow condition condition corollary condition add requirement follow choice sufficient guarantee weak consistency empirical predictor exponential mix r algebraic mix notation imply proof consider case exponential mixing case rhs clearly converge covering number fast rate convergence achieve balance term equation lead choice case algebraic mix second term rhs order use sufficient condition guarantee term converge r claim order derive bound expect error need assumption concern covering number space particular know work haussler covering number upper bound follow l b measure p assume pseudodimension guarantee finite cover number structural risk nonparametric prediction structural risk minimization result section provide error bound estimator form minimize empirical error fix class ddimensional function clear complexity class function play crucial role procedure class rich manifest large covering number clearly estimation error term large hand bias class function restrict complexity lead poor approximation rate wellknown strategy overcome obtain consider hierarchy functional class increase complexity sample size optimal tradeoff estimation approximation determine balance term procedure develop late vapnik term structural risk minimization recent approach collectively term complexity regularization extensively study recent year bear mind context add complexity exist case regression recall result derive section assume fix lag vector d general optimal value unknown fact infinite order achieve optimal performance nonparametric setting crucial size lag choose adaptively add complexity need incorporate optimal performance face unknown memory size achieve let e let sequence function define fdn upper bound observe pass lugosi recently consider situation pseudodimension unknown covering number estimate empirically datum line thought potentially useful pursue assume upper bound pseudodimension fdn know case class function use practice example line standard approach introduce new empirical function account empirical error complexity cost penalize complex model large complexity index lag size let empirical error predictor complexity penalty specific form constant definition choose hindsight achieve optimal rate convergence constant positive constant obey l e similarly possible choice d value choose accordance corollary let minimize empirical error class function assume class fdn compact minimizer exist far let r function minimize complexity penalize loss min follow basic result establish consistency structural risk approach yield upper bound performance let sequence class e fdn mapping rd r expect loss function select accord principle upper bound min inf main merit demonstration procedure achieve optimal balance approximation estimation retain attribute particular optimal lag d predictor belong predictor converge rate know advance type obtain respect size non parametric rate convergence predictor discuss paper reference barron complexity regularization application artificial neural network nonparametric functional estimation relate topic page academic press l p nonparametric curve estimation haussler decision theoretic generalization pac model neural learning application information computation haussler sphere packing number subset boolean bound j combinatorial theory series adaptive model selection use empirical complexity submit annal d e memory universal prediction stationary random process ieee convergence empirical process springer vapnik estimation dependence base empirical datum springer m theory learning rate convergence empirical process stationary annal probability
assignment multiplicative mixture natural image abstract analysis natural image gaussian scale use account statistic filter response inspire hierarchical cortical representational learning scheme gsm pose critical assignment problem work filter response generate common multiplicative factor present new approach solve assignment problem probabilistic extension basic gsm perform inference model use gibb sample demonstrate efficacy approach synthetic image datum understand statistical structure natural image important goal visual neuroscience neural representation early cortical area decompose image likely sensory input way sensitive sophisticated aspect probabilistic structure structure play key role method image processing code aspect natural image topdown modeling coordination nearby location scale orientation topdown perspective structure model use know scale mixture model gsm gsm involve multidimensional gaussian dimension capture local structure linear filter multiply collection common hide scale variable mixer variable capture coordination gsm wide implication theory cortical receptive field comprehensive framework mixer variable provide topdown bottomup characteristic natural image statistic statistical dependency fact marginal distribution receptive filter high hindsight idea bear close relationship multiplicative bottomup image analysis framework statistical model gain control coordinate structure address image work domain speech approach unsupervised specification representation early cortical area rely structure idea learn linear filter model simple cell base coordination find combination transform way find high order filter complex cell critical specification datum obvious neighborhood arrangement linear filter share mixer variable mixer variable scale wavelet suggest method find neighborhood base bayesian inference gsm random variable section consider estimate component base information neighborhood mode failure inference local global base observation section propose extension gsm generative model mixer variable overlap probabilistically solve neighborhood assignment problem use gibb sample demonstrate technique synthetic datum section apply technique image data gsm inference gaussian mixer variable simple ndimensional version gsm filter response l synthesize multiply ndimensional gaussian value g common mixer variable l assume uncorrelate diagonal covariance matrix analytical calculation assume v distribution parameterize strength prior exp ease develop theory know repeat figure b marginal distribution result gsm sparse highly joint conditional distribution element l l follow bowtie shape width distribution dimension increase large value positive negative dimension inverse problem estimate variable g v filter response formally illpose regularize prior distribution posterior distribution particularly relevant derive analytically posterior mean l b l b l b exp l b l b ll q l l l b l b l q l b l modify function second kind l force sign mixer variable positive note l row local estimate row estimate accord filter output posterior estimate numerically noise removal mixer prior gsm specify hierarchy mixer variable wainwright consider treebased arrangement practice natural sensory datum heterogeneous collection advantageous learn arrangement example approach relate suggest describe l filter response synthetic case facilitate image g l l actual distribution l eg eg l eg l ev l l filter global distribution filter filter local joint conditional l l c mixer multiply multiply l distribution figure generative model filter response generate multiply variable mixer variable mixer variable marginal joint conditional statistic bowtie sample synthetic filter response joint conditional statistic intensity proportional bin count column independently rescale fill range intensity leave actual distribution mixer column estimate base different number filter response c distribution estimate mixer variable note mixer variable value definition positive distribution estimate variable g e joint conditional statistic estimate generate log mixer value filter learn linear combination small collection underlie value consider problem term multiple mixer variable linear filter cluster group share single mixer pose critical assignment problem work filter response share mixer variable study issue use synthetic datum group filter response l l generate mixer variable figure attempt infer component gsm model synthetic figure empirical distribution estimate conditional mean mixer variable l base different assume assignment estimation base filter response estimate match actual distribution example local estimate base single filter response peak away assignment include filter response estimate good inference compromise estimate v global include filter response actually generate c column e consider joint conditional statistic component actual generative model l filter number filter number filter number l gibb fit assume l mixer gibb fit assume distribution distribution distribution l filter number gaussian filter number infer multiply filter number pixel l l figure generative model filter response generate gaussian variable mixer variable mixer variable choose probabilistically filter response sample distribution b actual probability filter association gibbs estimate probability filter association correspond c statistic generate filter response gaussian mixer estimate gibb sample estimate respective g g number filter response increase estimate improve provide right group filter response mixer variable specifically mean estimate independent e column note estimation base single filter response joint conditional distribution gaussian appear correlate independent second column estimation base filter response example joint conditional distribution estimate dependent independent bowtie shape e column mixer variable joint statistic deviate actual estimation local global observe qualitatively similar statistic estimation base coefficient natural image neighborhood size discuss context quality noise removal assume gsm model neighborhood inference solve assignment problem plot figure suggest possible infer assignment ie work filter response share common mixer learn statistic result joint dependency hard assignment problem filter pay mixer computationally soft assignment problem probabilistic relationship filter response mixer computationally behave real world stimulus likely capture possibility filter response coordinate somewhat different collection different image consider rich mixture gsm generative model figure model generation filter response single image patch multiply gaussian variable single mixer variable set assume pij satisfy assign mixer variable assignment assume independently patch use m inference learn model proceed stage accord expectation maximization filter response use gibb sample e phase find possible appropriate posterior assignment suggest use gibb sample solve similar assignment problem context dynamic tree model second m phase collection assignment multiple filter response update association probability pij sample mixer assignment estimate gaussian mixer component gsm use table section restrict filter response sample associate mixer variable test ability inference method find association probabilistic mixer variable synthetic example figure true generative model probabilistic overlap mixer variable generate sample filter accord generative model run gibbs sample procedure set number possible neighborhood iteration weight converge near proper probability plot actual probability distribution filter association mixer variable estimate association nonzero estimate closely match actual distribution estimate procedure consistently find correct association large example datum generate mixer variable example actual estimate distribution mixer component gsm note joint conditional statistic mixer gaussian independent variable generate synthetic example gibbs procedure adjust datum generate different parameter equation related mixer allow range image coefficient behavior image datum validate inference model use synthetic datum turn natural image derive linear filter multiscale orient pyramid filter prefer orientation spatial position spatial subsample pixel phase quadrature pair single spatial frequency peak image ensemble image standard image compression database plant leave sample run method parameter synthetic datum possible neighborhood parameter figure figure depict weight pij coefficient obtain mixer variable schematic template association representation follow c actual datum mixer variable neighborhood coefficient phase orientation spatial grid grid phase neighborhood illustrate probability coefficient generate mixer variable neighborhood image patch yield maximum log likelihood p neighborhood b prefer vertical pattern receptive field second localized region horizontal preference average image patch maximum log likelihood mixer variable group phase quadrature pair b c quadrature pair extract cortical datum component ideal complex cell model tendency group phase phase position position phase position x position neighborhood example max patch average neighborhood example max patch average l l c neighborhood mixer gibb fit assume gibb fit assume distribution distribution distribution coefficient phase l l l ev l figure schematic mixer variable neighborhood representation probability coefficient associate mixer variable range black white leave vertical horizontal filter orientation phase phase plot separately pixel spatial grid right summary representation filter shape replace orient line filter approximately pixel diameter filter pixel image ensemble neighborhood obtain gibbs sample pixel patch maximum log likelihood p average maximal patch c image ensemble neighborhood statistic representative coefficient spatially vertical filter infer gaussian mixer variable orientation space phase grouping bear interesting similarity recent suggestion maximal patch wavelet filter advantage span wide spatial extent possible current ica technique analysis parameter phase grouping control compare analysis representation obvious advantage extend analysis correlated wavelet filter simulation large number neighborhood obtain association estimate mixer gaussian variable accord model d representative statistic coefficient infer variable learn distribution gaussian mixer variable close assumption estimate exhibit joint conditional statistic roughly independent mixer variable weakly dependent far demonstrate neighborhood inference image ensemble interesting intuitive consider inference particular image image class figure ab demonstrate example mixer variable neighborhood derive learn patch image corel neighborhood compose quadrature pair spatial configuration rich neighborhood neighborhood average example max patch max patch average example max patch max patch figure example gibb image image pixel spatial neighborhood span pixel b example mixer variable neighborhood leave example mixer variable neighborhood average patch yield maximum likelihood p right image mark example patch yield maximum likelihood p previously report unsupervised hierarchical method example mixture neighborhood capture spatial configuration appear particularly relevant region mark image patch yield maximum log likelihood p b mixture neighborhood capture horizontal configuration horizontal stripe example demonstrate probabilistic mixture coefficient correspond horizontal stripe link vertical stripe horizontal stripe discussion work study natural image statistic recently evolve issue hierarchy wavelet induction unsupervised learning model base cortical development coordinate statistical structure wavelet component include bottomup bowtie hierarchical representation complex cell topdown viewpoint result new insight inform model idea form essential work paper link impressive engineering result image coding process critical aspect hierarchical representational model way structure hierarchy induce address hierarchy question use novel extension gsm generative model mixer variable level hierarchy enjoy probabilistic assignment filter response low level assignment learn use gibb sample illustrate attractive property use synthetic variety image datum ground method inference posterior distribution class random variable gsm mixer gaussian place particular emphasis generative model statistical property component obvious question raise work neural correlate different posterior variable gaussian variable characteristic resemble output normalize simple cell mixer variable obviously related output quadrature pair neuron orientation energy motion energy cell normalize different information source subsequently use great interest acknowledgement work fund foundation pd grateful hoyer simoncelli discussion reference c mallow scale mixture normal distribution royal stat soc m wainwright e p simoncelli scale mixture statistic natural image solla muller editor neural information processing system volume page cambridge mit press m wainwright e p simoncelli willsky random cascade wavelet tree use modeling analyze natural apply computational harmonic analysis special issue wavelet application hyvarinen unifying framework lowlevel statistical property natural image sequence optical society r w e p simoncelli image compression joint statistical characterization wavelet domain ieee image proc schwartz e p simoncelli natural signal statistic sensory gain control nature field relation statistic natural image response property cortical cell opt soc h e schreiner temporal statistic natural sound m s solla editor adv neural info processing system volume page mit press l statistic natural image scale phy rev letter c b nonlinear aspect primary vision entropy reduction decorrelation symposium society information display volume page statistic natural image model cvpr page choi baraniuk bayesian wavelet domain image modeling use hide markov tree image j p property natural image resemble flow phy rev e p simoncelli parametric texture model base joint statistic complex wavelet coefficient computer vision description generation invariant signal signal process t k model b editor handbook econometric hyvarinen p hoyer emergence topography complex cell property natural image use extension solla t k editor neural information processing system volume page cambridge mit press p hoyer hyvarinen multilayer sparse code network learn contour code natural image vision research m learn higherorder structure natural image network computation neural system sejnowski slow feature analysis unsupervised learning invariance neural computation c p extract slow subspace natural video lead complex cell editor artificial network page olshausen field emergence simplecell receptive field property learn sparse factorial code nature sejnowski independent component natural scene edge filter vision research u srivastava model clutter natural image ieee tran anal mach m wainwright e simoncelli adaptive wiener denoise use gaussian scale mixture model wavelet domain image proc page computer society m wainwright e p simoncelli image denoise use scale mixture gaussian domain ieee tran image process c dynamic tree m kearn s solla cohn editor information processing system volume page cambridge mit press e p simoncelli freeman multiscale transform theory special issue wavelet
anytime provable bound suboptimality sebastian school computer abstract real world planning problem time limit anytime planner suited problem find feasible solution quickly continually work improve run paper propose anytime heuristic search tune performance bind base available search time start find suboptimal solution quickly use loose bind tighten bind progressively time allow time find provably optimal solution improve bind reuse previous search effort result significantly efficient anytime search method addition theoretical analysis demonstrate practical utility experiment simulate robot kinematic arm dynamic path planning problem outdoor introduction optimal search infeasible real world problem limited time want find good solution time provide condition anytime prove useful usually find possibly highly suboptimal solution fast continually work improve solution allocate time unfortunately rarely provide bound suboptimality solution cost optimal solution know algorithm control suboptimality provide suboptimality bound valuable allow judge quality current plan decide continue search base current suboptimality evaluate quality past planning episode allocate time future planning episode accordingly control suboptimality bound help adjust tradeoff computation plan quality search inflated heuristic actual heuristic value multiply factor suboptimal prove fast domain provide bound suboptimality heuristic construct anytime algorithm suboptimality bound run succession search decrease factor naive approach result series solution suboptimality factor equal corresponding factor approach control suboptimality bind waste lot computation search iteration duplicate effort previous search try employ incremental heuristic search eg suboptimality bound search iteration long guarantee end propose anytime algorithm efficient anytime heuristic search run inflated heuristic succession reuse search effort previous execution way bound satisfied result substantial speedup achieve state value correctly compute previous iteration efficiency different domain evaluation simulate robot kinematic arm degree freedom fold speedup succession search demonstrate problem plan path robot account robot dynamic anytime heuristic search know anytime describe execute inflated heuristic continue improve solution algorithm control suboptimality bind select factor search experiment able decrease bound gradually significantly fast advantage guarantee examine state search algorithm property important provide bind time produce plan mention later describe number interesting idea applicable weight heuristic normally input heuristic consistent successor s denote cost edge positive consistency turn guarantee heuristic admissible large true cost reach goal heuristic use result state expansion consequently fast search heuristic violate property result solution long guarantee optimal pseudocode inflated heuristic figure easy comparison present later maintain function state real number cost current path node assume path find estimate total distance start goal maintain open state plan expand open sort expand state appear short path start goal initialize open list start state line time expand state line remove open update gvalue ss neighbor decrease insert open terminate soon goal state expand open insert sstart open f sstart expand remove small value open successor visit s s s open figure heuristic weight figure leave column search decrease right column correspond search iteration set result standard heuristic result solution guarantee optimal solution suboptimal suboptimality bound factor length find solution large time length optimal solution left column figure operation algorithm heuristic simple grid world example use grid black cell obstacle denote start state denote goal state cost cell neighbor heuristic large distance cell goal cell expand stop search soon expand goal state actually expand goal state path find search arrow search inflated heuristic expand substantially cell solution suboptimal reuse search result work execute multiple time start large decrease prior execution result search solution guarantee factor optimal run search scratch time decrease expensive explain reuse result previous search save computation explain improvepath function leave column figure path section explain main function column figure function series decrease let introduce notion local inconsistency term state locally inconsistent time gvalue decrease line figure time state expand suppose state good state min decrease min s word decrease introduce local inconsistency gvalue gvalue successor expand hand inconsistency correct gvalue successor line figure turn successor locally inconsistent way local inconsistency propagate child series expansion eventually child long rely s gvalue lower insert open list definition local inconsistency clear open list consist exactly locally inconsistent state time gvalue lower state insert open time state expand remove open time gvalue lower open list view set state need propagate local inconsistency consistent heuristic guarantee expand state set violate consistency result search state multiple time turn restrict state expand suboptimality bind hold implement restriction check state gvalue lower insert open previously expand line figure set expand state maintain close variable procedure fvalue return procedure main open close incon procedure insert sstart open fvalue improvepath remove small fvalue open min close close publish current suboptimal solution successor visit decrease state incon open update open accord fvalue close s close improvepath insert open fvalue min publish current suboptimal solution insert incon figure restriction expand state open long contain locally inconsistent state fact contain locally inconsistent state expand important track locally inconsistent state starting point inconsistency propagation future search iteration maintain set incon locally inconsistent state open line figure union incon open exactly set locally inconsistent state use starting point inconsistency propagation new search iteration difference improvepath function condition improvepath function reuse search effort previous execution locally inconsistent insert open result condition search stop soon equal minimal f value state open list condition use improvepath function line figure allow avoid expand possibly state value note long maintain value variable improvepath function change prohibitively expensive update value state instead fvalue function compute return f value state open iterative execution search introduce main function column figure perform series search iteration initialization function series decrease s improvepath function new open list construct content set incon open list sort current value state line figure improvepath function solution suboptimal factor suggest suboptimality bind compute ratio upper bind cost optimal solution minimum value locally inconsistent state lower bind cost optimal solution valid suboptimality bind long ratio large equal equal cost optimal solution actual suboptimality bind compute minimum ratio line figure think use actual suboptimality bind decide decrease search iteration set small delta experiment suggest decrease small step beneficial reason small decrease result improvement solution fact actual suboptimality bind previous solution substantially value large decrease hand result expansion state search useful suggestion implement prune open contain state unweighted value large equal execution improvepath function mainly save computation state locally consistent gvalue correct state precisely example right column figure series function state locally inconsistent end iteration identical second improvepath function expand cell contrast cell expand search search suboptimality factor decrease finally improvepath function set expand cell solution optimal total number expansion cell expand function single optimal search scratch expand cell theoretical property present theoretical property proof property algorithm refer use s denote cost optimal path sstart let define greedy path sstart path compute trace backward follow start state pick state arg min sstart theorem improvepath function exit state min open g cost greedy path sstart large correctness follow theorem execution improvepath function terminate large minimum value open mean greedy path start goal find factor optimal iteration decrease turn upper bind gradually decrease suboptimality bind find new solution satisfy bind theorem improvepath state expand locally inconsistent improvepath gvalue lower current execution second theorem formalize computational saving come search inflated heuristic search iteration guarantee expand state expand state gvalue improvepath function correctly compute previous search iteration set locally inconsistent state need update neighbor propagate local inconsistency experimental study robotic arm evaluate performance simulate degree freedom dof robotic arm figure base arm fix task endeffector goal navigate obstacle indicate rectangle action define change global angle particular joint joint arm rotate opposite direction maintain global angle remain joint workspace cell compute distance cell cell contain goal account cell occupy obstacle distance heuristic order heuristic overestimate true cost joint angle endeffector cell single action result statespace state dof robot arm state dof robot arm memory state allocate demand d arm trajectory uniform cost c nonuniform cost anytime nonuniform cost sec cost sec cost figure row robot arm experiment row robot arm experiment trajectory downsample anytime algorithm figure plan trajectory robot arm initial search search sec comparison search optimal trajectory infeasible run memory quickly plot figure b improve quality solution bound suboptimality fast manner succession search anytime experiment initially set algorithm experiment section decrease step suboptimality succession search anytime control experiment apparently perform lot computation result large decrease end hand reach optimal solution way evaluate expense anytime property run optimal search slightly simple environment optimal search feasible optimal search require min state expand find optimal solution require min state expand decrease step provably optimal solution find overhead experiment figure b action cost experiment figure c action nonuniform cost change joint angle close base expensive change high joint angle result nonuniform cost heuristic informative search expensive experiment start run algorithm minute end achieve solution substantially small cost succession search anytime suboptimality bind succession search anytime decrease cost solution gradually read graph differently reach suboptimality bind expansion sec succession search reach bind expansion minute fold anytime reach expansion minute fold similar result hold compare work algorithm spend obtain solution cost figure execution time comparison state expand identical additionally demonstrate advantage expand state search iteration compare search anytime search perform expansion anytime perform expansion mainly state expand robot laser b map c optimal d search optimal d search e d search d search sec sec sec figure outdoor robot navigation experiment cross position robot time solution find figure df result experiment dof robot arm action nonuniform cost algorithm start figure e second plan cost trajectory find suboptimality bind guarantee substantially small algorithm example trajectory figure d contain step extra change angle joint base arm fact change low joint angle expensive comparison trajectory figure e graph figure compare performance algorithm randomized environment similar environment figure d environment random goal location obstacle random location outside wall graph additional time algorithm require achieve suboptimality bind result different environment comparable normalize bind divide maximum good bound achieve run memory average environment time achieve good bind sec difference second end anytime graph correspond overhead factor outdoor robot navigation motivation work efficient robot large outdoor environment optimal trajectory involve fast motion turn speed environment particularly important advantage momentum find dynamic static plan use state space orientation velocity high dimensionality large environment result large statespace planner computationally infeasible robot plan optimally time discover new obstacle model error solve problem build planner d planner use fast planner use search result serve heuristic d planner interleave search execution good plan far perform search backward start search sstart actual goal state robot goal current state robot sstart change robot search tree remain valid search iteration heuristic estimate distance robot position operation line figure figure robot use navigation laser construct robot environment test system scan convert map environment figure c obstacle black size environment meter map cell meter d statespace consist state state space state robot initial state upper circle goal low circle ensure safe operation create high cost obstacle square corner figure fragment map grayscale proportional cost d plan figure c sharp degree turn obstacle require robot come complete stop optimal d plan result wide turn velocity robot remain high trajectory plan compute start figure e trajectory d plan somewhat bad optimal d plan time require optimal d planner sec time planner generate plan figure e ms result robot run start execute plan early robot run optimal d planner beginning path second receive goal location figure d contrast time robot run advanced figure plan converge optimal decrease conclusion present anytime heuristic search work continually decrease suboptimality bind solution find new solution satisfy bind way execute series search decrease suboptimality bound search try reuse possible result previous search experiment algorithm efficient previous anytime search successfully solve large robotic planning problem acknowledgment work support contract darpa program reference b planning heuristic search artificial intelligence m analysis planning conference artificial intelligence personal communication m incremental advance neural information processing system nip cambridge r e artificial intelligence m s thrun formal analysis tech pearl heuristic intelligent search strategy computer problem solve r e multiple sequence alignment use proc conference artificial intelligence student abstract s approximate reasoning use anytime algorithm approximate computation academic publisher
use feedforward neural network monitor change correlation center abstract report change normalize use conjunction network monitor change alertness operator continuously time previously eeg spectral amplitude change index change behavioral error rate auditory detection task report time increase frequency detection error task accompany pattern increase decrease spectral coherence frequency band eeg channel pair relationship eeg coherence performance vary subject subject topographic spectral profile appear stable session session change change correlation eeg waveform record different scalp site neural network estimate alertness correlation change eeg signal introduction human recording potential oscillation change dramatically frequency amplitude topographic distribution change complex differ subject recently use principal component analysis conjunction network change performance sustained auditory detection task estimate near realtime change eeg spectrum scalp channel report loss auditory detection task performance accompany change spectral coherence eeg signal record different scalp site extent frequency content coherence change link change differ subject subject appear stable session session second coherence change link associate significant phase difference correlation measure apply eeg waveform change incorporate coherence andor correlation information neural network algorithm estimate alertness eeg spectrum enhance accuracy robustness contribute design practical neural interface perform realtime monitoring change operator alertness method concurrent eeg behavioral datum collect purpose develop method monitor alertness operator complex system adult participate session push detect auditory target brief increase level background noise maximize chance observe decrement session conduct small experimental chamber subject eye close target ms increase intensity white noise background threshold present random time interval mean rate min short probe frequency target noise interval eeg collect electrode locate site system refer right sample rate diagonal channel record use eye movement artifact correction rejection session subject choose analysis basis include detection continuous performance measure local error rate compute convolve performance index smooth window advance performance datum step target hit define target respond window target eye movement artifact remove datum use selective regression procedure datum contain large artifact reject analysis complex eeg spectra compute advance point data window datum step multiply window convert frequency domain use complex coherence compute channel pair spectral epoch coherence study error rate smooth window s rectangular window use smooth coherence estimate finally complex coherence convert coherence amplitude phase result correlate local error rate correlation measure eeg waveform compute channel smooth window smooth use causal exponential window window use smooth error rate time correlation study use feedforward neural network monitor result u e b l frequency time task min time task min b figure change coherence amplitude upper trace correlate simultaneous change error rate half auditory detection task low trace indicate channel pair b concurrent change coherence phase upper trace local error rate session relation coherence change detection performance minute session fig subject detect target present coherence amplitude remain high minute subject fail single detection trace coherence amplitude fall low overall correlation session coherence error rate time series channel pair range session coherence phase performance fig b portion session coherence phase lag hz channel pair subject perform poorly degree phase lag appear activity frontal site lead activity frontal site ms overall correlation session coherence phase error rate channel pair range correlation coherence amplitude error rate frequency fig upper trace include broad band strong negative correlation hz correlation coherence phase performance confine narrow frequency band low trace estimate significance coherence correlation surrogate coherence record collect time use asynchronous block contiguous eeg datum channel correlation result surrogate coherence time series error rate compute distribution absolute correlation determine subject data value conservative assumption complete independence adjacent frequency p significance level maximum absolute correlation correlation spectrum heuristic estimate significance level surrogate datum session subject coherence correlation large absolute value imply coherence amplitude change frequency significantly related change alertness subject sejnowski spectral topographic stability c e pair subject o l o frequency frequency b figure correlation spectra correlation movingaverage coherence error rate session small letter indicate frequency analyze fig b cluster analysis correlation coherence amplitude error rate frequency mean set channel pair derive cluster analysis similar coherence correlation spectra pair scalp channel superimpose mean second session subject sign size spectral topographic structure correlation coherence amplitude error rate frequency stable session channel pair frequency band fig b mean spectral correlation session subject cluster similar correlation spectra identify cluster analysis result session size structure correlation spectra second session replicate result session spectral stability monotonic relationship coherence auditory detection performance suggest coherence use predict change performance level eeg datum collect continuously eeg waveform correlation performance case coherence phase lag datum small correlation change phase lag performance investigate movingaverage correlation eeg signal different scalp channel use predict change possibly low computational cost study relationship rate change movingaverage correlation eeg waveform bandpass session find strength topographic structure significant relationship performance measure stable variable subject subject select eeg channel pair time correlate highly error rate use train regression network feedforward perceptron estimate error rate movingaverage correlation feedforward neural network hidden unit respectively weight bias network adjust use error backpropagation conjugate gradient descent use minimize error network output actual error rate use feedforward neural network monitor alertness time crossvalidation use prevent network overfitte training datum session pair network architecture time course error rate estimate time use different random initial weight test generalization ability model second session subject procedure simulate potential realworld alertness monitoring application datum operator use train network estimate subsequent session eeg datum accuracy error rate estimation test session identical neural network hide unit accurate linear regression figure time course actual estimate error rate pair train test session result subject equivalent table average correlation estimation error actual estimate error rate time series session subject use feedforward neural network hide unit result use hide unit equivalent diagonal cell result train session cell test session nonlinear perceptron improve estimation performance multivariate regression reduce rm estimation error test session p increase mean correlation actual estimate error rate time series p discussion spectral coherence eeg waveform different scalp site measure nearly year subject increase number clinical behavioral developmental eeg study coherence value know high sleep transition note precede increase coherence frequency result datum subject perform sustained auditory detection task condition suggest coherence increase decrease depend subject analysis frequency electrode site analyze individual subject spectral topographic structure coherence change appear stable session session eeg correlation coherence relate change correlation eeg waveform reflect change activity site possibility use movingaverage correlation measure activity monitor state change animal discuss knowledge approach previously apply human eeg origin function understand decrease eeg coherence result subcortical brain system coordinate activity separate cortical eeg generator emergence activity project surface similarly increase coherence result increase cortical generator volume enhance activity generate single cortical subcortical site measure change eeg coherence correlation cognitive task possible role variable brain cognitive dynamic investigate extent eeg coherence andor correlation j measure combination spectral amplitude measure allow practical robust continuous time estimation alertness level auditory detection task environment estimate actual error rate rm g time task min test rm p time task min figure change detection rate exponential window estimate use feedforward perceptron correlation pass eeg signal select pair scalp channel panel training session panel testing session solid line actual error rate time course dash line estimate correlation error indicate table result alertness monitoring use eeg pairwise correlation test set test set subject training set rm training set test set subject b training set rm use feedforward neural network monitor alertness acknowledgment work support grant naval research center office naval research view express article author reflect official policy position department government acknowledge contribution mark collect process datum thank suggestion reference inphase brain activity sleep dynamic characteristic human eeg healthy subject brain human brain potential onset sleep sejnowski t estimate alertness eeg power spectrum submit publication mn removal ocular artifact eeg comparison time frequency domain method simulated real datum m coherence fluctuation performance eeg spectrum generalization parameter estimation feedforward net experiment neural information processing system t d coherence sleep left subject brain cognition rumelhart learn internal representation error propagation parallel distribute processing normal adult coherence measure relationship record
wasserstein training restrict abstract machine able learn highly complex multimodal structure distribution parameter model usually learn minimize kullbackleibler divergence training sample learn model propose work novel approach machine training assume meaningful metric observation metric represent wasserstein distance distribution derive gradient respect model parameter minimization new objective lead generative model different statistical property demonstrate practical potential datum completion denoise metric observation play crucial role introduction powerful generative model use approximate large class datum distribution handwritten character speech segment multimodal datum boltzmann machine share similarity neural network capability extract feature multiple scale build hierarchical datum representation restricted boltzmann machine rbm special type boltzmann machine compose layer latent variable define probability distribution p set binary observe variable state represent binary vector parameter vector learn empirical probability distribution px list observation rbm train use informationtheoretic divergence example minimize respect divergence p p sample empirical measure p model distribution p instance divergence approach result wellknown maximum likelihood estimator mle yield gradient form kl p log p log p p bracket notation indicate expectation respect p alternative choice euclidean distance distribution generally divergence m estimator result comparable gradient term try adjust fitting term grow large possible department brain cognitive conference neural information processing system nip explore work different scenario choose p large average close data point sense necessarily x coincide exactly adopt geometric criterion define closeness observation mean application machine metric observation readily available example consider hamming distance binary vector metric motivate practical consideration geometric draw consider wasserstein distance distance measure choice consider theory prove statistical consistency consider practically good knowledge paper describe practical derivation minimum estimator machine scale thousand observation describe paper recent advance fast approximation distance derivative play important role practical implementation computation describe approach detail like measure wasserstein distance result considerably different perspective provide approach difference illustrate figure probability p close kl perspective empirical measure p far measure p wasserstein sense conversely different probability p miss mark kl viewpoint achieve low wasserstein distance p proceed rest paper let mention wasserstein distance broad appeal machine learning distance instance introduce context supervise inference use compute predictive loss output multilabel classifier ground truth domain adaptation model model large overlap data distribution small overlap low large distance small distance high high low figure empirical distribution px gray define set state d possible model define set state size circle indicate probability mass state model kl wasserstein divergence px explanation divergence low high overlap distance minimum wasserstein distance estimation consider probability p p q p set probability map p q px omit summation sign consider cost function define typically distance r constant smoothed wasserstein distance equal p q min h pq p p q set joint probability x x px x x h log shannon entropy optimization problem strictly convex program equivalent dual formulation involve instead realvalued function rx play important role paper max e rx smooth distance true wasserstein distance correspond case equation entropic term easily verify definition match usual linear program use describe distance use mle principle metric consideration play key role define density p density euclidean distance kind metric grant work equation recover dual formulation rightmost regularizer converge indicator function feasible set dual optimal transport problem consider paper case considerably facilitate computation result divergence p q case differentiable wrt variable look dual formulation equation gradient equal center optimal dual variable center step ensure orthogonality respect sensitivity analysis clear interpretation quantity measure cost unit mass place p x compute wasserstein distance p q decrease p q favorable transfer mass p point high place point low idea use simple application chain rule minimize fix target probability p quantity p p respect proposition let p z ef parameterized family probability distribution differentiable function write let center optimal dual solution p p describe equation gradient smoothed distance respect p p p g p proof result direct application chain rule p p p t p p mention rightmost term optimal dual variable potential p jacobian linear map x p p x g p consequence p result equation integral wrt term multiply comparison kl fitting error target distribution p play direct role formation gradient p wrt term equation gradient incorporate knowledge p different way consider support p point correspond high potential cost compute distance p p high potential mean probability p lower decrease p p vary accordingly sample approximation gradient equation intractable involve solve optimal smoothed transport problem probability define d state practice replace expectation wrt p empirical distribution form sample model p e generate model define pcd sample sample e size e use differentiate sample generate model empirical observation dual potential center p measure uniform weight simplify approximation gradient b p p e solution discrete smooth wasserstein dual empirical practical term tion p p respectively support size n vector size coefficient pcd sample compute follow notation simple describe term generic probability p q mind practice training simulate empirical measure p p compute optimal variable correspond p q recover p cost grow product pq size support p q p px algorithm known adapt setting alg precise description ease notation consider arbitrary ordering set d identify element index let ip order family index set pi define accordingly respective length p q form matrix size p q choose positive vector u r r random repeat u converge metric operation u t u convergence optimal variable p u geometric mean vector u ensure center application restrict boltzmann machine restrict boltzmann machine rbm generative model binary datum compose binary observed variable binary explanatory variable vector d represent state observed variable vector represent state explanatory variable rbm associate configuration observed variable probability p x define p bj partition function normalize probability distribution parameter wj bj rbm learn datum know state observed variable explanatory variable independent bj logistic map z conversely know state explanatory variable observe variable probability distribution define sample independently lead efficient alternate gibbs sample procedure p rbm model explanatory variable analytically marginalize yield follow probability model p j log bj free energy associate model p ef partition function gradient rbm write rbm free energy form wasserstein gradient obtain compute gradient x inject equation b p p p bj gradient respect parameter bj obtain mean comparison gradient divergence p p p p wasserstein gradient way kl gradient express simple form simple small possible sample size case e occur center constraint section gradient stability kl regularization gradient gradient depend indirectly data distribution p problem sample p generate model strongly differ example come p weighting e generate sample represent desire direction parameter space case gradient point bad local minimum closeness empirical sample optimization perspective ensure add regularization term objective incorporate usual quadratic term kl term force proximity p direct dependence gradient optimization problem p p p kl p p k start point regularization hyperparameter select determine starting point analogous initial phase propose wasserstein procedure finetune standard rbm force finetune deviate pretraine solution experiment perform experiment demonstrate rbms learn distribution metric perspective explore main characteristic learn distribution optimize wasserstein objective investigate usefulness learn model practical problem data completion denoise metric observation occur performance evaluation consider dataset mnistsmall subsample version original mnist dataset digit retain subset uci plant dataset contain spread plant specie mnistcode dimensional code vector associate mnist digit additional detail supplement training validation evaluation rbm model investigate train batch mode use p pcd approximation p sample gradient update step alternate gibb sample start sample previous time step choose pcd sample e coefficient e occur size training set gradient obtain solve smoothed wasserstein dual p p smooth parameter distance ip denote hamming distance binary vector use center parameterization rbm descent perform holdout validation quadratic coefficient weight coefficient number hide unit rbm set heuristically dataset learning rate set heuristically pretraine phase modify min train final objective distance p p compute test distribution pcd sample end training procedure sample fast approximation true unbiased sample generate anneal state supplement comparison pcd sample result analysis contour plot figure effect hyperparameter distance kl regularizer active equivalent minimize standard rbm reduce regularization wasserstein distance effectively minimize small choose small wasserstein distance increase stability reason mention section experiment observe kl pretraine necessary order reach low wasserstein distance lead degenerate solution relation hyperparameter minimization criterion consistent dataset case wasserstein rbm produce low wasserstein distance standard rbm rbm e e rbmw parameter mnistcode e rbm e e rbmw parameter inf parameter plant parameter parameter mnistsmall e e high rbm e e rbmw parameter inf low figure wasserstein distance function hyperparameter good rbms sense rbmw red good rbms standard sense ie force inf minimum kl blue sample generate standard rbm wasserstein rbm precisely approximation figure rbmw produce reduce set clean prototypical example noise produce regular rbm generate rbmw welldefine contour round shape reproduce variety shape present datum similarly plant spread generate rbmw form compact contiguous region prototypical real spread diverse datum sample generate standard rbm finally rbmw generate code decode close actual mnistsmall plant mnistcode digit generation binary unit rbm binary unit binary unit pixel pixel rbmw rbm rbm binary unit figure example generate standard wasserstein rbms image plant dataset automatically generate template create user image mnistcode produce decoder right plot figure superimpose true data distribution gray distribution generate standard rbm blue wasserstein rbm particular plot project distribution pca component true distribution standard rbm distribution uniformly cover datum generate rbmw consist finite set small dense cluster scatter input distribution word wasserstein model bias cluster systematically ignore region rbm small plant rbmw large datum rbm small large datum rbm small datum rbmw large figure twodimensional comparison distribution learn rbm rbmw smooth parameter plot obtain project learn distribution component true distribution rbmw distribution obtain vary parameter figure analyze effect wasserstein smoothing parameter learn distribution observe dataset strong smooth strong shrinkage effect distribution look red distribution strongly depart data distribution red distribution actually superior consider smooth wasserstein distance performance metric figure validate shrinkage effect verify shrinkage effect observe figure training truly expect property model distribution analyze effect simple distribution parameter space enumerate figure plot wasserstein distance sample size dimensional gaussian distribution parameterized model distribution p parameter interpret shrinkage parameter wasserstein distance compute use euclidean metric rescale expect distance pair point model parameter metric euclidean p p p metric model parameter figure wasserstein distance sample p sample p model parameter smooth use euclidean metric interestingly choice wasserstein smooth parameter true distance compute use library good model p empirical sense shrink version p smoothing strong good parameter dirac distribution locate origin overall experiment validation observation wasserstein rbms learn shrink distribution note finite sample size prevent reach favor shrink model datum completion denoise order demonstrate practical relevance wasserstein distance minimization apply learn model task datum completion datum denoise use metric crucial datum completion datum denoise performance generally measure term distance true datum complete denoise datum euclidean distance realvalued hamming distance binary datum locate probability mass result minimization incur severe penalty completion denoise performance metric task useful practical application datum completion use step apply discriminative learn network svm datum miss feature datum denoise use dimensionality reduction step train supervised model let input h compose visible variable v hide variable data completion setting datum completion experiment illustrate figure distribution p possible reconstruction sample use alternate gibbs sampler expect hamming distance true state reconstruct state model distribution iterate possible reconstruction p e p hx reconstruction probability distribution compute expect hamming error biasvariance decomposition mnistsmall hide randomly locate image patch size plant mnistcode hide random subset variable result figure leave compare type model standard rbm rbm rbm estimation model use gaussian kernel choose divergence model validation datum minimize rbmw comparable model particular interest structure expected hamming error standard rbm large error come variance entropy wasserstein rbm bias term contribute relate observe figure data point area cover red point reconstruction systematically near red cluster incur systematic bias datum denoise consider simple noise process predefined subset variable denote h know number l bit flip occur randomly remain variable denote setting experiment illustrate figure e noisy version result flip l variable h expect hamming error original completion denoise hide pixel original image assign pixel incomplete ip pixel pixel original image possible image reconstruction possible image reconstruction noisy image figure illustration completion denoise setup image select known subset pixel hide corrupt noise possible reconstruction particular hamming distance original example expect hamming error compute weight hamming distance probability model assign reconstruction denoise completion hamming distance bias plant mnistcode rbm rbmw mnistsmall rbm rbmw bias rbm rbmw plant rbm rbmw mnistcode bias rbm rbmw rbm rbmw figure performance completion denoise task density estimation standard rbm wasserstein rbm total length bar expect hamming error dark gray light gray section bar biasvariance decomposition l e state visible variable distance l p e l hx e p hx iterate note original example necessarily set state noise model assumption mnistsmall datum choose randomly locate image patch size generate l random bit flip select patch plant mnistcode generate l bit flip randomly input variable figure right denoise error term expect hamming distance dataset rbmw comparable model completion task main difference rbms biasvariance ratio wasserstein rbm tend large bias experiment consider simple noise model consist fix number random bit flip small predefined subset variable denoise highly corrupted complex datum require combine wasserstein model flexible noise model propose conclusion introduce new objective restricted boltzmann machine rbm base smooth distance derive gradient wasserstein distance dual formulation use effectively train rbm usual kullbackleibler kl divergence wasserstein objective account metric datum consider scenario rbm produce distribution strongly depart standard rbms outperform practical task completion denoise generally demonstrate empirically learn probability distribution incorporate indirectly desire metric substitute training procedure desire metric directly learning objective wasserstein training direct approach density estimation regularize kl training future work aim far explore interface learning wasserstein minimization aim scale newly propose learning technique large complex datum distribution acknowledgement work support program national research foundation fund work support grant m acknowledge support young researcher grant correspondence reference h sejnowski learning algorithm cognitive science e minimum distance estimator statistic probability letter t enhance gradient train restrict machine computation r d optimal transport domain adaptation pattern analysis machine intelligence ieee transaction distance computation optimal transport advance information processing system page m doucet fast computation wasserstein proceeding conference machine learning icml page g e m hinton recognition restrict boltzmann machine advance neural information processing system page m poggio learn wasserstein loss nip page e hinton training product expert minimize contrastive divergence neural computation p j robust statistic springer bottou bengio p gradientbased learning apply document recognition proceeding ieee b m freitas inductive principle restricted machine learn proceeding international conference artificial intelligence statistic aistat page deep boltzmann machine center trick neural network trick trade second edition lnc page springer l distance multidimensional scaling image retrieval proceeding arpa image understand workshop page r salakhutdinov e hinton deep boltzmann machine proceeding twelfth international conference artificial intelligence statistic aistat page srivastava r salakhutdinov multimodal learn deep boltzmann machine machine learn research r salakhutdinov robust machine recognition denoise ieee conference computer vision pattern recognition page t training restrict boltzmann machine use approximation likelihood gradient machine learn proceeding international conference icml page plant database optimal transport old new volume springer
fit qiteration advantage weight regression theoretical computer science biological cybernetic abstract recently fit qiteration fqi base method popular increase sample efficiency stable learning process high quality result policy method remain hard use continuous action space frequently occur task robotic technical application greedy action selection commonly use policy improvement step particularly problematic expensive continuous action cause unstable learning process introduce optimization bias result highly nonsmooth policy paper use action selection policy improvement step use fqi simplify inexpensive regression result able derive new computationally efficient fqi deal high dimensional action space introduction reinforcement learn address problem autonomous agent improve behavior use experience time step t agent observe current state choose appropriate action subsequently agent feedback quality action reward rst observe state goal agent maximize accumulate reward expect future paper focus learn policy continuous multidimensional control problem state space action continuous multidimensional meaning discretization start prohibitively expensive reinforcement learning widely study problem rigorous convergence proof hold true continuous state action continuous state space convergence guarantee exist case bad performance generate easily method transfer straightforwardly continuous action current approach circumvent continuous action space focus problem rely discrete set action learn policy drive goal minimum time need action maximum acceleration start acceleration maximum velocity maximum goal sufficiently close point approach traditional control work large class minimum time control problem limited approach cost function relevant realworld incorporate complex constraint biological system movement use variance endpoint physical technical system incorporation optimization criterion essential importance minimum time policy prone damage car similar policy highly robot environment result reduce complex immediate reward function require large set action employ consider use continuous action fit qiteration fqi base batch mode reinforcement learn estimate stateaction value function q use greedy operator qs action improve policy work discrete action space greedy operation hard perform highdimensional continuous action reason application fit qiteration base method restrict lowdimensional action space efficiently discretize paper use stochastic softmax policy instead greedy policy allow reduce policy improvement step use fqi simple regression greedy operation qs action replace greedy operation parameter space value function result allow derive new computationally efficient algorithm base regression test algorithm different benchmark task ie pendulum dynamic version puddleworld dimension action selection algorithm able produce high quality policy fit qiteration fit qiteration fqi assume experience agent current time form h si ai ri si task learning estimate optimal control policy historical datum fqi approximate stateaction value function qs iteratively use supervised regression technique new target value regression generate ri ri max q regression problem find function define list datapoint pair regression procedure h si ai q fqi view approximate value iteration stateaction value function previous experiment function approximator neural network radial basis function network regression tree employ context performance bound value function approximation wide range function approximator performance bound hold true continuous action space case variant fqi unfortunately knowledge experiment variant exist literature additionally clear apply variant efficiently nonparametric function approximator fqi prove outperform classical online rl method application fqi rely greedy action selection equation algorithm frequently require discrete set action generalization continuous action straightforward use greedy operator continuous action space hard problem use expensive optimization method need high dimensional action return value greedy operator result optimization bias cause unstable learning process include oscillation divergence comparison algorithm use crossentropy optimization method find maximum qvalue implementation maintain gaussian distribution belief optimal action sample nce action distribution good nce action high qvalue use update parameter distribution process repeat iteration start uniformly distribute set sample action inherently offline method historical datum estimate optimal policy use online learning fqi algorithm new episode collect currently good infer policy fqi algorithm restart fit qiteration advantage weight regression different method policy update continuous action space reinforcement learning regression author action selection problem immediate reward rl set continuous action formulate expectationmaximization base subsequently reduce regression weighted regression apply ease highdimensional action space greedy operation action space need directly follow work follow general idea weight regression value estimation section consider task estimate value function v stochastic policy stateaction value function q value function calculate integral action space hard perform continuous action approximate value function evaluation integral consider quadratic error function ds s s s z use find approximation v value function denote state distribution follow policy square function use inequality probability density function derive upper bind s s s solution v minimize upper bind error b original error function proof compute square replace term value function error function upper bind error b s v s v s ds s s z z qs s z error function additive constant depend difference original error function upper bind approximate straightforwardly sample si ai ai gain follow behavior policy b ai b define state distribution follow behavior policy b term b si ai ensure weight state action prefer b know method importance sampling order tractable factor b ai si b si si set n minimization equation define weight regression problem weighting u weight regression procedure o ai ai u ai si fqi advantage weight regression input si ai ri si l number iteration initialize s si ai ri ai ai si ai si u end result order approximate value function need carry expensive integration action space state sufficient know qvalue finite set stateaction pair policy improvement use softmax policy policy improvement step fqi policy base advantage function qs additionally assume knowledge mean standard deviation s advantage function state s quantity estimate locally approximate additional regression policy define exp r exp control policy assume advantage distribute s normalize advantage value tion denominator constant state use term exp directly weight regression define equation result value function s use replace greedy operator fqi advantage weight regression algorithm qfunction query step history h furthermore state action pair ai use query fqi need determine policy subsequent datum collection policy obtain way regression advantage use instead reward weighting optimize long term cost instead immediate regression lawer base fqi propose new computationally efficient fit qiteration use locally weight regression function approximator similar kernel base method algorithm need able calculate similarity state si dataset state simplify notation denote sj sj calculate s diagonal matrix determine bandwidth kernel additionally algorithm need similarity measure wij action ai calculate kernel ai use state similarity wij estimate mean standard deviation advantage function state p approximate value function approximation qfunction use locally weight regression qfunction s sa sa t sa t sa sa stateaction matrix local weighting matrix consist state action similarity t vector qvalue equation q approximate combine weighting ai state similarity weight s value s sn t state matrix weight matrix bind estimate order prevent local regression add positive bias cause divergence value problem nonparametric value function approximator strongly increase computational complexity increase number datum point simple solution avoid problem introduce local mechanism state space old example area remove dataset approximate policy similar regression use stochastic policy s gaussian exploration approximation optimal policy mean variance s s p init wi ai p wi t denote action matrix variance automatically adapt exploration policy uncertainty optimal action init set initial exploration policy init set bandwidth action space set weight initial variance variance datum set experiment evaluation evaluate lawer benchmark task pendulum task task dynamic version puddleworld augment puddleworld velocity dimension compare algorithm treebased fqi cetree neural fqi fqi use crossentropy optimization find maximum qvalue optimization use nce sample dimensional nce sample dimensional nce dimensional control variable set nce use iteration enforce exploration collect new datum gaussian noise add policy treebased algorithm ensemble m tree use set number state action variable set use neural network hide layer neuron layer train network algorithm propose epoch experiment discount factor use immediate reward function quadratic distance goal position apply c evaluate learning process performance policy evaluate cycle determine accumulate reward episode start specified initial position represent confidence interval practice ridge regression use avoid numerical instability regression net number data collection net number data collection lawer tree time s c average reward average reward lawer tree time s d figure evaluation lawer fqi algorithm pendulum c plot average trial evaluation c learn torque trajectory c d learn torque trajectory c pendulum task task pendulum need position position state space consist angular deviation position angular velocity pendulum system dynamic ml mg sin u torque motor limit mass set m kg length link m time step set s experiment different torque c perform use iteration matrix d da set diag diag datum collection phase episode step collect start position episode start random position comparison algorithm c figure c figure b algorithm comparable performance fqi computationally efficient slightly decrease performance figure typical example learn torque trajectory start position lawer cetree figure c trajectory figure d c algorithm able discover fast solution setting solution second set qualitative difference trajectory regression lawer able produce smooth trajectory trajectory find method look figure influence parameter performance lawer work large range value task order performance lawer complex highly nonlinear control task use description system torque limit set kg length link m time step use l iteration use fqi algorithm phase agent observe episode start position start random position episode step matrix d da set diag da diag comparison lawer cetree algorithm figure state discretization treebased algorithm able learn fast end able produce policy high quality treebased dynamic puddleworld puddleworld task agent find way predefined goal area maze world figure agent negative reward difference standard puddleworld setting agent dimensional state space position use set create dynamic version puddleworld agent set force accelerate kdimensional point mass m kg c average reward average reward start goal lawer tree c number data collection b c figure evaluation average reward gain learning trial pendulum task different setting comparison lawer cetree task c set dimensional dynamic puddleworld tree number data collection average reward average reward u tree number data collection u u time s c time s d figure comparison cetree lawer dimensional dynamic puddleworld b comparison cetree lawer dimensional dynamic puddleworld c torque trajectory dimensional world learn torque trajectory learn cetree dimension puddleworld illustrate scalability algorithm multidimensional continuous action space respectively dimensional position limit velocity maximum force apply direction restrict time step set setting dimensional puddleworld figure c agent leave predefined area velocity set additional reward compare lawer cetree iteration use matrix d set diag da diag dimensional diag da diag dimensional puddleworld phase agent observe episode step start predefined initial position episode start random position figure comparison cetree lawer dimensional puddleworld figure b dimensional puddleworld result treebased algorithm advantage beginning learning process cetree algorithm problem find good policy dimensional lawer perform setting clearly comparison learn force trajectory figure lawer figure d cetree algorithm trajectory cetree algorithm random dimension control variable trajectory find lawer look smooth goal direct conclusion future work paper focus solve rl problem continuous action space fit qiteration base algorithm computational complexity max operator qs fqi algorithm intractable high dimensional continuous action space propose new method circumvent max operator use stochastic softmax policy allow reduce policy improvement step qs weight regression problem base result derive lawer new computationally efficient base experiment lawer able produce high quality smooth policy high dimensional action space use expensive optimization method calculate problematic suboptimal policy find computational cost use continuous action standard fqi need average s pendulum s world benchmark task cetree algorithm need s order magnitude slow comparable running time cetree lot work spend optimize implementation simulation run p comparison treebased fqi approach algorithm deal high dimensional state space distance kernel matrix choose appropriately user additionally uniform distance measure state space adequate complex control task degrade performance future research concentrate combine approach regression tree present acknowledgement paper partially fund project p author want thank mpi biological cybernetic academic work possible reference reinforcement moore generalization reinforcement learning approximate value function advance neural information processing system mit press p power law converge approach movement planning experimental psychology human perception performance vol r m alexander minimum energy cost hypothesis human arm trajectory biological cybernetic vol m m noise determine motor planning nature vol pp m fit qiteration experience datum efficient neural reinforcement learn method proceeding european conference machine learn ecml generalization reinforcement learn successful example use sparse coarse code advance neural information processing system mit press d p l batch mode reinforcement learn learn vol pp r szepesvari fit qiteration continuous mdps advance neural information processing system cambridge s m fit qiteration pp s schaal policy learning motor skill proceeding neural information processing d r crossentropy method operation research vol s reinforcement learning regression control proceeding international conference machine learning icml moore s schaal locally weight learn artificial intelligence vol
interference learn internal model inverse dynamic human dept brain cognitive science m abstract experiment perform reveal computational property human motor memory system human practice reach movement interact novel mechanical environment learn internal model inverse dynamic environment subject recall model testing session hour initial practice representation internal model memory interference attempt learn new inverse dynamic map immediately mapping learn suggest interference computational element use encode inverse dynamic map use learn second mapping predict lead initially learn skill introduction task use hand interact tool motor system develop model dynamic tool use model control couple dynamic arm tool shadmehr physical system theory tool mechanical analogue mapping force input change state output framework currently dept dept physiology m figure experimental setup robot low planar mechanism power torque motor act elbow joint subject endpoint robot house force hand series target display monitor face subject function robot produce novel force field subject learn compensate reach movement model develop motor control system learning process need approximate inverse mapping inverse dynamic map internal model tool interested understand representation nervous system use learn store internal model previous work measure way learned internal model extrapolate training datum shadmehr result suggest coordinate system learn map intrinsic joint muscle base hand base coordinate present mathematical technique estimate inputoutput property learn map explore issue motor memory store map similar input different output quantify internal model paradigm subject learn control artificial tool tool torque motor program produce variety dynamical environment fig task subject endeffector point point reach movement series target environment represent force field act subject hand typical case fig typical experiment begin turn null environment subject hand target smooth straight line fashion force field introduce dynamic task change hand trajectory significantly alter shadmehr practice typically movement hand trajectory return straight line path suggest practice lead formation internal model function inverse dynamic mapping ie desire trajectory presumably term hand position velocity prediction force encounter trajectory design method quantify force estimate output property internal model position force interaction point robot subject write dynamic link system fig term interference learn internal model inverse dynamic human follow couple vector differential equation iii f matrix function torque field produce environment force measure handle robot controller implement system subject qt reference trajectory plan motor system subject jacobian matrix describe differential transformation coordinate endpoint joint q p joint position subject robot subscript r denote subject robot matrix null environment ie e eq solution couple system q qt arm follow reference trajectory typically straight hand path gaussian velocity profile let controller accomplish task c co eq robot motor produce force field solution q qt new controller c c co e internal model compose subject c co ie change controller training period estimate quantity measure change interaction force trajectory training function fi function fi impedance subject arm view interaction approximate difference fi estimate change controller crucial assumption reference trajectory change training process order measure subject movement series environment environment unpredictable opportunity learn purpose perturb controller reference trajectory measure neighboring state environment fig present subject practice period adapt training fi estimate similar fashion difference function calculate measured arm trajectory result project hand velocity space computer limitation trajectory target direction use approximation result pattern force interpolate sum basis function fig change impedance arm estimate inputoutput property internal model learn subject find subject provide good result test group learn change effective impedance arm way approximate impose force field sufficient condition arm compensate force field allow hand follow desire trajectory alternate strategy simply arm muscle lead increase ability arbitrary environmental force figure b suggest practice lead formation internal model specific dynamic impose force field b figure change impedance subject arm learn force field force field produce robot training period change subject arm impedance training period ie internal model formation internal model memory wish determine subject retain internal model motor memory test naive subject handle robot sequence target null environment movement msec visual feedback timing movement movement subject able consistently reach target proper time trajectory constitute baseline set subject return day timing task point force field introduce subject attempt perform exact task target proper time sequence target introduce force perturb subject trajectory cause deviate straight line path note previous work shadmehr deviation decrease eventually subject trajectory presence force field come resemble baseline force present convergence trajectory perform baseline subject fig timing performance subject field order determine subject retain internal model force field memory return day hour later test force field half subject force present train previous day field half force field novel subject field field correlation value respect field ie force vector field degree rotation respective vector field subject test field train perform significantly p initial performance fig field novel perform naive level result suggest internal model form practice field specific field performance field interference learn internal model inverse dynamic human movement number figure measure performance training period movement naive subject short break minute interval movement mean standard error correlation coefficient hand trajectory null environment baseline trajectory measure exposure field trajectory force field hand trajectory field converge null field straight shape velocity profile mean period reach target goal reach target second e ft e e movement number movement number figure subject learn internal model specific field retain mean standard error movement period force field field initial practice session upper trace second session hour initial practice low trace b movement period different group subject initial training dark line field test field field hour later gray line performance record separate set naive subject field initial training day retain evidence performance follow day interference effect motor memory experiment tool subject learn control unusual subject learn inverse dynamic memory use enhance performance hour initial acquisition ask memory affect formation subsequent internal model previous section subject return day initial training memory learned internal model present interference decrement performance learn new field temporal distance significantly reduce learn movement number figure interference sequential learning uncorrelated force field low trace mean standard error movement period naive group subject initial practice force field field upper trace movement period group naive subject field minute practice movement field field model interfere learning new field new subject learn timing task null environment following day target force field field improvement performance short break minute walk lab read magazine new field field field respect field find significant reduction p ability learn field fig compare subject group initially train field word performance field shortly learn field significantly bad subject inability master task field order demonstrate field isolation difficult learn field new set subject initially learn field field find large decrement ability way explain decrement performance fig assume computational element represent internal model field use learn second field word second field force opposite field internal model badly biased represent second field muscle torque pattern predict movement target wrong direction connectionist literature phenomenon temporal interference network train element acquire large weight begin dominate inputoutput transformation second task present new map map similar input different output large error network perform poorly naive network network attempt learn new task error feed element presynaptic input cause activity element example computational element use nervous system model inverse dynamic mechanical system find firing pattern set cell reconstruct inverse dynamic representation eye interference learn internal model inverse dynamic human large synaptic weight learn hebbian ie weight change proportion pre postsynaptic large weight change effectively cause loss learn task computational stand point expect internal model field learn subject learn field evidence catastrophic interference subject present volume phenomenon interference sequential learning map term interference negative transfer psychological literature human interference observe extensively task involve memory eg task involve recognition word list find interference function similarity map task stimulus new learn task require response different recently learn significant interference interestingly interference decrease increase learning practice map task involve memory include motor learn question interference report interference sequential learning motor task involve response set light suggest interference observe cognitive confusion study report little interference subject learn motor task design task little cognitive component find shortly acquisition motor memory memory strongly interfere learning new inputoutput mapping interference significant hour memory initially acquire possible explanation initial learning place memory system time andor practice information memory transfer storage brain imaging study motor learning suggest subject motor task neural field motor cortex display increase new field report subject attempt learn new motor task successively case task consist sequence movement neural activity motor cortex low second task order ofthe task reverse remain decrement neural activity motor cortex correlate interference observe subject attempt learn different inputoutput mapping succession reference shadmehr r e catastrophic interference motor inform proc vol press condition transfer training r representation catastrophic connectionist network connection functional human learning determine regional blood flow shadmehr r adaptive behavior ofthe system environment soc impedance control approach manipulation theory et practice perfect functional mri study soc field human motor area involve preparation reach actual learn study evidence associative interference performance influence degree interpolate learn inhibition transfer specific response amer interference discrete motor task test theory dept psychology motor control learn behavioral emphasis human book experiment motor conflict adaptive representation dynamic learn motor task neuroscience m m inverse dynamic model eye movement control cell nature interference dependent stage amer lr mechanism memory science problem backpropagation learn procedure network th arm trajectory plan dynamic coordinate adaptation study exp brain press
direct feedback alignment provide learn deep neural network abstract artificial neural network commonly train gradient learn provide backpropagate error layer layer output layer hidden layer recently discover method feedbackalignment weight use propagate error backward symmetric weight use activation forward fact random feedback weight work evenly network learn feedback useful work feedback alignment principle use train hidden layer independently rest network initial condition error propagate fix random feedback connection directly output layer hide layer simple method able achieve training error convolutional network deep network completely error backpropagation method step biologically plausible machine learn error signal local symmetric reciprocal weight require experiment test performance mnist cifar good obtain backpropagation fully connected network combine dropout method achieve error permutation invariant mnist task introduction supervised learn achieve great success train deep neural network today method real competitor simplicity prove performance alternative exist machine learn different variant biologically inspire method training network method use local available signal adjust weight method combine finetune obtain good discriminative performance similar boltzmann machine learning use deterministic feedforward network case weak symmetric resemble recently introduce biologically plausible training method layer train reconstruct layer method require symmetric weight propagate target value instead gradient novel training principle feedbackalignment fa recently introduce author feedback weight use backpropagate gradient symmetric feedforward weight network learn use fix random feedback weight order reduce error essentially network learn learn result conference neural information processing system nip asymmetric weight explore conclusion work weight symmetry constraint significantly relaxed retain strong performance backpropagation algorithm biologically plausible reason require symmetric weight second require separate phase inference learn learning signal local propagate backward output unit require error derivative transport second signal network transport signal derivative nonlinearitie know mention method require error travel backward reciprocal connection biologically plausible sense cortical area know connect question error signal relay area reach distant area error signal represent second signal neuron participate forward pass error represent change activation neuron consider possibility error relay layer represent neuron participate forward pass low layer imply feedback path disconnected forward path layer long connect layer question arise neuron receive teaching signal disconnected feedback path work experimentally directly connect feedback path output layer neuron early pathway sufficient enable learn deep network requirement feedback random network adapt concept different backpropagation result similar method produce feature classification easy layer figure novel feedback path configuration far explore work method base feedback alignment principle direct feedbackalignment dfa indirect feedbackalignment figure overview different error configuration arrow indicate activation path black arrow indicate error path weight adapt learning denote weight fix random denote bi backpropagation b feedbackalignment direct feedbackalignment indirect feedbackalignment method let minibatche inputoutput vector want network learn simplicity assume network hidden layer figure target output scale let row denote weight connect layer unit hide layer let column vector bias unit hide layer activation network calculate b h b h b nonlinearity use hide layer nonlinearity use output layer choose logistic activation function output layer binary crossentropy loss function loss minibatch size gradient output layer e calculate log log m output unit minibatch index gradient hidden layer calculate e wt elementwise multiplication operator derivative nonlinearity gradient steepest descent directly minimize loss function linearize version network fa hide layer update direction calculate b e b bi fix random weight matrix appropriate dimension dfa hide layer update direction calculate b e b e bi fix random weight matrix appropriate dimension hidden layer number neuron bi choose identical hide layer hide layer update direction calculate b e fix random weight matrix appropriate dimension ignore learning rate weight update method calculate xt theoretical result provide gradient point direction steepest descent loss function provide different update direction experimental result indicate method able reduce error network nonlinear hidden unit surprising principle distinct different steepest descent feedback weight transpose forward weight fa feedback weight fix forward weight adapt approximately align pseudoinverse feedback weight order feedback useful feedbackalignment paper prove fix random feedback asymptotically reduce error condition happen freely follow network linear hide layer input datum mean standard deviation feedback matrix b satisfie b pseudoinverse b forward weight initialize output layer weight adapt minimize error let novel principle feedback alignment principle clear feedback alignment principle apply network nonlinear hide layer experiment layer add error backpropagate output follow theorem point mechanism explain feedback alignment principle mechanism explain asymmetric feedback path provide learn align backpropagate forward propagate gradient theorem hide layer feedforward neural network connect let hide layer activation let functional dependency layer weight matrix b bias vector nonlinearity let layer update accord update direction nonzero satisfy expectation e batch size large negative update direction minimize follow khk minimize maximize gradient maximize alignment l lk khk t direction order minimize proof let layer prescribed update descent direction order minimize use product rule hi positive scalar nonzero let ai define ai ai input layer use product rule gradient maximize ci ci hi ignore magnitude gradient write hi l project prescribed update indirectly maximize lk maximize component minimize gradient minimize prescribed update imply angle backpropagate gradient imply nonzero point direction vector steepest direction point direction apply output layer hide layer neural network achieve learn close feedback loop update direction e e feedback path connect output hide layer prescribed update directly minimize loss turn positive feedback provide update direction e reduce loss apply successively deep layer layer weight matrix update minimize layer time indirectly update direction useful suggest large class asymmetric feedback path provide gradient direction hidden layer long average choose feedback path e visit layer way backward weight fix random fa method choose direct feedback path bi e bi fix random dfa method choose direct feedback path g e b e connect hidden layer visit layer way forward method experimental section learning possible indirect feedback direct random feedback e bi e advantage nonzero nonzero random matrix bi rank probability close nonzero requirement order achieve feedback static ensure property preserve training addition static feedback easy maximize direction hi constant crossentropy loss use output target value sign error sample change mean quantity bi constant training bi constant task classify quantity addition constant sample class direct random feedback provide update direction hi magnitude vary magnitude error e forward weight initialize backpropagate error good starting point use asymmetric feedback update step possibility quickly turn quantity positive initial condition requirement feedback work experiment start bad initial condition direct random static feedback able turn quantity positive reduce training error hide layer growth bound layer layer saturate hidden layer update hi dfa hide layer update nonzero long error e nonzero limit growth nonlinearity hyperbolic tangent logistic sigmoid appropriate add nonlinearity hide layer hidden activation bound initial weight datum point nonlinearity limit initial growth direction experimental result indicate nonlinearity suited dfa hyperbolic tangent nonlinearity use hidden layer forward weight initialize linear activation function work initial weight error derivative unit bias incoming weight experimental result investigate learn useful feature hidden layer tanh network train input test image result feature visualize use figure method learn feature easy discriminate class hide layer cluster separate point visible improvement separation input hide layer indicate error dfa able learn useful feature deep hide layer furthermore experiment perform error dfa able learn useful hide representation deep layer tanh network train mnist hide layer fix random weight hide layer train epoch point training error hide layer training figure leave error curve network pretraine hide layer right error curve normal training tanh network figure visualization mnist input feature different color correspond different class row feature obtain row feature obtain dfa left right input image hide layer feature second hidden layer feature hide layer feature continue training error decrease epoch step repeat time layer train dfa expect different update direction error increase decrease epoch error curve present update direction provide different backpropagate gradient result hide representation reduce error similar way feedforward network train mnist cifar compare performance experiment perform binary crossentropy loss optimize mnist dropout experiment learn rate decay training time choose base validation set experiment learning rate roughly optimize use method learning rate constant dataset training stop train error reach number epoch reach minibatch size use momentum weight decay use input datum scale convolutional network datum whiten weight bias initialize network initial weight bias sample uniform distribution range feedback weight sample uniform distribution range model logistic tanh tanh adv table mnist test error direct feedbackalignment training error bracket high field indicate convergence result mnist summarize table adversarial regularization network train adversarial example generate regularization dropout probability use input layer network target propagation achieve error result method similar able train deep network initialization use good result dfa match good result tanh table cifar test error direct feedbackalignment training error bracket high result cifar summarize table convolutional network error inject maxpooling layer model identical use dropout paper nonlinearity network target propagation achieve error dropout experiment gap improve dropout convolutional network dfa bad tanh table cifar test error direct feedbackalignment training error bracket high result cifar summarize table improve dropout convolutional network dfa bad experiment perform verify dfa method feedback loop possible loop provide learn experiment perform single feedback loop figure d able train deep network hide layer neuron feedback connect hide layer hide layer train update direction loop start random initialization training error reduce test error reduce discussion experimental result indicate able fit training datum equally good performance test set similar lag little network clearly good add regularization help dfa successful train network hide layer proper weight initialization use able train deep network reason fail converge probably simple initialization scheme use proper initialization help fa similar way investigate dfa training procedure lot common supervised pretraine deep network important difference layer train simultaneously error deep network drive learn error shallow pretraine network network target hide layer adapt improvement loss contrast able decrease error case feedback depend weight layer dfa demonstrate novel application feedback alignment principle brain implement kind feedback step understanding mechanism provide learn brain learning possible feedback loop forward feedback path disconnect introduce large flexibility error signal transmit neuron receive error signal postsynaptic connect neuron directly presynaptic neuron indirectly error source locate synapsis away early disconnect feedback path lead biologically plausible machine learning feedback signal add hide layer nonlinearity derivative nonlinearity know learning rule local weight update depend presynaptic activity temporal derivative postsynaptic activity learning separate phase perform end extended forward pass error signal second signal neuron participate forward pass separate signal relay neuron local update rule link plasticity believe govern synaptic weight update brain disconnect feedback path great similarity controller use dynamical control loop purpose feedback provide change state reduce output error dynamical control loop change add state propagate forward output network change use update weight conclusion biologically plausible training method base feedback alignment principle present train neural network error feedback error backpropagation method symmetric weight reciprocal connection require error path short enable training deep network training signal local available synapse away weight initialization require method able fit training set experiment perform mnist cifar cifar performance test set lag little importantly work suggest restriction enforce backpropagation feedbackalignment backward pass visit neuron forward pass discard learn possible feedback path disconnect forward path reference biologically plausible deep learning d e rumelhart g e learn internal representation propagation nature topdown influence visual processing nature review neuroscience szegedy explain harness adversarial example ab geoffrey e hinton fast learning algorithm deep belief net neural computation geoffrey e hinton optimal perceptual inference proceeding ieee conference computer vision pattern difference target propagation machine learning knowledge discovery database page springer international publishing poggio important weight symmetry p tweed feedback weight support learn deep neural network ab salakhutdinov geoffrey e hinton deep boltzmann machine proceeding twelfth international conference artificial intelligence statistic aistat volume jmlr proceeding page m exact solution nonlinear dynamic learn deep linear neural network ab srivastava salakhutdinov dropout simple way prevent neural network overfitte learn research walk train deep nonlinear feedforward network initialization divide gradient run average recent network machine learn datum use machine learn research seung equivalence contrastive learn layer network neural computation
separate style content dept brain cognitive abstract seek analyze manipulate factor style content underlie set observation fit training datum bilinear model explicitly represent structure model adapt easily testing new style content allow solve general task extrapolation new style unobserved content classification content observe new style translation new content observe new style classification embed bilinear model probabilistic framework separable mixture model generalize early work factorial mixture model significant performance improvement benchmark speech benefit approach introduction pattern analysis synthesis task observed datum generate interaction underlie factor style content example character recognition task observe different letter different font fig different word different write style speech different phoneme different visual image face different people different lighting condition datum raise number learning problem extract hide structure raw observation receive significant attention unsupervised factorial learning kind prove tractable datum interact factor work supervised set label style content available training testing figure problem want solve label training set observation multiple style want extrapolate new style unobserved content class fig classify content observe new style fig ib translate new content observe new style fig paper treat problem common framework fit training datum separable model easily adapt testing new style content class write observation vector style content class c seek fit observation model particular functional form assume estimate vector c describe style s content c respectively w parameter separate style content extrapolation l d e r c e e e b j r c e e d e translation c e e r g h c b figure observation content letter different style font want extrapolate classify translate observation new style content class independent style content govern interaction term spirit model represent element row common independent column element column common independent row b c element common independent row column modular component solve problem illustrate fig example extrapolate new style unobserved content class fig combine content interaction parameter learn training style parameter estimate available datum new style bilinear model propose separate style content use bilinear model model linear factor hold constant simple model complex model subtle interaction style content empirical success linear model pattern recognition application model face vary identity constant illumination pose vary illumination constant identity pose bilinear model natural choice factor vary independently datum set computationally desirable property linear model extend bilinear model discuss section easy base efficient wellknown technique singular value decomposition expectationmaximization complexity control vary model dimensionality achieve compromise reproduction training datum generalization test finally approach extend model datum generate interact factor explore bilinear model eq symmetric model treat factor assume bilinear mapping ij parameter represent set basis function independent style content characterize interaction factor observation style content c generate mix basis function coefficient b tenenbaum freeman tensor product vector model exactly reproduce observation dimensionality equal number style class observe find coarser compact representation dimensionality decrease practical represent style content lowdimensional vector example linear combination basis style learn training describe new style obtain flexible asymmetric bilinear model let basis function depend style content example basis function allow depend style bilinear model bj wj simplifie sum index identify vector notation b c matrix basis function specific style independent content vector coefficient specific content independent style alternatively basis function depend content model parameterize function independently style content translate factor simultaneously fig far matrix representation style content flexible overfit training datum overfitting problem control additional constraint asymmetric model solve extrapolation classification task use training datum symmetric model figure illustrate example asymmetric model use separate style content collect small database image different people content class different head pose style image pixel treat dimensional vector subset datum fig fig b depict asymmetric bilinear model datum pose represent set basis vector image person represent set coefficient render image particular person particular pose basis vector mixed accord coefficient note basis vector pose look appropriate style pose bilinear structure model ensure correspond basis vector play correspond role pose e vector hold roughly mean face pose second modulate overall head size modulate head crucial adapt new style content class model fit task fig break training phase testing phase involve model fit training phase correspond row column fig learn parameter bilinear model complete matrix observation content class style testing phase correspond final row fig b final row column fig adapt model datum new style content class estimate new parameter new style content clamp parameter new old parameter combine accomplish desire classification extrapolation translation task section focus asymmetric model use extrapolation classification training separate style content face render b figure asymmetric model face vary identity head pose adaptation procedure symmetric model similar base describe procedure application extrapolation translation task training let number observation style s content c let l sum observation estimate c minimize asymmetric model eq find iterate fix point equation obtain set derivative error equal o ensure stability update parameter accord c typically use stepsize replace c c yield analogous procedure train model eq number observation available pair exist closedform procedure fit asymmetric model use let kdimensional vector denote mean observed datum generate style content c stack vector single s matrix compute define j matrix column matrix b j row finally identify b desire parameter estimate stack form b tenenbaum freeman model dimensionality choose standard way consideration require sufficiently good approximation datum measure mean square error metric look gap singular value spectrum test straightforward adapt asymmetric model incomplete new style order extrapolate style unseen content simply estimate eq use b c value learn training restrict sum e content class observe new style data content e style s synthesize b c extrapolate incomplete new content unseen style similarly adapt asymmetric model classification new style involved content class new datum possibly style unlabeled deal uncertainty embed bilinear model mixture model yield separable mixture model smm fit efficiently datum new style use specifically assume probability new unlabeled observation generate style content spherical gaussian center prediction asymmetric bilinear model e c total probability ep e use equal prior e assume content vector bc know training new style matrix find explain test datum alternate compute soft style assignment test vector current style matrix estimate estep estimate new style matrix set maximize y mstep mstep solve closed form use update rule m c test vector new style classify group vector content class e maximize application speech recognition example illustrate approach classification datum set benchmark connectionist learn algorithm datum consist sample vowel speaker originally collect datum vector consist parameter compute linear predictive analysis speech compare learning train categorize vowel speaker male female test sample remain speaker male female use procedure describe fit asymmetric bilinear model training datum label style speaker content vowel use learn vowel parameter b c smm test classification performance vary degree style information new speaker data separate style content style content label miss test vector style label present indicate change speaker content label miss smm label miss test datum loglikelihood augment prior favor temporal continuity style assignment smm training style problem difficult good approach obtain correct vowel classification test set multilayer perceptron neighbor classifier good performance standard technique try hastie recently obtain correct use discriminant adaptive near good result know approach model speaker style obtain correct smm use model dimensionality model variance use vowel class assignment initialize estep good initial condition important range model dimensionality variance setting reasonable performance apply method head pose datum fig train subject pose use smm learn style model new person simultaneously classify head pose obtain correct pose categorization average test subject compare correct performance match result demonstrate model style content substantially improve content classification new style style information available testing dramatically style available explicitly smm implicitly smm bilinear model offer easy way improve performance use style label frequently available classification task work conclusion discuss extrapolation translation problem summarize result figure extrapolation partially observe font unseen letter work training present letter font left shape topology describe letter warp black particle reference shape letter shape testing fit asymmetric model style matrix letter figure use good fit linear combination training font prior style matrix order control model complexity use fit style synthesize unseen letter compare actual letter style weight symmetric model independent particular style content class allow translation observation unknown style know training fit symmetric model observation test observation new style content class find value use know number iterate square fit parameter typically result vector unique uncertainty scale use approach translate shape lighting condition image face translate illumination color color measurement assume small work naturally combine current connectionist learn literature factorial learning learn family relate task b tenenbaum freeman training font r d e e b c c c e e e h g g h u h figure style extrapolation training datum letter font leave test datum letter right synthesize letter compare miss facilitate task transfer separable bilinear model provide powerful framework separate style content combine explicit representation factor computational efficiency linear model acknowledgement thank m helpful discussion tenenbaum reference r learn relate task time backpropagation adv neural info proc system volume page t freeman b learn bilinear model problem vision tr cambridge learning adv neural system volume page r connectionist generalization production example network p lowdimensional representation human face arbitrary lighting condition proc ieee cvpr page t hastie tibshirani discriminant adaptive near neighbor anal mach e hinton autoencoder minimum description length free energy neural info proc system volume d concept analogy basic book generic bilinear problem press r h matrix differential application statistic linear model surface opt soc l s family discovery neural info proc vol dynamic error propagation network phd engineering dept shape motion image stream factorization method m recognition
sensitivity analysis hmms application likelihood maximization abstract paper consider sensitivity analysis hide markov model continuous state observation space propose innitesimal perturbation analysis ipa ltere distribution respect parameter model describe methodology use algorithm estimate density sequential method design estimate gradient result ipa estimator prove asymptotically unbiased consistent computational complexity linear number particle consider application analysis problem identify unknown parameter model sequence observation derive estimator gradient loglikelihood use gradient method purpose likelihood maximization illustrate method numerical experiment introduction consider parameterized hide markov model hmm dene continuous state observation space hmm dene state process observation process yt parameterize continuous parameter compact subset state process markov chain value measurable state space initial probability measure mx ie assume sample markov chain use transition function independent number ie t ut u probability space practical situation p uniform ut uniform random number simplicity adopt notation u u rst transition function ie u u observation process yt t lie measurable space link state process conditional probability measure yt marginal density function assume observation conditionally independent state transition observation process parameterize parameter state observation yt process depend explicitly notation simplicity omit write dependence yt possible ambiguity main interest hmms recover state time sequence past observation write ltere distribution belief state pxn distribution condition information dene analogously predictive pxn contribution innitesimal perturbation analysis ipa estimate gradient refer derivative respect parameter ltere distribution precisely estimate function ltere distribution consider application problem parameter hmms consist estimate unknown parameter model serve generate sequence observation maximum likelihood ml approach search parameter maximize likelihood logarithm sequence observation dene log p p maximum likelihood estimator arg max asymptotically consistent sense converge surely true parameter condition mild assumption model theorem dm use ml approach parameter problem reduce optimization problem second contribution sensitivity analysis predictive distribution enable estimate gradient loglikelihood function use stochastic gradient method purpose optimize likelihood approach numerically illustrate parameter problem autoregressive model stochastic volatility model compare approach kalman lter likelihood ratio approach apply link work let mention interested continuous state case numerous application signal processing robotic naturally t framework general set exist closedform expression distribution space viterbi algorithm apply lineargaussian model kalman lter use paper use socalled sequential method know particle filter numerical tool apply large class model illustration challenging example problem parameter estimation stochastic volatility model nonlinear nongaussian continuous space parameterize continuous parameter describe experimental section usual approach parameter estimation consist perform maximum likelihood estimation mle ie search likely value parameter observe datum space problem expectation maximization popular method solve mle problem continuous space problem difcult use mainly expectation rely estimation posterior path measure intractable situation maximization complicated model belong linear exponential family alternative method consist use force optimization method base evaluation likelihood simulate anneal method approach optimization efcient high dimensional parameter space approach treat parameter state variable compute optimal lter case bayesian posterior distribution parameter marginal optimal lter know method stable certain condition perform practice large number time step solution consist use optimization procedure base evaluation gradient loglikelihood function respect parameter approach study continuous space hmms idea use likelihood ratio approach score method evaluate gradient likelihood approach suffer high variance estimator particular problem small noise dynamic tackle issue propose use marginal particle lter instead simple particle lter approximation method approach efcient term variance reduction computational complexity quadratic number particle instead linear particle method ipa approach propose paper alternative gradientbased maximum likelihood approach compare work gradient approach previously cite ipa provide usually low variance estimator likelihood ratio method numerical complexity linear number particle work relate socalle tangent lter approach describe dynamic come discretization diffusion process describe different setting ie policy gradient partially observable markov decision process similar estimator design setting result estimator bias usual scheme ipa estimator sequential monte method measurable test function r kxt kxt use notation yt g x general impossible write analytically specic case lineargaussian ltere paper consider numerical approximation base smc method mention method extended kalman lter quantization method markov method use build ipa estimator propose section basic smc method bootstrap filter detail approximate empirical distribution particle generic sequential t n sample sample set uit n dene importance sampling weight gt e resample set n index select weight end return sampling transition step generate successor particle population accord state dynamic previous population sample weight selection step resample replacement n particle set accord weight resample use avoid weight decrease consist select new position preserve consistency property ie simple version introduce choose selection index independent sample set accord multinomial distribution parameter idea replicate particle proportion weight variant propose literature resample method optimal term convergence issue eg law large number central limit theorem discuss dm purpose note mild condition asymptotically unbiased asymptotic expression bias consistent estimator innitesimal perturbation analysis hmms sensitivity analysis distribution follow decomposition gradient ltere distribution apply function problem nde estimator reduce nde estimator gt dominant innitesimal method estimate gradient expectation markov chain innitesimal perturbation analysis ipa method score function method likelihood ratio method instance p detailed presentation method use estimate know low variance general far know use context object section appropriate smoothness assumption proposition gradient expectation random variable equal expectation involve pair random variable x ef ef ef refer derivative respect state variable apply property estimate gt e e dene augment markov chain u u r rt follow recursive relation introduce augment markov chain equation use equation rewrite ef ef state condition previous derivation sound proposition equation valid follow condition path surely differentiable continuously differentiable gt continuously differentiable gt continuous differentiable let random subset gt fail differentiable require t gt proof proposition direct application notice require path equivalent require transition function continuously differentiable respect equation derive ipa estimator use smc inn r zni particle derive use smc algorithm augment markov describe ipa estimation t n n iid uit sample set set zti uit uit zti set set zti end select compute weight wit index proposition assumption proposition estimator dene bias consistent inn f surely addition asymptotic variance proof use general smc convergence property model dm apply chain random potential tion test function state consistent asymptotic expression bias order apply result test function use representation gradient estimator asymptotically unbiased consistent asymptotic variance central limit apply estimator remark notice computation gradient estimator require m dimension elementary operation linear number particle linear number parameter memory requirement gradient loglikelihood maximum likelihood approach problem parameter follow stochastic gradient method maximize loglikelihood t obtain estimate term sum use similar decomposition predictive distribution apply e xk k ipa estimator zti gt zni zni particle derive use smc algorithm augment markov chain describe previous subsection use similar argument detail proof proposition estimator asymptotically unbiased consistent result gradient describe step choose appropriately local convergence occur detailed analysis stochastic approximation likelihood maximization gradient ascent use ipa estimator number gradient step initialize t n n iid uit set uit set uit pn pn set set zti t p j compute weight set zti xkt index select end perform gradient ascent step end method method method figure plot parameter estimate model compare method kalman ipa use observation particle numerical experiment consider typical problem report result variance estimator autoregressive model ar simple lineargaussian hmms solve method kalman algorithm enable compare performance algorithm dynamic iid independent sequence random variable threedimensional parameter stochastic volatility model popular quantitative evaluate derivative option nonlinear nongaussian model method use dynamic iid t vt parameter parameter figure result gradient estimator parameter problem compare method kalman lter apply model lineargaussian unknown parameter use notice apparent bias method estimation kalman provide exact ltere distribution number observation use particle gradient iteration run time random starting point order illustrate draw method sensitive starting point observe term estimation accuracy competitive method kalman design specic model lineargaussian apply general model example stochastic volatility model figure set estimate use ipa n observation particle comparison kalman apply complicated variance study score ipa ipa score method provide gradient estimator general model compare variance corresponding estimator gradient ar model know exact value use kalman value parameter number figure plot parameter estimate apply stochastic volatility model use observation particle figure variance ipa score estimator partial derivative focus study problem volatility estimation challenge value respective performance algorithm case parameter use ipa estimator perform score estimator small value hand case huge variance state model use score estimator score method figure variance loglikelihood derivative ln compute ipa score method true parameter estimation compute let mention variance ipa score estimator increase number observation increase weak condition hmm lm ltere distribution gradient forget exponentially fast initial distribution property use estimator cm smoothing drastically reduce variance raise bias similar smooth discount provide efcient variance reduction technique ipa estimator conclusion propose sensitivity analysis hmms base innitesimal perturbation analysis provide computationally efcient gradient estimator provide interesting alternative usual score method analysis use estimate gradient loglikelihood gradientbased likelihood maximization approach purpose parameter finally let mention estimator higherorder derivative derive ipa approach enable use sophisticated optimization technique method reference cm r particle policy gradient neural information processing system stochastic particle method linear tangent ltere equation editor optimal control innovation application page press e mouline use particle maximum processing conference p interact particle system application doucet sequential method r c asymptotic maximum likelihood estimator general hide markov model r e mouline limit theorem weight sample application sequential annal statistic p doucet sharp propagation chaos estimate model theory probability application doucet vb parameter estimation general statespace model use particle method ann l method parameter track numerical experiment technical report gradient estimation perturbation analysis approach nonlinear nongaussian bayesian state estimation proceeding volume page smooth nongaussian model comput graph stat stochastic approximation algorithm application l exponential geometric hide markov model mathematic control system r rj hidden markov model international series operation management science uniformly convergent adaptive particle lter journal apply probability doucet ss particle method optimal lter derivative application parameter estimation optimization stochastic model interface simulation optimization academic particle method parameter estimation general state space lter statespace model presence unknown static parameter ieee transaction signal processing
semicrowdsource cluster generalize crowd labeling robust distance metric learning jin machine learn main challenge datum clustering define appropriate similarity measure object crowdclustere address challenge define pairwise similarity base manual annotation obtain crowdsource encouraging result key limitation crowdclustere cluster object manual annotation available address limitation propose new approach clustering semicrowdsource clustering effectively combine lowlevel feature object manual annotation subset object obtain crowdsource key idea learn appropriate similarity measure base lowlevel feature object manual annotation small portion datum cluster difficulty learn pairwise similarity measure significant noise variation manual annotation obtain crowdsource address difficulty develop metric learning base matrix completion method empirical study realworld image datum set propose outperform stateoftheart distance metric learning algorithm clustering accuracy computational efficiency introduction provide easy relatively inexpensive way utilize human capability solve difficult computational learning problem eg image annotation game divide large task number task refer human intelligence task hit ask human worker solve individual hit combine partial solution obtain individual hit form final solution past explore number machine learn task classification cluster crowdclustere exploit paradigm datum cluster key idea obtain manual annotation object crowdsource annotation form grouping object base perceive similarity assignment individual object eg image human worker pairwise similarity matrix compute acquire annotation use cluster object conventional clustering technique similarity measure define base feature object crowdclustere pairwise similarity derive manual annotation capture underlie similarity study perform significantly conventional clustering method sufficiently large number manual annotation object cluster figure propose framework semicrowdsourced cluster n object need cluster small subset object annotate crowdsource encouraging result obtain crowdclustere main shortcoming cluster object manual annotation available significantly limit application large scale clustering problem instance cluster thousand object feasible object manually annotate multiple worker address limitation study problem semicrowdsourced clustering annotation obtain crowdsource small subset object objective cluster entire collection object figure depict propose framework set object cluster objective learn pairwise similarity measure label object object feature vector x note available expect n object label crowdsource key semicrowdsourced clustering define appropriate similarity measure subset object manual annotation ie object end propose learn similarity function base object feature pairwise similarity derive manual annotation subset object apply learn similarity function compute similarity object perform datum clustering base compute similarity study computational simplicity restrict linear similarity function ie object oj feature representation xi respectively similarity m xj m learn learn linear similarity function pairwise similarity refer pairwise constraint similarity binary know distance metric learning study extensively literature key challenge distance metric learning semicrowdsourced clustering arise noise pairwise similarity obtain manual annotation accord large disagreement observe human worker specify pairwise similarity result pairwise similarity base majority vote human worker disagree true cluster assignment object example author scene datum set pairwise label obtain human worker inconsistent true cluster assignment large noise pairwise similarity distance metric learning lead poor prediction performance demonstrate empirical study propose metric learning algorithm explicitly address presence noise pairwise similarity obtain crowdsource propose algorithm use matrix completion technique noisy pairwise similarity regression analysis efficiently learn figure propose framework learn distance metric noisy manual annotation distance metric restore pairwise specifically propose cluster object consist component filter noisy pairwise similarity n object object pair pairwise similarity agree worker majority worker result filtering step partially observe similarity entry recover similarity matrix partially observe entry use matrix completion apply regression learn distance metric recover similarity matrix cluster pairwise similarity base learn distance metric figure basic step propose compare exist approach distance metric learning propose algorithm follow advantage explore matrix completion technique propose robust large noise pairwise utilize regression analysis propose algorithm computationally efficient handle positive semidefinite constraint key computational bottleneck distance metric learning learned distance metric high probability close optimal metric learn perfect true similarity ie similarity object cluster arbitrarily large finally note addition distance metric learning learn constrain clustering apply generalize information manual annotation acquire crowdsource work focus distance metric learning relate work discussion explore kernel learning constrain clustering technique semicrowdsource clustering find section semicrowdsource clustering robust distance metric learning present problem general framework semicrowdsourced clustering describe propose algorithm learn distance metric small set noisy pairwise similarity derive manual annotation problem definition framework let set object cluster let x feature representation rd vector d dimension randomly sample subset b o obtain manual object collection d denote d annotation crowdsource let m number hit use manual annotation collect kth hit define similarity matrix rnn bi bj share common annotation ie share common annotate keyword object assign cluster worker object annotate kth hit unlabeled pair note consider binary similarity measure study goal perfectly reconstruct ideal pairwise similarity base true cluster assignment ie object assign cluster objective semicrowdsourced clustering cluster n object base b feature m m similarity matrix m object d paper assume number cluster denote r priori relax requirement estimate number cluster heuristic eg consider number cluster rank complete matrix b entire collection object d generalize pairwise similarity subset propose learn distance metric similarity matrix m compute pairwise similarity n object use learn distance metric challenge learn appropriate distance metric set similarity matrix m straightforward approach combine multiple similarity matrix single similarity matrix compute rnn average similarity matrix average specifically let m pm aij bi bj label kth hit ie object bi indicate pair o bj annotate kth worker indicator function output e main problem true learn distance metric m simple strategy large disagreement worker determine pairwise average similarity correlate true cluster assignment subsection develop efficient robust algorithm learn distance metric set noisy similarity matrix learn distance metric set noisy similarity matrix illustrate figure propose algorithm consist step ie filter step matrix completion step distance metric learning step step datum preprocesse step follow idea propose filtering step filter uncertain object pair introduce threshold d similarity measure small d d average similarity matrix indicate worker corresponding object pair different cluster simply set similarly set similarity measure large object pair similarity measure range d treat uncertain object pair discard mark unobserved similarity matrix result partially observe similarity matrix aij aij d unobserved define set observe entry aij aij d matrix completion step construct partial clustering result generate different worker expect binary similarity measure incorrect introduce matrix e rnn capture incorrect entry perfect similarity matrix p e p p output matrix bij appropriately choose threshold expect observe entry correct result e sparse matrix reconstruct perfect similarity matrix follow matrix completion theory solve follow optimization problem b s t p b e p min b p nuclear norm matrix e ij norm e use b low rank assumption fact sparse matrix b b optimal solution high probability include supplementary document theoretical result problem distance metric learning step step learn distance metric complete similarity b common problem share distance metric learning algorithm high matrix computational cost constraint distance metric positive semidefinite study develop efficient algorithm distance metric learning deal positive semidefinite constraint algorithm base key observation high b positive semidefinite accord probability complete similarity matrix b theorem probability true cluster assignment property guarantee result distance metric positive propose distance metric learning base standard regression optimal distance metric m regression problem similarity matrix m b bj lm m rdd ij bi x b b bi feature vector sample object optimal solution denote bx bx m b pseudo inverse z straightforward verify m b directly use solution result overfitting similarity matrix address challenge smoothing technique potential bx m identity matrix size d smooth parameter use address overfitting curse dimensionality note computation simplify term singular value singular vector b omit detail express m space constraint let oj perfect similarity output state theoretical property m oj belong cluster straightforward oj r cluster assignment object learn ideal distance metric perfect similarity measure generalize regression problem follow m m rdd solution cx x let smoothed version ideal distance metric m m follow high probability difference small small assume feature representation object assume condition hold probability m stand spectral norm matrix detailed proof find supplementary material learn distance metric construct similarity matrix m apply spectral clustering m compute final data partition n object experiment section demonstrate empirically propose semicrowdsource clustering algorithm effective efficient datum set baseline parameter setting datum set realworld image datum set use experiment imagenet datum set subset large imagenet database subset contain image belong category horse cart pascal datum set subset pascal visual object class challenge database subset contain image belong class car dog cat bird choose specific image category yield relatively low classification performance competition pascal voc challenge indicate difficult cluster image use low level feature information image feature dataset download imagenet database research group learn recognition vision respectively perform follow ask human worker annotate image keyword choice hit total worker employ use amazon mechanical annotate image imagenet pascal dataset respectively average image annotate different worker keyword individual worker hit pairwise similarity image use section set image share common annotate baseline baseline method use reference point study base method cluster image directly use image feature distance metric learning raw e filter method run propose algorithm average similarity matrix matrix completion step comparison base method allow examine effect distance metric learning semicrowdsource clustering comparison raw method reveal effect filtering matrix completion step distance metric learn compare propose algorithm distance metric learning follow stateoftheart distance metric learning algorithm global distance metric learning rca relevant component analysis discriminative component analysis information theoretic metric learning lmnn large margin near stateoftheart distance metric learning eg neighborhood component analysis exclude comparison work class assignment instead pairwise similarity applicable case code baseline algorithm provide respective author lmnn principal component analysis pca use reduce datum lower dimension fair comparison distance metric learning algorithm apply n pairwise similarity matrix reconstruct constraint derive completion algorithm refer propose distance metric learning regression base distance metric learning rdml short propose semicrowdsource cluster algorithm parameter setting criterion use determine value d d d small large ensure retain pairwise similarity consistent cluster assignment second large small obtain sufficient number observed entry partially observe matrix data set set d follow heuristic propose determine parameter select generate balanced clustering result parameter set vary find clustering result essentially remain unchanged evaluation normalize mutual information short use measure coherence infer clustering ground truth categorization number sample image varied experiment perform pc processor gb main memory experiment repeat time performance average trial report try similarity measure cosine similarity measure tfidf weighting find yield performance simple similarity measure use work imagenet datum set pascal datum set figure sample image use image incorrectly place different cluster base method similarity correctly group cluster propose method similarity image incorrectly place different cluster base method similarity correctly group cluster propose method similarity c image incorrectly group cluster base method similarity correctly cluster different cluster propose method similarity figure sample image pair incorrectly cluster base method correctly cluster propose method similarity method base normalize distance metric m experimental result examine effect distance metric learning semicrowdsourced cluster figure compare cluster performance different metric learning algorithm base method learn distance metric observe distance metric learning ie propose rdml outperform base method fail improve cluster performance base conjecture failure rca method sensitivity noisy pairwise similarity fact rca yield performance base method pairwise similarity consistent cluster assignment compare baseline distance metric learning rdml propose distance metric learning yield good clustering result datum set value ie number annotate image consider furthermore performance rdml gradually stabilize number image increase consistent theoretical analysis imply modest number annotate image need propose algorithm learn appropriate distance metric observation particularly useful crowdclustere expensive reliably label large number image figure example image pair base method fail correct cluster assignment propose rdml method successfully correct mistake learn distance metric experiment evaluate impact filtering matrix completion step figure compare clustering result propose algorithm semicrowdsourced cluster raw method run propose distance metric rdml filtering matrix completion step base experiment follow observation propose distance metric learning algorithm perform raw method particularly number annotate image small gap propose semicrowdsource clustering method raw method decrease sample size increase result indicate importance filtering matrix completion step datum semicrowdsourced clustering finally interesting observe raw method outperform baseline method far effectiveness propose algorithm distance metric learn finally evaluate computational efficiency propose distance metric learning table propose distance metric learning significantly efficient baseline approach evaluate row table indicate run time table cpu time second learn distance cpu time sample size propose matrix imagenet datum set pascal datum set matrix completion step distance metric learning algorithm apply similarity matrix recover matrix completion computational cost matrix completion share distance metric learning algorithm use evaluation observe completion step particularly large sample size computationally demand problem investigate future work relate work discussion crowdclustere propose divide task cluster collection image number human intelligence task hit hit small subset image randomly sample collection worker ask cluster subset image multiple group use large number hit author ensure image collection include hit author extend definition hit crowdclustere ask worker annotate image keyword derive pairwise similarity image base commonality annotate keyword major limitation study point early cluster image manually annotate matrix completion technique propose crowdclustere different goal work matrix completion use estimate similarity matrix propose approach use matrix completion estimate distance metric crowdsource label generalize cluster image annotate crowdsource work closely relate distance metric learning learn distance metric consistent subset pairwise study distance metric learning report address challenge learn reliable distance metric noisy pairwise constraint limitation early study work relatively small number typically noisy pairwise constraint contrast semicrowdsourced clustering expect significantly large percentage pairwise similarity inconsistent true cluster assignment limitation distance metric learning restrict linear similarity function learning generalize distance metric learning nonlinear similarity function mapping data point high dimensional space kernel function plan learn base similarity function subset manually annotate object distance metric learn alternative approach incorporate manual annotation clustering process constrain clustering semisupervise cluster compare distance metric learn constrain clustering computationally expensive distance metric learning learn distance metric pairwise constraint apply learn distance metric cluster set object constrained clustering algorithm new set object need cluster exploit strength constrain clustering algorithm plan explore hybrid approach effectively combine distance metric learning constrained clustering approach accurate efficient semicrowdsourced cluster acknowledgment work support national science foundation iis research award reference hertz learn mahalanobis metric equivalence constraint jmlr constrain clustering advance theory application chapman cande tao power relaxation nearoptimal matrix completion ieee transaction information theory m cover element information theory p jain dhillon metric learning icml page largescale hierarchical image database cvpr m zisserman pascal visual object class challenge voc result p bayesian hierarchical model learn natural scene category cvpr page roweis geoffrey e salakhutdinov component analysis nip r p krause p crowdclustere nip learning distance metric contextual constraint image retrieval cvpr page metric learning optimization analyze acm partially observe optimization icml page dc gg introduction linear regression analysis volume son scholkopf alexander learning support vector machine optimization e image annotation proceeding normalize cut image segmentation pami adaptively learn crowd icml weinberger j metric learning large margin near neighbor classification nip p p multidimensional wisdom crowd nip h jin learn uncertain information automate tag acm distance metric learning application cluster nip jin distance metric learning comprehensive survey computer science crowdclustere sparse label matrix completion approach human computation
efficient principled learning thin junction abstract present truly polynomial algorithm structure junction tree attractive subclass probabilistic graphical model permit compact representation probability distribution efficient exact inference constant treewidth algorithm polynomial time sample complexity junction tree sufficiently strong dependency exist provide strong theoretical guarantee term kl divergence result true distribution present extension approach lead significant speed practice demonstrate method empirically real world dataset key new theoretical insight method bound conditional mutual information arbitrarily large set variable mutual information computation subset variable underlie distribution approximate junction tree introduction application eg medical diagnosis performance monitor probabilistic inference play important role decide patient treatment useful know probability know important able represent probability distribution compactly perform inference efficiently probabilistic graphical model successful compact representation probability distribution order use need define structure parameter value usually datum sample probability distribution learn structure datum crucial task formulation structure learning problem npcomplete structure learn algorithm guarantee output local optimum notable exception work abbeel learn structure factor graph provide probably approximately correct pac guarantee represent probability distribution compactly exact inference compact model abbeel remain intractable attractive solution use junction tree limited treewidth subclass permit efficient exact inference tree likely mle structure junction tree learn efficiently use chowliu representational power tree address problem learn fix treewidth learn likely npcomplete algorithm global guarantee learn polynomial algorithm pac guarantee guarantee term difference loglikelihood mle model variable independent result guarantee achieve constant fraction difference constant improve data increase imply algorithm pac guarantee complexity exponential contrast provide truly polynomial algorithm pac guarantee contribution paper follow theoretical result upper bound conditional mutual information arbitrarily large set random variable polynomial time particular assume efficiently computable mutual information oracle exist polynomial algorithm structure junction tree strong dependency provide degradation guarantee distribution approximately representable fix treewidth figure junction tree rectangle denote clique separator mark edge approach structure learn input oracle treewidth threshold l l set useful component q q l l q return heuristic allow practical empirical evidence approach realworld dataset bound treewidth graphical model general represent probability distribution p discrete variable need space exponential size junction tree limited treewidth allow compact representation tractable exact inference briefly review junction tree detail let c c cm collection subset element c clique let t set edge connect pair clique tree definition tree t junction tree iff satisfy run intersection property rip ci cj c ck unique simple path ci cj ci cj set sij ci cj separator correspond edge ij t size large clique junction tree treewidth tree example junction tree fig variable contain contain simple path large clique size treewidth junction tree distribution p representable use junction tree t c instantiate variable separator sij render variable different sij independent denote fact independent b c b c let clique reach ci use edge ij denote reachable variable viji ck ci sij ij example fig definition p factor accord junction tree t iff t viji sij distribution p factor accord junction tree treewidth p kjt representable case projection ptc p t c define ci p ci ptc equal p clarity consider maximal junction tree separator size p kjt representable factor accord maximal treewidth practice notion conditional independence strong instead natural relaxation require set variable low conditional mutual information denote entropy nonnegative iff b s intuitively new information extract b know definition t c junction tree p t viji sij notation note paper use small letter y denote variable letter denote set variable font d denote set set exist junction tree t c p p kjt representable case kullbackleibler divergence projection p t c p bound kl p ptc bind mean junction tree p instead p use tractable principled approximation ptc inference paper address problem learn structure junction tree data sample p structure learn paper address following problem datum multiple temperature reading sensor sensor network treat datapoint instantiation variable v seek find good approximation p assume p representable aim find junction tree p treewidth small possible note maximal treewidth consider constant problem input complexity approach exponential let initially assume oracle compute mutual find conditional b c exactly disjoint distribution input separator s oracle subset b c strict max set size address section use oracle approach qs set singleton evaluate s q possible k record find min pair q s merge list l junction tree t consistent list l iff return c hold sij viji form junction tree consistent l junction tree p tree find procedure implement use constraint satisfaction alg summarize idea algorithm follow outline include form class approach algorithm use mutual information test constrain set possible structure return consistent constraint unfortunately use directly impractical complexity exponential total number variable follow section discuss alg present efficient solution global independence local test problem inner loop alg line separator need oracle exponentially time q drawback address section second mutual information oracle s subset b size unfortunately know way compute mutual information estimate datum time sample complexity exponential previous work address problem particular approach exponential complexity general need estimate subset size new result state limit compute mutual information small subset variable let p kjt representable distribution let hold compute upper bind s use nk onk ie oracle involve variable bound quality approximation p projection junction tree t corollary condition hold p sij viji separator sij junction tree t n junction tree p partition algorithm weak conditional independency efficient upper bind oracle let turn reduce number oracle exponential polynomial notation note set b c denote b c notation efficient approach structure learn input oracle treewidth threshold l q l l q input list l component q s q l order increase q greedily check q record decomposition exist s s return corresponding junction tree return return tree find present approximate solution problem assume efficient approximation oracle exist key observation rely function submodular b allow minimization submodular function use evaluation combine approach partition conditionally independent subset use evaluation compute set size complexity approach exponential general approach contrast polynomial complexity q q approach use subroutine gain intuition suppose exist junction tree p separator subset b c different s junction tree definition mean b c s look subset b c true partitioning know set test possible way partition subset ax possible partitioning conclude variable separator junction tree include separator notice min use algorithm evaluate time instead time minimization exhaustive search initially assume variable form partition q test variable separator follow partition q separate q line alg process repeat large set variable size q converge set partition independent proposition time complexity k time complexity compute c b c k q important partitioning algorithm return partition similar connected component viji true junction tree p formally let define desirable property suppose junction tree p output algorithm threshold partition correct q viji q correct algorithm variable separator algorithm weak q sij small weak algorithm variable different separator correspond mutual information variable large ideally want correct weak algorithm separate variable different true junction tree introduce independency use instead line alg satisfie requirement relaxed version second q correct n weak implement use dynamic programming concrete form procedure step need alg practical adopt dynamic programming approach use purpose briefly review intuition detail consider junction tree t let sij separator t c set reachable ci use edge j denote set edge t connect clique t c junction tree p junction tree p viji sij subtree consist clique ci connect ci example fig subtree clique decompose clique include clique clique recursive structure suggest dynamic programming approach component q check small subtree cover variable q formally require follow property definition s q l si isi m qi s connect directly x qi ensure run intersection property subtree q set q sm qm decomposition unfortunately check decomposition exist equivalent npcomplete exact set cover problem requirement def unfortunately challenging issue address algorithm use complexity polynomial use simple greedy approach start candidate decomposition d add l d property def hold eventually def hold return decomposition return decomposition exist result procedure proposition separator size k time complexity onk combine arrive alg overall complexity dominate equal onk general greedy decomposition check miss junction tree consistent list component class distribution guarantee find junction tree intuitively require sij viji junction tree alg add component decomposition sij viji l requirement guarantee distribution variable clique junction tree sufficiently strongly interdependent certain level mutual information t c p edge t separator separator c x k k strongly connect alg output p sample complexity far assume mutual information oracle exist distribution p efficiently query real life datum ie sample p work probabilistic estimate b c accuracy probability use number sample computation time polynomial log entropy probability distribution discrete variable domain probability use size r estimate accuracy r rk log rk log rk sample p time employ oracle algorithm performance guarantee probabilistic theorem exist k strongly connect junction tree p sample base use k r nk time find tree p probability finally p kjt representable correspond junction tree strongly connect let use alg find probability arbitrarily close junction tree approximate p arbitrarily time polynomial log ie class strongly connect tree probably approximately correctly learnable class p distribution pac learnable p p learning algorithm output p probability time polynomial log corollary exist strongly connect junction tree p learn junction tree p probability use o log log sample p log log computation time evaluation mutual information alg require value threshold input tight quality guarantee need choose small find junction tree priori value know need procedure choose optimal natural way select binary search discrete random variable domain size r p x hold guarantee find junction tree clique connect separator restrict binary search range log binary search value alg check result minimize s onk complexity value possible find optimal check s course search process intuitively think set partition qs alg set connected component graph variable vertex hyperedge connect variable increase hyperedge disappear number connected component independent set increase specifically graph qs maintain separator s s add hyperedge connect variable annotate strength s qs ree qs return tree increase strength ie strength weak remain hyperedge remove hyperedge fig example evolution k far save computation time exploit observation subset connected component q add hyperedge qs change qs test hyperedge contain connected component increase component disconnected edge add component induce incorrect independency issue address second insight find junction tree particular value need component use tree insight lead simple procedure return tree t c check hyperedge component use form edge add return c value qs change iterate procedure find solution evaluation evaluate approach apply realworld sensor network temperature artificial sample alarm dataset implementation lpacjt use evaluation section baseline comparison use simple hillclimbe heuristic combination lpacjt hillclimbe intermediate result return use start point hillclimbe algorithm denote denote experiment run ghz runtime hour necessary entropy cache alarm datum sample know bayesian network treewidth learn model treewidth computational concern fig b loglikelihood learn model test datum depend training datum small training dataset lpacjt find model basic approach bad obs chowliu implementation use regularization outcome expect conclude dataset approach overfit hillclimbe large training set lpacjt result achieve likelihood true model limit model small treewidth chowliu perform bad limit model treewidth fig c example structure find lpacjt alarm datum lpacjt miss edge true model hillclimbe kind available replace variable variable connected tree leaf clique ci clique ci sij connect separator local example evolution training set size alarm loglikelihood temperature local training set size loglikelihood loglikelihood chowliu c alarm structure temperature sample run training point time second d temperature loglikelihood e temperature sample run traffic loglikelihood loglikelihood true model alarm obs chowliu local training set size traffic loglikelihood figure example evolution qs section structure learn experimental result example evolution test set likelihood good find model e node denote variable edge connect variable belong clique green edge belong true learn model blue edge belong learn model red true temperature data month deployment sensor nod datapoint variable discretize bin learn model treewidth location sensor shape loop problem learn thin junction tree data hard fig lpacjt perform good approach large training set algorithm expect lpacjt outperform chowliu algorithm significant margin datum available overfit small training set fig e evolution test set likelihood good high training set likelihood structure identify time structure identify minute final result hour traffic dataset contain traffic flow information measure minute location month select location area experiment discretize traffic flow value bin learn model treewidth algorithm include lpacjt result essentially quality relation prior work conclusion brief overview prior work refer reader fig closely related learn factor graph learn markov net approach guarantee low treewidth result instead settle guarantee low treewidth guarantee difference loglikelihood result fully independent model constant factor difference likely exponential complexity approach polynomial complexity quality guarantee hold strongly connect kjt representable distribution hold present truly polynomial algorithm learn junction tree limited treewidth base new upper bind conditional mutual information compute use polynomial time number sample algorithm guarantee find junction tree close kl divergence true distribution strongly connect kjt representable distribution special case guarantee strongly connected representable distribution believe new theoretical insight provide significant step understanding structure learning graphical model useful analysis approach problem addition theory demonstrate experimentally theoretical idea future use development fast effective structure learn heuristic approach model guarantee true distribution sample score tractable local poly score tree global score tree mixture local score compact local poly score score tractable poly positive poly poly constraint constraint tractable tractable poly paper figure prior work majority literature approach try maximize target function usually regularize likelihood perform conditional independence test restrict set candidate structure consistent result test tractable mean result guarantee limited treewidth compact limited connectivity graph guarantee column result local global optimum pac guarantee difference loglikelihood result fully independent model difference likely true distribution class distribution guarantee hold mean complexity poly exponential general poly special case pac mean pac different degradation guarantee acknowledgment work support nsf grant iis onr support p sloan fellowship thank helpful discussion share source code reference abbeel factor polynomial time sample jmlr complexity find embedding algebraic discrete method r bach junction tree nip m alarm monitor system case study inference technique belief network medicine choi h network approximation edge deletion approximate discrete probability distribution dependence tree ieee transaction information theory r g p l probabilistic network expert system information science statistic springer datum acquisition sensor network learning robust learning product distribution colt d srebro learning markov network maximum bounded treewidth krause nearoptimal value information graphical model m m learn mixture tree jmlr m bounded treewidth graphical model m minimize symmetric submodular function math program moore find optimal bayesian network dynamic programming center automate learning discovery p c r prediction search mit press m search simple effective algorithm learn network
periodic finite state controller efficient pomdp plan computer fi information computer fi abstract application robot control wireless communication require plan uncertainty partially observable markov decision process pomdps plan policy single agent uncertainty decentralized version decpomdp find policy multiple agent policy problem represent state controller fsc introduce novel class periodic fsc compose layer connect previous layer periodic find deterministic policy convert initial periodic infinitehorizon policy policy optimize new infinitehorizon yield deterministic periodic policy new expectation maximization algorithm yield stochastic periodic policy method yield result early planning method compute large solution regular fsc introduction machine learning application involve plan uncertainty planning necessary medical diagnosis control robot agent dynamic spectrum access wireless communication system planning task represent reinforcement learn problem action policy control behavior agent quality policy optimize maximize reward function single agent policy optimize partially observable markov decision process pomdps world state uncertain decentralize pomdps decpomdps optimize policy multiple agent act direct communication separate observation belief world state maximize joint reward function pomdp decpomdp method use representation policy value function graph finite state controller fsc present novel efficient method pomdp planning focus infinitehorizon problem policy operate introduce new policy representation periodic finite state controller intelligent restriction speed optimization yield solution periodic fsc compose layer subset state transition allow state layer final layer policy proceed layer periodic fashion policy optimization determine probability state transition action choice maximize reward work main contribution firstly introduce improved optimization method standard problem policy compression secondly method transform initial infinitehorizon periodic fsc introduce compression periodic fsc introduce expectationmaximization train algorithm plan periodic fsc result method perform early pomdp method pomdp method policy use periodic fsc enable compute large solution regular fsc online execution complexity deterministic fsc layer width stochastic discuss exist pomdp decpomdp solution method section formally define infinitehorizon decpomdp section introduce novel concept periodic describe stage method improve solution transform periodic infinitehorizon solution improve periodic solution novel decpomdps section section improved performance new method planning problem conclude section partially observable markov decision process pomdps decentralize pomdps decpomdp model family decision uncertainty pomdps optimize policy single agent uncertainty environment state decpomdp optimize policy agent uncertainty environment state state action agent environment evolve accord markov model agent policy optimize maximize expect reward action future infinitehorizon plan expect reward typically discount emphasize current action computationally pomdps decpomdp complex problem find solution bad case pomdps problem state art method store policy stochastic agent policy size bound parameter optimize expectation maximization advantage adapt example continuous probability distribution advantage factor problem alternative include formulate optimization nonlinear constraint satisfaction nlp problem solvable nlp solver iteratively improve linear programming fsc fix deterministic fsc fix size find search decpomdp problem specific goal state goaldirecte approach achieve good result nlp method yield good result infinitehorizon decpomdp problem recent variant nlp nlp base approach decpomdps adapt policy represent machine instead traditional moore machine representation base controller achieve equal solution moore controller size paper recognize need improve general pomdp solution introduce approach fsc periodic layer structure turn yield good result infinitehorizon tuple s p r b define infinitehorizon agent set environment state ai set possible action observation agent pomdp special case agent probability state action agent jointly denote observation function oo probability agent observe observation agent action environment transition state initial state distribution s rs realvalued reward execute action state brevity denote transition probability action observation probability reward function set agent time step agent perform action environment state change agent receive observation goal find policy agent maximize expect discount infinitehorizon reward e discount factor rst state action time t e denote expect value policy policy store set stochastic finite state controller fsc agent agent define tuple ai qi set initial distribution p node ai probability p ai perform action probability p qi transition time s time s r r q q t t t figure leave influence diagram decpomdp state controller q joint observation joint action reward r reward function dotted line separate time step right example new periodic finite state controller layer node layer possible transition arrow controller control agent layer active depend current time node active action choose depend transition probability action probability controller observe current node agent denote policy optimize optimize parameter ai qi figure leave illustrate setup periodic state controller stateoftheart optimize policy fsc find local optimum initialization yield solution initialize compact fsc straightforward reason dynamic programming difficult apply generic fsc fsc pomdps build use dynamic programming add new node yield large fsc apply decpomdps need piecewise linear value function general fsc probability distribution sparse time fsc start single node computation large fsc difficult fsc base method limit introduce periodic fsc allow use large controller small complexity increase efficient initialization new dynamic programming algorithm periodic fsc compose m layer controller nod node layer connect node layer layer connect second second layer layer connect width periodic fsc number controller nod layer loss generality assume layer number nod singlelayer periodic equal ordinary fsc periodic different action m transition probability layer ai layer m probability action m q qi layer m probability observe layer connect policy cycle layer t m m m denote remainder figure right q qi q qi ai example periodic introduce method solve decpomdps periodic policy periodic structure allow efficient computation deterministic controller optimize periodic stochastic fsc periodic deterministic controller use initialization stochastic controller algorithm discuss context decpomdps directly apply pomdps deterministic periodic finite state controller deterministic action node transition deterministic function current observation optimize deterministic periodic fsc compute policy transform periodic infinitehorizon policy connect layer layer result deterministic policy m prove new algorithm section periodic deterministic policy use initialization stochastic fsc optimizer base expectation maximization section deterministic controller briefly discuss exist method deterministic controller introduce improve method use initial solution controller stateoftheart point base optimize policy graph restricted width agent compute policy single belief instead possible belief belief world state sample use action heuristic policy build dynamic programming horizon t time step time step policy compute policy graph node assume node agent associate belief pomdp compute deterministic policy policy graph node mean find good action good connection node observation direct search decpomdp approach combination action observation node agent number combination grow exponentially number agent direct search work simple problem efficient way action combination action combination sample random policy agent improve policy agent turn hold agent fix guarantee find good policy belief yield good result policy generation introduce new algorithm improve use linear programming find policy agent fix joint action fixed policy agent use fast simple direct search follow initialize value function q construct initial policy graph agent start horizon project initial belief random trajectory horizon t yield sample belief bs state add graph agent node layer find good connection layer follow sample random connection agent agent turn optimize connection connection agent fix connect node maximize value compute use bs layer value function repeat convergence use random restart local minima good connection action combination policy current policy graph node run graph layer node decrease t run t use algorithm initialization use new policy improvement approach improve policy value monotonically use random trajectory belief projection instead project belief q world state controller q agent initially assume start controller node time step horizon t current policy graph yield distribution node match current policy start layer proceed layer optimize agent separately graph agent action agent observation optimize deterministic connection layer optimize policy action connection identical policy node layer sample new belief world state node new belief new policy find try sample belief try uniformly random belief find policy connection previous policy graph layer current node instead node policy compress policy graph change value step necessary happen naturally previous layer computational complexity om m n q method assumption fsc certain belief assume initialization step actual optimization optimization monotonically improve value fix size policy graph converge local optimum apply procedure decpomdps adapt improve deterministic infinitehorizon fsc section simple improvement speedup use linear programming find policy agent turn simple direct search fast use improve duplicate handle try sample belief avoid duplicate node try uniformly random q use current policy project q t time step t agent agent ai q q ai argmax q ai q ai pt ai ai ai node p policy q link qi p sample belief qj use compute new policy step q ai p q monotonic policy graph improvement belief decpomdps connection duplicate node recursion idea projection approach guarantee improve value find local optimum deterministic infinitehorizon controller initialize infinitehorizon problem transform deterministic policy graph compute section infinitehorizon periodic controller connect layer assume controller start policy graph node compute policy node layer belief sample time step m length controller period remain compute deterministic connection layer approximately optimal connection find use belief layer value function project layer graph layer approach yield efficient controller suitable problem long effective horizon optimize controller far change enable optimization infinitehorizon policy compute q time step project initial belief determine effective projection horizon compute policy upper bind optimal decpomdp policy dynamic programming projection horizon use number dynamic programming step need gather value correspond compute belief q layer need line p discount sum project q compute value q policy graph layer use line m step previous periodic layer current layer layer time complexity iteration infinitehorizon approach om m m s m q convergence guarantee approximation approximation error decrease exponentially period m expectation maximization stochastic infinitehorizon controller stochastic fsc provide solution equal large value compare deterministic number controller nod algorithm optimize stochastic fsc adapt use periodic fsc paper adapt expectationmaximization approach periodic fsc adapt version retain theoretical property regular monotonic convergence local optimum approach optimization policy write inference problem reward scale probability policy represent stochastic fsc optimize iteration maximize probability reward introduce algorithm decpomdp periodic stochastic fsc build method standard fsc detail s rs reward function scale probability rmin rmin rmin minimum maximum reward possible s conditional probability binary reward r parameter rr p optimize maximize reward likelihood p t p r t respect horizon infinite p t equivalent maximize expect discount reward decpomdp approach improve policy stochastic periodic finite state controller iteration describe estep mstep formula estep message m beta message m s compute layer periodic intuitively correspond discount weight average probability world state s fsc node q follow policy define q s intuitively expect discount total scale reward start current fsc nod message compute project initial distribution forward beta message compute project reward probability backward compute separate m m s layer m use projection horizon m m tm number layer m mean accumulate probability mass estep project step order reach valid t periodic fsc forward projection joint distribution world time step time step q q ai q m q compute project single trajectory forward start initial belief add message belong layer m m contrast m q project separately backward starting point m m similar message denote projection q r ai p m m q equation message q q t m q m m m t mean complexity estep periodic fsc m time complexity estep usual fsc total number node equal width periodic complexity increase linearly number layer mstep update parameter layer separately use beta p layer follow expect complete loglikelihood p r l t log p r l t l denote latent variable action observation state denote previous parameter denote new parameter periodic fsc p r l t t p r aq denote aq ai aq ai q q qi log expect complete loglikelihood transform product probability sum divide sum small sum sum contain parameter periodic layer denote sq m q mstep periodic update rule write ci m qi r m q q m m m m q qi q qi m m m q ai q q note initialization initialization procedure section yield deterministic periodic controller initialization deterministic finite state controller stable point controller mstep approach change probability allow stable point find optima add noise controller order produce stochastic controller improve experiment experiment run standard pomdp benchmark problem time limit hour type benchmark run propose infinitehorizon method deterministic controller denote improvement round describe section benchmark run propose periodic expectation maximization approach section denote periem initialize section improvement round infinitehorizon transformation section periem period use period use problem discount factor discount factor large discount factor main comparison method nlp removal dominate action pair implement use matlab server utilize solve nonlinear program use good parallel experiment run choose number run problem nlp decentralize tiger robot wireless network problem run pomdp problem report result literature table decpomdp result decentralized tiger robot meet grid wireless network box stochastic mar problem discount factor use wireless network problem decpomdp benchmark table pomdp result benchmark problem repeat discount factor use problem pomdp benchmark method confidence interval overlap good method bold propose method perform problem restricted policy size method pomdp problem periem perform outperform conclusion discussion introduce new class finite state controller periodic finite state controller periodic present method initialization policy improvement comparison result method outperform stateoftheart decpomdp stateoftheart restrict size pomdp method work pomdps general method period length base simply discount factor perform result achieve example run solution different period parallel addition expectationmaximization present optimization algorithm infinitehorizon problem adapt periodic fsc example nonlinear programming approach adapt periodic fsc brief separate value function separate fsc parameter use time slice periodic fsc number constraint grow linearly number time slice acknowledgment thank discussion decision making wireless network author belong research work support decision number publication reflect author view table decpomdp benchmark comparison result run nlp test section note goaldirecte special method apply problem goal table pomdp benchmark comparison method result run nlp test section time ai periem goaldirecte nlp nlp s robot ai nlp periem meet x grid ai periem nlp nlp goaldirecte wireless network ai nlp box push ai goaldirecte nlp periem s ai goaldirecte nlp periem bias nlp fix nlp s nlp value m nlp bias nlp s repeat nlp s nlp s s nlp nlp s s reference r e optimal control partially observable markov process finite horizon operation research page s formal model algorithm decentralized decision uncertainty autonomous agent system d pomdp planning approximate optimally reachable belief space proc robotic science system s dynamic programming decpomdp proc th page policy generation decentralized pomdps page p bound finite state controller advance neural information processing system m storkey probabilistic inference solve d optimize controller decentralize pomdps rd uai page press decentralize pomdps complexity new algorithm page efficient planning infinitehorizon decpomdp nd ijcai page s r complexity decentralize control markov decision process mathematic operation research anytime plan decentralized pomdps use expectation maximization bound policy iteration decentralized pomdps th page d optimal search algorithm solve infinite horizon decpomdps ecml page c achieve goal decentralize pomdps volume page b controller base machine centralized decentralize pomdps r h l policy iteration volume page optimal approximate qvalue function decentralize pomdps journal artificial intelligence research
optimal regret minimization postedprice auction strategic buyer andre abstract study revenue optimization learn algorithm postedprice auction strategic buyer analyze broad family algorithm problem include previously know algorithm family admit strategic regret favorable t introduce new algorithm achieve strategic regret differ lower bind factor exponential improvement previous good new algorithm admit natural analysis simple proof idea design general report result empirical evaluation compare algorithm previous state art consistent exponential improvement different scenario introduction auction long active area research economic game theory past decade online advertisement algorithmic study auction include design learn algorithm revenue maximization generalized secondprice auction secondprice auction study largely motivate widespread use vast historical datum collect advertisement platform use secondprice auction reserve price allocate advertisement space far propose revenue maximization auction critically rely assumption bid outcome auction draw accord unknown distribution assumption hold practice particular knowledge revenue optimization algorithm use seek publisher fact consistent empirical evidence strategic behavior find motivate analysis present paper interaction seller strategic buyer buyer act goal maximize surplus scenario consider postedprice auction simple mechanism fact match common situation auction admit single bidder set secondprice auction reserve equivalent postedprice auction seller set reserve price good buyer decide accept bid high reserve price order capture buyer strategic behavior analyze online scenario time price pt offer seller buyer decide accept leave scenario model repeat nonzero sum game incomplete information seller objective maximize revenue seek maximize surplus describe detail section literature nonzero rich work area focus characterize different type equilibrium directly relevant algorithmic question arise furthermore problem consider admit particular structure exploit design efficient revenue optimization algorithm seller perspective game view bandit problem robbin revenue reward price offer accessible seller precisely study continuous bandit set assumption oblivious buyer exploit seller behavior precisely author assume round seller interact different buyer author present tight regret bind log log t scenario buyer hold fix valuation regret bind face adversarial buyer use elegant reduction discrete bandit problem argue deal strategic buyer usual definition regret long meaningful consider follow example let valuation buyer assume algorithm regret ucb auer use t round seller possible strategy buyer know seller accept price small small value certain seller eventually learn offer price buyer considerably boost surplus theory seller incur large regret hindsight good fix strategy offer price round clearly optimal seller strong notion policy regret introduce appropriate analysis bandit problem adaptive adversary example describe sublinear policy regret similarly achieve notion regret study scenario adopt instead definition introduce precisely study problem notion regret match concept learn loss introduce face oblivious adversary use definition present upper low bound regret seller face buyer buyer surplus discount time order able achieve sublinear section gap upper low bound present t following analyze broad family monotone minimization algorithm problem section include algorithm algorithm family admit strategic regret favorable t introduce algorithm achieve strategic regret differ low bind factor section represent exponential improvement exist good algorithm set new algorithm admit natural analysis simple proof key idea design method buyer lie reject price valuation setup consider follow game play buyer seller good advertisement space repeatedly offer seller buyer round buyer hold private valuation good round t price pt offer seller decision buyer value buyer accept buy price buyer lie beginning game use seller set price buyer buyer play algorithm knowledge standard assumption mechanism design match practice define discount surplus buyer follow pt t value discount factor indicate strength preference buyer current surplus future performance seller measure notion define follow t buyer objective maximize discount surplus seller seek minimize regret note view discount factor buyer fully adversarial problem consist design algorithm achieve sublinear strategic regret regret motivation definition straightforward seller access buyer valuation set fix price good close value buyer control price offer option accept price order optimize utility revenue round seller v scenario high revenue achieve natural setting compare performance gain intuition problem let examine complication arise deal strategic buyer suppose seller attempt learn buyer valuation perform binary search natural algorithm face truthful buyer view buyer knowledge algorithm good interest lie initial round quickly fact exponentially decrease price offer seller seller incur t regret binary search approach aggressive buyer manipulate seller offer price lie value discussion suggest follow conservative approach section discuss natural family conservative algorithm problem monotone algorithm follow conservative pricing strategy introduce let p price pt reject round t low price offer round time price accept price offer remain round denote algorithm monotone motivation design clear suitable choice seller slowly decrease price offer press buyer reject price convenient obtain favorable price author present regret bind algorithm t careful analysis p bind far tighten t t t discount factor know seller sublinear regret monotone remain suboptimal certain choice consider scenario set buyer long incentive lie algorithm binary search achieve logarithmic regret regret achieve monotone guarantee argue monotone specific admit single parameter complex algorithm monotonic idea achieve favorable regret let analyze generic monotone define definition buyer valuation define acceptance time time price offer seller use accept proposition decrease sequence price exist truthful buyer valuation suffer regret q t t proof definition regret p t consider case t t case t t imply statement proposition assume condition family monotone algorithm definition root offer price t offer price accept n offer price pn r round ln end end let p pt p offer price reject p t offer price end t offer price end let v uniformly distribute view appendix t t p t ev p e righthand minimize e plug value yield t imply m monotone suffer regret t face truthful buyer tight lower bind mild condition price offer definition sequence convex verify pt pt pt instance convex sequence price offer monotone seller offer price form decrease convex sequence seek control number lie buyer slowly reduce price follow proposition lower bind regret algorithm family proposition let decrease convex sequence price exist valuation buyer regret monotone define price t c t proof proposition appendix proposition discount factor know monotone fact asymptotically optimal class result present suggest dependency t improve monotone sense family conservative achieve favorable regret guarantee entirely different algorithmic idea introduce section describe new algorithm achieve substantially advantageous strategic regret combine fast convergence property binary algorithm truthful set method penalize behavior buyer nearly optimal algorithm let algorithm revenue optimization use truthful buyer denote tree associate t round t tree height t node t label price pn offer right leave child denote respectively price offer accept buyer label rn price offer reject label finally denote left right subtree root node l rn respectively figure depict tree generate propose describe later b figure tree t associate algorithm propose modify tree t r buyer hold fix valuation consider algorithm increase price price accept decrease rejection formalize follow definition definition algorithm consistent l pn pn node t consistent algorithm define modify algorithm parametrize integer r design face strategic buyer offer price define follow modification price reject buyer seller offer price r round pseudocode motivation modify follow simple observation strategic buyer lie certain reject price boost surplus future force buyer reject price round seller ensure future discount surplus negligible buyer truthful proceed formally analyze algorithm particular quantify effect parameter r choice buyer strategy measure spread price offer need definition node t t define right increment similarly define left increment l pn pn price offer ar define path t node path define time tn number round need node reach ar note r great path choose necessarily reach leave t finally let sn function represent surplus obtain buyer play optimal strategy node reach function s satisfie follow recursive relation sn max proof define weight tree t t t node reachable assign weight edge follow way edge t t form rn weight set tn set easy function evaluate weight long path node t follow elementary graph algorithm equation hold previous lemma immediately necessary condition buyer reject price proposition reachable node price pn reject buyer follow inequality hold r pn l r proof direct implication price pn reject buyer tn definition buyer surplus obtain follow path rn bound particular true path reject accept price surplus path t price seller offer price reject furthermore consistent bound follow t pn proceed upper bind pn pn nl l nl t combine inequality conclude tn pn t nl r t nl r r pn r nl rearrange term inequality yield desire result pn r let consider follow instantiation introduce algorithm track feasible interval b initialize increment parameter initialize algorithm work phase phase offer price price reject price k reject new phase start feasible interval set k increment parameter set process continue t point phase start price offer remain round hard number phase need algorithm log t e surprising fact algorithm achieve regret olog log t seller face truthful buyer modification ar admit particularly favorable regret bind penalize fast search proposition value regret admit follow upper bind r log t e note r upper bind coincide proof accumulate regret way price offer pn reject case regret price accept regret let log t e number phase run different price reject buyer rejection phase price reject r round cumulative regret rejection upper bound second type regret bound straightforwardly phase let ai bi denote corresponding search parameter feasible interval respectively bi regret case buyer accept price interval bound bi ai hand bi readily follow bi price pn offer phase regret obtain acceptance round bound bi bi denote number price offer ith round finally notice view definition bi correspond reject price proposition exist node necessarily distinct bi bi r l r ni immediate node write bi r r r r inequality hold t price offer algorithm combine bound regret type yield result upper bind discount factor know seller leverage information optimize upper bind respect parameter m let r r v regret t log log proof theorem fairly technical defer appendix help define condition logarithmic regret achieve e log log t use inequality valid obtain log log log t follow log log log log let compare regret bind discussion certain value exponentially regret achieve algorithm argue knowledge upper bind require need monotone t regret bind monotone uninformative order properly compare algorithm assume case regret algorithm t log t linear regret guarantee monotone favorable bind o t t t t monotone achieve regret strictly regret log log log attain lower bind follow low bound derive previous work let fix algorithm exist valuation buyer fact stochastic setting buyer valuation random variable fix distribution d proof theorem select point mass reduce scenario fix price set algorithm play truthful buyer exist value c log log t universal constant regret regret v number round logscale number round logscale regret regret number round logscale number round logscale figure comparison monotone different choice regret plot function number round know algorithm figure value accessible algorithm figure combine result lead immediately follow corollary algorithm exist buyer valuation max c log log t universal constant c compare upper bound previous section bound corollary log log log t hand choose r case proposition olog log t upper low bound match olog factor empirical result section present result simulation compare monotone experiment carry follow buyer valuation discrete set false valuation vb select set algorithm run buyer seller believe valuation vb instead value vb achieve good utility buyer choose regret algorithm report figure consider set experiment value parameter leave unknown algorithm value r set choice motivate discussion follow theorem large value t expect achieve logarithmic regret plot left right figure depict result apparent stationarity regret consequence scale plot regret fact grow logt second set experiment allow access parameter algorithm value r choose optimally base parameter set t t ensure regret o t t t worth note algorithm design assumption knowledge value experimental result exponentially performance monotone fact performance version comparable comprehensive series experiment present appendix conclusion present detailed analysis revenue optimization algorithm strategic buyer reduce gap upper low bound strategic regret logarithmic factor furthermore algorithm present simple analyze reduce truthful scenario limit important property previous algorithm admit believe analysis help gain deep understanding problem serve tool study complex scenario strategic behavior repeat secondprice auction auction general market strategy acknowledgment thank discussion topic paper work partly fund award iis reference r control optimization u learn price repeat auction strategic buyer proceeding nip page r dekel tewari online bandit learn adaptive adversary regret policy regret proceeding icml p auer p fischer finitetime analysis multiarmed bandit problem machine learn p auer freund r e schapire problem comput b reserve price auction proceeding soda page b m strategic bidder behavior sponsor search auction decision support system d machine learning approach revenue maximization sponsor search proceeding ijcai page r d value know demand curve bound regret online postedprice auction proceeding foc page algorithm multiarmed bandit problem journal learn p r theory auction competitive econometric society page m theory algorithm revenue optimization secondprice auction reserve proceeding icml game introduction game theory page springer learn repeat game incomplete information social choice prediction optimization learn repeat econometric society page m m price internet advertising auction field experiment proceeding page acm h robbin aspect sequential design experiment herbert robbin select paper page springer auction competitive
rate distortion function spin glass state toy model laboratory mathematical neuroscience abstract apply statistical mechanic inverse problem linear mapping investigate physics optimal lossy compression use technique toy model demonstrate shannon result rate distortion function widely know theoretical limit compression fidelity criterion derive numerical study sparse construction model provide suboptimal compression introduction study similar statistical physics statistical physics information science expect direct common objective formulate information theory base concept entropy actually happen difficult physics disorder system spin glass theory particular naturally include important aspect information science field expect develop far current perspective future area relation particularly strong shannon code theory classical spin system disorder theory disorder statistical system trigger work link recently examine area error correction compression decode recent result topic base replica technique exception basic characteristic channel capacity entropy rate achievable rate region capture concept phase transition firstorder jump optimal solution arise research field far categorize socalled decode scheme term information theory system require perfect reproduction input alphabet spin glass technique useful describe physics system fidelity criterion certain degree information distortion assume reproduce alphabet framework rate distortion theory processing information require regard concept distortion practically input alphabet represent continuous variable statistical physics employ approach paper introduce prototype suitable study analyze information distortion describe concept statistical physics specifically study inverse problem decode problem use framework break disorder system accord analysis simple model provide optimal compression scheme arbitrary degree encoding procedure remain npcomplete problem practical encoder paper organize follow section briefly review concept rate distortion theory main result relate purpose section introduce toy model section obtain consistent result information theory conclusion section detailed derivation report review rate distortion theory briefly recall definition concept rate distortion theory state simple version main result end section let discrete random variable assume source produce sequence jm symbol randomly draw distribution assume alphabet paper use vector notation represent sequence convenience explanation jm t m encoder describe source sequence m g jm illustrate figure note decoder represent estimate m represent length source sequence represent length rate define note relation m hold compression consider r hold definition distortion function mapping r set source alphabet pair set nonnegative real number measure cost represent symbol intuitively distortion dj symbol definition general case reproduction alphabet source alphabet set follow distortion measure adopt definition hamming distortion dj result probable error distortion relation hold e represent expectation p probability argument distortion measure far define basis extend definition sequence definition distortion sequence m define m m distortion sequence average distortion symbol element sequence definition distortion associate code define expectation respect probability distribution rate distortion pair r d sequence rate distortion code limit m closure set exist achievable rate distortion pair rate distortion region source finally define function describe boundary definition rate distortion function rd infimum rate r r rate distortion region source distortion d restrict binary source hamming distortion measure simplicity assume binary alphabet draw randomly ie source biased rule compression redundancy find description rate rd require describe source expect proportion error equal d simplified case accord boundary write follow rate distortion function binary source hamming distortion rd d represent binary entropy function encoder decoder figure rate distortion encoder decoder general scenario section introduce toy model lossy compression use inverse problem decode realize optimal encoding scheme previous section assume binary alphabet draw randomly source hamming distortion measure select boolean representation binary alphabet ie set set represent codeword rest paper bit reproduction let m bit source sequence bit codeword sequence encoding problem write follow distortion d boolean matrix dimensionality m find bit satisfie j criterion hold accord m bit source sequence note apply arithmetic additive operation framework decode linear encoding remain npcomplete problem mapping recently expand work focus limit case follow construction matrix treat practical case let boolean matrix characterize row c column finite usually small number define particular code rate code set arbitrary value select combination use c control parameter define rate r value small ie relation hold boolean matrix result sparse matrix contrast consider construct case extensively big value assume hold code parameter region include case result optimal code conclude follow section new finding analysis use statistical physics physics model onestep rsb scheme similarity code type ising spin system point formulate mapping code ising spin system hamiltonian context error correction facilitate current investigation map problem ising model finite connectivity follow use ising representation alphabet element source sequence rewrite value mapping reproduction sequence generate product relevant binary sequence element ising representation ik ik index ik correspond row produce ising version note additive operation representation translate multiplication ising set change notation simplicity use technique consider source dimensionality m n respectively infinite rate r nm finite explore system capability examine hamiltonian si introduce dynamical variable si find suitable ising provide reproduction sequence decode stage element sparse connectivity tensor value corresponding index bit choose ie corresponding index matrix index represent system degree connectivity calculate partition function apply replica method follow calculation calculate energy calculate average nth power partition function prepare replica introduce inverse temperature interpret measure system sensitivity distortion follow calculation optimal value naturally determine consistency break scheme consider use integral representation dirac function enforce restriction c bond index ik z rise set order parameter q represent replica index average respect probability distribution ik ik consider source sequence simplicity assume symmetry use different representation order parameter related conjugate variable dx x dx ck q k normalization constant represent probability distribution relate integration variable l denote number relate replica index paper integral limit denote integral range obtain expression free energy source bit express term probability distribution m l z l j d tanh tanh z l d tr tanh s l denote average randomness j saddle point equation respect probability distribution provide set relation d l tanh l use result obtain free energy easily perform straightforward calculation find observable quantity include internal dd m m record reproduction error term consider symmetric complete solution problem easily unfortunately set equation solve numerically general exist analytical solution equation consider case dominant solution emerge correspond spin glass phase solution valid general form x low possible free energy bit entropy r ln positive r mean true solution replica symmetric step onestep break replica usually divide nm group contain m aspect avoid use freedom m actually onestep rsb scheme consider provide exact solution random energy model consider analysis restrict case spin glass solution calculate replica symmetric onestep reduce solution r yield rsb rsb m g r obtain root equation enforce nonnegative symmetric ln tanh r ln free target bit estimation model estimator product performance measure information corruption energy e accord onestep rsb framework free energy calculate probability distribution rsb satisfy saddle point equation characteristic inverse temperature g replica symmetric entropy disappear equal let hamming distortion distortion associate code fraction free energy arise spin glass phase tanh substitute spin glass solution expression use fact replica symmetric entropy disappear consistent g determine use simple algebra relation rate r nm distortion d form r coincide rate distortion function retrieve surprisingly observe firstorder jump analytical solution recently approach family code characterize linear encoding operation result different picture optimal boundary construct energy model limit capture concept firstorder jump analysis model view kind inverse problem provide exception optimal condition information theory describe concept firstorder phase transition view point statistical physics investigate solution satisfy case c saddle point equation appear difficult analytical argument resort numerical evaluation represent probability distribution x bin model carry integration use method note characteristic inverse temperature evaluate numerically use set select value c demonstrate performance stable solution numerical result obtain suboptimal property figure strongly imply analytical solution stable solution conjecture verify carry large scale simulation conclusion point note firstly find consistency rate distortion theory onestep rsb scheme secondly conjecture analytical solution consistent shannon result stable solution situation currently work verification acknowledgment thank comment manuscript thank valuable reference research support special researcher program reference statistical physics spin glass information processing press t m cover element information theory introduction replica theory disorder statistical system press model code nature statistical mechanic typical performance code phy rev r d figure stable solution stable solution value calculate use method use bin model approximate probability distribution rsb start initial condition distribution converge continuous suboptimal performance l solid line indicate rate distortion function rd inset snapshot distribution l t statistical mechanic datum compression theorem j phy statistical mechanic code b c e shannon code theorem discrete source fidelity record page rate distortion theory mathematical basis datum compression prenticehall statistical mechanic lossy datum compression use perceptron code theorem lossy datum compression code proceeding ieee international symposium information theory page m m theory scientific m graph spin glass random network fix finite phy ll b random energy model exactly solvable model disorder system phy rev b
deep knowledge trace abstract knowledge machine model knowledge student interact establish problem computer support education effectively model student knowledge high educational impact task inherent challenge paper explore utility use recurrent neural network rnn model student learn rnn family model important advantage previous method require explicit encoding human domain knowledge capture complex representation student knowledge use neural network result substantial improvement prediction performance range knowledge trace dataset learn model use intelligent design allow straightforward interpretation discovery structure student task result suggest promising new line research knowledge trace exemplary application task introduction education promise open access world class instruction reduction grow cost learn develop promise build model large scale student trace datum popular educational platform knowledge trace task model student knowledge time accurately predict student perform future interaction improvement task mean resource suggest student base individual need content predict easy hard skip delay intelligent tutoring system attempt content promising result human tutoring produce learn gain average student order standard deviation machine learning solution provide benefit high quality teaching world free knowledge trace problem inherently difficult human learning ground complexity human brain human knowledge use rich model appropriate previous work education rely order markov model restricted functional form paper present formulation deep knowledge trace apply flexible recurrent neural network deep time task knowledge trace family model represent latent knowledge state temporal dynamic use large vector artificial neuron allow latent variable representation student knowledge learn datum main contribution work novel way encode student interaction input recurrent neural network gain auc good previous result knowledge trace demonstration knowledge trace model need expert annotation discovery exercise influence generation improved exercise curricula correct incorrect line graph intuition slope line solve solve graph linear equation square root exercise index predict probability exercise attempt figure single student predict response solve exercise master finding intercept transfer knowledge graph linear equation task knowledge trace formalize observation interaction student particular learning task predict aspect interaction instantiation knowledge trace interaction form tuple qt combine tag exercise answer qt exercise answer correctly prediction model provide tag exercise answer predict student exercise correct figure visualization trace knowledge single student learn grade student answer square root problem correctly single exercise incorrect subsequent interaction student solve series graph exercise time student answer exercise prediction answer exercise type correctly interaction visualization prediction time relevant subset exercise type previous work exercise tag denote single concept human expert assign exercise model leverage require expert annotation demonstrate absence annotation model learn content relate work task model predict human learn inform field diverse education psychology neuroscience cognitive science social science perspective learning understand influence complex level interaction include affect motivation identity challenge present far expose level learning fundamentally human cognition highly complex process field cognitive science particularly relevant theory human mind learning process recursive drive analogy problem knowledge trace pose heavily study intelligent community face aforementioned challenge primary goal build model capture cognitive process useful bayesian knowledge trace bayesian knowledge trace bkt popular approach build temporal model student learning bkt model learner latent knowledge state set binary variable represent understanding single concept hide markov model use update probability binary variable learner answer exercise concept correctly incorrectly original model formulation assume skill learn forget recent extension model include guess estimate estimate prior knowledge individual learner estimate problem difficulty extension knowledge trace suffer difficulty binary representation student understanding second meaning hide variable mapping exercise ambiguous rarely meet model expectation single concept exercise technique develop create refine concept category mapping current standard cognitive task analysis iterative process domain expert ask learner thought process solve problem finally binary response datum use model transition impose limit kind exercise model dynamic probabilistic model partially observable markov decision process pomdps use model learner behavior time case learner follow openende path arrive solution present extremely flexible framework require exploration exponentially large state space current implementation restrict discrete state space meaning latent variable intractable practice potential overcome limitation simple model performance factor analysis framework learn factor analysis predictive power comparable bkt obtain predictive result model ensemble method use combine bkt model combination support adaboost random forest regression logistic regression feedforward neural network deliver superior result bkt learner model rely ensemble technique limitation include requirement accurate concept label recent work explore combine item response theory model switch nonlinear kalman filter knowledge trace approach promise present restricted functional form expensive inference latent variable method present recurrent neural network recurrent neural network family flexible dynamic model connect artificial neuron time propagation information recursive hide neuron evolve base input system previous activation contrast hide markov model appear education dynamic rnn high dimensional continuous representation latent state notable advantage rich representation rnn ability use information input prediction later point time especially true long short term memory lstm popular type rnn recurrent neural network competitive stateoftheart time series instance speech text translation image large training datum available result suggest successful trace student knowledge formulate task new application temporal neural network deep knowledge trace believe human learning govern diverse property material context presentation individual involve difficult quantify rely principle assign attribute exercise structure graphical model apply different type rnn rnn model sigmoid unit long short term memory lstm model problem predict student response exercise base past activity model traditional recurrent neural network rnn map input sequence vector output sequence vector yt achieve compute sequence hidden state view successive encoding relevant information past observation useful future prediction figure illustration variable relate use simple network define equation tanh bh h yt h h x figure connection variable simple recurrent neural network input xt dynamic network onehot encoding compressed representation student action prediction yt vector represent probability dataset exercise correct tanh sigmoid function apply elementwise model parameterize input weight matrix recurrent weight matrix initial state h readout weight matrix bias latent readout unit bh long short term memory network complex variant rnn prove powerful latent unit retain value explicitly clear action forget gate naturally retain information time step believe easy train additionally hide unit update use multiplicative interaction perform complicated transformation number latent unit update equation lstm significantly complicated rnn find input output time series order train rnn lstm student interaction necessary convert interaction sequence fix length input vector xt use method depend nature interaction dataset small number m unique exercise set onehot encoding student interaction tuple qt represent combination exercise answer exercise answer correctly m find separate representation qt degrade performance large feature space onehot encoding quickly large dataset large number unique exercise instead assign n input tuple rn m set input correspond random lowdimensional representation onehot highdimensional vector motivate compress sensing sense state ksparse signal dimension recover exactly random projection scaling additive constant onehot encoding sparse signal student interaction tuple exactly encode assign fix random vector length log m current paper deal vector technique extend easily capture aspect complex student interaction fix length output yt vector length equal number problem entry represent predict probability student answer particular problem correctly prediction read entry correspond qt optimization training objective negative log likelihood observed sequence student response model let qt onehot encoding exercise answer time t let binary cross entropy loss prediction loss single student l yt objective minimize use stochastic gradient descent minibatche prevent overfitting training dropout apply compute readout yt compute hidden state prevent gradient backpropagate time truncate length gradient norm threshold model paper consistently use hidden dimensionality minibatch size facilitate research publish code relevant preprocesse data educational application training objective knowledge trace predict student future performance base past activity directly useful instance formal testing long necessary student ability undergo continuous assessment explore experimentally section model power number improve curricula big potential impact model choose good sequence learn item present student student estimate hide knowledge state query rnn calculate expect knowledge state assign particular exercise instance figure student answer exercise test possible exercise compute expect knowledge state choice predict optimal problem student revisit solve use train test classic curricula rule education literature mix exercise different topic block student answer series exercise type choose entire sequence exercise maximize predict accuracy phrase markov decision problem evaluate benefit use algorithm appendix optimal sequence problem discover exercise relationship model far apply task discover latent structure concept datum task typically perform human expert approach problem assign influence jij direct pair exercise p correctness probability assign rnn exercise second timestep student answer exercise correctly characterization dependency capture rnn recover associate exercise dataset test ability predict student performance dataset simulate assistment dataset measure area curve auc datum evaluate result use fold cross validation case hyperparameter learn training datum compare result deep knowledge trace standard bkt possible optimal variation bkt additionally compare result prediction simply calculate marginal probability student particular exercise correct overview c dataset student exercise tag answer marginal bkt bkt assistment table auc result dataset test bkt standard bkt bkt report result literature assistment result use lstm deep knowledge trace simulate datum simulate student learn concept test predict response control setting run experiment generate student answer exercise draw concept dataset student answer sequence exercise student latent knowledge state skill concept exercise single concept difficulty probability student exercise difficulty correct student concept skill model c use classic item response theory e c probability random guess set student learn time increase concept skill correspond exercise answer understand different model incorporate datum provide model hide concept label instead input simply exercise index exercise answer correctly evaluate prediction performance additional simulated test student number concept repeat experiment time different randomly generate datum evaluate accuracy mean standard use sample student usage interaction eighth grade common core include exercise complete student different exercise type contain personal information researcher work paper access dataset use govern agreement design student accordance notice provide particularly relevant source learn datum student interact site extended period time variety content student topic work trajectory material benchmark dataset order understand model compare model evaluate model assistment skill public benchmark dataset assistment online simultaneously teach assess student grade school mathematic good knowledge large publicly available knowledge trace dataset result dataset deep knowledge trace substantially outperform previous method dataset use lstm neural network model lead auc notable improvement performance standard bkt auc especially compare small improvement bkt provide marginal table figure b assistment produce gain previous report result respectively gain report auc compare large gain achieve dataset prediction result synthetic dataset provide interesting demonstration capacity deep knowledge trace lstm model predict student response oracle perfect knowledge model parameter fit latent student knowledge variable figure order accuracy par oracle model mimic function incorporate latent concept difficulty exercise prior distribution student knowledge increase concept skill happen predict probability true positive rate test accuracy oracle rnn rnn rnn marginal number number concept block mix false positive rate exercise index c figure leave result simulated datum right predict knowledge assistment different exercise curricula error bar standard error mean rnn lstm exercise bkt prediction degrade substantially number hide concept increase mechanism learn concept test ability choose exercise subset concept curricula method use model simulate student answer question evaluate student know exercise repeat student measure average predict probability student future simulation time question correct block strategy notable advantage number mix figure c block perform par solve exercise deep look far future choose problem come curricula student high predict knowledge solve problem prediction accuracy synthetic dataset suggest possible use model extract latent structure assessment dataset graph model conditional influence synthetic dataset reveal perfect clustering latent concept figure direct edge set use influence function equation interesting observation exercise concept occur far apart time example synthetic dataset number depict sequence th exercise hide concept problem problem concept ask able learn strong conditional dependency analyze dataset use technique result graph articulation concept th grade common core relate figure node number depict exercise tag restrict analysis order pair exercise b b appear time remainder sequence determine result conditional relationship product obvious underlying trend datum compare result baseline measure transition probability student answer b answer probability dataset use model answer b correctly student early answer correctly baseline method generate graph appendix relationship education expert discovery network learn coherent model discussion paper apply rnn problem knowledge trace education improvement prior stateoftheart performance assistment dataset particularly interesting novel property new model need expert annotation learn concept pattern operate student input vectorize disadvantage rnn simple hide markov method require large training datum suited online education environment small environment hide concept hidden concept hide concept hide concept hide concept simulate datum datum scatter plot linear function intercept interpret function graph construct inconsistent system recognize number system equation theorem proof linear equation solution system equation scientific notation intuition theorem multiplication scientific notation view function line graph intuition line parallel line equation distribution angle system equation graph proportional relationship fraction repeat equation word problem exponent rule cube root function slope line angle scientific notation line graph exponent system equation fraction linear model bivariate datum understand equation word problem theorem system equation exponent function plot line good fit segment addition vertical angle integer sum system equation solve intercept angle compare proportional relationship recognize function exponent solution linear equation scatter plot find intercept linear function slope triangle similarity repeat fraction segment distance formula graphical solution system volume word problem convert fraction linear non linear function construct scatter plot age word problem square root interpret feature linear function solve repeat fraction graph system equation compare feature function construct linear function frequency bivariate datum order graph linear equation compare feature function angle addition postulate computing scientific notation angle parallel line figure graph conditional influence exercise model observe perfect clustering latent concept synthetic datum th common core exercise influence arrow size indicate connection strength note nod connect direction edge magnitude small thresholde label add hand fully consistent exercise cluster application rnn knowledge trace provide direction future research investigation incorporate feature input time explore educational impact hint generation dropout prediction validate hypothesis pose education literature space repetition model student forget vector input possible track knowledge complex learning activity especially interesting extension trace student knowledge solve programming task use recently develop method program hope able model student knowledge time learn program ongoing plan test efficacy plan control experiment use propose exercise site acknowledgment thank guidance support support grant number reference notice baraniuk r compressive sense ieee signal processing magazine j b learning factor general method cognitive model evaluation improvement intelligent tutoring system springer identity belong model implication current direction psychological science cognitive computer solve problem user model springer t r knowledge trace model acquisition knowledge user modeling interaction d r t accurate student model contextual estimation guess probability bayesian knowledge trace intelligent tutoring system springer r z g b b t ensemble prediction student knowledge intelligent tutoring system user model personalization springer d l item response theory handbook industrial psychology e handbook motivation publication m h k address assessment challenge online system assess user modeling interaction t m d c evolution language implication cognition theoretical framework analogy cognitive science b e t intelligent tutoring system springer m ar speech recognition deep recurrent neural network acoustic speech signal processing international conference ieee h s s long shortterm memory neural computation ei ei deep alignment generate image description m r m l r v m m c incorporate latent factor knowledge trace predict individual difference learn proceeding conference educational datum mining m m b p m m c p integrate knowledge trace item response theory framework proceeding international workshop personalization approach learn environment l s c baraniuk r g timevarye learning content analytic sparse factor analysis proceeding th acm international conference discovery datum mining acm l e p p r role affect cognitive processing academic motivation cognition perspective development m c recurrent neural m network base language model annual conference z h t introduce item difficulty knowledge trace model user modeling personalization springer r p c h r performance factor new knowledge trace online p h p m m learn program embedding propagate feedback student code p m l generate hint infer problem solve policy proceeding second acm conference learn scale ls acm p m d c s p model student learn program proceeding acm symposium computer science education p m c r foundation intelligent tutoring system r e g t l s p fast teaching planning artificial intelligence education springer effect space mix practice problem research education s m s l cognitive task analysis psychology press r learning algorithm continually run fully recurrent neural network neural computation m r bayesian trace model artificial intelligence education springer pp
adaboost consistent statistic computer science abstract risk probability error classifier produce adaboost investigate particular consider stop strategy use adaboost achieve universal consistency provide adaboost stop sample sequence risk classifier produce approach baye risk baye risk l introduction boost algorithm important recent development classification algorithm belong group voting method example produce classifier linear combination base weak classifier empirical study boost good classification algorithm theoretical result complete explanation effectiveness assumption underlie distribution population boost converge baye risk number iteration infinity population version assume infinite sample size imply similar result adaboost especially result example adaboost prediction error asymptotically suboptimal t t number iteration author modify version adaboost consistent modification include restrict l norm combine classifier restrict step size analyse boost algorithm prove process consistency property certain assumption process consistency mean exist sequence adaboost sample size stop iteration risk approach baye risk impose strong condition underlying distribution distribution predictor absolutely continuous respect measure tion continuous proof constructive hint algorithm stop prove consistency result adaboost assumption probability distribution step algorithm large like obtain simple stop rule guarantee consistency require modification paper provide constructive answer mention issue consider adaboost modification provide simple stopping rule number iteration t fix function sample assume class base classifier finite span class sufficiently rich assumption clearly necessary setup notation describe adaboost procedure formulate coordinate descent algorithm introduce definition notation consider binary classification problem measurable feature space set binary label sample sn xi observation distribute random variable p unknown distribution goal construct classifier base sample quality classifier misclassification probability sn course want probability small possible close baye inf infimum possible measurable classifier conditional probability infimum achieve baye produce classifier linear combination base classifier hh assume class h finite dimension max define e boost procedure describe follow set choose number iteration set following hold h step size algorithm step output ft final classifier use hull scale x hi set function hi r need define l norm inf hi h define function l l l l l l l set truncate function o set classifier base class denote define derivative arbitrary function q direction h q second derivative q f h define similarly consistency boost procedure need follow assumption assumption let distribution p class h r inf measurable function class assumption satisfied possible distribution p sufficient condition assumption example class class indicator rectangle indicator define hyperplane binary tree number terminal node equal consider tree terminal node form successive univariate split dimensionality begin simple lemma theorem theorem t following hold pseudodimension class proof adaboost consistency base following result build result koltchinskii resemble continuous function define lipschitz constant l maximum absolute value argument m max x function r q e t r t e sup r c r e sup r probability r r r m r t m r ln proof equation constitute lemma proof equation similar begin n e sup r e t t use contraction theorem function l e sup r l e sup l e sup t proceed find supremum notice function t bound clip absolute value equal rescale t e xi use dudley entropy integral bind rhs p l d cover number upper integration limit use bind n l p pseudodimension obtain c ln e d r n t xi notice constant c depend t transform use inequality dp theorem r dp xi upperbound pseudodimension class t r n t ln e sup xi constant c independent t prove second statement use bound difference inequality theorem p m sup sup r r t obtain change pair yi complete proof allow choose number step describe complexity linear combination base function addition parameter govern size deviation function essential proof consistency easy adaboost ie ex choose t far deal statistical property function minimize turn algorithmic need follow simple consequence proof let function convex let q inf assume c c h c c h h c h reference function sequence function produce boost follow bind hold m m ln c h h b h h proof statement version result implicit proof theorem m qfm trivially true m consider case convexity qfm m p let hi correspond good representation small l norm linearity derivative x m h m hh qfm fm m m assumption path follow bound qfm assumption depend q fm inf qfm r hand qfm m qfm inf qfm b qfm b combine r q fm b expansion time m m function path m q b qfm b q qfm conclude combine m m b m use m m recall m fm m m m combine sequence decrease m m m m p b b m m m m m bi z m bm ln bx b m m m complete proof m allow upper bind difference risk function output adaboost risk appropriate reference function theorem assume r let tn number step run adaboost let n let minimizer function n large high probability following hold ftn proof follow directly adaboost rn h hxi xi rf condition satisfied replace equation b fn fn n trivially true notice guarantee probability r r mn rhs expression r fn r result follow immediately equation use fact hand formulate main result theorem assume l r tn adaboost stop step return sequence classifier surely satisfy proof exponential loss function l imply r let n let minimizer r minimizer rn ftn ftn ftn n n inequality hold probability inequality true sufficiently large hold s r r n r choice appropriate choice n example rf r assumption appeal arrive r ftn r eventually use theorem conclude ftn adaboost consistent stop step discussion adaboost consistent stop sufficiently early tn iteration tn baye risk l unclear number increase result imply x function class h adaboost algorithm achieve training error step tn know happen gap subject research analyze adaboost boost algorithm use loss function proof rely property exponential loss similar conclusion version boost boost ln case assumption second derivative hold f h result inequality trivial factor preclude find useful bind subject future work find analog handle acknowledgment gratefully acknowledge support nsf award reference freund schapire decisiontheoretic generalization online learn application boost journal computer system bag predictor machine learn arc classifier discussion annal statistic department statistic infinite theory predictor ensemble technical report department weak base hypothesis implication boost regression classification annal statistic gabor lugosi consistency regularize boost method annal statistic tong statistical behavior consistency classification method base risk minimization annal statistic tong boost early stop convergence consistency annal statistic process consistency adaboost annal statistic p theory generalized boost machine learn research learn theoretical foundation cambridge university press v koltchinskii empirical margin distribution bound generalization error combine classifier annal statistic talagrand probability banach space central limit empirical process theory application gabor lugosi probabilistic theory classification bound
stochastic variational inference nonparametric model computer abstract present stochastic variational inference nonparametric model traditional variational inference algorithm require truncation model variational distribution method adapt complexity fly study method dirichlet process mixture model hierarchical dirichlet process topic model large datum set method perform previous stochastic variational inference algorithm introduction model emerge important tool build probability model flexible latent structure complexity bnp model use posterior inference adapt model complexity datum example datum observe dirichlet process dp mixture model create new mixture component hierarchical dirichlet process topic model create new topic general posterior inference bnp model intractable approximate posterior approach inference model advantage mcmc directly operate unbounded latent space increase model complexity add new mixture component naturally fold sample step easily require store configuration hide variable order number datum point scalable mcmc typically need parallel hardware computational complexity scale linearly datum fast massive datum alternative variational inference find member simplified family distribution approximate true posterior generally fast mcmc recent innovation let use stochastic optimization approximate posterior massive streaming datum mcmc variational inference algorithm bnp model operate unbounded latent space truncate model variational distribution maximum model complexity particularly limit approach hope bayesian nonparametric posterior adapt model complexity stream datum paper develop stochastic variational inference model let easily apply bayesian nonparametric datum analysis massive streaming datum work author splitmerge technique use truncation operation difficult design unknown apply stochastic inference setting consider particular present new general inference locally collapse variational inference apply model require truncation principled mechanism increase model complexity fly demonstrate algorithm mixture model topic model large datum set improved performance truncated stochastic variational inference model goal develop efficient stochastic variational inference model describe algorithm wide class hierarchical bayesian model apply algorithm bnp model consider general class hierarchical bayesian model figure let global hide variable prior p hyperparameter local variable datum sample zi hidden xi observe joint distribution variable hide observe factorize p pxi pxi idea local variable conditionally independent global variable convenience assume global variable continuous local variable zi discrete assumption necessary large range model represent use form eg mixture model model factor model hierarchical model example consider dp mixture model document cluster document model bag word draw distribution vocabulary mixture component distribution vocabulary mixture proportion represent stickbreake process global variable contain proportion component local variable mixture assignment document generative process draw mixture component stick dirichlet k beta document draw mixture assignment b word draw word return general model eq inference interested posterior hide variable observe datum ie p zn model posterior intractable approximate use meanfield variational inference variational inference variational inference try find distribution simple family close true posterior describe meanfield approach simple variational inference assume fully factorize family distribution hidden variable q global variational distribution qzi local variational distribution want minimize kldivergence variational distribution true posterior variational theory equivalent maximize low bind log likelihood observe datum obtain bind log pxn pxn pn eq log p log q log lq log odd method figure graphical model hierarchical bayesian model global hide variable local hidden observe variable hyperparameter fix random variable meanfield method b frequency word new figure result assign document d q case b figure dirichlet axis large likely assign component approach uncertainty assign incorrectly case b locally collapse approach correctly case meanfield variational inference locally collapse variational inference initialize m n set local variational distribution qzi pxi end set global variational distribution q log pxn zn end return initialize q m n set local distribution qzi zi sample qzi obtain empirical qzi end set global variational distribution q log pxn zn end return q maximize wrt q define eq optimal condition q log pxn zn qzi pxi typically equation use coordinate ascent iteratively optimize factor hold fix factorization global local variable ensure local update depend global factor facilitate speedup parallel stochastic variational inference bnp model value zi potentially unbounded mixture assignment dp mixture need truncate variational distribution truncation necessary variational inference mathematical structure bnp model difficult grow truncation variational inference way tend posterior variance contrast mathematical structure variance right conditional distribution allow gibb sample bnp model effectively explore unbounded latent space locally collapse variational inference describe locally collapse variational inference mitigate problem posterior variance variational inference far apply model good mechanism increase truncation fly outline approach difference traditional variational algorithm lie update local distribution qzi algorithm oppose meanfield update eq collapse global distribution q locally method locally collapse variational inference note algorithm similar low variance uncertainty model high approach lead different approximation posterior implementation use collapse gibbs sampler sample equation local gibb sample step fast far algorithm require truncation gibb sampler bnp model operate unbounded space update suppose set sample qzi construct empirical distribution qzi plug eq solution q log pxn zn form eq meanfield approach intuitive comparison locally collapse consider toy document clustering problem vocabulary size use twocomponent bayesian mixture model fix equal prior proportion suppose stage component document assignment obtain approximate posterior component parameter q consider case dirichlet dirichlet consider dirichlet case q relatively low variance q high variance difference q case low probability word case new document word word frequency case document likely assign component large figure difference locally collapse variational inference case approach correctly word low probability case b ignore uncertainty result incorrect clustering approach correctly case justify approach adaptation variational inference justification optimize qzi correspond lower bind loose optimize lower bind meanfield approach issue local mode exclude experiment find predictive distribution inference possible explanation outline section supplement algorithm understand approximate expectation relate algorithm algorithm closely relate collapse variational inference apply variational inference marginalize model integrate global hidden variable estimate posterior variance optimization local variable zi depend local variable difficult apply large scale algorithm apply intermediate model treat q prior consider single data point xi hide structure zi let develop stochastic algorithm fit massive datum set algorithm relate recently propose hybrid approach use gibb sample stochastic variational inference advantage sparsity text document topic model approach use meanfield update eq local hide topic variable document group optimal qzi approximate gibb sampler adaptation fast sparse update idea use stochastic locally collapse variational inference extend algorithm variational inference allow fit approximate posterior massive datum set assume model figure exponential family satisfy conditional conjugacy global distribution p conjugate prior local distribution pxi p t pxi hxi zi txi notation base measure h sufficient statistic t log different family conjugacy term t form t assume global variational distribution family prior q condition batch update q pn txi zi term txi define txi txi zi analysis conditional conjugacy assumption batch update parameter eq easily turn stochastic update use natural gradient suppose parameter t step random observation sample obtain empirical distribution appropriate learning rate t t t t t correspond stochastic update use noisy natural gradient optimize lower bind note natural gradient approximation qzi eq suboptimal lower bind minibatch common strategy use stochastic variational inference use small batch sample update suppose batch size s set sample use formulation q t zt choose factorize factorization potentially lead problem new component instantiate bnp model stochastic variational inference bnp model describe locally collapse variational inference general set main interest paper bnp model approach lead variational algorithm describe approach dp mixture model description present beginning detail topic model global variational distribution variational distribution global hide variable mixture component stick proportion u dirichlet parameter beta parameter sufficient statistic term txi define eq summarize txi txi zi txi zi indicator function suppose time t obtain empirical distribution qzi observation use update dirichlet parameter beta parameter u t t t k qzi unbounded number mixture component need represent explicitly suppose t component associate datum update indicate dirichlet prior distribution similar gibbs sampler model truncate automatically stick accord size local empirical distribution qzi mixture assignment hidden variable obtain analytical form use r pxi k k d w p q u document length gamma function mini batch analytical form sample probability create new component t sample obtain empirical distribution qzi t create new component discussion locally collapse analogous collapse gibb sample mixture model explore new mixture component initialize single sample locally collapse variational inference powerful trigger toy example role distribution dirichlet similar potential new component want maintain gibbs sample mixture model note difference approach find use meanfield method grow shrink truncation use splitmerge approach difficult design far transfer stochastic setting contrast approach present grow truncation natural consequence inference easily adapt stochastic inference experiment evaluate method mixture hdp topic model compare variational inference focus stochastic method large datum set dataset analyze large document collection nature datum contain document journal nature year token vocabulary size contain m document year m token vocabulary size standard stop word word appear time percent document remove final vocabulary choose tfidf set test set document evaluate predictive power test set training evaluation metric evaluate different algorithm use log xi high likelihood exact compute heldout likelihood intractable use approximation detail approximate likelihood question heldout likelihood metric compare different model heldout likelihood metric nonetheless suited measure inference accomplish specific optimization task define model experimental setting mixture set component dirichlet parameter concentration parameter hdp topic model set topic dirichlet parameter concentration parameter b description topic model stochastic variational set truncation level hdp run algorithm hour model final stage output assess convergence vary size intend compare hdp want algorithm work method stochastic approach set learning rate accord t start new method component datum use learning rate schedule large weight component effectively leave room create new component fly set learn rate size corpus algorithm time document stochastic meanfield algorithm set low bind learn rate sn find work practice mimic usual trick run gibb use sequential prediction initialization datum point initialize run gibb sampler remove component document topic word hdp topic model time process k document meanfield method batchsize batchsize batchsize time hour time hour time hour hour method method method method meanfield batchsize batchsize batchsize number topic batchsize batchsize batchsize batchsize batchsize method method model method method method method time end end batchsize batchsize number topic number topic end end number number topic likelihood batchsize batchsize likelihood likelihood heldout likelihood nature nature figure result mixture heldout likelihood comparison approach robust batch size predictive performance b infer number mixture similar left figure b number mixture component infer hour method tend mixture small batch size stochastic meanfield approach work result small number mixture right figure b different method infer number mixture stochastic mean field approach shrink approach grow b method meanfield method method result figure result dp mixture model figure heldout likelihood comparison dataset approach robust batch size usually predictive performance small batch size stochastic meanfield approach work figure b infer number mixture nature similar method tend mixture stochastic meanfield approach approach shrink truncation approach need truncation grow number mixture datum require figure result hdp topic model figure heldout likelihood comparison dataset similar mixture approach robust batch size predictive performance time small batch size stochastic approach work figure b infer number topic similar similar dp method tend topic approach stochastic meanfield approach shrink truncation approach grow number topic datum require possible explanation method result approach follow approach rely random initialization place parameter truncation initialization use performance degrade explain small batch size stochastic meanfield tend work sample dominate effect random initialization leave room late sample approach mitigate problem allow new create datum require compare hdp good result hdp comparison meaningful different setting hyperparameter compute heldout likelihood tractable intractable hdp use importance sample approximate time hour time hour method robust batch size predictive performance time infer number topic similar left figure b number topic infer hour method tend topic small batch size stochastic meanfield approach work result small number topic right figure b different method infer number topic similar stochastic mean field approach shrink approach grow method time hour figure result topic model heldout likelihood comparison approach batchsize batchsize number topic number topic stochastic method batchsize batchsize b end end batchsize nature likelihood likelihood nature heldout likelihood number topic nature detail importance sampling usually correct ranking different topic model significantly probability conclusion future work paper develop stochastic variational inference algorithm model bnp model apply large dataset extension bnp model indian buffet process nest chinese restaurant process straightforward use stickbreake construction explore algorithm behave true streaming set program learn machine interesting future direction acknowledgement support fellowship support onr nsf career p sloan foundation grant reference nonparametric principle practice press cambridge dirichlet process application bayesian nonparametric problem annal statistic teh m m hierarchical dirichlet process journal freitas doucet introduction mcmc machine learning learn m introduction variational method graphical model machine learn neal r markov chain sample method dirichlet process mixture model graphical statistic asuncion p smyth distribute algorithm topic model learn research smola architecture parallel topic model proc m latent variable model international conference web search datum mining wainwright m m model exponential family variational inference trend machine learn hoffman m m eprint hoffman m bach online inference latent allocation advance information processing system nip m blei online variational inference hierarchical dirichlet process international conference artificial intelligence statistic aistat blei m dirichlet process analysis m collapse variational dirichlet process mixture model international joint conference artificial intelligence ijcai teh m collapse variational inference hdp advance information processing system nip m accelerate variational dirichlet process mixture advance neural information processing system nip variational inference nest chinese restaurant process advance information processing system nip hill datum analysis use regression model cambridge d finite mixture model m m density estimation inference use statistical blei dirichlet allocation journal machine learning research griffith infinite latent feature model indian buffet process advance neural information processing system nip teh d ghahramani stickbreake construction indian buffet process international conference intelligence statistic aistat blei d griffith m nest chinese restaurant process inference topic hierarchy journal acm sethuraman constructive definition dirichlet prior bishop c pattern recognition machine learn flexible large scale topic modeling package use variational inference mapreduce international world wide web conference m online model selection base variational baye neural computation opper naive mean field theory equation page mit d information theory inference learn cambridge press asuncion m p smyth smoothing inference topic model uncertainty artificial intelligence uai collapse variational baye inference conference machine learning icml collapse variational baye inference hierarchical dirichlet process international conference knowledge discovery data mining page minka t divergence measure message pass tech research teh m collapse variational bayesian latent dirichlet allocation advance neural information processing system nip m hoffman sparse stochastic inference latent dirichlet allocation international conference machine learning icml natural gradient work efficiently learn neural computation robbin h stochastic approximation method annal mathematical statistic pp read leave human interpret topic model advance neural information processing system nip griffith t m find scientific topic proceeding wallach h murray r salakhutdinov et evaluation method topic model international conference machine learning icml distribution derive stable annal probability architecture language learning artificial intelligence
backpropagation neuromorphic computing abstract solve real world problem embed neural network require training algorithm achieve high performance compatible hardware run real time remain energy efficient deep learning use backpropagation recently achieve string success domain dataset neuromorphic chip run spike neural network recently achieve energy efficiency bring advance resolve backpropagation use neuron synaptic weight neuromorphic design employ spike neuron discrete synapsis approach treat spike discrete synapsis continuous probability allow train network use standard backpropagation train network naturally map neuromorphic hardware sample probability create network merge use ensemble averaging demonstrate train connect network run truenorth chip use dataset high performance network ensemble achieve accuracy image high efficiency network ensemble achieve accuracy image introduction neural network today achieve stateoftheart performance competition range field success raise hope begin network lab embed system tackle real world problem necessitate shift darpa public release distribution unlimited think system design neural network hardware substrate collectively meet performance power space speed requirement basis efficient substrate neural network operation today neuromorphic design achieve high efficiency neuromorphic architecture use spike provide event base computation communication consume necessary use low precision synapsis memory computation datum movement local allow parallel distribute operation use constrain connectivity implement neuron fanout efficiently dramatically reduce network traffic design choice introduce apparent backpropagation use train today successful deep network use neuron synapsis typically operate limit number input neuron build system advantage algorithmic insight deep learning efficiency neuromorphic main contribution demonstrate learning rule network topology apparent backpropagation neuromorphic hardware essence learning rule train network offline hardware support connectivity continuous value input neuron output synaptic weight value constrain range far impose constrained value represent probability spike occur particular synapse network train use backpropagation direct representation spike low synaptic precision deployment system bridge world network topology use mix approach neuron access limited set input previous layer source choose neuron successive layer access progressively network input previous effort success subset element bring backpropagation use train network spike neuron weight converse network synapsis continuous output neuron probabilistic backpropagation approach demonstrate network binary neuron binary synapsis connectivity work present novel demonstrate time offline training methodology use backpropagation create network employ spike neuron synapsis require bit precision weight constrain connectivity achieve good accuracy date mnist compare network use spike neuron high precision synapsis network use binary synapsis neuron demonstrate network run realtime truenorth chip achieve far publish power efficiency digit recognition classification accuracy run image second compare low power approach classification accuracy run image second deployment hardware use truenorth chip example deployment system approach generalize neuromorphic hardware truenorth chip consist core core contain axon input synapse spike neuron information flow spike neuron axon core axon potentially neuron core gate binary synapsis neuron consider variety dynamic include describe axon assign axon type use index lookup table unique neuron provide sign bit integer synaptic strength correspond synapse approach require bit synapse state additional bit synapse lookup table scheme network training approach employ type multilayer network deployment network run platform support spike neuron discrete synapsis low precision limited connectivity training network use learn binary synaptic connectivity state bias network share topology deployment network represent input neuron output synaptic connection use continuous value constrain range overview provide figure table value correspond probability spike occur synapse provide mean map training network deployment network provide continuous differentiable space backpropagation describe deployment network training methodology procedure map training network deployment network deployment network deployment deployment network follow feedforward methodology neuron sequentially update layer input network represent use stochastically generate spike value input unit probability write p spike state input unit continuous value range derive rescale input datum pixel scheme allow representation datum use binary spike preserve datum precision expectation training input input connect synapsis connection probability neuron spike strength probability figure diagram input synapsis output neuron deployment training network simplicity synapsis depict sum neuron input compute ij bj index cij binary indicator variable represent synapse synaptic strength bj bias term identical common practice neural network factor synaptic weight cij sij focus learning effort reason describe function follow thresholding equation ij dynamic implement truenorth set neuron leak equal learned bias term drop fractional portion threshold membrane potential set synapse parameter use scheme describe represent class label use multiple output neuron layer network find improve prediction performance network prediction class simply average output neuron assign class table network component network input synaptic connection deployment network variable value p p training network variable value training network training follow backpropagation methodology iteratively run forward pass layer layer compare network output desire output use loss function iii propagate loss network determine loss gradient synapse bias term use gradient update network parameter training network forward pass probabilistic representation deployment network forward pass synaptic connection represent probability use cij cij cij synaptic strength represent use sij deployment network assume draw limited set value consider additional constraint set block multiple synapsis share value truenorth efficiency learn optimal value sij condition require stepwise change allow value optimization local synapse simple approach learn bias synapse connection probability fix synapse strength use approach describe network initialization input training network represent use probability input occur deployment network neuron note equation summation weight variable bias term assume independence input sufficient number approximate probability distribution summation gaussian bj variance derive probability neuron fire use complementary cumulative distribution function error function p layer replace input previous layer represent probability neuron produce variety loss function suitable approach find training converge fast use log loss log pk class binary class label class present pk probability average spike count class great use approximation equation mean variance term set averaging process training network backward adaptation backpropagation use neuron synapse equation gradient synapse use chain rule compute e bias similar computation replace cij equation bj differentiate equation produce sij e sij sij describe assume synapse strength neuron balance positive negative value neuron receive input expect close cij right term equation contain denominator expect small left term contain denominator condition computational efficiency approximate equation drop right term factor remainder e sij similar treatment use correspond gradient respect term equal network update use loss gradient synapse bias term synaptic connection probability change accord cij cij learning rate synaptic connection probability fall outside range result update rule near valid value change term handle similar fashion value clip fall range large value support use truenorth neuron parameter training procedure describe amenable method heuristic apply standard backpropagation result use mini batch size momentum dropout learning rate decay fix schedule training iteration start multiply epoch transformation training datum iteration rotation shift pixel rescale mapping training network deployment network training perform offline result network map deployment network hardware operation deployment depend system requirement utilize ensemble sampling training network increase overall output performance ensemble method train sample training network member system output class determine average neuron member network assign class synaptic connection state set accord cij cij use independent random number draw synapse ensemble member data convert spike representation input use p use independent random number draw input member ensemble network initialization approach network initialization describe allow optimize efficient neuromorphic hardware employ bit synapse approach synaptic connection probability initialize uniform random distribution range initialize synapse strength value begin principle core maximize information transfer maximize information neuron minimize redundancy neuron method explore detail approach infomax goal data dependent pursue second initialization time tune space possible weight core represent matrix synapse strength value s approach wish minimize redundancy neuron core attempt induce product distribution output pair neuron simplify problem note sum weight input pair neuron bivariate gaussian distribution force covariance weight input guarantee input independent furthermore function pairwise independent random variable remain pairwise neuron output guarantee independent axon type axon type axon type axon strength neuron neuron neuron sum weight input equation desirable purpose maintain balance dynamic weight use mix positive negative value sum figure synapse strength value sij axon row neuron column array learning procedure fix value network initialize imply assume input learn probability synapse synaptic connection state decorrelate transmit state bias term simplify covariance strength matrix result input neuron core share synaptic strength approach use truenorth reduce memory ir rearrange term ir sij note equation covariance assumption input equal mean variance constant far assume covariance q input constant use equation equation ir hcj sj hcj sj hcj sj hcj sj minimize absolute value inner product column w force ij ir maximally uncorrelated constraint inspire observation apriori ie knowledge input datum choose strength value absolute value inner product column effective weight matrix minimize sum effective weight neuron practically achieve assign half neuron half balance possible permutation assignment occur equally possible neuron core evenly distribute possible axon type axon core result matrix synaptic strength value figure configuration provide optimal weight subspace constraint backpropagation operate fashion find desirable synaptic state b cm core neuron input window layer input input network figure network configuration use result describe core network design minimize core count core network design maximize accuracy truenorth chip use run deployment network chip cm run real time neuron update consume run benchmark network use neuron measure accuracy measure energy network configuration run chip ensemble size right data point network topology network topology design support neuron response local regional global feature respect connectivity truenorth architecture neuron core share access set input number input limit network use multilayer feedforward scheme layer consist input element row column channel array image remain layer consist truenorth core connection layer use slide window approach input core layer draw r r input window figure r represent row column dimension represent feature dimension input layer row column unit input element feature input channel input remain layer row column unit core feature neuron core target layer locate input window upper left corner source layer core target layer shift input window right successive core slide window edge source layer reach window return left shift s process repeat feature randomly constraint neuron select target core allow input element select multiple time scheme similar respect use convolution network employ independent synapsis location specific network employ associate parameter figure result apply training method describe mnist dataset examine accuracy energy tradeoff use network run truenorth chip figure b network small multilayer truenorth network possible number pixel present dataset consist core distribute layer correspond neuron second network build primary goal maximize accuracy compose core distribute layer figure corresponding neuron network layer use r network s core network s core network subsequent layer network use r s parameter result pyramid shape core layer final layer draw input source core neuron source core employ neuron core target maximum neuron test network ensemble member run truenorth chip realtime image encode use single time step different spike sample use input line target pixel available measure active power network operation power entire chip consist core report energy number active power fraction power core use high overall performance observe achieve core train network use member ensemble total core measure use classification low energy achieve core network operate ensemble measure use classification achieve accuracy result plot accuracy energy figure network classify image second discussion result backpropagation operate probabilistic domain use train network naturally map neuromorphic hardware spike neuron extremely synapsis approach summarize constrain network provide direct representation deployment system train constraint contrast approach network agnostic final deployment system train follow training constrain normalization discretization method provide spike representation low precision weight require training rule approach offer advantage decrease training error direct correspondence decrease error deployment network conversely approach allow use training method unconstrained training guarantee produce reduction error hardware constraint apply look forward expand approach complex dataset deep convolution network great deal success use learn weight convolutional filter learning method introduce independent specific network structure sparsity constraint certainly adapt use convolution network second biology provide number example map sensory datum binary spike representation draw approach improve performance linear mapping scheme use work approach gradient base learning method method exist probabilistic component contrastive divergence far describe use approach truenorth provide concrete use case reason training approach use spike neuromorphic hardware believe work particularly timely recent year backpropagation achieve high level performance number task reflect real world task include object detection complex scene detection speech recognition wide range sensor find device range platform truenorth provide low power substrate process sensory datum bridge backpropagation efficient neuromorphic computing hope work provide important step build scalable system real world applicability acknowledgment research sponsor advanced research project contract c view opinion finding contain paper author interpret represent official view policy government reference m large scale visual recognition challenge international computer vision deep learning detection international conference computer vision c case e r coate scale endtoend speech recognition p chandrasekaran jm r system simulation proceeding ieee vol e brown core circuit vol t s muller p muller m m j network universal neuromorphic compute substrate frontier neuroscience vol p r integrate circuit scalable communication network interface science vol rumelhart r learning representation backpropagate error nature vol p e neural network adaptation hardware implementation handbook neural computation e physics publishing university publishing e weight discretization paradigm optical network international society spike deep convolutional neural network international computer vision vol p u m m spike deep network weight threshold balance international joint conference neural network press muller round method neural network low resolution synaptic weight preprint m learning stochastic bit stream neural network network vol pp train binary multilayer neural network image classification use expectation e d scalable implementation spike deep belief network international joint conference neural network ieee press p r p m e risk cognitive computing building block efficient digital neuron model core international joint conference neural network srivastava krizhevsky r r salakhutdinov improve neural network prevent feature detector sejnowski approach blind separation blind deconvolution neural computation vol pp bottou bengio p gradientbased learning apply document recognition proceeding ieee vol pp e hinton r r salakhutdinov reduce dimensionality datum neural network science vol
stable fit reinforcement learning geoffrey computer abstract describe reinforcement learning problem motivate algorithm seek approximation q function present new convergence result introduction background imagine agent act environment time environment state choose finite set state agent perceive allow choose action finite set action environment change state time new state choose probability distribution depend agent experience realvalue cost choose distribution depend finite mean environment markov decision process mdp reinforcement learning problem control mdp minimize expect discount cost discount factor define function q cost state time choose action behave optimally discover q solve problem step simply choose minimize information mdps watkin bertsekas distinguish class problem online offline offline problem model mdp state action describe distribution cost state concern online problem knowledge mdp limit discover interact solve online problem approximate transition cost function proceed offline problem indirect approach try learn q function intermediate step direct approach approach work problem stable fit reinforcement learning direct approach extract information observation indirect approach introduce additional error extra approximation step concern direct algorithm watkin qiearne find q function small mdps online offline convergence probability online case prove large mdps exact qiearne expensive represent q function require space overcome difficulty look inexpensive approximation q function offline case algorithm purpose prove converge online case provably convergent algorithm point rely gradient descent large stochastic problem observe independent transition state compute unbiased estimate gradient algorithm use state approximate q function modify apply online problem result qiearne repeat small update control policy interleave period evaluation change submit paper paper contain different algorithm solve online mdps addition new paper b prove result large class approximator algorithm handle restrict version online problem case markov chain mdp action available time step prove converge arbitrary linear approximator dayan decision process linear transition function quadratic cost function socalled linear problem algorithm guarantee converge practice researcher mixed success approximate reinforcement learn moore remainder paper divide section section summarize convergence result offline prove contraction property useful later section extend convergence result algorithm base simple function approximator section treat problem section offline discount problem standard offline qiearne begin mdp m initial q function goal learn good approximation optimal q function m accomplish goal perform series update component correspond state x action define p q m q expect cost perform action state probability action state lead state discount factor offline qiearne converge discount mdps tm contraction max norm vector q r q contraction mapping unique fix point q sequence converge linearly worth note weight version offline qiearne guarantee converge consider iteration adtm positive learning rate arbitrary fix diagonal matrix weight iteration update q rapidly occur instance visit state frequently come possibility later weight iteration max norm contraction sufficiently small q function q r r ii suppose small large element ad b let b small diagonal element ad consider state x action write correspond element ad ad ad adtm ad max norm contraction factor fix point weight qiearne fix point unweighted qiearne q equivalent adtm lq difficulty standard weight unweighted qiearne mdps state completely infeasible compute value way avoid difficulty fit qiearne find function tm cheap compute t m perform fit iteration instead standard offline iteration mapping implement function approximation scheme assume represent q fit offline qiearne iteration guarantee converge unique fix point max norm bound error finally define fit weight qiearne iteration m condition satisfied example state aggregation fit weight qiearne guarantee converge max norm m adtm adtm q q q range note guarantee range line composition max norm max norm contraction max norm contraction fix point fit weight qiearne necessarily fix point fit qiearne represent exactly linear c c adtm c q range perpendicular range particular c c range let q fix point stable fit reinforcement learn weighted fit iteration q adtm adtm linear addition condition convergence bind error fit weight qiearne offline problem weighted version fit useful version involve work iteration contraction factor good error bind tight require m addition condition convergence unweighted iteration hand section weighted apply online problem online discount problem consider follow algorithm natural generalization markov decision problem algorithm start initial q function repeat follow step let policy choose accord predetermine tradeoff exploration exploitation function agent start state allow follow policy random number step step result trajectory agent state action cost ct state yt action appear optimal compute estimate error ct q q observe entire trajectory define ei vector component sum et t compute weight vector accord update rule learn rate b comment type mapping appropriate online algorithm assume distribution independent event relate ith subsequent trajectory bound define expect number time agent visit state choose action ith trajectory assume policy positive let di diagonal matrix element notation write expect update sarsa matrix form qi ai exception fact di change iteration iteration equation look similar offline weight fit qiearne update sarsa guarantee converge benign case figure counterexample sarsa mdp start state agent choose upper low path decision force arc expect cost actual cost randomize step box pair arc aggregate agent learn identical q value arc box use discount learning rate ensure sufficient exploration agent choose apparently suboptimal action time parameter result similar behavior particular anneal help learn q value righthand box step qfunction approximate state aggregation apply mdp figure learn q value problem happen matrix di change q function fluctuate slightly upper path appear arc goal follow learn q value decrease low path appear cost arc weight heavily q value increase arc initial state expect value state lead constrain value path appear infinitely oscillation continue hand represent optimal q function q matter di expect sarsa update fix point q small diagonal element bound away large bound choose contraction fix point q factor let learn rate satisfy ei q guarantee theorem generally linear represent c vector c bind error q fix point expect sarsa update iteration choose previous q qi minor modification theorem distance qi region iii q q c converge sequence converge bad region size determine stable fit reinforcement learn accurately represent frequently visit state finally follow fix exploration policy trajectory matrix d case contraction prove previous section convergence appropriate learning rate guarantee theorem problem m discount qiearne operator tm long max norm contraction instead long policy guarantee set terminal state tm contraction weight norm proof previous section substitute weight max norm unweighted case addition variable determine trial end set step t terminal subsequent step error choice independent ith trial finite mean result constant di discussion prove new convergence theorem online fit reinforcement learning algorithm base watkin qiearne sarsa sarsa fix exploration policy allow use function approximator mapping max norm satisfy m prototypical example function approximator state aggregation similar result large class approximator b acknowledgement material base work support national science research fellowship grant number opinion finding conclusion recommendation express publication author necessarily reflect view arpa government reference l residual algorithm reinforcement learning function tion machine learn proceeding twelfth international d p bertsekas parallel distribute computation numerical method moore generalization reinforcement learning approximate value function touretzky editor advance neural information processing system volume s reinforcement learning apply linear quadratic hanson l gile editor advance neural information processing system volume p dayan convergence learn stable function approximation dynamic programming machine learn proceeding twelfth international conference online fit reinforcement learning moore r editor proceeding workshop value function approximation proceeding available tech report s convergence stochastic iterative dynamic programming algorithm neural computation p t m learn soft aggregation touretzky editor advance neural information processing system volume p r reinforcement learning replace trace machine learn learn predict method temporal difference machine learn neural network program proceeding iii page featurebased method programming p laboratory information decision asynchronous stochastic approximation qiearne machine learn h watkin learn delay reward phd thesis
learn unambiguous reduce sequence description dept computer co abstract want neural net algorithm learn sequence limit conventional gradient descent approximation thereof instead use sequence learn implement follow method history compression matter final goal train network predict input previous unpredictable input convey new information ignore predictable input let unexpected input information time step occur input higherlevel network kind work slow time scale build hierarchy network principle reduce description event sequence information ease supervise reinforcement learning task alternatively use recurrent network collapse multilevel predictor hierarchy single recurrent net experiment system base principle require computation time step training sequence conventional training algorithm recurrent net finally modify method define fashion continuous fashion schmidhuber introduction follow method supervised sequence learning propose simple recurrent net net eg sequential recursive autoassociative memory backpropagation time mozer focus accelerate version recent higherorder network continuous time method equivalent follow method sequence learn reinforcement learning propose extended reinforce algorithm recurrent network adjust adaptive system network hierarchically organize exception approach waste resource limit efficiency focus input instead focus relevant input method second drawback long time lag event occurrence related error information carry corresponding error information time detailed analysis address problem system describe hand address problem manner different present history compression major contribution work adaptive method remove redundant information sequence principle implement help method mention introduction consider deterministic discrete time predictor necessarily neural network state time sequence p describe environmental input vector zpt internal state vector output vector zpt time predictor start internal start state time predictor compute zpt time predictor furthermore compute information input time reconstruct pair zpt zpt zpt zpt time t predictor able predict input previous new input mean information observe input sequence far compress input vector zpt suffice know element vector zpt correctly predict observation imply discriminate sequence know input corresponding time step learn unambiguous reduce sequence description occur information lose ignore expect input know principle history compression theoretical point view important know time unexpected input occur potential ambiguity different input sequence lead short sequence predict input practical task need know critical time step section selforganize predictor hierarchy use principle history compression build selforganize hierarchical neural chunk system l basic task formulate prediction task time step goal predict input previous input external target vector certain time step simply treat input predict architecture hierarchy predictor input level hierarchy come previous level pi denote ith level network train predict input previous input pi conventional dynamic recurrent neural network mention introduction adaptive sequence processing device time step input recurrent predictor current external input create new higherlevel adaptive predictor adaptive predictor previous level p stop improve prediction happen mechanism p switch exclude potential instability cause ongoing modification predictor time step p fail predict input beginning training sequence usually predictable receive input concatenation input p unique representation corresponding time step activation hide output unit update perform activation update procedure ensure feed unambiguous description input sequence observe p theoretically justify principle history compression general receive input time p exist learning different hierarchical connectionist system base similar principle recently aware related idea personal communication hierarchical approach sequence generation pursue instance employ limited feedforward network time window approach case number previous input consider basis prediction remain fix unique time representation theoretically necessary provide unambiguous information failure occur section unique representation time input occur contrast reduce description refer unambiguous schmidhuber algorithm higherlevel predictor difficulty learn predict critical input predictor credit assignment path short compare p happen incoming input global temporal structure discover p related approach problem credit assignment reinforcement learn method simplification improvement recent chunking method describe multilevel predictor hierarchy safe way learn deal sequence multilevel temporal structure eg speech experiment multilevel predictor quickly learn task practically network eg collapse hierarchy disadvantage predictor hierarchy know advance level need disadvantage level explicitly separate possible collapse hierarchy single network outline section detail need conventional recurrent network cone distinction automatic attend event describe similar distinction context learn time step receive current external input function term force emit certain desire target output certain time target input second term force time step predict input crucial term explain concern second term en function predict input include potentially available teaching vector unique representation current time step new input c new input process c input time step early train predict higherlevel input internal state input employ conventional net c perform activation update contribute high level internal representation input history note accord principle history compression c feed unambiguous reduce description input history information mean prediction consider redundant beginning episode usually predictable feed chunking level credit assignment path short compare c able develop useful internal representation previous unexpected input event final term error function force reproduce internal representation predict state able create useful internal representation early stage process learn unambiguous reduce sequence description sequence receive meaningful error signal long error second kind occur internal representation turn discriminate information enable improve lowlevel prediction receive input input predictable collapse operation ideally time emphasize incremental multilevel predictor hierarchy describe section formal proof net online version free instability imagine situation previously learn prediction term function relative weighting different term function represent potential problem experiment relative weighting necessary experiment experiment multilevel chunking architecture involve grammar produce string bs local temporal structure training string detail task differentiate string long suffix conventional algorithm completely fail solve task great number input sequence similar chunk system soon discover certain hierarchical temporal structure input sequence decompose problem able solve training sequence net chunking system potential collapse level test net algorithm detail conventional algorithm learning rate training sequence performance improve prediction task involve o time step relevant event net chunking system able solve task quickly efficient approximation apply iteration error propagation past perform time step test run require train sequence final weight resemble hope conventional algorithm hide unit learn bridge step time lag mean strong chunk system need computation time step conventional method require training sequence continuous history compression history compression technique formulate define fashion input unit activation predictable certain time rise unexpected event unexpected event update internal state higherlevel predictor update place accord conventional activation spread rule schmidhuber current neural net concept partial possibility update higherlevel net little bit nearly expect input practical application use define acceptable continuous history compression base follow idea follow denote ith component vector use local input representation component zpt force sum interpret prediction probability distribution possible zpt zt interpret prediction probability output entropy interpret measure predictor confidence bad ease predictor expect possible event equal probability information relative current predictor convey event observe accord log define update procedure base mozer recent update function let highly informative event strong influence history representation informative likely event strength update unexpected event monotonically increase function information event convey update procedure use pollack recursive autoassociative memory store unexpected event yield entirely local learning learn extended acknowledgement thank conduct experiment thank mark ring useful comment early paper research support award grant external research grant reference learn represent state master thesis u adjust supervised learning lippman editor advance neural processing system page learn unambiguous reduce sequence description finding structure time technical report report center research language m learning algorithm analog fully recurrent neural network international joint conference neural network volume page c l gile miller learning extract accept publication neural computation m order parallel distribute processing approach technical report ic report institute cognitive science review schmidhuber paper recurrent network adjust neural network review unsupervised learn model action planning tenth annual conference cognitive science society page m c mozer backpropagation algorithm temporal system m c mozer connectionist music composition base psychophysical constraint technical report m c mozer induction multiscale temporal structure lippman touretzky editor advance neural information processing system appear c learn delay reinforcement tr college science technology medicine pearlmutter learn state space trajectory recurrent neural network neural computation time dependent adaptive neural network editor advance neural information processing system page pollack recursive distribute representation artificial intelligence m ring autonomous construction sensorimotor hierarchy neural network technical report m ring incremental development complex behavior automatic construction hierarchy editor machine learn proceeding eighth international workshop page utility drive dynamic error engineering schmidhuber r target training method linden editor proceeding distribute adaptive neural information processing d e rumelhart e hinton r learn internal representation error propagation d e rumelhart editor parallel distribute processing volume page mit press schmidhuber local learning algorithm dynamic feedforward recurrent network connection science schmidhuber recurrent network adjust adaptive international joint conference neural network c volume page h schmidhuber adaptive decomposition time editor artificial neural network page science publisher h schmidhuber fix size storage time complexity learn algorithm fully recurrent continually run network accept publication neural computation j h schmidhuber learn complex extended sequence use principle history compression accept publication neural computation j h schmidhuber learn control alternative recurrent net accept publication neural computation j h schmidhuber m c mozer continuous history compression technical report dept e shannon mathematical theory communication p generalization propagation application recurrent gas market model neural network r theory connectionist system complexity exact gradient computation algorithm recurrent neural network technical college computer science efficient gradientbased algorithm online training recurrent network trajectory neural experimental analysis realtime recurrent learn connection science r gradientbased learning algorithm recurrent network computational complexity backpropagation theory architecture application nj press vi recurrent network
multistep learning underlie structure statistical model dept mathematic abstract multistep learn final learning task accomplish sequence intermediate task intuition successive step level transform initial datum representation suited final learning task related principle arise propose theoretical framework study learn multiple task transform inductive bias learner widespread multistep learning approach semisupervise learn step supervise author analyze propose version pac learn framework augment compatibility function link concept class unlabeled data distribution propose analyze multistep learning approach spirit baxter define learning problem generatively joint statistical model determine natural way class conditional distribution possible marginal abstract form compatibility function allow analyze discrete setting tool analysis define notion uniform shattering statistical model use condition marginal conditional model imply advantage multistep learning approach particular recover general version result mild hypothesis multistep approach learn feature invariant successive factor finite group invariance sample complexity requirement additive multiplicative size subgroup introduction classical pac learn framework valiant consider learning problem unknown true distribution p fix concept class c consist deterministic function aim learning select hypothesis realizable case good recover formally class pac learnable learning algorithm high probability select c arbitrarily low generalization error possible distribution d x distribution d govern sampling point z algorithm obtain training sample error generalization error modification model notion learnable model probability resp decision rule kearn schapire allow treat function case analogously polynomial dependence algorithm sample size reciprocal conference neural information processing system nip probability bound far require framework efficient learning framework consider bad case error sense require generalization error small arbitrary distribution d assume concept class c regardless true underlie distribution d addition choice hypothesis class inductive bias algorithm address classic measure complexity hypothesis space dimension rademacher complexity overview allow prove upper bound generalization error set variant anneal rademacher average begin koltchinskii use obtain upper bound widespread strategy semisupervise learning know fit framework valiant haussler kearn schapire perform step use unlabeled training datum draw distribution follow second step use label training datum joint distribution study author follow work cover compare value unlabeled label immediate observation tie possible marginal d concept class c record possible conditional pyx benefit unlabeled datum arbitrary convey information true joint distribution generate label datum framework completely independent propose augment pac learn framework addition compatibility function d record compatibility believe concept c d class distribution function require learnable d use reduce concept class c subclass use subsequent supervised learning step good compatible function subclass complexity blum framework essence allow true joint distribution c d existence good compatibility function sense blum implicitly assume joint model believe small return point section paper study property multistep learning strategy involve multiple training step consider advantage break single learning problem sequence learning problem start assume true distribution come class joint distribution ie statistical model p prove underlie structure certain kind p differential availability label unlabeled datum imply advantage multistep learn sample size structure need existence representation sufficient statistic classification regression interest common setting assumption hold manifold learning groupinvariant feature learn setting respectively t determine marginal px px concentrate submanifold t determine group action pyx invariant action learn case correspond respectively learn manifold feature groupinvariant feature approach exist poggio discussion assume fix method framework restrict setting tool analysis define variant dimension statistical model use prove useful lower bind generalization error assumption true distribution come p allow establish gap finite sample size error achievable purely supervise learner achievable semisupervise learner claim asymptotic gap purpose analysis differential finite availability datum dictate multistep learning approach application respectively manifold learning example analyze groupinvariant feature example relate result discuss relevance framework commonality framework baxter transfer learn work baxter consider learn inductive bias hypothesis space algorithm mean group g transformation pyx low bound definition weak distributionfree target learning task base experience previous source learn task purpose define learning environment e class probability distribution y unknown probability distribution q e assume e restrict possible joint distribution arise generative assumption assume joint distribution come p use prior q framework baxter study reduction generalization error algorithm learn new task define p e access sample sample m learn task p pm choose randomly accord compare algorithm access sample p analysis produce upper bound generalization error term cover number lower bind obtain term dimension specific case shallow neural network prove low bind term variant dimension use minimax analysis setup assume learning problem specify joint probability distribution p particular regression classification decision function r determined entirely postulate statistical model p assume p p simplified notation depend conditional pyx entire joint distribution main type learn problem framework address reflect type noisefree concentrate single x classification ep noisy regression ep yx case parameter define learning goal depend pyx assume learner know model p type learn problem ie hypothesis class concept class c p p precise type list concept class kearn second type class decision rule type class kearn schapire specific choice loss function seek worstcase bound learn rate distribution p p result type learn problem state presentation simple detailed proof type ie assume label binary classic argument discrete adapt framework smooth extend argument handle proceed modification discrete cf kearn schapire remark presence noise bound obtain detail technical version definition use leave subsequent paper define follow probabilistic version shattering dimension definition class probability distribution let suppose exist disjoint set n si reference probability measure q subclass pn p cardinality follow property q low bound marginal p pn subset r b r b e p pn si si ei p shatter sn uniformly use uniform shatter dimension p large p shatter collection subset x uniformly provide measure complexity class p distribution sense indicate variability expect constrain lie region s measure corresponding marginal reference measure q serve low bind marginal ensure uniformly assign variability conditional trade uniformity corresponding marginal distribution remark uniformity measure technical requirement reference distribution q automatically satisfied marginal px p pn uniform simplicity situation consider example weak condition term q postulate definition sufficient main result theorem binary noisefree p shatter s sn uniformly subclass pn p specify uniformity measure p constant si induce shatter sn usual sense case choose arbitrarily omit mention value binary noisy uniform shattering express term scale uniform shatter dimension p use lower bind size require powerful learner class problem proof spirit purely combinatorial proof low bound use essentially add condition p term allow convert risk calculation combinatorial problem lower bind result consider alternative step learning strategy use underlie structure imply model p obtain upper bound correspond risk underlying structure assume representation t rk datum pyx express term g parameter t generally know statistic sufficient dimension reduction assumption dimension compare dimension x paradigm feature extraction use machine dimension high original dimension set important reduce complexity concept space depend p assume example t depend p marginal px possible group action data possibly time underlie structure true joint distribution p p representation t capture structure induce p hand regression function depend conditional general natural factorization p px p px determine marginal q px collection q possible conditional pyx arise joint p p marginal px generally sufficient statistic t induce similar factorization p pt p pt marginal model respect t conditional pyt need learn know marginal q imply collection t q possible conditional relevant learn know reduce original problem pyx pyt come p p come p reduce class q t p note similarity assumption blum good compatibility function reduce concept class case concept class c consist define pyt p p marginal come joint model p postulate correspond subset c pt pair q use p t indicator function subset abstract binary version compatibility function recall compatibility function value function c d satisfy practical condition function typically sense assumption joint model p sufficient statistic t general form compatibility function link assumption t learn imply original learning problem factor learn structure t learn parameter g reduce hypothesis space goal understand learn rate wish quantify benefit achieve use factorization term bound expect loss ie risk sample size m draw iid p p assume learner provide sample z z zi xi draw distribution p use algorithm z m c h select z approximate let z denote specific loss absolute square hinge logistic loss define z global expectation l norm pointwise loss z z px z bad case expect loss ie minimax risk good learn algorithm knowledge good learn algorithm oracle knowledge sup order regard class principle bad case expect loss supremum p expect loss determine determine px supremum px supremum px q find bad case expect error good mean infimum supremum describe case qm algorithm know order supremum t change respect infimum learner select good algorithm use knowledge clearly definition section lower bind rm upper bind qm establish gap qm main result uniform shatter dimension n imply lower bind bad case expect error rm sample size m particular setup specify previous section small dimension n result distinct gap rate learner oracle access learner consider framework define previous section assume dimension p uniform shatter dimension m sample size m bc m depend type loss presence noise c depend noise assume standard definition binary noisefree noisy set b absolute squared hinge logistic loss noisy setting b absolute loss b squared loss noisefree set c m noisy setting require p satisfy strong notion uniform shatter obtain c noisy case note sample size m uniform shatter dimension m low bind simple form m bound use section derive implication remark state simple upper bind stick use dimension order focus presentation lower bind use new complexity measure upper bind improve replace corresponding upper bind assume instead shattering dimension d proof upper bind hold erm algorithm classic argument example corollary focus low bind stick simple definition uniform shattering definition omit proof final statement theorem slightly involved let m comment result general let sm set uniformly shatter use family pm p denote union assumption measure reference measure dominate marginal px p pm definition divide argument prove lower bind average pm lower bind supremum sup bc m supremum p suffice prove define likely label joint distribution p pm notation extend noisy case definition noisefree case uniform shatter condition imply noisy case noisefree case x s m write z z m m z sm c specify note set vl m m occupy exactly l si m define partition m recall p pm m x m m m m m m b x c m p c b m m z claim bound computation perform depend know x time measure q m m m m l complete proof assume fixed arbitrary prove b simplify discussion refer set contain component si datum need notation element pm l m denote unique element pm l let m si index set si datum assumption l m subset l m p pm determine lx collect l lx define pl pm l l family partition pm pd m l probability distribution importantly p pd determine si datum imply r function p simplify notation work single pd write z regression function datum true regression function p underlie distribution set si let majority si use reference measure q focus unseen si data lie ie use vi specify correspondence element p pd subset p pd kp specific p pd associate x set si kp condition x opposite imply low bind x pointwise loss function consider absolute square hinge logistic value differ case case x x si x b m si sum kp let kp obtain m assume l define equation mk possible cardinality m m m b m m b x m m m use m m m arbitrary subset lower bind hold family pd b m x construction section case prove different level shatter different shattering subset follow corollary immediate consequence theorem setting state binary noise let c m p shatter subset c m learning achieve bad case expect error use training sample size m uniform shattering hold n lower bind apply regardless sample size shattering hold approach sufficiently slowly possible asymptotic obstacle learn contrast section extreme situation e case learning impossible application conclusion manifold learning describe simple finite dimensional version example let x rd d fix consider simple type dimensional manifold union linear segment connect circular fashion figure let px collection marginal distribution support assign uniform probability curve type correspondence element px curve describe p element p pn know h p p version use x reader verify lower bind hold case use m instead m m m figure example m dash curve label solid curve figure figure m piece use prove uniform shattering set case e curve m choose distinct point remove disconnect m let component label label let p class joint distribution y conditional describe marginal px noisefree setting binary m circular coordinate m consider reduce class p p p m p p dimension hand n p shatter set appendix figure e follow corollary bad case expect error bound e sample linear piece allow ie high impractical number label example contrast example example arbitrarily close groupinvariant feature simplified example smooth version figure let let x grid real line segment picture rectangular array vertical stick grid point consider special point stick let px contain uniform distribution assume noisefree setting e segment assign label special point determine e point determine family conditional distribution family p e joint distribution reader verify p uniform shatter dimension note true distribution pe e label invariant action zn define follow z z zn j let group element vertical stick flip stick stretch need special point determine e stick second stick space action identify let t projection space labelling space label invariant action group access t result concept class dimension hand instead access projection s action subgroup e ps p p uniform shatter dimension class p general setting overall complexity requirement twostep learning n learning conclusion use notion uniform shatter demonstrate manifold learning invariant feature learn situation learning impossible learner access large label datum use twostep semisupervise approach suitable manifold groupinvariant feature learn unsupervised fashion example provide complexity advantage observe poggio form intermediate groupinvariant feature accord subgroup large transformation group acknowledgement author grateful chance student paper directly inspire discussion cut short soon author thank helpful input preliminary reference m s reverse hierarchy theory visual perceptual learning trend bengio regularize autoencoder learn datum generate distribution blum model learn label unlabeled datum learn theory volume page springer lnc baxter model inductive bias learning artificial intelligence research m manifold regularization geometric framework learn label unlabeled example machine learn research unlabeled datum provably help worstcase analysis sample complexity semisupervise learning colt page j m hierarchical development primate visual cortex reveal early middle temporal area cover relative value label unlabeled sample ieee transaction information theory l lugosi probabilistic theory pattern recognition volume application haussler generalize pac model sample size bound metric uniform convergence result page m kearn schapire efficient distributionfree learning probabilistic concept computer system science m kearn u introduction computational learning theory koltchinskii rademacher penalty structural risk minimization ieee transaction information theory group invariant scatter httparxivorgab m foundation machine learning manifold regularization semisupervise learn theoretical analysis machine learn research poggio l j computational ventral stream sketch theory deep architecture work r shalevshwartz s access unlabeled datum speed prediction time icml l valiant theory learnable communication acm
feature selection classification matrix datum large margin small covering number obermayer electrical engineering computer science abstract investigate problem learn classification task dataset describe matrix row column matrix correspond object row column object belong different set entry matrix express relationship interpret matrix element produce unknown kernel operate object pair mild assumption kernel correspond dot product unknown feature space minimize bind generalization error linear classifier obtain use cover number derive objective function model selection accord principle structural risk minimization new objective function advantage allow analysis matrix positive definite symmetric square consider case row object interpret feature suggest additional constraint impose sparseness row object method use feature selection finally apply method datum obtain dna microarray column object correspond sample row object correspond gene matrix element correspond expression level benchmark conduct use standard classification support vector machine neighbor standard feature selection new method extract sparse set gene provide superior classification result introduction property set object describe matrix row column correspond object element describe relationship typical case socalle pairwise datum row column matrix represent object dataset fig entry matrix denote similarity value express relationship object pairwise datum b c d e g feature vector b c d e f g figure typical example matrix datum text pairwise datum row object coincide b feature vector column object differ form row object interpret feature typical case occur object describe set feature fig b case column object object characterize row object correspond feature matrix element denote strength feature express particular object following consider task learn classification problem datum consider case class label assign column object training set matrix class label want construct classifier good generalization property possible choice select classifier support vector machine family use principle structural risk minimization model selection recent success theoretical property previous work large margin classifier dataset object describe feature vector svms operate column vector matrix problem arise number feature large comparable number object feature selection svms prone overfitte complexity regularization implicit learning method sparse number support vector classifier sparse number feature use classification relate result number feature provide upper bind number essential support vector previous work large margin classifier dataset object describe mutual similarity center idea matrix similarity interpret gram matrix obermayer work line far restrict case gram matrix positive definite method suggest modify matrix order restore positive definiteness row column object set pairwise datum contribution extend gram matrix approach matrix datum row column object belong different set long expect matrix positive definite square new objective function derive section algorithm construction linear classifier derive use principle structural risk section concern question condition matrix element interpret vector product feature space specialize pairwise datum section sparseness constraint feature selection introduce section section finally contain evaluation new method dna microarray datum benchmark result standard classifier base standard feature selection procedure large margin classifier matrix datum following consider set z object describe feature vector z base feature vector construct classifier define classification function h denote dot product hyperplane perpendicular distance parameterize unit normal origin hyperplane margin respect min set allow treat normal vector normalize margin normalize accord canonical form separation hyperplane hyperplane large margin obtain minimize margin equal generalization error linear classifier bound probability log log l provide training classification error x bound draw iid unknown distribution object l denote number training object denote margin expect cover number class function map datum object t theorem proposition order obtain classifier good generalization property suggest minimize proper constraint know general probability distribution object particular support know order mini problem approximate range maxi value training set minimize quantity m instead eq let x matrix feature vector l object set z z matrix feature vector p object set object set label summarize label use label matrix let consider case feature vector unknown t z correspond scalar product training set datum matrix correspond label matrix principle structural risk minimization implement minimize upper bind m wk m maxi constraint impose training set account use expression slack variable minimize obtain optimization problem m t m t min t b t m penalize wrong classification m absolute value exceed classification m set note quadratic expression objective function convex follow x x t w fact x t positive semidefinite dual variable constraint impose let training set vector case treat unique exist unique choose accord section second exist set t t identity t t z optimality condition require follow derivative x t l t y lagrange multiplier slack m variable obtain ensure z z z m m ti condition b m m t following set m m m c m z t x m imply norm obtain follow dual problem subject m add constraint m example add classifier select accord new example u classify accord sign u ui b b optimal classifier select optimize eq long m hold true possible object assume draw error bound eq outlier reject condition m enforce large training set number rejection small probability p m outlier occur bound confidence use additive bound log p m l note outlier misclassifie trivial bind generalization error order kernel function measurement scalar product section assume matrix derive scalar product feature vector z describe object set z practical purpose information available summarize matrix feature vector know unclear exist order apply result section practical problem follow question remain answer condition measurement operator interpret scalar product feature vector matrix interpret matrix evaluation order answer question use follow theorem let l h denote set function p set infinite vector ai converge theorem singular value expansion let hilbert space let l h let kernel l h define hilbertschmidt operator h z dz p exist expansion n sn z gn converge l sense singular value h correspond orthonormal function corollary classification let assumption hold let k let dot product h define h e e s following hold true x tk adjoint operator follow sum convergence absolutely uniformly eq linear classifier map vector feature space define second mapping feature space e z e z dirac delta recover discrete observe wk problem arise belong set measure obey singular value decomposition occur z set function tell measurement apply object express z h x zi define dot product space define feature matrix feature vector column object matrix z z p feature vector p row object apply result section pairwise datum interesting special case occur row column object coincide kind datum know pairwise datum object classify serve feature vice versa section expand singular value decomposition introduce different mapping feature space use map row column object perform eigenvalue decomposition consequence eigenvalue negative follow eigenvalue expansion let definition assumption let p h h let symmetric exist expansion en converge l sense eigenvalue tk correspond orthonormal eigenfunction corollary r space classification let assumption z k x hold true define p p h e e e e s denote sign sign following hold true p p p h sign h p l sense follow sum en convergence absolutely uniformly eq linear classifier space s discrete case z normal vector comparison corollary kwk t assume converge unfortunately general continuous positive definite compact sum converge uniformly absolutely sparseness feature selection mention text optimization problem additional regularization term need choose regularization term enforce sparseness use feature selection choose regularization parameter separate positive negative dual optimization problem t min sparse ie classification function pp ui ui b contain term save number measurement ui new object yield improve classification performance reduce number feature z application dna microarray datum apply new method dna microarray datum publish column object sample different brain tumor kind sample obtain patient treat similar way sample label accord patient respond row object correspond gene gene tag use probe bind pair fluorescence bound level gene expression measure rise real value matrix entry represent level gene expression corresponding sample detail task construct classifier predict outcome basis sample new patient major problem classification task limited number sample large number gene feature selection good generalization construct classifier use step procedure step apply new method matrix column object avoid biased feature selection choose fairly large order obtain sparse set feature second step use select feature apply method reduce matrix small value use regularization instead feature selection classification statistic svm statistic comb statistic knn statistic comb e feature selection classification e table benchmark result dna microarray datum explanation text table classification error number wrong classification e different number select feature different value feature selection method denote statistic method datum provide classification standard svms weight comb near knn combine comb procedure use classification method result table result leaveoneout crossvalidation procedure classification error different number select feature method compare classification gene standard svms weight near neighbor knn combine classifier method feature selection base correlation feature class use method use c feature selection step rise select feature feature selection procedure classifier low misclassification rate feature construction classifier use step feature selection method clearly outperform standard method number misclassification factor select gene acknowledgment thank anonymous reviewer hint improve paper work fund reference e m vapnik training algorithm optimal margin classifier th annual acm workshop computational learning theory page support vector network machine learn r d p m m r m e molecular classification cancer class discovery class prediction gene expression monitor science r obermayer pairwise proximity datum nip page t r scholkopf m obermayer r c williamson classification proximity datum page vapnik gene selection cancer classification use support vector machine mach learn obermayer classification pairwise proximity datum support vector learning workshop bengio pairwise datum cluster deterministic ieee analysis mach l p m m m c poggio r t r prediction central nervous system outcome base gene expression nature pairwise clustering equivalent classical kmean learning workshop bengio smola learning kernel support vector machine optimization mit press cambridge shawetaylor l bartlett c williamson m framework structural risk comp learn page shawetaylor l bartlett c williamson m structural risk minimization datadependent hierarchy ieee transaction information theory shawetaylor cristianini generalisation soft margin technical report computer science vapnik nature statistical learning theory springer m poggio feature selection svms nip page
gish blanz compare performance connectionist statistical classifier image segmentation problem gish w blanz abstract development image segmentation system real time image processing application apply classical decision analysis paradigm view image segmentation pixel classification task use supervise training derive classifier system set example particular pixel classification problem study test suitability connectionist method statistical method likelihood classifier second degree polynomial classifier solution real world image segmentation problem research classifier derive use method performance classifier training datum set separate entire test image measure introduction apply machine paradigm development image segmentation system use real time image processing application view image segmentation classical decision analysis task pixel scene describe set measurement use set measurement classifier choice determine region object scene pixel belong perform image segmentation decision analysis task provide advantage exploit inherent find decision compare performance connectionist statistical classifier analysis system use supervise training derive classifier set example particular pixel classification problem classifier derive use machine paradigm exhibit property generalization apply datum represent set problem similar example problem pixel classification scheme classifier derive solely characteristic problem datum approach eliminate dependency qualitative characteristic problem datum characteristic explicitly derive classification algorithm classical decision method employ statistical technique compare connectionist system set alternative statistical method classification problem classifier derive use supervise training find connectionist alternative comparable case preferable statistical alternative term performance problem vary complexity comparison study alternative method term cost implementation solution architecture digital lsi term cost analysis connectionist architecture simple implement statistical architecture complex classification problem property connectionist method attractive implementation choice system require hardware implementation difficult application study evaluate connectionist method classifier component real time image segmentation system classification problem use real world pixel classification task use image size pixel pixel variable data quality typical problem production system use solve test suitability connectionist method incorporation system performance requirement system feasibility exploit simple connectionist architecture provide system implement hardware method image segmentation system image segmentation system use describe summarize figure system design perform low level image segmentation real time production feature extraction classifier system component implement hardware classifier derive training phase user outline region object interest training image system perform low level feature extraction training image result feature extraction input user combine automatically system form training datum set system apply supervised training method use training datum set order derive coefficient classifier perform pixel classification task feature extraction process capable compute class feature pixel feature high power use gish blanz describe pixel image selection feature base analysis result extraction process independent supervise learn paradigm use derive classifier identical feature extraction process apply training running phase particular image segmentation problem coefficient classifier train phase segment image run phase training image test image figure diagram real time image segmentation system image segmentation problem image segmentation problem use study research describe image series image chamber high speed camera process segmentation task determine area gas image pixel image classify different class gas gas figure exact determination area gas possible use pixel classification great success pixel classification step great likelihood real time image segmentation system use successfully problem classifier set classifier use study compose connectionist base parallel distribute processing model describe statistical method maximum likelihood classifier baye polynomial classifier base second degree polynomial set classifier use general study compare performance compare performance connectionist statistical classifier figure problem classify pixel region alternative set classification problem classifier adaptation procedure describe study implementation adaptation classifier study perform software simulation connectionist classifier implement common connectionist classifier network hide layer network fully connect connection layer number unit input output layer determine number feature vector describe binary encoding scheme class pixel belong respectively number unit hidden layer free network use study unit input layer unit hidden layer unit layer network activation achieve use continuous nonlinear logistic function define connectionist adaptation procedure backpropagation learning rule define problem momentum term hold presentation pattern ill training datum set term trial network weight unit update presentation pattern trial training datum set problem generate automatically image segmentation system training datum set consist approximately element feature vector vector describe pixel vector label belong region interest training datum set construct entire training image compose vector representative pixel region interest image gish classifier test study adapt training datum set connectionist classifier define converge problem test network convergence determine result separate test iii test difference network output target output average entire training datum set reach minimum second test performance network classify training datum set measure number misclassification network reach minimum actual network performance classify pattern measure output vector real output unit output layer assign value application decision threshold binary encoding scheme output vector element value element correspond class h network produce output vector element value element value pattern generate output consider reject test problem study classifier set reject pattern test datum sample statistical classifier rejection threshold set result performance classifier connectionist likelihood linear quadratic cubic polynomial measure training datum set test datum represent entire image series chamber image image label image training datum set construct performance classifier summarize table classifier able classify training datum set comparably misclassification likelihood classifier quadratic polynomial classifier unable perform entire test image connectionist classifier alternative test study deliver acceptable performance test image connectionist classifier low error rate test image deliver training datum sample linear polynomial cubic polynomial classifier perform test image exhibit high error rate test image image segmentation problem connectionist method generalize training datum set solution acceptable performance figure result classification perform connectionist classifier test image image actual test image include left figure conclusion result demonstrate feasibility application connectionist decision analysis method solution world image segmentation problem compare performance connectionist statistical classifier ii reject reject t l rain datum image image image error reject b l pattern pattern reject training datum let table performance inclusion connectionist classifier supervised segmentation system allow meet performance requirement real world problem constraint application solution real time machine vision problem represent new processing method solution strategy remain consistent decision analysis paradigm connectionist derive solely quantitative characteristic problem data connectionist architecture remain simple need accord qualitative characteristic specific problem apply connectionist architecture independent image size apply identical architecture successfully image range size pixel pixel pixel pixel research date network apply machine vision entire image explicitly map network pixel image correspond different unit network layer example pixel map representation scale large image size idealize toy research image significant problem statistical pattern classification method require problem data satisfy assumption statistical model unfortunately real world problem datum complex variable quality rarely use guide choice appropriate method solution particular problem image segmentation problem report study performance result problem datum actually satisfy assumption statistical model underlie likelihood classifier polynomial gish blanz figure level assign region black light gas gas original image left figure classifier gaussian fit problem datum polynomial classifier provide slightly fi t connectionist method provide fit require solution problem notable alternative method study perform training datum set extensive testing different entire image require order demonstrate true performance actual problem datum set result connectionist method choice system require simple readily implement hardware flexibility handle problem describe large datum robustness require problem datum meet model compare performance connectionist statistical classifier reference o p e hit pattern scene e blanz lowlevel image segmentation theory editor advance machine application architecture springerverlag e blanz combine decision theoretic syntactic approach image segmentation machine application l gish w e compare method research e blanz b b implementation low level image architecture e feature selection multiple class process proc int pattern l et parallel mit press e rumelhart geoffrey e internal representation error propagation et ai editor parallel chapter mit press cambridge e blanz gish architecture apply image segmentation report model mechanism visual pattern recognition ieee man cybernetic smc model associative processor ieee man cybernetic smc
variational approach learn curve manfre neural compute engineering apply abstract combine replica approach statistical physics variational approach analyze learn curve analytically apply method gaussian process regression main result derive relation empirical error measure generalization error posterior variance introduction approximate expression generalization error finite dimensional statistical datum model obtain large datum limit use asymptotic expansion method yield approximate relation empirical true error use assess quality train model unfortunately approximation scheme easily applicable popular nonparametric model model support vector machine svms apply approach statistical physics average case learn performance machine far tool statistical physics successfully apply variety learn problem elegant method suffer drawback datum average perform exactly assumption data distribution limit infinite datum space dimension try overcome limitation combine replica method variational approximation bayesian model method allow express useful datum average expectation mean approximate measure derivation measure require assumption datum density assumption input dimension main focus article gaussian process regression demonstrate strength present method solve problem state end previous nip paper base simple somewhat truncation cumulant expansion process model method explicit approximation generalization error sample fluctuation furthermore compute correction theory demonstrate possibility derive approximate universal relation average empirical true error practical interest early version approach restrict assumption idealize datum distribution appear setup notation assume class elementary predictor neural network function bayesian formulation prior distribution class function assume set observation conditionally independent input assign likelihood term form posterior expectation denote functional express form partition function normalize posterior denote expectation respect prior interested compute average posterior expectation different training datum set datum example independently generate distribution section derive measure enable compute analytically approximate combine datum posterior average approach utilize statistical mechanic approach learn aim analysis serve generate compute socalle average free energy function suitable datum average posterior expectation partition function b use replica trick perform average compute integer continuation perform end obtain denote expectation prior measure eq transform simple form introduce canonical tion function m r q r evaluate predictor datum density point respect true data density canonical partition represent version original model fluctuate number example chemical potential km expect value yield simply sufficiently large replace sum eq term c neglect relative fluctuation recover original canonical free energy m r km r m n variational approximation interesting case partition function compute closed form different use variational approach approximate tractable hamiltonian easy write term inf expansion canonical free energy respect difference r bracket average respect effective measure induce r act space variable know prior term present upper bind lead differentiate bind respect usually preserve inequality sensible thing expect optimization respect variational equation ensemble choose integral local quantity input variable ie form s z specialize prior local quadratic expression fi l suitable trial hamiltonian lead function variational parameter optimize important explicit input variable order non uniform input density account perform variation term locality variational explicit function local moment straightforward variation yield z extend variational solution value optimal parameter notation aa use correspond guide success method physical application instance physics interpretation fi note approach equivalent variational approximation original posterior contain information statistic training datum r order compute approximate use distribution induce prior o combine datum posterior average consider expected local follow algebra posterior method approximate variational approach e second consider noisy local mean square prediction error posterior mean predictor case e calculate fluctuation respect datum average example t e e regression gaussian process statistical model assume datum generate noise t prior function mean covariance use definition eq yield set variational equation particularly easy input distribution regression model use translationally invariant homogeneous finite interval variational equation solve term eigenvalue study learn curve average generate process use gaussian process prior apply average theory adapt notation simply replace term learn curve fluctuation practical situation differ typical case analysis data generating process unknown assume fix result learn curve condition particular teacher left panel fig example display mean square prediction error circle solid line sample fluctuation error bar respect datum average cross line target random fixed realization p prior periodic radial basis function example simple process regression model use kernel noise input dimensional independent uniformly distribute represent simulation datum typical property theory line accurate sufficiently large number example datum theory line simulation symbol correction free energy generalization error fluctuation number m example datum number m example data figure gaussian process regression use periodic radial basis function kernel input homogeneous dimension density leave generalization error input fluctuation datum noise right correction free energy symbol subtract contribution true value energy obtain simulation line contribution decrease set equal value noise variance correction variational approximation strength method quality approximation characterize systematically improve paper restrict characterization consider case datum set equal posterior independent datum interesting model posterior variance estimate consider term expansion energy correction variational free energy evaluate d b d line right panel different value model noise consider input density input dimension model use periodic e z d p kernel symbol fig difference true value energy obtain simulation term eq correction term find qualitatively accurate emphasize discrepancy free energy term expansion eq medium example datum learning curve inherit behaviour universal relation relate training error empirical posterior variance z b z theory periodic periodic periodic theory periodic periodic periodic figure illustration left eq right error measure scale symbol simulation result radial basis function rbf regression homogeneous input distribution periodic additionally left figure example input lie twodimensional manifold embed cross case free energy use eq stationarity free energy respect variational parameter obtain follow relation yz use fact posterior variance independent datum simply estimate model datum set equal case eq yield relate empirical posterior variance local posterior test input similarly derive expression training error use eq combination interesting note relation contain assumption datum generating process hold general model likelihood illustration eq fig example process regression radial basis function kernel left panel fig learn start upper right corner rescaled empirical posterior variance initially decrease increase number example datum right panel learn start low left corner rescaled training error noisy datum set initially increase increase number example datum theory line hold sufficiently large number example datum accuracy increase input dimension eq test real datum common benchmark set datum find eq hold small medium size training datum set question approximate universal relation practical use example relation training error generalization error involve know posterior variance relation useful case large number datum input output label available regression independent output label use extra input point estimate application technique complicated model possible technically eq far involved replace o e process prior model hard condition margin support vector machine classification maximum margin classification ensure particular interest computation empirical estimator use practice model selection calculation fluctuation error bar estimator prominent example efficient approximate leaveoneout estimator svms work issue progress acknowledgement like thank inspire discussion work support grant reference amari ieee transaction neural network p statistical mechanic learn press neural information processing system p t tresp ed m opper lecture note computer science p ed m m spin glass theory world scientific r p r quantum mechanic path integral p p neural information processing system p m kearn s solla cohn ed p neural information processing system t s
relationship spike datum electrical computer engineering department goal neuroscience identify neural network correlate important behavior environment work propose strategy identify neural network characterize time connectivity pattern use convolutional dictionary learning link spiketrain datum local field potential lfps multiple area brain analytical contribution model dynamic relationship spike describe relationship spike lfps analyze ability predict datum region base spike information iii development clustering methodology allow similarity neuron multiple region result base datum set spike datum record simultaneously brain region mouse introduction fundamental challenge neuroscience largescale integration problem distribute neural activity lead precise unified cognitive moment paper seek examine challenge perspective extracellular electrode insert brain extracellular electrode insert brain pick type signal local field potential lfp represent local oscillation frequency hz single action potential know spike typically occur frequency khz lfps represent network activity sum long distance action potential represent precise activity cell tip electrode action potential treat information transfer brain relationship behavior activity equally precise precise activity individual neuron lfp network highly form disease lead interest understand mechanism lfps potential interact create specific type behavior new recording technique allow simultaneous recording large number brain region provide opportunity study interaction type multidimensional datum pose significant challenge require new analysis technique challenging characteristic recording network represent dynamic space time neuron local area different function relate lfp oscillation specific way different frequency lfp oscillation relate single neuron specific way new model propose examine relationship neuron neural network accommodate characteristic lfp brain region model convolution element observe spike train critically convolutional factor allow dynamic bin lfp spike time model dictionary element bin time series clustering model propose neuron dictionary element scale version autoregressive template share neuron cluster allow identify neuron similar dynamic functional connectivity brain region finally provide strategy explore frequency band characterize functional connectivity use novel dataset mouse model use identify interaction different neuronal define jointly activity single cell lfps method lead understanding relationship brain activity behavior underlie brain disease model datum notation datum use consist multiple lfp spiketrain time series measure simultaneously m region mouse brain spike sort perform spike datum vb implementation j single unit assume detect multiple region henceforth refer single unit neuron number observe neuron depend datum consider infer discuss multiple insert single brain region experiment describe typically detect neuron m region insert discuss far present result analysis objective examine degree relate predict datum brain region use spike datum brain region analysis allow identification neural network degree neuron region predictive lfps let represent time series lfp datum measure particular brain region sample record regular grid temporal interval spike train j different neuron sort represent set vector bin manner temporally lfp datum zt number fire time bin represent nonnegative integer propose model lfp datum represent superposition signal associate residual capture signal spike datum contribution information assume generate convolution element reside interval l l t model relate convolutional learn observe spike sort represent signal convolve learn dictionary dj model dj time evolve motivate expectation neuron contribute differently specified lfp datum base latent state brain relate observe animal activity time bin set contiguous window likewise dictionary element similarly bin djb contribution represent convolution djb bin size tradeoff time discretize computational cost experiment example bin choose second wide datum minute datum choose convenience second data set time long similar result find window narrow second wide minute model lfp contribution multiple neuron jointly lfp voltage time window b represent djb represent convolution operator let dj dj djb represent sequence dictionary element use represent lfp datum window perspective neuron impose clustering prior dp draw dirichlet process scale parameter probability measure note cluster shape dictionary element scaling r concern base measure impose autoregressive prior temporal dynamic define ar process t t identity matrix prior use constitute b column dp atom k element vector draw stickbreake process h vi beta place prior gammaa b prior uniform gammaa b respectively complete model place prior b j implementation truncate stickbreaking representation employ use stick simplify implementation effective practice k large size infer special case model clear example simply draw neuron allow contribute unique dictionary shape represent nonclustering model result author consider similar model time evolution dj consider neuron assume contribute way represent lfp independent time far single neuron consider clustering consider version model infer set inference let likelihood clustering model b j b p nonclustering model recover set truncation level stickbreaking process timeinvariant model recover set number cluster model recover use single bin single recent method propose provide quick approximation dirichlet process mixture model critically model latent assignment variable conditionally independent dp parameter propose model assumption hold observation superposition draw dirichlet process factorize variational distribution q propose approximate posterior distribution nonclustering model arise special case clustering model inference fit distribution q base bayesian hierarchical cluster vb dirichlet process splitmerge method propose model fit framework method learn merge cluster adapt present section factorize distribution q form standard form distribution assume q gammaa b q vec gammaa b facilitate inference distribution split variational distribution spike train nonclustering model represent special case clustering model j note factorize posterior property q cluster nest representation cluster number cluster represent variational algorithm find q minimize divergence true intractable posterior find q locally maximize evidence low bind objective log px lq eq log px z log facilitate inference approximation develop let number time point define entry time point t let rjk residual contribution neuron remove define let rl entry l efficiently estimate time write log djb djb djb djb p p define key update let rjk denote yk b block indexing b b bin efficiently calculate block matrix firstorder autoregressive process explicit equation exist let p rjk update pk p parameter update cluster latent variable update sequentially t b b b use calculate mean distribution evaluate use forward smooth block matrix enable efficient computation detail update find section supplemental material approximate distribution q q q standard merge step model initialize cluster find q nonclustere model initialization important superposition measurement model proceed merge local mode vb procedure follow randomly choose cluster merge propose new variational distribution q cluster calculate change variational lower bind accept merge variational lower bind increase intelligent sampling significantly improve performance sample c radial basis function pairwise clustering consider computationally infeasible problem approach merge cluster similar develop algorithm require efficient estimation difference low bind propose new variational distribution q propose k b calculate let log rjk difference lower bind calculate q p lq eq log log explicit detail calculation variable find section supplementary material block nature allow complete calculation value l linear datum use model stop merge row reject integrate nest laplacian approximation nonclustering model vb inference method assume separable posterior nonclustering model integrate nest laplacian approximation use estimate joint posterior animal invariant cluster animal invariant cluster table mean heldout rfe model predict denote timeinvariant model clustering denote dynamic model clustering min min min holdout rfe amplitude au joint model prediction dictionary element cell single neuron time second experiment time minute figure leave mean holdout rfe predict compare dynamic timeinvariant model point single neuron dictionary cell predict minute minute minute experiment start right holdout rfe experiment time timeinvariant nonclustere clustering model predict assume comparison constitute independent validation vb inference nonclustere version model inference procedure detail supplemental section inference find significantly slow vb approximation experimental result vb vb predictive performance quantitatively similar nonclustere model provide confidence vb result experiment result mouse introduce novel environment data set group mouse consist male mouse number male control mouse number far describe animal total wire thalamus ventral area average electrode area filter hz sample neuronal activity record use acquisition processor individual spike train single unit detect animal dataset animal begin minute place novel environment minute analysis minute data sequence bin second chunk bin experiment choose l dictionary element cover second spike crossvalidation perform use leaveoneout analysis time bin use error metric reduction fractional error figure leave average holdout rfe timeinvariant model dynamic model single spike train predict dynamic model strong improvement scale single cell result typical dynamic model high holdout rfe detect cell animal region indicate dynamic model generally outperform timeinvariant model dynamic dictionary element cell predict figure middle begin experiment cell link slow oscillation animal initially place new environment illustrate minute data point amplitude dictionary element drop close animal new environment illustrate minute data point cell original periodic dictionary element begin appear example cell lfps clearly timeevolve relationship leaveoneout performance timeinvariant nonclustere clustering model predict animal neuron figure right result thalamus experiment time minute cluster cell location number cell dictionary second cluster factor evolution cluster factor evolution thalamus number cell dictionary second experiment time minute cluster cell location number cell dictionary second cluster factor evolution experiment time minute cluster cell location thalamus figure example cluster predict convolutional factor duration experiment location cell cluster dynamic consistent duration cell predict cluster contribution raw residual frequency energy frequency rfe cell predict thalamus time experimental time time min figure leave rfe function time frequency bin cell predict lfp change predictive property minute middle total energy unexplained residual cell predict thalamus lfp frequency band right rfe use cluster cell figure right change time indicate strong increase lfp mouse place novel environment use dynamic improve result dramatically cluster holdout result improvement holdout performance mean holdout rfe result animal table animal miss region recording result region supplemental table similar result dataset little quantitative difference clustering nonclustering model clustering result interpretation reason procedure cluster model equivalent performance evidence neuron shape dynamic repeat dynamic pattern reduce concern dynamic result failure distinguish distinct neuron similarly clustering neuron shape single electrode result algorithm cluster electrode strong evidence truly different neuron cluster additionally neural action potential shape drift time cell cluster come different electrode region strong evidence dynamic drift neuron cluster dynamic shape result neural distribution region example clustering shape histogram cell location cluster predict figure figure base dictionary element evolution duration experiment note left middle plot dynamic effect minute cell primarily come ventral area right plot fairly stable factor cell ability predict lfp constitute functional connectivity neuron circuit electrode lfp neural circuit transfer information specific frequency oscillation scientific interest know functional connectivity group neuron function frequency frequency relationship explore filter lfp signal predict signal remove use filter interval hz bandwidth rfe calculate heldout time frequency cell thalamus use predict frequency band result figure leave figure increase rfe band animal new location rfe band region nonclustere cluster nonclustere cluster dms m table mean heldout rfe animal sleep cycle region mean factor cell v mean factor cell time second time second amplitude au amplitude amplitude mean factor cell time second figure predictive pattern individual neuron predict multiple region leave cell good single cell predictor v lfp middle v cell relationship right nucleus cell equivalent predictive ability good v cell figure middle raw energy frequency band high novel environment cell explain additional energy band figure result use cluster figure note change minute slight change convolutional change neural fire pattern result sleep datum set second data set record mouse different sleep cycle hour different region brain use medial frontal cortex core area motor m frontal cortex average electrode area filter sample hz l set second total neuron single unit detect use spike sort datum split minute time bin leaveoneout predictive performance high dynamic single cell model neuron predict mean holdout rfe record region brain table model cluster model good perform model region previously publish work look signal individual neuron experiment find dictionary element v cell electrode cell dataset timeinvariant match timeinvariant dictionary shape dictionary element single v cell predict multiple region figure middle simplicity subset brain region record suggest v cell connection v region brain region record model cell brain region functional connectivity v good individual predictor cell figure leave additional example cell cell rfe good v cell shape figure right sleep state typically define dynamic change functional connectivity brain region measure eeg lfps record scalp little know single neuron contribute interact network change sleep covariate second datum score awake sleep state use method sleep state average time define time bin sleep state individual element p m d c cell h p m dictionary element s m t awake sleep cell time o number cell awake sleep amplitude amplitude amplitude au cluster cluster cluster predict region figure leave cluster predict region brain match know pattern individual cell cluster predict motor cortex positive negative relationship amplitude sleep frequency frequency awake sleep frequency awake sleep mean rfe cluster frequency mean mean sleep frequency figure mean animal awake leave cluster convolution factor stable minor difference sleep awake prediction middle right cluster figure leave right depict vary pattern mouse sleep state ond score sleep state animal awake individual second score sleep state figure middle cluster strongly positively correlate sleep figure cluster correlate sleep figure neuron location mean waveform shape sleep case cluster cell cluster come different region concern maximally correlated cluster result address concern pvalue find cluster strongly correlated pvalue correction multiple test furthermore cluster detect correlation amplitude sleep state isolated rfe change function frequency sleep state cluster neuron use bandwidth frequency bin figure middle mean rfe use cluster figure middle cluster associate positively sleep shift frequency peak increase ability predict animal sleep likewise cluster perform predict animal comparison figure leave include frequency result cluster stable dictionary element rfe comparable dramatic shift peak frequency sleep awake state conclusion novel model method develop account timevarye relationship neuron lfps context experiment significantly improve predictive performance realize account temporal dynamic far cluster model reveal neuron similar relationship specific brain region frequency predictable lfp change know dynamic animal state future work idea incorporate attempt learn network structure lfps consider common input explore network neuron future experiment design place additional electrode single brain region goal detect neuron single brain region record lfps region method propose facilitate exploration diversity neuron difference functional connectivity individual neuron scale research report fund aro darpa thank reviewer helpful comment reference rodriguez phase synchronization largescale integration nature rev b m pp temporal structure neuronal activity work memory macaque parietal cortex nature neuroscience c e inference movement local field potential motor cortex nature neuroscience singer neural oscillation rev m analysis dynamic brain oscillation advance trend neuroscience vogelstein m spike sort joint dictionary learn mixture modeling ieee genetic mouse model l hierarchical beta process convolutional factor analysis deep learning icml gibb sample method stickbreake ferguson bayesian analysis nonparametric problem annal m neuron circuit linear estimation local field potential mc online variational inference dirichlet process mixture model nip bayesian hierarchical clustering icml dm blei dirichlet process mixture bayesian analysis sj variational baye generalized autoregressive model process approximate bayesian latent gaussian model use integrate nest laplace approximation royal stat soc l kalman filter mixture model spike sorting nonstationary data method m stimulus contrast modulate functional connectivity visual cortex nature local field potential indicate network state account neuronal network functional connectivity descent sleep lm control state p characterization partially observe population spike neuron nip nk infer spike train local field potential neurophysiology p dm efficient online inference model nip
avlsi cricket ear abstract female cricket locate male song produce behaviour underlying physiology study depth cricket auditory system solve complex problem unique manner present analogue large scale integrate avlsi circuit model process result test circuit agree simulation know behaviour physiology cricket auditory system avlsi circuitry extend use robot previously model neural circuitry understand complete sensorimotor pathway understand insect carry complex sensorimotor task help design simple sensory robotic system insect sensor evolve filter match extract highly specific datum environment solve particular problem directly little need process example include head fly use vision sense estimate head cricket cricket body time difference d sound arrive head small s membrane locate cricket reach low detect directly timing neural spike wavelength cricket song significantly great width cricket body intensity difference low absence information cricket use phase determine direction possible male cricket produce pure song school electrical information perception action behaviour figure cricket auditory system input channel sound directly membrane sound input pass double central membrane medial induce phase delay reduction gain sound transmission weak effectively input system physics cricket auditory system understand system figure use pair sound receiver acoustic input external surface body acoustic connect interference occur sound travel cricket produce directional response frequency song amplitude firing rate auditory afferent neuron vary sound source cricket sound different input phase output match sound straight ahead input symmetric respect sound source sound song frequency phase signal close come alignment signal increase conversely decrease crossover amplitude allow cricket track sound source figure simplified version auditory system use acoustic input implement hardware simple neuron network require direct robot carry simple simulator create model behaviour auditory system figure different frequency datum figure use average typical value paper choose gain delay simulation figure model internal auditory system cricket sound arrive input transmission auditory receptor simulator implement model delay input model external sound transmission result simulator use check system different frequency gain understanding response impractical check effect movement complex sound simulator necessity simulate sound production avlsi chip design implement model allow complex experiment movement run experiment run real world figure model auditory system cricket use build simulator avlsi implementation box experiment simulator circuit publish reader refer paper detail present paper present detail circuit use avlsi implementation circuit chip implement avlsi box figure comprise allpass delay filter gain circuit secondorder bandpass filter firstorder bandpass filter firstorder filter support circuitry include reference voltage current single avlsi chip include necessary circuitry model complete auditory system cricket complete model auditory system obtain use appropriately connect chip allpass delay filter need implement instead figure relative delay pathway arrive sum node count delay circuit implement filter order extend frequency range delay firstorder allpass delay circuit cascade secondorder allpass delay circuit result addition firstorder delay secondorder delay allow approximately flat delay response wide bandwidth decrease delay corner frequency firstorder filter cancel delay secondorder filter frequency figure secondorder section allpass delay circuit circuit use base datum present design delay s way bias current manipulation figure standard include feedback necessary fully differential design figure simple differential pair figure firstorder allpass delay circuit leave secondorder allpass delay right differential output delay circuit convert current multiply variable gain implement figure gain cell include differential pair source transistor source improve linearity current gain cell implement avlsi default gain set hold default input high appropriately bias current value correct andor explore gain configuration current cell figure allow gain program digital mean post current input current figure divide branch recursively current branch second branch branch current use control switch hold default low set appropriately gain set save output program bit cc gain cell set single bit shift fashion sum output gain circuit current domain simply involve connect wire natural option filter follow use current domain filter case choose implement filter use transistor operate weak inversion figure basic building block filter cell multipli cell block diagram block connect create necessary filtering block cell filter firstorder n slope factor voltage bias current figure input current cell use build secondorder filter multipli cell simply loop ai configuration cell particular response cover correspond equation high frequency filter figure implement filter figure corner frequency khz low frequency filter divide biological filter response example figure separate narrow secondorder bandpass filter khz frequency wide bandpass filter firstorder filter khz corner frequency follow firstorder lowpass filter khz corner frequency filter add reproduce biological filter filter response adjust post bias current allow processing match error figure gain cell use convert differential voltage input delay cell current output gain cell current cell bias generator use create necessary current bias chip main block delay gain cell filter bias current external chip chip fabricate use m technology design use ic design tool method chip test use sound generate computer play chip response chip record computer completion output chip gain circuit current external circuit build discrete component use enable output probe figure circuit diagram filter building block cell multipli block diagram filter use avlsi initial experiment perform tune delay gain recording directional frequency response sound generate computer chip input simulate delay sound appropriate time simple solution use microphone use result avlsi chip test measure gain delay successfully tune appropriate value chip compare simulation check faithfully model system result test approximately cricket frequency figure apart drop amplitude signal response circuit similar simulator difference expect deal noise simulate version perfect signal example gain frequency response bandpass filter figure note filter peak khz significantly song frequency cricket khz mistake observe real cricket state introduction range testing result circuit simulator publish d s s avlsi auditory sensor research model hearing field model cricket auditory system previously build reproduce acoustic input response frequency co specific song generate output correspond set relevant auditory receptor result match biological datum inconsistency error specification address future iteration design complete implementation frequency impractical complexity size issue serve clear purpose figure amplitude left dotted right solid measure response khz simulation leave avlsi chip plot amplitude response sound source rotate cricket figure curve bandpass filter aim work understand simple sensorimotor loop cricket insect step circuitry robot carry experiment compare exist new datum allow refine model neural circuitry involve model sensory afferent neuron hardware necessary order reduce processor load robot include chip auditory system single chip conserve space robot belief experience result intelligent preprocessing carry sensor level neural circuit necessary accurately model behaviour remain simple acknowledgment author thank neuromorphic fund research paper reference r match filter neural model external world v physics directional hearing cricket comparative physiology tune cricket news h b robot attract p husband editor proceeding artificial life page mit book r new neural circuit tran soc r directional hearing cricket t bias current generator wide dynamic range analog integrate circuit signal processing jin cell new method implementation arbitrary differential equation ieee symposium circuit system sound frequency cricket auditory receptor neuroscience complex auditory behaviour emerge simple nature r b r new technology test model cricket outdoor robot platform robotic autonomous system
distribute powerlaw graph compute theoretical empirical analysis le dept comp dept comp lab novel software tech dept comp dept comp abstract emergence big graph variety real application social network machine learning base distribute dgc framework attract attention big data machine learn community dgc framework graph partition gp strategy play key role affect performance include workload balance communication cost typically degree distribution natural graph real application follow skewed power law gp challenging task recently method propose solve gp problem exist gp method achieve satisfactory performance application paper propose novel vertexcut method degreebase hashing dbh dbh effective use skewed degree distribution theoretically prove dbh achieve low communication cost exist method simultaneously guarantee good workload balance furthermore empirical result large powerlaw graph dbh outperform state art introduction recent year emergence big graph large variety real application web social network service furthermore machine learning datum mining algorithm model graph machine learning base distribute dgc framework attract attention big data machine learn community perform distribute parallel cluster machine server partition graph machine graph partition gp dramatically affect performance dgc framework term workload balance communication cost gp strategy typically play key role dgc framework ideal gp method minimize communication cost simultaneously workload machine approximately balanced exist gp method divide main category edgecut vertexcut method edgecut try evenly assign vertex machine cut edge contrast vertexcut try evenly assign edge machine cut vertex figure illustrate edgecut vertexcut partition result example graph figure edge ac cut machine store vertex set respectively figure vertex cut machine store edge set ad respectively edgecut machine cut edge maintain local replica vertex edge datum vertexcut machine associate cut vertex maintain mirror local replica vertex mirror shaded vertex figure edgecut workload machine determine number vertex locate machine communication cost graph determine number edge span different machine vertexcut workload machine determine number edge locate machine communication cost graph determine number mirror vertex edgecut b vertexcut figure strategy graph partition shaded vertex mirror respectively traditional dgc framework graphlab use edgecut method gp recently author powergraph find vertexcut method achieve performance edgecut method especially powerlaw vertexcut attract attention dgc research community example adopt random vertexcut method greedy variant provide heuristic constrain solution improve random vertexcut method large natural graph usually follow skewed degree distribution powerlaw distribution challenge different vertexcut method result different performance powerlaw example figure toy powerlaw graph vertex high degree figure b partitioning strategy cut vertex e c d figure c partition strategy cut vertex e find partition strategy figure c figure b number mirror figure c small mean communication cost intuition underlie example cut vertex result mirror vertex powerlaw degree distribution use facilitate unfortunately exist vertexcut method include powergraph rarely use degree distribution gp achieve satisfactory performance natural try combine edgecut vertexcut use powerlaw degree distribution lack theoretical guarantee c sample bad partition good partitioning figure partition sample graph vertexcut paper propose novel vertexcut gp method degreebase hashing dbh distribute powerlaw graph compute main contribution dbh briefly outline follow dbh effectively exploit powerlaw degree distribution natural graph theoretical bound communication cost workload balance dbh derive dbh achieve low communication cost exist method simultaneously guarantee good workload balance dbh implement execution engine powergraph powergraph application support dbh empirical result large real graph synthetic graph dbh outperform stateoftheart method problem formulation let e denote graph set vertex e set edge let v denote cardinality set vi neighbor e degree vi denote measure number neighbor note need consider gp task undirected workload mainly depend number edge matter direct undirected computation base computation base direct graph use undirected counterpart direct graph partition result assume cluster p machine vertexcut assign edge corresponding vertex p machine cluster assignment edge unique vertex replica different machine dgc framework base vertexcut workload computation machine roughly linear number edge locate machine replica vertex incur communication synchronization goal vertexcut minimize number replica simultaneously balance number edge machine let m e p machine edge e e assign p span vertex different machine number replica v different machine similar powergraph replica vertex choose treat mirror master let m denote machine master v locate goal vertexcut formulate follow e m e m m e max m m p p m denote machine factor replication factor e max e e m e m edgeimbalance p m max m m good balance m small possible degree natural graph usually follow skewed powerlaw distribution d probability vertex degree d power parameter positive constant low skewed graph powerlaw degree distribution gp challenge vertexcut method achieve performance edgecut method powerlaw exist vertexcut method random method powergraph method effective use powerlaw distribution achieve satisfactory performance degreebase hashing section propose novel vertexcut method degreebase hashing dbh effectively exploit powerlaw distribution gp hash model refer certain machine index machine denote define kind hash function function vertex hash vertex machine edgehash function edge edge hashvi hash edge e machine hashing model include main component assignment master replica vi uniquely assign machine equal probability machine randomized hash function vertex hashvi edge assignment edge e assign p machine hash function edge hashvi vj easy find hash model vertexcut gp method assignment easily implement expect achieve low score contrary edge assignment complicated different edgehash function achieve different replication factor different edgeimbalance score note replication factor reflect communication cost edgeimbalance reflect key hashing model lie edgehash function edge hashvi vj degreebase hash example figure observe powerlaw graph replication factor define total number replica divide total number vertex small cut vertex relatively high degree base intuition define edge hashvi follow vertex hashvi dj edge hashvi vertex mean use function define edgehash function furthermore edgehash function value edge determine degree associate vertex specifically edgehash function value edge define function value associated vertex small degree method degreebase hash dbh dbh effectively capture intuition cut vertex high degree performance dbh method vertexcut briefly summarize degreebase hash dbh input set edge e set vertex number machine p output assignment m p edge initialization count degree di parallel e e hash edge parallel dj m e vertex hashvi m vertex end end theoretical analysis section present theoretical analysis dbh method comparison random vertexcut method random powergraph constrain solution grid adopt baseline analysis base randomization assume graph undirected duplicate edge graph mainly study performance term replication factor edgeimbalance define section space limitation proof theoretical result supplementary material partition graph firstly assume degree sequence di ni fix follow expect replication factor produce different method random assign edge evenly p machine randomized hash function result directly assume sequence vertex ni corresponding degree sequence ni simple randomized vertexcut p machine expect replication factor x px e p use grid hash function vertex p p candidate machine compare random simply replace p follow corollary corollary use grid hash expect replication factor p machine p e p use dbh method section obtain following result fix sequence define number adjacent edge hash neighbor accord edgehash function define assume sequence vertex ni corresponding degree sequence ni adjacent edge hash define h dbh method p machine expect replication factor px px d e p dbh method small expect replication factor random turn analysis balance constraint fix degree sequence follow result dbh method theorem dbh method p machine sequence ni define edgeimbalance p max e e m e m m master vertex evenly assign machine want randomized assignment close perfect balance problem study model uniformly ball p bin p lemma maximum number master vertex machine bound follow r m max m m p m partition powerlaw graph change sequence fix degree sequence random sample generate powerlaw distribution result upperbound provide method random grid dbh let minimal degree dmin ni sample degree distribution parameter expect replication factor random machine approximately bound px p p p dmin degree sequence powerlaw distribution upper bind expect replication factor increase decrease imply random yield bad partition powerlaw graph skewed corollary replace p similar result corollary use grid method expect replication factor p machine approximately bound px p p dmin note p p p p corollary tell grid reduce replication factor motivate skewness degree distribution assume edge hash dbh method expect replication factor dbh p machine approximately bound px p dmin note p p p p dbh method reduce replication factor term increase decrease mean dbh reduce replication factor powerlaw skewed note grid dbh method actually use different way reduce replication factor grid reduce replication factor p grow approach combine obtain improvement focus paper finally dbh guarantee good workload balance powerlaw distribution theorem assume edge hash dbh method dmin ni ni define vertex evenly assign constant ep ne p exist expect edgeimbalance p machine bound high probability p vi note satisfy work theorem result tight bind large theorem dbh method reduce replication factor simultaneously guarantee good workload balance empirical evaluation section empirical evaluation real synthetic graph use verify effectiveness dbh method cluster experiment contain machine connect machine core gb ram dataset graph dataset use experiment include synthetic synthetic powerlaw graph generate combination synthetic direct graph direct graph sample powerlaw degree distribution different power parameter respectively collection synthetic graph separate subset subset parameter table subset parameter table b graph table c experiment additional realworld graph table dataset synthetic synthetic e graph m m m m m e m m m baseline evaluation metric experiment adopt random powergraph grid baseline empirical comparison method hybrid adopt comparison combine edgecut vertexcut pure vertexcut method important metric replication factor reflect communication cost test speedup real application use total execution time pagerank force iteration speedup define dbh alg execution time pagerank method random grid method achieve good workload balance experiment report graphlab release use powergraph engine grid gp method adopt graphlab replace original random gp method detailed information find result figure replication factor subset synthetic graph find dbh method achieve low replication factor random grid replication factor dbh reduce compare random compare grid random grid dbh grid dbh replication factor replication factor s s s s s s s s s s s replication factor replication factor figure experiment subset synthetic graph denote different dataset table table b number machine grid figure replication factor find achieve good performance figure b relative speedup dbh random grid pagerank computation grid replication factor replication factor execution speedup figure experiment graph number machine figure replication factor execution time pagerank graph number machine range find dbh achieve good performance case grid dbh grid dbh execution time replication factor number machine replication factor number machine execution time figure experiment graph number machine range conclusion paper propose new vertexcut graph partition method degreebase hash dbh distribute framework dbh effectively exploit powerlaw degree distribution natural graph achieve promising performance theoretical empirical result dbh outperform stateoftheart method future work apply dbh big data machine learn task acknowledgement work support program aa fundamental research fund central university reference law internet framework compression technique proceeding th international conference world wide web wiener graph structure web computer network computation partition skewed parallel distribute system transaction mathematical software distribute computation natural graph proceeding th symposium operating system design implementation processing distribute dataflow framework proceeding th symposium operating system design implementation l scalable graph framework proceeding international workshop graph datum management experience system graph partition scheme proceeding international conference parallel processing park social network news medium proceeding th international conference world wide web computation pc proceeding th symposium operating system design implementation large network dataset collection url m graphlab new framework parallel machine learning proceeding conference uncertainty artificial intelligence m distribute graphlab framework machine learning proceeding international conference large datum basis system graph processing proceeding conference management p measurement analysis online social network proceeding th acm conference internet measurement ball simple tight analysis randomization approximation technique computer science page springer streaming graph partition large distribute graph proceeding th acm international conference knowledge discovery datum mining streaming graph partition massive scale graph proceeding web search datum mining partition graph proceeding international conference datum
simulation thalamocortical circuit compute directional head rat abstract region rat brain contain neuron know headdirection celis encode animal directional head spatial navigation paper present model headdirection cell suggest thalamocortical circuit compute rat head direction integrate angular velocity head time model implement use testable prediction structure function rat headdirection circuit headdirection cell rat navigate space neuron headdirection celis encode animal directional head horizontal plane ranck muller ranck headdirection cell record brain area include postsubiculum ranck anterior thalamus variety theory propose headdirection cell play important role spatial learning navigation brown sharp touretzky basic firing property headdirection cell fire action potential rat head face particular direction respect static surround environment regardless animal location environment headdirection cell influence position rat head respect body influence direction neuroscience science park simulation thalamocortical circuit compute directional head rat o head direction figure directional tuning curve headdirection cell head respect stationary reference frame spatial environment headdirection cell directional preference entire population cell encode direction animal face figure example headdirection cell directional tuning curve plot firing rate function rat head direction tuning curve cell fire rat head face preferred direction degree cell fire rapidly direction close degree stop fire altogether direction far degree velocity integration propose headdirection cell rely process calculate rat current head direction base previous head direction angular velocity head turn headdirection cell compute directional position head integrate angular velocity head time velocity integration hypothesis support experimental finding brain region associate headdirection cell contain angular velocity cell neuron fire proportion angular head velocity sharp press second headdirection cell postsubiculum modulate angular head velocity peak fire rate high head turn direction recently find headdirection cell anterior thalamus postsubiculum anticipate future direction rat head sharp anticipatory headdirection cell sharp discover headdirection cell anterior thalamus shift directional preference left clockwise turn right turn shift occur systematically function velocity way cell anticipate future direction rat head illustrate consider cell fire head face specific direction near future cell behave case consider imagine rat head turn clockwise approach direction left case anticipatory cell fire head face left left turn clockwise predict arrival future second head turn approach right anticipatory cell fire head right head cell fire head face summary anticipatory head direction cell shift directional preference leave clockwise turn right turn head behavior formalize denote cell prefer present head direction denote angular velocity head denote future head direction cell anticipate t constant time delay cell activity anticipate arrival equation assume measure degree increase clockwise direction v positive clockwise head turn negative head turn sharp demonstrate equation provide good approximation headdirection cell behavior anterior thalamus anticipatory time delay initial report suggest headdirection cell anterior thalamus anticipate future head direction average time delay cell encode present head direction anticipate sharp muller recent evidence suggest individual neuron anterior thalamus temporally tune anticipate rat future headdirection different time delay msec cell lag present headdirection sharp model section describe model account property headdirection cell postsubiculum anterior thalamus propose connect form thalamocortical circuit section present simulation result implementation model use neural element figure illustrate basic circuit compute rat headdirection circuit consist type cell present headdirection phd cell encode present direction rat head anticipatory headdirection ahd cell encode future direction rat head av cell encode angular velocity rat head av cell active clockwise turn av cell active turn angular speed cell fire inverse proportion angular speed head regardless turn direction cell fire low rate fast turn high rate slow turn modulate headdirection avhd cell headdirection cell fire excitatory mb cell anterior inhibitory figure model rat headdirection system simulation thalamocortical circuit compute directional head rat head turn direction avhd cell fire preferred direction head turn clockwise avhd cell fire preferred direction head turn functional characteristic model ahd cell directly neighbor indirectly inhibit neighbor avhd cell act inhibitory ahd cell send excitatory feedback connection omit figure clarity active remain active turn inhibitory input rate firing modulate inhibitory input rat turn head cell represent current head direction fire inhibit neighbor condition rat turn head lateral inhibition exceed lateral excitation activity spread direction layer ahd cell rat begin turn head avhd cell turn allow activity spread direction example clockwise head av cell active inhibit layer avhd cell result cell stop inhibit right neighbor activity spread right layer ahd cell ahd cell continue inhibit neighbor left activity direction activity spread right speed propagation ahd layer govern cell slow head turn cell fire high rate strongly inhibit ahd cell slow speed propagation fast head turn cell fire low rate weakly inhibit ahd cell allow activity propagate quickly inhibition cell ahd cell fire fast head turn figure agreement experimental datum sharp ahd cell send topographic projection cell phd cell receive excitatory input ahd cell anticipate head soon face cell prefer direction ahd cell activity anticipate phd cell activity transmission delay cell assume msec simulation present weight connection ahd cell cell small ahd cell fire action potential phd cell begin fire time delay cell account anticipatory firing correspond parameter equation anatomical characteristic component model assume reside specific brain region ahd cell assume reside anterior thalamus postsubiculum respectively cell observe sharp press model predict find body mb receive input rs mb project avhd cell observe ai model predict find receive input inhibit note lateral excitation cell feature model incorrect table summarize anatomical evidence simulation result model illustrate figure implement use neural element represent single spherical table anatomical feature reference feature model ranck sharp ai sharp prediction model prediction model phd cell ahd cell cell project ps project project avhd cell cell diameter time constant range msec synaptic connection simulate use trigger conductance result present demonstrate behavior model compare property model experimental datum begin simulation small current inject ahd cell cause initiate sustained fire cell represent simulated rat initial head direction behavior simulate inject current av cell amplitude yield fire proportional desire angular head velocity activity headdirection cell figure present simple simulation illustrate behavior headdirection cell model simulate rat begin face direction degree course msec rat quickly turn head degree right return initial starting position degree average velocity head simulation similar speed actual rat perform fast head sharp course simulation activation propagate cell degree cell degree cell comparison experimental datum examine model reproduce fire property cell simple simulation perform firing rate model ahd cell examine simulate rat perform degree clockwise direction result summarize figure activity phd cell turn right angular velocity time figure simulation example simulation thalamocortical circuit compute directional head rat o angular head velocity o angular head velocity figure compare property real simulated headdirection cell compare simulation datum experimental datum experimental datum figure average result cell record cell record cell anticipate future head direction exhibit angular separation clockwise preference separation occur cell section cell magnitude angular separation proportional angular head velocity great separation occur fast turn separation slow turn eq left panel figure model ahd cell exhibit similar pattern angular separation sharp report firing rate cell differ way cell fire high rate cell cell high rate fast turn slow turn fire rate regardless turn speed figure right panel model reproduce finding discussion conclusion paper present neural model rat headdirection system model include neural element firing property similar actual neuron rat brain model suggest thalamocortical circuit compute directional position rat head integrate angular head velocity time comparison model propose neuron encode headdirection angular velocity connect form linear associative mapping network refine idea theoretical circuit incorporate headdirection angular velocity cell incorporate anticipatory headdirection cell find model incorporate anticipatory cell develop touretzky manuscript recently present theoretical analysis circuit suggest anticipatory headdirection cell influence angular velocity angular acceleration head cell influence angular velocity angular acceleration limitation model current form model suffer significant limitation example directional tuning curve model headdirection cell narrow actual headdirection cell present form model accurately track rat headdirection limited range angular head limitation address advanced version model t acknowledgment work support fellowship number fellowship thank reference sharp anticipatory headdirection cell anterior evidence thalamocortical circuit angular head velocity compute head direction journal neuroscience sharp temporal tuning anticipatory headdirection cell anterior thalamus rat submit brown m sharp simulation spatial learning neural network model hippocampal formation model hippocampal function network ad model headdirection system neuron program simulation nerve equation analysis modeling academic publisher organization cortical afferent sector nucleus comparative neurology e learning sense direction cognitive neuroscience green representation body motion trajectory rat sensory motor cortex neuron society abstract vector encoding foundation spatial cognition computational mechanism m e cognitive neuroscience bj m green bj representation motion spatial rat cortex ranck headdirection cell deep freely rat society neuroscience abstract projection nucleus rat study transport journal comparative neurology h topographic organization subcortical projection anterior nucleus rat journal comparative neurology sharp pe press multiple cell rat postsubiculum multiple regression analysis comparison hippocampal area model neural basis rat sense direction ed advance neural information processing system mit headdirection cell record anterior nucleus rat journal neuroscience muller headdirection cell activity anterior thalamus postsubiculum predict animal future directional head society neuroscience abstract ranck headdirection cell record postsubiculum freely rat description quantitative analysis jm cortex rat characterization fourth region cortex connection journal comparative neurology navigation model combine place code headdirection path integration information society neuroscience abstract representation spatial orientation intrinsic dynamic headdirection cell ensemble theory submit
efficient reinforcement learning high dimensional linear quadratic system abstract study problem adaptive control high dimensional linear quadratic lq system previous work establish asymptotic convergence optimal controller adaptive control scheme recently average cost lq problem regret bind apart form logarithmic factor bind scale exponentially p dimension state space work consider case matrix describe dynamic lq system sparse dimension large present adaptive control scheme achieve regret bind op t apart logarithmic factor particular algorithm average cost time optimum cost t comparison previous work dense dynamic require time scale exponentially dimension order achieve regret time optimal cost believe result prominent application emerge area computational advertising particular target online advertising advertising social network introduction paper address problem adaptive control high dimensional linear quadratic lq system formally dynamic linear quadratic system b ut utt r p ut r control action time r state time ct r cost time t sequence random vector normal entry matrix q positive semidefinite matrix determine cost step evolution system describe matrix finally high dimensional system mean case p r fundamental theorem control theory assert lq system optimally control simple linear feedback pair b pair q observable optimal controller explicitly compute matrix describe dynamic cost paper assume condition hold matrix b unknown task adaptive control system learn control time early work adaptive control rely certainty equivalence principle scheme time t unknown parameter estimate base observation collect far optimal controller estimate system apply controller converge optimal controller case minimum variance cost general converge suboptimal controller subsequently introduce random exploration add noise signal solve problem converge suboptimal estimate aforementioned work concern asymptotic convergence optimal controller order achieve regret bound parameter estimation particular face uncertainty principle effective method confidence set find s high probability s small system control use optimistic parameter estimate optimum cost asymptotic convergence average cost problem asymptotic result extend provide bind cumulative assume control policy define average cost t t t far define cumulative regret t cost control policy time t j optimal average cost t o hide algorithm propose cumulative regret logarithmic factor lower bind provide armed bandit problem lower bind general case suggest scaling time cumulative regret optimal focus scaling regret time horizon regret propose algorithm scale poorly dimension specifically analysis prove regret bind current paper focus application state control dimension large time horizon interest powerful reinforcement learn application regret depend dimension general little achieve p number degree freedom p large number observation t p estimator arbitrary inaccurate prior knowledge unknown parameter b b sparse accurate estimation feasible particular prove suitable condition unknown parameter noise drive system ie control dynamic model linear stochastic differential equation estimate accurately sample result directly applicable general feedback gain l b sparse closed loop gain b l need sparse furthermore system dynamic correlate past observation estimate gain matrix finally notion cost obtain bound cost scale p work extend result suitable condition unknown parameter sparse high dimensional lq system accurately estimate r observation equip efficient learning method sparse high dimensional lq system adaptively control result perspective note expect cost time p noise cumulative cost time t bound pt compare regret bind t cumulative cost bound time optimum cumulative cost word algorithm perform close optimal step contrast result need pp step order achieve regret time optimal cost sparse high dimensional lq system appear engineering application particularly motivate emerge field application marketing use dynamical optimal control model advertising history decade cf survey model partial differential equation use describe advertising time translate basic problem find advertising maximize net focus work model temporal dynamic advertising control variable variable interest level exist rich literature study spatial behavior devise marketing scheme model space generalize include notion combination spatial temporal dynamic model optimal advertising consider simple temporal dynamic model extend allow state control variable spatial dependence introduce component control describe spatial dynamic control equivalent abstract linear control system form concern optimal control interaction dictate model assume know work deal discrete noisy version dynamic estimate know sparse model consider state variable live infinite dimensional space spatial model marketing usually consider state variable large number dimension eg number code high dimensional state space control recur application particular modern social network customer classify highly way potentially customer represent class number class complexity interaction unlikely formulate effective model class interact far nature interaction change time change landscape internet service information available customer important efficiently learn realtime datum nature interaction notation bundle unknown parameter variable b rpq p r interaction matrix m p denote standard pnorm kp corresponding operator norm m mi represent ith row matrix m m submatrix m form row column set s denote s cardinality integer denote set n algorithm employ face uncertainty principle episodic fashion beginning episode construct confidence set guarantee include unknown parameter high probability algorithm choose small expect cost estimate parameter episode apply optimal control estimate parameter episode confidence set construct use observation episode length episode choose increase geometrically allow accurate estimate shrinkage confidence set constant factor episode detail step pseudo code follow construct confidence set define start episode let controller choose episode t system control system dynamic write b b obtain solve follow beginning episode initial estimate optimization problem row separately u t reinforcement learn algorithm input precision failure probability initial cmin identifiable controller l e confidence set controller output series estimate let max log log let apply control observe trace construct confidence set calculate estimate e calculate set l e light t estimator b u know estimator term cost function normalize negative log likelihood measure fidelity observation second term impose sparsity constraint rpq define distance max row matrix worth note ksparse matrix constant distance scale p q particular absolute value element bound max kmax b algorithm construct confidence set episode estimator b rpq input parameter algorithm fix choose ensure probability design controller let minimum expect cost interaction matrix b denote l optimal controller achieve expect cost e implement principle choose beginning episode estimate e argmin apply episode optimal control correspond e xt t recall b optimal controller follow relation q r b t equation l b t r pseudo code algorithm summarize table main result section present performance guarantee term cumulative regret learn accuracy present algorithm order state theorem need present assumption system e let solution rpq define l follow lyapunov equation e t t closed loop system b l stable solution equation exist normal stationary distribution covariance proceed introduce identifiable regulator definition ksparse matrix rpq define l e t solution eq let l fine l cmin identifiable respect satisfy follow condition s q b lk condition simply state system control use regulator l closed loop autonomous system asymptotically stable second condition similar refer sparse recovery literature mutual incoherence condition example result exist matrix family satisfy condition let set index nonzero entry specific row second condition state correspond entry extended state variable sufficiently word trajectory correspond group state variable observe approximate linear combination condition think high order dependency consider entry extended state variable dynamic directly influence entry influence indirectly entry condition roughly state indirect influence sufficiently weak direct influence exist vast literature applicability condition scenario know hold condition necessary successful recovery relaxation discussion similar condition impose sparse recovery refer reader reference define min result state system learn efficiently trajectory observation control identifiable regulator consider lq system eq assume b ksparse let ut lxt l cmin identifiable regulator respect define max let denote number sample trajectory observe exist eq satisfie d probability regularize square solution p large particular achieve second result state equip efficient learning lq system eq t log suitable assumption control define neighborhood rpq assumption assert l close assumption exist c l identifiable wrt l sup define sup max max note theorem consider lq system eq constant cmin assume initial cmin identifiable regulator l far assume l cmin identifiable probability cumulative regret table bound log hide logarithmic factor o analysis proof prove theorem state set sufficient condition solution regularize square distance define true parameter subsequently prove condition hold high probability define x let wn matrix contain gaussian noise realization far let denote row define normalize gradient hessian likelihood function t l follow proposition proof find provide set sufficient condition accuracy regularize square solution proposition let support u k define definition assume exist k min follow condition hold cmin cmin k kg b ss u u regularize square solution satisfie d sequel prove condition proposition hold high probability assumption satisfied lemma order proof defer appendix b concentrate infinity norm mean state assume b let b exp prove condition eq bind follow lemma absolute deviation b mean b ij hij element h let q b hij exp q follow corollary bound corollary let q b s proof corollary apply union bind ij hij proof theorem condition proposition hold condition true assumption identifiability l respect order b imply second constraint b assume constraint ensure hold q require cmin obtain log cmin b aggregate b cmin corollary condition h cmin log cmin merge condition eq conclude condition proposition hold probability log proof proof highlevel idea proof similar proof main theorem decomposition gap cost obtain algorithm optimal cost upper bind term decomposition separately cost decomposition write optimality equation average cost dynamic programming e t ut e zt t t e e estimate use time t et field generate variable t notice average cost occur initial state utt et t et ft e et utt e e t e wt t utt e t k et e xt b xt utt k consequently t e t c c t t t e t e e t t et t xt b xt utt good event proceed define follow event probability space bind term c c provide low bind probability event p e p logt t technical follow establish upper bound c c c event e e following hold probability r t c p log log lemma event e e following hold log log t event e e following hold probability r c log log p log t following hold true pe e pe position prove theorem proof theorem use cost decomposition eq event e e e t c c c utt c c c e t fact inequality stem choice t t event e c c c use bound c c c desire result acknowledgment author thank anonymous reviewer comment support fellowship reference szepesvari regret bound adaptive control linear system proceeding th annual conference learn theory page e dual effect certainty equivalence separation control automatic control ieee transaction m learning network stochastic differential equation advance neural information processing system page bertsekas dynamic programming deterministic stochastic model prenticehall dynamic programming optimal control rd edition m adaptive control linear time invariant system good principle communication information system e r spatial model marketing marketing letter m achieve optimality adaptive control good approach decision control proceeding th ieee conference volume page kakade stochastic linear optimization bandit feedback proceeding annual conference learn theory colt r dynamic optimal control model advertising recent development management science page l h regulator revisit tracker automatic control ieee transaction p becker new family optimal adaptive controller markov chain automatic control ieee transaction robbin asymptotically efficient adaptive allocation rule advance apply mathematic square estimate stochastic regression model application identification control dynamic system annal statistic c distribute dynamic advertising journal theory application t dynamic optimization distribute model journal optimization theory application dynamic optimal control model survey review page relax programming method identify sparse signal information theory ieee transaction wainwright sharp threshold highdimensional noisy sparsity recovery quadratic programming lasso information theory ieee transaction p model selection consistency machine learn
template feature code computer vision hierarchical feedforward network successfully apply object recognition level hierarchy feature extract encode follow pool step processing pipeline common trend learn feature code template refer codebook entry filter overcomplete basis recently approach apparently use template obtain promising result secondorder pool op paper analyze op codingpoole scheme find testing phase op automatically adapt feature code template input feature use template learn training phase finding able bring common concept codingpoole scheme op feature quantization allow significant accuracy improvement op standard benchmark image classification caltech voc introduction object recognition scheme inspire biological vision base feedforward hierarchical architecture eg level hierarchy algorithm usually divide step feature coding spatial pool feature code extract set input feature set template filter overcomplete basis codebook similarity response transform use nonlinearitie finally spatial pooling extract single vector set transform response specific architecture network layer specific algorithm codingpoole layer usually set recognition task secondorder pool op alternative algorithm aforementioned codingpoole scheme op introduce medical imaging analyze magnetic resonance image achieve stateoftheart traditional computer vision task surprising fact op formulate feature code template contrast common codingpoole scheme template learn training phase testing phase template remain fix learn value motivate intriguing property op paper try reformulate op codingpoole scheme find actually compute similarity feature code template rest codingpoole scheme remain op template specific input fix learn value op template input require learn formulation able bring common concept codingpoole scheme feature quantization allow achieve significant improvement accuracy author contribute equally op image classification report experiment challenging benchmark image classification caltech voc preliminary section introduce op codingpoole scheme identify common terminology literature serve basis new formulation op introduce follow section algorithm analyze section usually layer hierarchical network object recognition input algorithm set feature vector come output previous layer raw image let xi set input feature vector algorithm set n feature vector rm index output algorithm single vector denote different dimensionality input vector follow subsection present algorithm terminology templatebased method introduce formulation op appear literature apparently use template codingpoole base evaluate similarity template templatebased method build similarity input vector set template depend terminology template denote filter codebook overcomplete basis refer template denote set template rm p paper input feature vector xi dimensionality m set template fix learn value training phase possible learn algorithm analyze necessary algorithm interesting purpose start compute similarity measure input feature vector template bk p let xi similarity function depend algorithm define vector contain similarity xi set template bk p matrix column vector bk compute algorithm analyze apply nonlinear transformation result response merge pool operation pool consist generate single response value template denote function include nonlinear transformation pool operation rm p r include operation function literature usually present separate step finally output vector build use p bk p n depend algorithm common concatenate output neighboring region generate final output layer present terminology apply method base evaluate similarity template method fisher vector sequel algorithm basis reformulate method popular bagofword variant fall category eg method consist assign input vector xi set template vector quantization build histogram assignment correspond average pool operation present use terminology compute similarity template usually base distance compute vector quantization pooling let s number template input vector assign let result assignment vector result apply vector entry set rest indicate assignment finally compute pooling assignment correspond template ie final output vector concatenation result pooling different template gp vector use second order statistic similarity feature template fisher vector build vector template bk xi bk xi bk bk learn constant normalization factor learn constant matrix model note eq similarity feature vector xi template bk final output vector p p detail refer reader use terminology simple rewrite term define use indicate fisher vector different k template vector obtain compute transformation original learn template involve input set feature norm idea importance template similarly method note relate fix template bk final output vector bf pool secondorder pool op introduce medical imaging describe voxel produce diffusion tensor image process tensor field op start build correlation matrix set feature column vector ie m m transpose vector r square matrix symmetric positive definite spd matrix contain secondorder statistic set spd matrix form manifold conventional operation euclidean space use metric propose spd matrix metric consist map spd matrix tangent space use logarithm matrix logk tangent space standard euclidean metric use logarithm spd matrix compute practice apply logarithm individually eigenvalue final output vector op write vec logk vec logk km eigenvector corresponding eigenvalue vec operator vectorize logk apparently similarity set template absence template op look different templatebased method recently achieve stateoftheart result computer vision task object detection semantic segmentation description reason motivate far analyze op relation templatebased method template section introduce formulation relate op templatebased method new formulation base compare final representation vector define final vector build denote hyr inner product final representation vector set input feature vector respectively use r s indicate respective representation set clear section analyze hyr instead divide analysis subsection subsection rewrite formulation templatebased method section inner product hyr subsection op op base evaluate similarity template subsection analyze characteristic template op input reformulation templatebased method rewrite generic formulation templatebased method describe section inner product final output vector algorithm section express gk r gq bsq kp xi bk v similarity function template depend function include nonlinearitie pooling similarity input feature vector template eq arise naturally algorithm section analyze term formulation method inner product final output vector write hyr g r gp r t gps r r gq bsq kp qp kp step introduce outer summation indicator function eliminate unnecessary cross term compare equation eq identify bsq indicator function return bsq vector inner product final fisher vector r g r b gp gk r gq kp qp indicator function appear reason method final template set input vector respectively compare rf similarity bk eq equal op codingpoole base template similarity reformulate op way templatebased method previous subsection allow relate op templatebased method op use similarity template rewrite definition op hyr use property tr trace function matrix hyr supplementary material derivation hyr km qm square matrix eigenvector m compare generic formulation templatebased method method vector bsq template bsq hxi fix bsq hxi p table summary table element formulation method vector similarity function template identify op note op sum m number eigenvector eq p number template finally eq correspond logk point express op similar way method find similarity input feature vector template purpose use definition eigenvalue eigenvector eigenvector orthonormal p derive follow equivalence replace find eigenvalue write use similarity input vector eigenvector hxi finally integrate derivation obtain x hyr gk r gq km qm hxi analyze eq equation form general equation templatebased method note eigenvector role set template ie ek p m observe bsq square inner product eigenvector square inner product input vector eigenvector pool operation logarithm average similarity table summarize corresponding element describe method template define template template depend input set feature vector fix predefine value case op template correspond eigenvector compute set input feature vector template op fix value learn training phase interestingly final template vector partially input vector note bk obtain modify fix learn template bk input feature vector finally note op number template equal dimensionality input feature vector op number template increase change input vector length m follow question m template allow sufficient generalization object recognition set input vector analyze question section application quantization op observe experiment section performance op degrade number vector set input feature increase reasonable template sufficient number different vector increase different sparse quantization input output n set high value xi vector entry rest end ti introduce algorithm increase robustness op variability input vector quantize input feature vector compute op quantization discard detail reduce variability vector experiment section report allow prevent degradation performance object recognition number input feature vector increase quantization algorithm use sparse quantization sq change dimensionality feature vector sq fast compute increase computational cost sparse quantization op quantization xi use quantization set ksparse vector let set ksparse vector ie k define q k set binary vector k element set q k set cardinality equal quantization vector codebook ci mapping v close element k quantize vector v case ci ie codebook contain set ksparse vector previously introduce type important advantage sq general quantization compute efficiently naive way compute general quantization evaluate near neighbor v ci costly compute large codebook highdimensional contrast compute select k high value set sq vi entry vector v sq dimension index set instead refer reader detailed explanation depict implementation sq op highlight simplicity computational cost sq negligible compare cost compute op use set ksparse vector work practice follow experiment section analyze op image classification dense sample sift descriptor setup common image classification allow direct comparison previous work report result caltech voc dataset use standard evaluation benchmark mean average precision accuracy class implementation detail use standard pipeline image classification use flip blur image extend training set pipeline caltech image resize maximum height width pixel standard protocol dataset voc size image remain original extract sift patch regular grid different scale caltech extract pixel scale pixel diameter voc sift sample pixel scale diameter op compute use sift descriptor input use spatial pyramid caltech generate pool region divide image region voc region generate final descriptor image concatenate descriptor pooled region apply power normalization final feature dimension work practice finally use linear svm classifier class parameter svm set use library feature coding check result replace op bagofword baseline change parameter caltech replace average pool bagofword maxpoole normalization perform codebook learn randomly pick set patch codebook entry work encoding evaluate use codebook entry entry performance increase significantly computational cost result caltech use random split image class training rest testing fig result different spatial pyramid configuration different level quantization note introduce quantization sift feature dimensional vector note use sq increase performance compare use use level pyramid level improvement accordance observation small region sift vector variability small limited template able capture meaningful information big region small performance degrade introduction quantization run experiment bagofword maxpoole op surpass op op accuracy report sift descriptor compare version sift experiment normal sift code find difference accuracy mainly come use image maximum pixel width height usually literature pixel note way discard information benefit confirm resize image pixel code accuracy similar report sq code accuracy exactly difference sift parameter test code maximum pixel accuracy increase high report close stateoftheart result use sift descriptor result voc fig b run experiment caltech note impact sq evident caltech table report accuracy addition mean average precision report fig b follow evaluation procedure describe pyramid use accuracy increase contrast caltech op sq performance similar implementation bagofword adverse condition image high variability voc high number input vector use sq obtain huge improvement op accuracy good report result voc op sq obtain improvement baseline conclusion find op pose codingpoole scheme base evaluate similarity template template op input rest analyze method practice formulation use improve performance op image classification currently analyze template deep hierarchical network pyr sq caltech pascal voc mean average precision mean accuracy sq select set sparse quantization sparse quantization average train person horse cow table dog car cat pyr op sq pyr op pyr op pyr op sq figure result different number nonzero entry sq note introduce quantization sift feature dimensional vector caltech use image class training b voc table pascal voc classification result average score provide average report result op sq second level pyramid pyr op sq second level pyramid pyr op sq level pyramid pyr acknowledgment thank support reference c c diffusion tensor imaging concept application magnetic resonance imaging visualization processing tensor field springer p region covariance fast descriptor detection classification p local covariance matrix l image representation application eccv r semantic segmentation secondorder pool selforganize neural network model mechanism pattern recognition shift position biological cybernetic m poggio hierarchical model object recognition distinctive image feature scaleinvariant keypoint cox science model search hyperparameter optimization dimension vision architecture icml r j darrell rich feature hierarchy accurate detection semantic segmentation cvpr l r fergus p object category m zisserman pascal visual object class voc challenge c r l c visual categorization bag keypoint workshop statistical learning computer vision coate importance encode training sparse coding vector quantization icml nest sparse quantization efficient feature code j image classification vector theory practice geometric mean novel vector structure symmetric positivedefinite matrix matrix analysis application positive definite matrix press sparse quantization patch description cvpr r e library large linear classification jmlr kernel object categorization iccv image classification use coding local image descriptor eccv
algorithm information filtering abstract work study information filtering model relevance label associate sequence feature vector realization unknown probabilistic linear function building analysis restricted version model derive general filtering rule base margin ridge regression estimator rule observe label vector vector relevant experiment realworld document filtering problem performance rule close online classifier allow observe label empirical result complement theoretical analysis consider randomized variant rule prove expect number mistake large optimal filtering rule know hide linear model introduction system able filter piece information crucial importance application consider stream discrete datum individually label relevant accord fix relevance criterion instance certain topic email spam case datum user behavior case filter use drop stream user datum likely relevance criterion point view filter view simple online binary classifier standard online pattern classification task classifier observe correct label prediction relevance data element know filter decide forward datum element user learn protocol partial feedback know adaptive filtering information community formalize filtering problem follow element arbitrary data sequence characterize feature vector associated relevant time filtering system observe th decide forward datum forward relevance label reveal system research support project ist use information adapt filter criterion data forward relevance label remain hidden th instance data sequence th example simplicity assume pair kind error filter system judge relevance feature vector example classify false positive relevant system similarly false negative classify system false negative remain unknown filter system score accord overall number wrong relevance judgement false positive false negative count mistake paper study filtering problem assumption relevance judgement generate use unknown probabilistic linear function design filter rule maintain linear hypothesis use margin information decide forward instance performance measure regret ie number wrong judgement filtering rule rule know probabilistic function use generate judgement finitetime bound regret hold arbitrary sequence instance result kind aware prove apple model apple model relevance judgement choose probabilistically compare bound report preliminary experimental result suggest method oppose general transformation develop apple framework matter fact general transformation margin information account section introduce probabilistic relevance model preliminary observation section consider restricted version model prove regret bind simple filtering rule simple fil section generalize filter rule good performance volume algorithm employ ridge fil linear square inspire section prove unrestricted probabilistic model regret bind randomized variant p ridge fil general filtering rule fil randomized variant run kernel adapt case unknown linear function drift time learn model notational convention preliminary relevance value random variable mean relevant exist fixed unknown vector relevant probability random variable assume independent assumption way sequence generate model want perform algorithm know consider filtering algorithm predict value dynamically update weight vector intend current approximation suitable confidence threshold fix sequence instance use denote margin denote margin define expect regret filtering algorithm time observe conditional probability space b c ed use denote random variable predicate true integrate possible value obtain d d inequality inequality use section analysis simple fil p ridge fil algorithm simplified model start analyze restrict model datum element unknown probability relevant want perform filtering rule consistently optimal action ie forward analysis model use section guide design good filtering rule unrestricted model let sample average let number data element time step fraction true positive element forward obviously optimal rule consider instead empirical rule rule mistake probability event suffice happen increase quickly datum forward irrespective sign estimate confidence level small respect problem argument large deviation bound require small case unknown fix use condition look use empirical value control large deviation approach work algorithm simple fil implement line reasoning form follow simple rule forward expect regret time simple fil define probability simple fil mistake time probability optimal filtering rule mistake result logarithmic bind regret time theorem expect cumulative regret simple fil number step proof sketch bind actual regret definition filtering rule time step follow cd loss generality assume bind imply write separately imply apply bound sum value independent random variable obtain bind adapt technique let apply bound finally easily verify piece desire result d d cd d cd d cd d d cd d cd d cd cd c linear square filter order generalize simple fil original unrestricted learning model describe section need estimate target vector let matrix column feature vector time step let vector corresponding observe relevance label index drop note hold consider square estimator pseudoinverse belong column space unbiased estimator b remove assumption rank add identity allow replace pseudoinverse standard inverse b d frequency fmeasure category measure filtering task measure define precision fraction relevant document recall fraction forward document relevant plot filter rule ridge compare ridge correct label classification ridge balanced ridge fil recall high precision need forward document believe relevant order confidence estimator converge fast note case cause ridge fil achieve slightly measure ridge figure k d d obtain sparse version ridge regression estimator sparsity fact store forward instance ie relevance label estimate directly margin far modify line technique analyze quantity regression estimator precisely estimate define use write expectation formula hold matrix let number forward instance order generalize estimator analysis need find large deviation bind form sufficiently fast able find simple fil bound report experimental result algorithm base inspire analysis simple fil exhibit good empirical behavior datum section prove bound base analysis simple fil expect regret randomized variant algorithm use experiment variant able prove regret bind scale essentially square root contrast logarithmic regret simple fil experimental result run experiment use filtering rule estimator note rule fil natural generalization simple fil unrestricted learning model particular simple fil use relevance threshold form ridge fil simple fil margin define differently test algorithm k let forward label update follow forward probability forward label update update ridge fil parameter real initialization loop b d dd d d figure pseudocode filter ridge fil performance analyze document filtering problem base story volume select topic frequency set document plausible range filtering application topic define filter task relevance judgement assign base document label topic document map real vector use bagofword representation particular token use generalpurpose remove replace digit single special character document vector build remove word occur time corpus use tfidf encoding form word frequency document df number document contain word total number document tfidf coefficient set finally document vector normalize length measure choice threshold affect filter performance run ridge fil set dataset standard online binary classifier receive correct label classification figure illustrate experimental result average measure ridge ridge fil respectively threshold compensate pretty partial feedback filter setup hand standard perceptron achieve measure classification task inferior ridge finally test filtering rule transformation base transformation consideration margin exhibit poor performance include plot probabilistic ridge filter section introduce probabilistic filtering algorithm derive ridge regression class linear probabilistic relevance function ridge fil sketch figure algorithm probability value input parameter maintain linear hypothesis forward update accord follow rule compute standard regression algorithm use inverse matrix new vector obtain project unit ball projection wrt distance function imply b note d d hand forward consequently update probability analysis p ridge fil inspire analysis related different problem base relate expected regret trial measure progress follow useful use notation figure let trial update follow inequality hold b d denote determinant matrix theorem let expect cumulative regret figure run proof sketch trial forward place define random update occur variable let regret trial trial set trial regret update rule lower bound lower bound probability regret use trivial bind probability instead let constant specify write easy verify conditional space use eq write proof sketch theorem fact d b follow bregman function easily figure projection convex set d d d wrt appendix follow b d d d desire inequality d b p ridge fil ridge fil d c c c c parametrization require knowledge turn remove assumption cost slightly involved proof inequality drop factor combine result turn term bound term virtue point work conditional space distinguish case case second case set case use sum notice proof theorem obtain bad case far upper bound d d d d d c c conclude proof d c d dd reference long pm associative reinforcement learning use linear probabilistic concept proc icml auer p use upper confidence bound online learning ieee page relative loss bound online density estimation exponential family distribution machine learn censor iterative method interval programming optimization theory application analysis gradientbased algorithm online computer system science n c secondorder perceptron proc colt page springer n long pm worstcase quadratic loss bound prediction use linear function gradient descent r sequential sample algorithm unify analysis low bound proc page lnc springer littlestone long pm apple information computation herbster m track good page probability inequality sum bounded variable ridge regression bias estimation nonorthogonal problem statistical learning theory son e tenth text retrieval conference competitive online statistic international statistical review
normalize spectral map synchronization abstract estimate map large collection object dense correspondence image shape fundamental problem wide range domain paper provide theoretical justification spectral technique map synchronization problem ie input collection object noisy map estimate pair object connected object graph output clean map pair object simple normalize spectral method normspecsync project block eigenvector data matrix map space exhibit surprisingly good behavior normspecsync efficient stateoftheart optimization technique admit similar exact recovery condition demonstrate usefulness normspecsync synthetic real dataset introduction problem establish map eg point correspondence transformation collection object connect wide range scientific problem include fuse partially overlap range scan structure motion object analyze organize geometric datum collection dna sequence model fundamental problem domain socalled map synchronization input noisy map compute pair object utilize natural constraint composite map cycle identity map obtain improved map importance map synchronization algorithmic problem remain limited early work formulate map synchronization solve combinatorial optimization formulation restrict problem susceptible local minimum recent work establish connection constraint property matrix store pairwise map block cast map synchronization inference technique exhibit improvement theoretical practical particular admit exact recovery condition ie underlie map recover noisy input map limitation optimization method scale largescale dataset contrast optimization demonstrate spectral technique work remarkably map synchronization focus problem synchronize permutation introduce robust efficient algorithm consist simple step step compute eigenvector data matrix encode input map second step round block conference neural information processing system nip matrix permutation matrix simple possess remarkable ability particular exact recovery condition match stateoftheart optimization technique efficient property enable apply propose algorithm thousand object spectral map synchronization consider input observation pair object contrast technique consider incomplete pairwise observation provide theoretical justification practical noise model algorithm section describe propose algorithm permutation synchronization begin problem setup section introduce algorithmic detail section problem setup suppose object sn object represent m point eg feature point image shape consider map sj pair object follow convention encode map ij permutation matrix pm pm space permutation matrix dimension m m t m m m rm vector element input permutation synchronization consist noisy permutation g connected object graph describe widely use pipeline generate input establish object graph connect object similar object use object descriptor image apply pairwise object matching method compute input pairwise map eg image d shape output consist improved map object begin define datum matrix obs encode initial pairwise map block xij dj sj degree object si note way encode datum matrix different sense follow common strategy handle irregular graph use normalize data matrix propose algorithm motivate fact input pairwise map correct correct map pair object recover lead eigenvector proposition suppose exist latent map eg groundtruth map object xi g denote matrix collect eigenvector column underlie pairwise map compute corresponding matrix block matrix t ij key insight propose approach input map noisy ie block corrupt lead eigenvector x obs stable perturbation analyze stability property section motivate design simple twostep permutation synchronization approach normspecsync step compute lead eigenvector second step normspecsync round induce normspecsync input base max initialize set initial guess orthonormal eigenvector max obs k rk end set corresponding xi solve output matrix block permutation follow elaborate step analyze complexity provide lead eigenvector computation need compute lead m eigenvector obs propose use generalized power method justify observation usually exist gap m m fact input pairwise map correct easy derive lead eigenvector x m x m x g second large eigenvalue normalize adjacency matrix later m m persistent presence corrupted pairwise map stability eigenvalue perturbation pm denote xij w t ij underlie groundtruth map dj obey fix k need round lose generality set paper solve follow constrain optimization problem project space permutation norm arg min kxi optimization problem describe socalled linear assignment problem solve exactly use complexity om note pn optimal solution invariant global scaling omit dj t m generate time complexity normspecsync step generalize power method consist matrixvector multiplication factorization complexity multiplication leverage sparsity m number edge complexity factorization analyze laser generalize power method converge linearly set max provide sufficiently accurate estimation lead eigenvector total time complexity generalize power method m nm time complexity step summary total complexity normspecsync m nm logn comparison complexity sdp formulation solve use fast admm method alternate direction multipli method m normspecsync exhibit significant speedup compare sdp formulation analysis section provide analysis normspecsync generalized model noise model noise model consider parameter m p specifically assume observation graph fix independently edge e m probability pij probability p pij pm random permutation remark noise model describe assume underlie permutation identity map fact assume generalized noise model probability pij probability p predefine underlying permutation object si object independent turn model describe equivalent m probability pij probability p independent random permutation mean sufficient consider model describe remark fundamental difference model propose use lowrank matrix recovery observation pattern ie fix model follow random model argue assumption practical observation graph construct compare object descriptor dependent distribution input object hand fix significantly complicate analysis normspecsync main contribution paper main state main result let dmin denote second value normalize adjacency matrix assume dmin noise model describe normspecsync recover underlie pairwise map high probability pc dmin constant proof proof combine stability bound consider projection step proposition consider permutation matrix pm matrix xij xk arg pm proof proof straightforward fact xk xk vary graph density vary graph density vary graph density graph density graph density graph density normspecsync second sdp second c diffsync second figure comparison normspecsync sdp diffsync noise model describe mean correspond element xij nonzero element dominant row column max kj end proof second bind concern stability lead eigenvector assumption t d w m obs easy prove theorem comb proof hard major difficulty require control block lead eigenvector require l bind stability result eigenvector base l norm space constraint defer proof appendix supplemental material imply normspecsync nearoptimal respect information theoretical bind describe fact clique p c align low bind factor follow model describe assume observation graph sample density factor q object connect independently probability q case bind easy dmin p c ln stay factor low bind indicate normspecsync experiment section perform quantitative evaluation normspecsync synthetic real example experimental result normspecsync superior stateoftheart map synchronization method literature organize remainder section follow section evaluate normspecsync synthetic example section evaluate normspecsync real example quantitative evaluation synthetic example generate synthetic datum follow procedure describe section specifically synthetic example control parameter g m p specify input graph m describe size permutation matrix p control noise level input map input map follow generalized erdosrenyi model ie independently edge input graph probability p input map m random permutation simplify discussion fix vary observation graph p evaluate normspecsync exist algorithm vary vertex degree vary vertex degree normspecsync specsync figure comparison specsync irregular observation dense graph sparse graph study performance normspecsync respect density graph experiment control density follow standard model parameter q independently edge connect probability q pair fix p q generate example apply normspecsync ratio underlie permutation recover figure illustrate success rate normspecsync grid sample p q blue yellow color indicate succeed fail example respectively color indicate mixture success failure normspecsync noise graph align theoretical analysis result compare normspecsync specsync advantage normspecsync irregular observation graph end generate use different model specifically let degree vertex uniformly distribute illustrate figure small ie vertex similar degree performance normspecsync specsync similar large irregular normspecsync tend noise specsync advantage utilize normalize data diffsync proceed compare normspecsync permutation synchronization method base diffusion distance diffsync exhibit similar computation efficiency significantly noise diffsync illustrate figure normspecsync sdp finally compare normspecsync sdp formulate permutation synchronization solve semidefinite program illustrate figure b exact recovery ability normspecsync sdp similar align theoretical analysis result normspecsync noise model consideration computationally normspecsync efficient sdp average run time second contrast sdp second average quantitative evaluation real example section present quantitative evaluation normspecsync real dataset cmu evaluate normspecsync cmu cmu dataset cmu dataset contain image image mark feature point experiment estimate initial map pair image use consider observation graph clique observation graph gf initial map compute pair image sparse observation gsparse construct connect similar image experiment connect edge image difference descriptor small average descriptor difference pair image note gsparse high variance term vertex correspondence correspondence euclidean distance pixel euclidean distance pixel correspondence correspondence sdp geodesic distance diameter geodesic distance diameter figure comparison specsync diffsync sdp cmu dataset consider observation graph sparse observation graph connect potentially similar object degree cmu similar cmu contain image exhibit slightly big variability cmu construct observation graph initial map similar fashion quantitative evaluation measure cumulative distribution distance predict target point groundtruth target point figure leave compare normspecsync sdp formulation specsync diffsync sparse observation graph normspecsync sdp superior diffsync performance normspecsync specsync gf similar normspecsync slight advantage ability handle irregular performance normspecsync sdp similar sdp slow normspecsync example gsparse sdp second second evaluate normspecsync dataset consist different pose human subject uniformly sample point model consider observation graph gf sparse observation graph gsparse gsparse construct way use shape context descriptor measure similarity d model addition initial map compute stateoftheart technique compute dense correspondence shape quantitative evaluation measure cumulative distribution geodesic distance predict target point groundtruth target point illustrate figure right relative performance normspecsync algorithm similar cmu particular normspecsync advantage specsync gsparse term computational efficiency normspecsync far sdp conclusion paper propose efficient algorithm normspecsync solve permutation synchronization problem algorithm adopt spectral view mapping problem exhibit surprising behavior term computation complexity exact recovery condition theoretical result improve exist method aspect include fix graph practical noise method experimental result demonstrate usefulness propose approach multiple opportunity future research example like extend normspecsync handle case input object partially overlap scenario develop analyze suitable procedure subtle example extend normspecsync rotation synchronization apply spectral decomposition round iterative manner acknowledgement like thank anonymous reviewer detailed comment improve paper author like thank support ii proof architecture section provide proof detailed proof defer supplemental material reformulate observation matrix normalize adjacency matrix d ad decompose t dominant eigenvalue correspond clear eigenvector reformulate observation matrix p m m ground truth result relate term m noise come specifically noise come randomness term t m uncertainty measurement graph structure use represent spectral norm graph disconnect disconnected close impossible recover ground truth noise term consist random matrix bind spectral norm block complete graph spectral norm bound p consider graph structure pd bind min measure distance s m let m u u want distance s m small distance function dist define r matrix represent form xt define kf specifically bind distance s m construct series olog distance ak s m small use triangle inequality s m close sketch proof able exist rotation matrix r m order n straightforward prove intuitively measurement eigenvector close ground truth second moment close formally speak ui m sj m ui m sj m rr sj m r m sj m ui m m m hand notice m m sj m need order detail include supplemental material reference automatic threedimensional modeling reality tech optimization structure motion ieee conference computer vision pattern recognition cvpr pp m object geometric paper pp explore collection d model use fuzzy correspondence transaction graphic proc vol dna genomic science vol m m visual relation use loop constraint ieee conference computer vision pattern recognition cvpr optimization structure motion ieee conference computer vision pattern recognition cvpr ieee nguyen m l optimization approach improve collection shape map symposium geometry processing consistent shape map programming computer proc symposium geometry processing vol l singer exact stable recovery rotation robust synchronization ab nearoptimal joint object match online available httparxivorgab r solve matching problem permutation synchronization advance neural information processing system d r g permutation diffusion map application image association problem computer vision advance neural information processing system histogram orient gradient human detection proceeding ieee computer society conference computer vision pattern recognition cvpr volume volume cvpr freeman sift flow dense correspondence different scene proceeding th computer vision pp blend intrinsic map acm transaction graphic vol acm p r s assignment problem industrial apply mathematic robust principal component analysis acm vol online available information recovery pairwise measurement approach vol learning graph match computer iccv ieee international conference ieee pp m r c random sample consensus paradigm model fit application image analysis automate acm vol r t graph vol pp
regularize distance metric learning theory dept computer science engineering image national institute health abstract paper examine generalization error regularize distance metric learn appropriate constraint generalization error regularize distance metric learning independent dimensionality suitable handle high dimensional datum addition present efficient online learn regularize distance metric learning empirical study datum classification face recognition propose algorithm effective distance metric learning compare stateoftheart method efficient robust high dimensional datum introduction distance metric learning fundamental problem machine learning pattern recognition critical realworld application information classification cluster numerous algorithm propose examine distance metric learning usually classify category metric learning supervise metric learning unsupervised distance metric learning refer manifold learning aim learn underlie lowdimensional manifold distance pair datum point preserve example algorithm category include local linear embed supervise metric learning attempt learn distance metric information label instance pairwise constraint search optimal distance metric data point class close data point different class far example algorithm category include work focus supervised distance metric learning large number study supervise distance metric learning survey reference study address generalization error distance metric learn paper examine generalization error regularize distance metric learn follow idea stability analysis appropriate constraint generalization error regularize distance metric learning independent dimensionality datum suitable handle high dimensional datum addition present online learn regularize distance metric learning regret bind note online metric learning study approach advantageous computationally efficient handle constraint sdp cone regret bind mistake bind dataset separate mahalanobis distance verify efficacy efficiency propose algorithm regularize distance metric learn conduct experiment datum classification face recognition empirical result propose online algorithm effective metric learning compare stateoftheart method robust efficient high dimensional datum regularize distance metric learning let zi xi yi denote label example vector dimension m class label study assume norm example upper bound r r let distance metric learn distance datum point calculate ax follow idea maximum margin classifier follow framework regularized distance metric learning d ij derive class label loss function output small value large positive value large value large negative assume continuous lipschitz constant l regularizer measure complexity distance metric d introduce ensure bounded domain reveal later constraint active constraint constant d sublinear p constraint affect generalization error distance metric learning generalization let ad distance metric learn algorithm training example let denote empirical loss ij convenience presentation write zi highlight dependence example denote loss true distribution zi empirical loss loss true distribution define estimation error dd d ad order behavior estimation error follow analysis base stability uniform stability algorithm determine stability training example replace specifically algorithm uniform stability sup ad u u stand new training set obtain replace d new example z far define n uniform stability behave advantage use stability analysis generalization error regularize distance metric learn example pair use training distance metric difficult directly utilize result statistical learning theory analysis derive generalization error bind regularized distance metric learning uniform stability derive constant regularize distance metric learning framework generalization error bind uniform analysis section follow closely omit detailed proof analysis utilize inequality state follow inequality random variable function l r sup f vi vl ci follow statement hold exp ci use inequality distance metric learning uniform stability follow inequality number training example d result follow lemma condition inequality hold let collection randomly select training example collection example replace example z dd bound follow dd measure large loss distance metric combine result derive bind generalization error use let denote collection randomly select training example ad distance metric learn algorithm uniform stability probability following bind d ad generalization error regularize distance metric learning behave sublinear d summarize follow proposition proposition trace constraint activate p dg z z proof follow directly ad c sup bind uniform stability need follow proposition proposition distance metric follow inequality hold example zu zu zu proposition follow directly fact z lipschitz continuous r example follow lemma bound ad ad let denote collection randomly select training example randomly select example let ad distance metric learn algorithm ad adiz proof lemma find result proposition follow theorem stability norm base theorem uniform stability algorithm use norm regularizer denote bound follow cl r comb follow theorem generalization error metric use let collection randomly select example ad distance metric learn algorithm probability follow bind true loss function ad learn use d ad cl r g min dg remark important feature estimation error converge order choose d low dependence dp p propose framework regularize distance metric learning robust high dimensional datum extreme case set d constant estimation error independent dimensionality datum section discuss efficient algorithm solve assume hinge loss max b z classification margin design online learn regularize distance metric learning follow theory gradient base learn define potential function online learn regret bind online learn let online learn run learn rate sequence yt t assume r training example distance m m m r b m t max yt m max yt t online learn regularize distance metric learning input predefine learning rate initialize receive pair training example xt yt yt compute class label yt yt yt training pair yt classify correctly s yt m project matrix m end end proof theorem find appendix b note online learning require compute m project matrix m sdp cone expensive high dimensional datum address challenge notice m m equivalent optimization problem m m approximate t t compute follow t arg min t t follow solution optimization problem theorem optimal solution t problem express yt proof theorem find supplementary material finally quantity compute solve follow optimization problem optimal value compute efficiently use conjugate gradient method note compare online metric learning propose online learn metric learning advantageous computationally efficient avoid project matrix sdp cone provable regret bind present mistake bind separable dataset experiment conduct extensive study verify efficiency efficacy propose algorithm metric learning convenience discussion refer distance metric learning examine efficacy learn distance metric employ near neighbor knn classifier hypothesis distance high classification accuracy set experiment accord experience compare algorithm follow stateoftheart algorithm distance metric learning baseline euclidean distance metric distance metric inverse covariance matrix training sample ie xing propose distance metric learning base large margin near informationtheoretic metric learning base relevance component analysis rca set maximum number iteration xing method number target neighbor parameter table classification error knn classifier uci datum set use different metric standard deviation include table pvalue test method dataset method xe lmnn rca tune cross validation range algorithm implement run use matlab experiment run processor machine ram operation system experiment comparison stateoftheart algorithm conduct experiment datum classification follow dataset uci repository class feature instance class feature instance glass class feature instance class feature instance class feature instance segmentation class feature instance class feature instance waveform class feature instance class feature instance dataset randomly select sample training use remain sample testing table classification error metric learning method dataset average run standard deviation observe propose metric learning deliver performance comparable stateoftheart method particular dataset classification accuracy propose algorithm close lmnn yield overall good performance baseline algorithm consistent result study effective algorithm distance metric learn far verify propose method perform statistically baseline method conduct statistical test use test test nonparametric statistical hypothesis test comparison related sample know safe student assume normal distribution table find regularized distance metric learning improve classification accuracy significantly compare mahalanobis distance xing method rca significant level perform slightly comparable lmnn run time second classification accuracy image resize ratio image resize ratio b figure face recognition accuracy knn running time online algorithm dataset vary image size result high dimensional datum evaluate dependence regularize metric learning algorithm datum dimension test task face recognition face database use study consist image face distinct subject picture subject subject image different time varied lighting condition different facial expression facial detail original size image pixel level examine sensitivity datum dimensionality vary datum dimension ie size image compress original image size different size image aspect ratio preserve image compression achieve interpolation output pixel value weight average pixel near neighborhood subject randomly face image training set test set ratio distance metric learn collection training face image use predict subject test image conduct experiment time report classification accuracy average subject run figure average classification accuracy knn classifier use different distance metric learning algorithm run time different metric learning algorithm dataset figure b note exclude xing method comparison extremely long computational time observe increase image size dimension regularized distance metric learning yield stable performance indicate high dimensional datum contrast baseline method performance vary significantly size input image change yield stable performance respect different size image high computational cost figure arise solve bregman optimization problem iteration highdimensional datum conclusion paper analyze generalization error regularize distance metric learning appropriate constraint regularized distance metric learning robust high dimensional datum present efficient learning algorithm solve related optimization problem empirical study face recognition datum classification propose approach robust efficient high dimensional datum comparable stateoftheart approach distance learning future plan investigate different regularizer effect distance metric learning acknowledgement work support national science foundation research laboratory research office opinion finding conclusion recommendation express material author necessarily reflect view nsf aro appendix proof proof introduce bregman divergence proof lemma function matrix bregman divergence matrix b compute follow b b tr define function follow kxkf ij furthermore function ad adiz dn adiz ad adiz ad c adiz zi adiz z ad z ad ad adiz inequality follow fact second step hold matrix ad adiz minimize objective function respectively adiz ad ad adiz adiz dn b ad adiz adiz lead result appendix proof proof denote s follow b m m d bf use relation b m m yt b assume r training example r r r b yt yt result reference andre stability generalization learn gabor lugosi prediction learning nonparametric statistic approach p jain dhillon metric learning proceeding th international conference machine learn s roweis metric learning collapse class advance information processing system ch semisupervise distance metric learning collaborative retrieval proceeding ieee conference computer vision pattern cvpr ch learning distance metric contextual constraint image retrieval proceeding ieee conference computer vision pattern recognition cvpr s t roweis think globally fit locally unsupervised learning low dimensional manifold machine learn research shalevshwartz singer online batch learning proceeding international conference machine learn page t hertz d m adjustment learning relevant component analysis proceeding seventh european conference computer vision volume page r introduction conjugate gradient method report jin h retrieval regularize metric learning acm c langford global geometric framework nonlinear dimensionality reduction science kernel relevant component analysis metric learning ieee international joint conference neural network weinberger distance metric learning large margin near neighbor classification advance neural information processing system jin learn uncertain information application automate proceeding international conference distance metric learning application cluster advance neural information processing system r jin distance metric learning comprehensive survey university tech jin r efficient algorithm local distance metric learn proceeding national conference artificial proceeding
simple local model complex dynamical system computer science computer science abstract present novel mathematical formalism idea local model dynamical system model certain prediction certain situation result restrict local model far simple complete model system combine local model produce detailed model demonstrate ability learn collection local model largescale example preliminary empirical comparison learn collection local model model learn method introduction building model good prediction world complicated task human remarkable ability split task chunk instance activity park complex interact component people dog ball answer question joint state impossible simple answer abstract question ball bounce ignore detail happen moment question dog difficult answer general dog complicated object behavior depend factor certain situation relatively easy prediction ball reasonably predict dog consideration potentially relevant fact short human lot simple localize piece knowledge allow prediction particular aspect world restrict situation combine abstract prediction form concrete detailed prediction course substantial effort exploit structure ai focus static domain temporal concern idea apply dynamical setting main contribution provide novel mathematical formulation local model dynamical system certain prediction certain situation combine complete model finally present empirical illustration use local model background paper focus learn model discrete dynamical system leave consideration control system future work time step system emit observation finite set observation sequence observation test let t set possible test length time step history simply sequence past observation use letter represent null history observation emit prediction test t history h denote conditional probability sequence t occur sequence occur set history define t t pt use model prediction definition complete model generate prediction t t h model prediction conditional prediction system instance want prediction set possible future occur man ball time leave park represent type prediction use union test outcome definition union test t t set test t prefix t t prediction union test sum prediction pt h model provide expert learn experience system form datum set observation sequence emit system complexity represent learn model depend complexity system model measure complexity adopt linear dimension define rank system dynamic matrix infinite matrix prediction th entry t closely related number underlie state hide markov model define formally note system simple mean small linear dimension present main contribution work start precisely define local model combine create complete model local model contrast complete model local model limit prediction certain prediction certain situation definition set test interest t set history interest local model model generate prediction interest pth t t assume general test interest union test paper place constraint semimarkov property close relationship concept option literature assumption relax future work word require order determine current history interest need look happen history interest formally definition set history interest semimarkov iff h t t imply t t simple example consider d ball bounce system figure agent observe line pixel location ball black rest white ball line change direction hit edge time step probability ball stick place probability square current direction figure ball bounce natural local model onestep prediction pixel p test interest set onestep test p black set onestep test p white history interest local model answer question chance ball pixel p note order answer question need observe color pixel neighbor p refer example model restricted local model test interest history interest end pixel p black local model essentially answer question ball p chance stick order prediction local model ignore detail prediction test interest history interest refer local model model b general example expect detail world irrelevant prediction interest ignore order simplify local model approach similar et test history interest convert primitive observation sequence abstract observation sequence ignore unnecessary detail complete model system use local model original primitive system abstraction proceed step figure construct intermediate system prediction test update history interest far abstract system ignore detail irrelevant prediction test interest abstract detail local prediction incorporate history interest intuitively local model ask prediction history way simplify update prediction history interest essentially history interest occur observation sequence happen awake update history interest sequence observation happen history interest bridging test set bridge test t b induce set history interest definition test t t bridge test iff t denote prefix t h h t conceptually transform primitive observation sequence sequence abstract observation observation correspond bridging test transform sequence temporally extended t e sequence figure note primitive system small number observation t e system infinitely infinity bridge test figure mapping experience original update history interest model t e system experience system simple model original system experience abstract system consider ball bounce size system linear dimension ok intuitively ball possible direction possible position recall model b apply ball land particular pixel bridge test possible way ball travel edge probability bridge test depend current direction ball e system linear dimension regardless possible formally e system complex original system proposition linear dimension dynamical system semimarkov set history interest linear dimension induce t e system e proof sketch linear dimension system rank system dynamic matrix correspond system matrix correspond e system submatrix original system column row correspond history test sequence bridging test submatrix great rank matrix contain good model system model system prediction test t t history interest h specifically prediction test history interest express prediction union test t e following note history interest write corresponding sequence bridge test use subscript t e distinguish prediction t e prediction original system proposition primitive test t t original system union test e pth pt e proof present constructive proof suppose t write sequence bridge test st trivially t correspond sequence bridge test rewrite concatenation test t t t t long prefix t sequence bridge test null t t b pth know pt e calculate note def set bridge test t prefix t b probability t probability bridge test p history interest let b result test interest union test prediction interest p t t use model t e simply h pt e model t e simple complete model system prediction history interest prediction test far simplify modeling task focus predict test interest incorporate test interest recall model example history interest bridging test single observation t e exactly equivalent original system note order prediction interest know ball neighbor pixel need distinguish observation ball nearby group rest abstract observation ball far general attempt abstract away unnecessary detail bridge test bridge test equivalent respect prediction interest specifically define partition mapping t e observation bridge test t b abstract observation use model abstract system observation figure local model follow property able express test interest union sequence abstract observation abstract history contain detail accurate prediction test let consider satisfy ease discuss special case assume test interest union onestep test ie t t t t partition observation contain exactly test interest natural example satisfy assumption local model onestep prediction particular dimension observation fundamental barrier treat test interest arbitrary union test development general case complex note union test t equivalent t e union test consist bridge test begin observation t t partition s t partition b bridge test t accord observation choose refinement thereof satisfy criterion s satisfy instance d ball bounce order accurate prediction pixel suffice observe pixel ignore rest distinguish color neighboring pixel problem treat explicitly et define accurate partition definition observation abstraction accurate respect iff primitive history contain abstract observation t t system abstract t e observation bridge test require accurate refinement refinement satisfy criterion furthermore accurate history result prediction test interest use abstract history exactly prediction test interest access primitive history accurate refinement satisfy criterion furthermore accurate refinement exist partition distinguish observation trivially accurate general expect able abstract away detail finally model abstract system far simple model original system t e system complex proposition linear dimension dynamical system linear dimension local model m nm e proof sketch row column correspond abstraction t e linear combination row column t e rank abstract rank t e learn local model test history interest accurate abstraction learn local model translate primitive trajectory t e trajectory use history interest translate t e trajectory abstract trajectory use accurate abstraction figure train model datum experiment use pomdps markov model local model representation combine local model consider collection local model m local model m m test interest history interest exact model abstract system induce accurate history h set model m m h available prediction test interest wish prediction specifically interest local model case combine abstract coarse prediction individual model joint prediction modeling assumption allow efficiently combine prediction local model definition local model mutually conditionally independent iff subset m m t t tk prediction intersection equal product prediction ti domain expert specify structure collection local model satisfy property good possible assumption collection local model use prediction individual model compute prediction test intersection test interest multiply prediction compute prediction union test interest use standard formula b history h collection local model use prediction union test construct test interest model include test course prediction practical necessary collection local model selectively focus important prediction ignore approximate important prediction save representational complexity course collection local model complete model instance note model prediction o h complete model prediction express term onestep prediction onestep test intersection test interest model h m complete model m mutual conditional independence property hold prediction use m approximate local model m prediction interest exactly useful future work explore bound error approximation learn collection local model paper assume test history interest accurate refinement model train local model individually abstract datum fair knowledge assume analogous provide structure graphical model learn distribution parameter common practice automatically split system simple local model interesting challenging problem ground future research hope cast structure learn problem light framework new progress relationship structured representation briefly discuss especially relevant alternative modeling technology aim exploit local independence structure dynamical system dbn dynamic baye network dbn representation exploit conditional independence structure main difference collection local model specify independence structure hidden variable value observe representation express structure entirely term prediction observation structural assumption verify use statistical test datum dbn assumption directly verify dbn decompose world state set random variable table local model structure m apply history end ball hit brick b m onestep tion color pixel b ball hit brick b ball position p come direction brick ball color pixel absence presence ball color pixel p color m additionally distinguish bridging test type special brick hit type special brick recently hit configuration brick adjacent p step bridge test store conditional probability distribution variable value previous time step distribution local model onestep prediction variable variable dbn specify variable ignore predict value essentially accurate refinement identify detail local model ignore history interest relate concept relational model relational model treat state world conjunction predicate state evolve use update rule consist precondition specify rule apply change state update rule essentially local model pre play role history test interest relational model typically focus markov world address partial essentially generalize update rule main strength relational model include firstorder variable update rule allow sophisticated parameter generalization use parameter tie experiment incorporate formalism variable framework recently introduce essentially special collection local model relate maximum entropy model represent prediction weight product feature future past experimental result large scale example section present preliminary empirical result illustrate application collection local model example modify version game figure observation pixel image image pixel ball wall pixel brick ball hit brick brick disappear ball hit wall bounce randomly select angle episode end figure game brick version type special brick ball hit dark brick brick require hit break ball hit light brick brick require hit break place brick regular medium gray probability dark light probability system stochastic partially observable special brick markov roughly observation underlying state decomposition local model specify table naturally local model predict brick row ball row background row behave structure satisfy mutual conditional independence property pixel predict model history fully detailed pixel onestep prediction model apply tradeoff complexity individual model total number local model structure select approximately local model course train model impractical improve data efficiency training time parameter tie system behavior object depend position advantage type local model note brick pixel p possible position p ball possible direction ball come include case step ball simply appear pixel local pomdp likelihood ratio likelihood ratio size size training episode local pomdp training episode figure leave result d ball bounce problem error bar omit avoid dbn structure use node binary shade node hide link t node t omit simplicity total ball model direction combine translate trajectory associate position use train single share model local model maintain state underlie model parameter share model type associate different position note position matter time step ball appear place result model bad prediction time step clarity presentation ignore timestep result local model use lookup table base markov representation overall system markov local model learn local model firstorder markov responsible predict happen brick ball hit model secondorder markov local model state episode drop likelihood ratio learn curve collection local model figure trial train model number episode end brick step measure likelihood wrt test episode report average trial parameter tie model assign probability test sequence datum sparsity issue solid line likelihood ratio log likelihood training trajectory true system divide log likelihood model ignore episode cause infinite log likelihood dash line figure result ar proportion episode drop likelihood ratio approach proportion bad episode approach imply learn good model episode learn comparison experiment compare parameter learning result collection local model method simple example complexity easily control recall d ball bounce learn model d ball bounce size use collection local model parameter tie use pomdps local model respectively flat model pomdp dbn collection local model follow structure pixel type model predict color pixel time step history ball immediate neighborhood pixel model ignore pixel predict model apply ball pixel jointly predict color pixel neighbor model distinguish bridge test ball leave right stay pixel step collection local model satisfy mutual conditional independence property allow prediction primitive onestep test game example trial train model number episode length measure log likelihood test episode length initialize local pomdp state flat pomdp state different problem size dbn use graphical structure figure c train use graphical model stop maximum iteration training free parameter detail parameter sweep choose local respectively size size domain report likelihood ratio average trial result figure collection local model perform outperform flat model dash line flat model performance degrade size world increase collection local model affect problem size local datum local pomdps learn good model ultimately learn model unexpected result dbn training perform bad flat pomdp training explanation effect fact different graphical structure cause different local issue clearly result thorough empirical comparison wide variety problem conclusion present novel formalization idea local model preliminary empirical result collection local model learn largescale system datum complexity parameter learning compare favorably representation support support nsf grant ii opinion finding conclusion recommendation express material author necessarily reflect view nsf reference learn probabilistic model relational structure journal machine learn research hide markov model advance information processing system nip page m pack learn symbolic model stochastic domain artificial intelligence predictive representation state advance neural information processing system nip page herbert observable operator model discrete time neural computation matthew predictive state representation new theory model dynamical system uncertainty artificial intelligence page framework temporal abstraction reinforcement learn artificial intelligence decision tree method find mdp national conference artificial intelligence abstraction predictive state representation artificial intelligence build incomplete accurate model international symposium artificial intelligence survey partially observable markov decision process theory model management science friedman independence bayesian network uncertainty artificial intelligence page approximate predictive state representation autonomous agent system maximum entropy approach natural language process computational linguistic family predictive representation state neural information processing system nip page graphical model learning discovery predictive state representation dynamical system reset international conference machine learn icml
slice sample covariance hyperparameter dept computer iain school popular way specify dependency random variable probabilistic model covariance structure specify use unknown hyperparameter integrate hyperparameter consider different possible explanation datum prediction integration perform use markov sample nongaussian observation standard hyperparameter sampling approach require careful tuning converge slowly paper present slice sample approach require little tuning mix strong introduction probabilistic model incorporate multivariate gaussian distribution explain dependency variable process gp model generalize linear mixed model common example nongaussian observation model infer parameter specify covariance structure difficult exist computational method split complementary class deterministic approximation simulation work present method sample approach easy apply recent work murray develop slice sample variant elliptical slice sample update strongly couple apriori gaussian variate nongaussian observation previously agarwal demonstrate utility slice sampling update covariance parameter hyperparameter observation model question possibility slice sample general setting work develop new slice sampler update covariance hyperparameter method use robust representation work wide variety problem technical requirement little need tuning easy apply model consider generative model datum depend vector latent variable gaussian distribute covariance set unknown hyperparameter model common machine learn statistical science use standard notation gaussian distribution mean covariance m m m use m indicate draw distribution density l latent value l l l l l input space lengthscale l prior draw lengthscale figure draw prior use different lengthscale squared exponential covariance b posterior draw generic form generative model consider summarize covariance hyperparameter variable conditional likelihood p datum method discuss paper apply covariance arbitrary positive definite function parameterize experiment focus popular case covariance associate input vector kernel kxi dj hyperparameter d signal variance control overall scale latent variable characteristic lengthscale convert distance input covariance corresponding latent value f nongaussian likelihood wish sample joint posterior z f ph like avoid implement new code tune algorithm different covariance conditional likelihood function markov chain inference markov chain transition operator t z define conditional distribution new position position z operator leave target distribution invariant z t z standard way sample joint posterior alternately simulate transition operator leave conditional p datum p invariant fairly mild condition markov chain target distribution recent work focus transition operator update latent variable datum fix covariance update hyperparameter fix latent variable need leave conditional posterior ph invariant simple algorithm metropolishasting operator possibility include slice sample hamiltonian alternately fix unknown appeal implementation result markov chain slow explore joint posterior distribution figure latent vector sample use covariance different lengthscale sample highly informative lengthscale hyperparameter use especially short lengthscale sharpness figure b dramatically limit markov chain update hyperparameter fix latent value transition fix transition fix input current hyperparameter dist covariance function output hyperparameter propose q draw uniform f q return accept new state return current state input current state proposal dist covariance function output solve n variate propose q compute imply value draw uniform p q q return accept new state return current state whiten prior conditional likelihood weak strong prior smoothing assumption introduce latent gaussian model extreme limit data ie l constant target distribution prior model p sampling prior easy alternately fix work strongly couple strategy reparameterize model unknown variable independent prior independent random variable identify generative procedure multivariate gaussian distribution vector independent normal draw independently hyperparameter deterministically transform n l l notation paper square root covariance matrix matrix square root use decomposition convenient reserve c principal square root square root behave power example choose update hyperparameter fixed instead fix original latent variable deterministically link hyperparameter update actually change sample figure result use whitened variable different hyperparameter follow general trend vary lengthscale use construct posterior hyperparameter fixed apparent apply baye rule generative procedure obtain change variable p datum p datum p l datum l ph metropolishasting operator distribution acceptance rule depend latent variable conditional likelihood instead prior variable automatically update respect prior limit new hyperparameter propose prior accept surrogate data model previous algorithm ideal statistical application illustrate figure ideal weak datum limit latent variable distribute accord prior example likelihood restrictive proposal acceptable strong data limit latent variable fix likelihood ideal likelihood term example strong prior ignore regression problem latent variable analytically allow hyperparameter accept reject accord marginal posterior p datum latent variable require sample directly conditional posterior p datum build method apply nongaussian likelihood create auxiliary variable model introduce surrogate gaussian observation guide joint proposal hyperparameter latent variable current state whiten surrogate datum proposal observation input space figure regression problem gaussian observation illustrate gray bar current state sampler short lengthscale hyperparameter long lengthscale propose current latent variable lie straight line long lengthscale plausible whiten prior section update latent variable straight line ignore observation proposal use surrogate datum section set observation noise set latent variable draw plausible propose lengthscale close current state augment gaussian model auxiliary variable g noisy version true latent variable p g s arbitrary free parameter set hand fix value value depend current hyperparameter discuss automatically set auxiliary noise covariance section original model f define joint auxiliary distribution p g hyperparameter possible sample distribution opposite order draw auxiliary value marginal distribution p s sample model value condition auxiliary value p r standard manipulation g r s g auxiliary model latent variable interest draw posterior surrogate datum g describe sample process draw spherical gaussian n lr r condition whitened variable surrogate datum g update hyperparameter imply latent variable remain plausible draw surrogate posterior current hyperparameter illustrate figure leave joint distribution invariant update follow conditional distribution derive generative model p p ph metropolishasting contain ratio term acceptance rule slice sample metropolishasting algorithm discuss far proposal distribution q set tune efficiency algorithm depend careful choice scale proposal distribution slice sample family adaptive search procedure robust choice scale parameter surrogate datum surrogate data slice sampling input dist q model output draw surrogate datum compute imply latent variate lr mg propose compute function m draw uniform p q p h q h return accept new state return current state input scale model output draw surrogate datum compute imply latent variate lr randomly center bracket uniform min max min draw uniform determine threshold u draw proposal max compute function m ph return shrink bracket minimum min shrink maximum max apply possible slice sample scalar hyperparameter surrogate datum model section free parameter scale initial proposal distribution careful tuning parameter require initial scale set large value width prior width proposal shrink acceptable range exponentially quickly procedure use adapt initial scale small assume hyperparameter effective reparameterization improve performance auxiliary noise covariance surrogate datum g noise covariance define distribution specify plausible region latent variable update noise covariance determine size region baseline algorithm section result limit case surrogate datum current latent variable equal acceptance ratio reduce observation uninformative current state tend prior limit acceptance ratio reduce choose base preliminary run tuning likelihood term factorize fi measure likelihood restrict variable individually p fi fi gaussian fit moment match laplace approximation match second derivative mode fit close approximation possible analytically perform numerically distribution onedimensional gaussian fit variance vi set auxiliary noise level result posterior variance site s thresholde moment matching procedure simplify step assume density filtering expectation propagation expensive use markov chain relate work discuss sampler jointly update latent variable hyperparameter hyperparameter far joint narrow conditional posterior eg figure b allow generic way jointly sample realvalued variable hmc method implement tune use hmc jointly update latent variable hyperparameter hierarchical model improve sample et propose robust representation sample gaussian model use approximation target posterior distribution reparameterization unknown variable close independent approximation replace likelihood gaussian form proportional log argmax diagonal suggest diagonal approximation look laplace approximation function probability density likelihood fit result approximate r find noise datum current latent variable draw approximate posterior n f mf suggest use reparameterization mf fix new variable update hyperparameter p datum likelihood reparameterize variable independent hyperparameter hope approximate nongaussian likelihood result parameterization markov chain mix rapidly expand common loglikelihood maximum define example approximating probit logistic likelihood binary classification poisson observation count expansion flat approximation prior likelihood term flat reparameterization approach reduce section alternative s auxiliary covariance propose use instead surrogate datum sampler section view use reparameterization treat mg arbitrary random reparameterization proposal density q reparameterize space multiply jacobian proposal density original parameterization propose reparameterization include metropolishasting acceptance probability p min p s l r line linear algebra confirm acceptance ratio result alternatively substitute acceptance probability similar obtain apply metropolishasting propose et difference new latent variable compute use different mean surrogate datum method extra term random fix choice reparameterization surrogate data sampler easy implement previous reparameterization work surrogate posterior centre current latent variable mean point estimate maximum likelihood require pick noise covariance poorly produce method fix reparameterize work badly true posterior distribution tail et point center approximate gaussian likelihood reparameterization current state compute jacobian transformation intractable construction surrogate data model center reparameterization current state experiment empirically compare performance approach sample datum set regression classification cox process inference problem detail rest section code supplementary material result summarize figure follow discussion section experimental configuration run independent chain different random seed iteration sample iteration quantify mixing chain estimate effective number sample complete datum likelihood trace use compare cost metric number hyperparameter setting consider require small number covariance decomposition time complexity number likelihood evaluation total time single core experiment design test mixing hyperparameter sample joint posterior discuss approach update variable transition operator latent variable fix hyperparameter require whitened variable remain fix latent variable hyperparameter constrain satisfy l surrogate datum sampler ergodic joint posterior distribution eventually explore update change hyperparameter require expensive computation involve covariance compute covariance set hyperparameter sense apply cheap update latent variable method apply update elliptical slice sample latent variable hyperparameter update consider apply elliptical slice sample reparameterize representation simplicity comparison independently work use surrogate datum reparameterization update latent variable fix hyperparameter method implement method update gaussian covariance hyperparameter method use slice sampler apply follow model representation fix fix latent function whiten prior use surrogate datum noise level set match site posterior use laplace approximation poisson likelihood classification problem use moment match laplace approximation work use surrogate datum noise variance set expansion loglikelihood infinite variance truncate large value method fix reparameterization base posterior binary classification ionosphere evaluate different method perform binary gp classification fix apply method ionosphere dataset use training datum dimension use logistic likelihood prior lengthscale signal variance method reduce method apply maximum loglikelihood infinity gaussian regression synthetic observation reparameterization et hyperparameter latent variable exactly independent random surrogate datum model effective use gaussian regression problem assess bad surrogate datum method compare ideal reparameterization synthetic datum set input point draw uniformly unit hypercube gp unit lengthscale draw uniform observation noise variance apply fix method likelihood site method coincide auxiliary noise match observation noise cox process inference test method inhomogeneous poisson process gaussian process prior sample hyperparameter mean offset model apply point process dataset record mining event bin day tree location region scale unit square split bin result mining problem initially highly variable mining experiment quick chain iteration fix effective sample likelihood evaluation effective sample covariance construction ionosphere synthetic ionosphere synthetic effective sample second ionosphere synthetic figure result experimental comparison mcmc method inference datum set figure group bar experiment vertical axis effective number sample complete data likelihood unit cost cost likelihood evaluation leave covariance construction center second right mean standard error run group bar rescale number group effective sample method bar height bar miss method text discussion ionosphere classification problem site method work baseline slightly prefer involve derivation synthetic test post method perform similarly expect exist post method advantage realize particular dataset post method slight time advantage implementation detail notable mining problem poisson likelihood close exist approximation work new propose method approximation poisson fit poorly site count dataset discretize twodimensional space lead large number majority bin count mining expand likelihood likelihood contribution bin count perform similarly work good result come use approximation fine discretization result different site reparameterization work empirical investigation use slice sampling easy implement use representation discuss combine mcmc method recently use cox process new surrogate datum representation offer stateoftheart performance advanced method applicable important message result fix latent variable update hyperparameter accord conditional posterior commonly use work poorly simple reparameterization whiten prior discuss section work problem smoothness important posterior site approximation difficult advanced method present simple whitening reparameterization consideration perform mcmc inference hyperparameter acknowledgement thank anonymous reviewer useful comment work support ist programme european community pascal network ist publication reflect author view advanced research reference iain murray elliptical slice sample machine learn research proceeding international conference artificial intelligence statistic aistat m sample annal statistic slice sampling simulation base spatial datum model statistic compute process machine learning markov chain explore posterior distribution annal statistic efficient sampling inference use control variable advance neural information process system page mit press letter m neal use hamiltonian dynamic appear handbook chapman method spatial generalized linear mixed model graphical statistic expectation propagation approximate bayesian inference proceeding th annual conference uncertainty artificial intelligence uai page correct version available learn hyperparameter neural network model use hamiltonian dynamic master thesis computer available good auxiliary sampling use datum m neal regression classification use gaussian process prior editor bayesian statistic page press rasmussen assess approximate inference classification machine learning research p classification return ionosphere use neural network r g note interval biometrika model spatial pattern royal statistical society series mark royal statistical society series appear
fire rate prediction optimal balanced network group neural theory neural theory neuroscience programme centre unknown abstract fire rate spike network relate neural input connectivity network function important problem firing rate key measure network activity study neural computation neural network dynamic difficult problem spiking mechanism individual neuron highly nonlinear individual neuron interact strongly connectivity develop new technique calculate fire rate optimal balanced network particularly interesting network provide optimal spikebased signal representation produce spike activity dynamic balance excitation inhibition calculate firing rate treat balanced network dynamic algorithm optimise signal representation identify calculate firing rate find solution fire rate calculation relate network firing rate directly network input connectivity function allow explain function underlying mechanism tune curve variety system introduction firing rate neuron arguably important neural dynamic neural computation recording firing rate neuron observe increase muscle large diversity fire rate response stimulus observe range tuning curve tuning curve diversity computational role firing rate response fire rate determine network connectivity neural input attempt answer question use variety experimental theoretical technique approach deal nonlinearity neural mechanism strong interaction neuron mediate network connectivity significant progress use linear approximation example experimentally record firing rate variety system describe use receptive field capture linear relationship stimulus fire rate response recent year find linear approximation fail capture important aspect neural activity similarly theoretical study linear approximation use simplify nonlinear firing rate calculation variety network model use approximation recently use linear response theory calculation lead important insight neural network connectivity input determine fire rate calculation apply restrict subset situation assumption apply develop new technique calculate fire rate directly identify nonlinear structure tightly balanced network balanced network theory come regard standard model cortical activity account large proportion observe activity dynamic balance excitation inhibition recently find tightly balanced network efficient coding signal represent optimally subject cost observation allow interpret balanced network activity optimisation algorithm directly identify nonlinear relationship fire rate input connectivity neural computation provide algorithm use technique calculate firing rate variety balanced network model explore computational role underlie network mechanism monotonic firing rate tuning curve tuning curve tune curve inhomogeneity optimal balanced network model calculate fire rate balanced network consist connect network drive input signal ik m input m dimension input response input neuron produce spike train denote si sn spike train time spike produce membrane potential vi exceed spike threshold simple rule capture essence neural mechanism membrane potential follow dynamic m vi neuron connection strength neuron connection strength input neuron neuron spike membrane potential reset ri ti write equation work focus network connectivity symmetric simplify analysis certain case matrix interested network balance excitation inhibition coincide optimal signal representation choice network connectivity spiking threshold certain condition satisfied possible proceed fire rate calculation derive condition begin calculate sum total excitatory inhibitory input receive neuron network solve equation implicitly m vi rk temporal filtering neuron train t temporal filtering input t rk excitatory inhibitory input receive include summation eqn rewrite slope loss function follow r c c constant use expression derive condition connectivity satisfy network operate optimal balanced state balanced network excitation inhibition cancel produce input order magnitude spike threshold small relative magnitude excitation inhibition tightly balanced network consider cancellation precise large network limit active neuron use equation tight balance condition equivalent loss function eqn minimise implication choice network connectivity spiking threshold loss function minimum guarantee require positive definite secondly spike threshold neuron choose act minimise cost function spike condition write spike use equation rewrite spike rk finally c x time figure optimal balanced network example schematic balanced neural network provide optimal spikebased representation signal tightly balanced network produce output blue panel closely match signal black panel population spiking activity represent use plot middle panel spike represent dot randomly choose neuron red middle panel plot total excitatory input green panel total inhibitory input red panel sum excitation inhibition black panel fluctuate spike threshold thin black line panel indicate network tightly balanced spike produce sum exceed spike threshold c firing rate tuning curve measure simulation balanced network line represent tuning curve single neuron error value equation cancel term use equation write spike condition spike threshold neuron set condition relax considerably loss function additional linear cost term condition satisfied network tightly balanced interested network tightly balanced optimal equation balance excitation inhibition coincide optimisation loss function important result relate balanced network dynamic neural computation specifically allow interpret spike activity tightly balanced network algorithm optimise loss function eqn interesting optimisation easily map useful computation particularly interesting example identity matrix recent work connectivity learn use spike plasticity rule use connectivity rewrite loss function eqn follow e ri r second term equation cost term neuron spike term quantify difference signal value linear readout compute use linear decoder ft network connectivity close produce spike train optimise equation produce output signal value remainder work focus optimal balanced network form connectivity illustrate property system simulate network neuron find network produce spike train fig middle panel represent great accuracy broad range signal value fig b panel expect optimal performance coincide tight balance excitation inhibition fig panel reminiscent cortical observation example network optimise represent dimensional signal measure fire rate tuning curve use fix value vary use signal produce interesting nonlinear tuning curve fig c especially signal value neuron fall silent section attempt understand tuning curve nonlinearity calculate fire rate analytically firing rate analysis quadratic programming goal calculate firing rate neuron tightly balanced network model function network input recurrent network connectivity connectivity surface difficult problem individual neuron complicate nonlinear integrateandfire dynamic interact strongly network connectivity loss function relationship develop allow circumvent problem possible firing rate measure use experiment theoretical study usually temporal averaging window use define firing rate neuron z et t dt exponentially weight temporal average choose temporal average match dynamic synaptic filter neural network suppose network optimise follow cost function r c bt r vector positive linear weight find optimal spiking threshold network bi apply technique network threshold ti case firing rate short membrane potential leak easily framework long slow synaptic process allow write t ri t need multiply ensure firing rate report unit spike second calculate firing rate use relationship exploit algorithmic nature tightly balanced network network produce spike train minimise loss function eqn firing rate network minimise constraint fire rate positive fi firing rate prediction solution constrain optimisation problem know quadratic program optimisation quadratic loss function quadratic function constrain firing rate positive value quantity definition illustrate firing rate prediction use simple network recurrent connectivity simulate system measure spiketrain fire rate neuron fig left panel use equation obtain theoretical prediction fire rate find firing rate prediction match spiketrain measurement great accuracy middle panel right panel use firing rate solution understand relationship firing rate input connectivity function neuron active solve equation exactly fire rate relate network connectivity accord neuron silent neuron compensate adjust firing rate slope example silent firing rate neuron increase denote second row similarly prediction simulation measurement b figure calculate fire rate example tuning curve measurement obtain simulation network leave representation error e network signal value leave tuning curve prediction obtain use quadratic programming middle predict representation error e predict fire rate closely match measure firing rate neuron signal value right phase diagram network activity simulation leave panel fire rate evolve silent state minimum cost function ex leave panel fluctuate minimum increase discrete step size decrease exponentially leave panel measure firing rate trajectory right panel network evolve minimum cost function ex right panel silent silent firing rate neuron increase row nonlinear change firing rate cause constraint understand functionally attempt network represent accurately constraint system large network firing rate prediction difficult write analytically interaction individual neuron constraint nonetheless number general observation tune curve shape general interpret tune curve shape solution quadratic programming problem write piecewise linear function m mx matrix entry depend region signal space occupy example system discuss signal space partition region region active silent second region neuron active region silent active fig left panel region different linear relationship signal firing rate boundary region occur point signal space active neuron silent silent neuron active n region use quadratic programming describe spike dynamic underlie nonlinear network return example measure temporal evolution fire rate find network suboptimal state firing rate rapidly evolve optimum series discrete step size fig left panel stepsize spike ri ri accord equation fi fi accord equation network reach optimal state impossible remain firing rate begin decay exponentially firing rate definition exponentially weight summation eqn fig b middle panel eventually firing rate decay far optimal solution spike fire network close optimum way spike dynamic interpret quadratic programming firing rate continue fluctuate optimal spiking value fluctuation noisy dependent initial condition network noise unusual algorithmic structure standard probabilistic description spike analyse tuning curve shape quadratic programming framework relate fire rate network connectivity input explore computational function tuning curve shape network mechanism generate tuning curve investigate system monotonic tuning curve system tuning curve constitute large proportion firing rate observation begin consider system monotonic tuning curve similar example consider recurrent connectivity system recurrent connectivity tuning curve shape largely determine form matrix determine contribution tuning curve computational function role linear decoder illustrate simulate response network dimensional signal varied fix use different configuration fig system produce monotonically increase decrease tuning curve fig find neuron positive value positive firing rate slope fig blue tuning curve neuron negative value negative firing rate slope fig red tuning curve value space tuning curve individual neuron space manipulate regularity add random noise connectivity obtain inhomogeneous highly irregular tuning curve fig b inhomogeneity little effect representation error inhomogeneous monotonic tuning reminiscent tune neural system include oculomotor system oculomotor system represent eye position use neuron negative slope represent left eye position neuron positive slope represent right eye position relate model system signal variable interpret represent central eye position positive negative value simulation measurement prediction b figure relationship fire rate stimulus connectivity network neuron dot represent contribution neuron signal representation firing rate column consider signal straight line thin black line simulate network neuron measure fire rate column measurement closely match predict firing rate rd column point th column represent firing rate individual neuron stimulus b similar noise add connectivity representation error panel column column similar network connectivity noise c similar b consider signal circle thin black line dot represent contribution neuron signal representation firing rate column signal produce tuning curve column predict accurately rd column b leak membrane potential noise figure performance quadratic programming firing rate prediction mean prediction error absolute difference prediction measurement average neuron second increase line standard deviation prediction large line b mean prediction error line standard deviation prediction error line increase noise prediction error remain represent right leave eye position respectively use relationship develop tuning curve computational function interpret oculomotor tune attempt represent eye position optimally tuning curve produce network represent circular variable sin orientation signal fig c tuning curve individual neuron space value space add noise connectivity tuning curve inhomogeneous highly irregular inhomogeneity little effect signal representation example firing rate prediction closely match firing rate measurement network simulation fig success algorithmic approach calculate firing rate depend success spike network optimise cost function resolution spike algorithm determine leak membrane potential noise large firing rate prediction error large fluctuation optimal firing rate value average prediction error average time neuron remain small similarly membrane potential noise increase fluctuation optimal firing rate average prediction error remain small noise large generate spike input fig discussion conclusion develop new algorithmic technique calculate fire rate tightly balanced network approach require approximation directly identify nonlinear relationship fire rate connectivity input optimal representation identify relationship problem system neuroscience largely mathematical language use describe information representation different language use describe neural network spike statistic tightly balanced network essentially solve problem match firing rate statistic neural activity structure representation nonlinear relationship identify solution quadratic programming problem previous study interpret fire rate result constrain optimisation problem population code model network spike neuron recent study spike network use solve optimisation problem network require positive negative spike difficult biological spiking firing rate tuning curve calculate allow investigate poorly understand feature experimentally record tuning curve particular able evaluate impact tuning curve inhomogeneity neural computation inhomogeneity experimental study difficulty interpret theoretical study treat form noise average find tuning curve inhomogeneity necessarily noise necessarily signal representation propose tuning curve inhomogeneous simply interpretation tuning curve shape quadratic programming approach firing rate calculation promise useful area neuroscience datum analysis possible train framework use neural datum predict firing rate response sensory stimuli study computational impact neural damage tune curve computation acknowledgement like thank helpful comment manuscript grateful funding grant sd grant sd membrane potential noise include network model add wiener process noise term membrane potential equation eqn noise constant reference impulse produce sensory nerve physiology ck distribution neural activity perceptual decisionmake progress neurobiology g orientation selectivity cat cortex invariant stimulus contrast experimental brain research e functional circuitry neural integrator nature neuroscience wiesel receptive field binocular interaction functional architecture cat visual cortex physiological soc field close understand neural computation receptive field auditory neuron biological cybernetic linearity cortical receptive field measure natural sound journal neuroscience official journal society neuroscience sompolinsky theory correlation stochastic physical review e impact network structure response spike time correlation plo computational biology e pouget insight simple expression information connected population spike neuron neural computation sompolinsky chaos neuronal network balanced excitatory inhibitory activity neural computation sompolinsky chaotic balanced state model cortical circuit neural computation b ar network activity vivo generate dynamic balance excitation inhibition journal neuroscience official journal society neuroscience learn optimal spikebase representation advance neural information processing system dynamic encode population neuron m predictive coding dynamical variable balanced spike network plo biology m spikebase population coding work memory plo comput e l convex optimization cortex statistic geometry springer e behavioral constraint determine optimal sensory representation spikebase population code work memory plo comput e
