{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/png;base64,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\" style=\"height: 100px\">\n",
    "<center style=\"color:#888\">Módulo Advanced Data Science<br/>Natural Language Processing</center>\n",
    "\n",
    "# S1. Worksheet. Herramientas clásicas de NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:05:28.295330Z",
     "start_time": "2021-09-10T18:05:28.282708Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install -U nltk spacy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:05:28.311301Z",
     "start_time": "2021-09-10T18:05:28.301319Z"
    }
   },
   "outputs": [],
   "source": [
    "#! pip install -U nltk spacy sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:05:35.421016Z",
     "start_time": "2021-09-10T18:05:28.317299Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook vamos a repasar herramientas clásicas (usadas en modelos de machine learnin, aunque menos en *deep learning*) que pueden sernos útiles para trabajar con texto, y que nos van a permitir dar solución a muchas de las peculiaridades del lenguaje natural.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Pregunta:</b> \n",
    "Un texto es una unidad compleja, que se puede dividir en párrafos, frases, palabras, sílabas o incluso caracteres. ¿A qué nivel crees que tratan los modelos de ML el texto? ¿Cuáles crees que son las ventajas e inconvenientes de trabajar a más alto o bajo nivel?\n",
    "</div>\n",
    "\n",
    "\n",
    "## Simplificación de los datos\n",
    "\n",
    "Uno de los principales problemas de las representaciones clásicas de texto es la dimensionalidad, ya que nos basamos en un diccionario, que fácilmente puede tener $10-100$ K tokens. Así, surge un problema técnico que nos lleva a intentar reducir la dimensionalidad de los vectores utilizados. \n",
    "\n",
    "Podemos ver un texto como una unidad de comunicación humana bastante compleja. Un documento, hasta el más pequeño, está hecho no sólo de palabras, sino de un sinnúmero de relaciones semánticas que solo pueden ser descodificadas por quienes dominan ciertos códigos. En fin, que son un desastre y un dolor de cabeza para quienes están acostumbrados a la información estructurada (e.g., en tablas o esquemas).\n",
    "\n",
    "Extraer automáticamente información relevante de los textos es una tarea trivial para un ser humano, pero un verdadero reto para una máquina. Muchas veces no nos interesa conocer todos los significados de un texto, sino solamente algunos pertinentes para realizar una tarea. Aunque las computadoras (aún) no entienden el lenguaje natural, son muy competentes leyendo superficialmente grandes cantidades de texto en segundos.\n",
    "\n",
    "Una buena técnica para obtener la información relevante de un texto consiste en eliminar los elementos que puedan ser irrelevantes, y resaltar más lo que de verdad contengan información. Las siguientes herramientas tienen el mismo objetivo, y es el de atacar el problema de la dimensionalidad, intentando simplificar a las máquinas el texto libre que reciben.\n",
    "\n",
    "### tokenización\n",
    "\n",
    "Vamos a eliminar las palabras que tienen poco interés para nosotros. El primer paso es delimitar las palabras del texto, y convertir esas palabras en elementos de una lista, los **tokens**. Este procedimiento es conocido como tokenización. Este proceso también está repleto de ambigüedades:\n",
    "* hispano-romano, astur-leonés\n",
    "* aren't, o'neill\n",
    "* whitespace, white space, \n",
    "\n",
    "\n",
    "Hay diversas librerías que nos facilitan la tokenización, y este proceso se puede hacer usando reglas, o mediante modelos estadísticos. Dos librerías que facilitan esta tarea son `spacy` y `nltk`. O se puede hacer directamente en Python si la regla es sencilla, incluso usando expresiones regulares, tema que trataremos en la siguiente sesión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:05:35.437015Z",
     "start_time": "2021-09-10T18:05:35.426020Z"
    }
   },
   "outputs": [],
   "source": [
    "texto = '''El producto cuesta 3€. Este procedimiento es conocido como tokenización. Vamos a implementar esto en Python. Lo ha hecho la empresa TEST S.A. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:05:35.468993Z",
     "start_time": "2021-09-10T18:05:35.441004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El',\n",
       " 'producto',\n",
       " 'cuesta',\n",
       " '3€.',\n",
       " 'Este',\n",
       " 'procedimiento',\n",
       " 'es',\n",
       " 'conocido',\n",
       " 'como',\n",
       " 'tokenización.',\n",
       " 'Vamos',\n",
       " 'a',\n",
       " 'implementar',\n",
       " 'esto',\n",
       " 'en',\n",
       " 'Python.',\n",
       " 'Lo',\n",
       " 'ha',\n",
       " 'hecho',\n",
       " 'la',\n",
       " 'empresa',\n",
       " 'TEST',\n",
       " 'S.A.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería `nltk` tiene un amplio rango de [tokenizers](https://www.nltk.org/api/nltk.tokenize.html), tanto para palabras, como para frases y sílabas. Incluso hay uno específico para tweets `nltk.tokenize.TweetTokenizer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:05:35.499991Z",
     "start_time": "2021-09-10T18:05:35.473003Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El',\n",
       " 'producto',\n",
       " 'cuesta',\n",
       " '3€',\n",
       " '.',\n",
       " 'Este',\n",
       " 'procedimiento',\n",
       " 'es',\n",
       " 'conocido',\n",
       " 'como',\n",
       " 'tokenización.',\n",
       " 'Vamos',\n",
       " 'a',\n",
       " 'implementar',\n",
       " 'esto',\n",
       " 'en',\n",
       " 'Python.',\n",
       " 'Lo',\n",
       " 'ha',\n",
       " 'hecho',\n",
       " 'la',\n",
       " 'empresa',\n",
       " 'TEST',\n",
       " 'S.A.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "tokenizer.tokenize(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:05:50.201601Z",
     "start_time": "2021-09-10T18:05:50.175153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El',\n",
       " 'producto',\n",
       " 'cuesta',\n",
       " '3€.',\n",
       " 'Este',\n",
       " 'procedimiento',\n",
       " 'es',\n",
       " 'conocido',\n",
       " 'como',\n",
       " 'tokenización.',\n",
       " 'Vamos',\n",
       " 'a',\n",
       " 'implementar',\n",
       " 'esto',\n",
       " 'en',\n",
       " 'Python.',\n",
       " 'Lo',\n",
       " 'ha',\n",
       " 'hecho',\n",
       " 'la',\n",
       " 'empresa',\n",
       " 'TEST',\n",
       " 'S.A',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.NLTKWordTokenizer()\n",
    "tokenizer.tokenize(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El recomendado por `nltk`, y que permite elegir el idioma, es `nltk.tokenize.word_tokenize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:06:17.655870Z",
     "start_time": "2021-09-10T18:06:17.604275Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['El',\n",
       " 'producto',\n",
       " 'cuesta',\n",
       " '3€',\n",
       " '.',\n",
       " 'Este',\n",
       " 'procedimiento',\n",
       " 'es',\n",
       " 'conocido',\n",
       " 'como',\n",
       " 'tokenización',\n",
       " '.',\n",
       " 'Vamos',\n",
       " 'a',\n",
       " 'implementar',\n",
       " 'esto',\n",
       " 'en',\n",
       " 'Python',\n",
       " '.',\n",
       " 'Lo',\n",
       " 'ha',\n",
       " 'hecho',\n",
       " 'la',\n",
       " 'empresa',\n",
       " 'TEST',\n",
       " 'S.A',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt') # descarga los archivos necesarios\n",
    "tokenize = lambda x: nltk.tokenize.word_tokenize(x, language='spanish')\n",
    "tokenize(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `spacy`, además de la librería, debes descargar el modelo de la lengua que vas a utilizar. \n",
    "\n",
    "Para descargar el modelo del español, por ejemplo, escribe en tu terminal lo siguiente: \n",
    "\n",
    "`python -m spacy download es`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T07:54:06.162927Z",
     "start_time": "2021-09-12T07:54:03.818845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[x] No compatible package found for 'en_core_news_sm' (spaCy v3.1.2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install spacy\n",
    "#!{sys.executable} -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:09:49.773844Z",
     "start_time": "2021-09-10T18:09:49.083817Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm') # Crea un objeto de spacy tipo nlp, que tokeniza directamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si aplicamos el modelo de lenguaje `nlp` al texto, directamente lo tokeniza, pero además aplica PoS tagging y NER por defecto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T07:35:55.971373Z",
     "start_time": "2021-09-12T07:35:55.942381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El producto cuesta 3€. Este procedimiento es conocido como tokenización. Vamos a implementar esto en Python. Lo ha hecho la empresa TEST S.A. \n",
      "El\n",
      "producto\n",
      "cuesta\n",
      "3€.\n",
      "Este\n",
      "procedimiento\n",
      "es\n",
      "conocido\n",
      "como\n",
      "tokenización\n",
      ".\n",
      "Vamos\n",
      "a\n",
      "implementar\n",
      "esto\n",
      "en\n",
      "Python\n",
      ".\n",
      "Lo\n",
      "ha\n",
      "hecho\n",
      "la\n",
      "empresa\n",
      "TEST\n",
      "S.A.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(texto) \n",
    "print(doc)\n",
    "for t in doc:\n",
    "    print(t.text)\n",
    "# tokens = [t.text for t in doc] # Crea una lista con las palabras del texto\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si lo **único que queremos es tokenizar, es mucho más eficiente** decirle a spacy que sólo haga eso, o crear un tokenizador a partir de `nlp`. Se puede hacer de muchas formas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T08:11:57.018097Z",
     "start_time": "2021-09-12T08:11:56.999504Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['El', 'producto', 'cuesta', '3€.', 'Este', 'procedimiento', 'es', 'conocido', 'como', 'tokenización', '.', 'Vamos', 'a', 'implementar', 'esto', 'en', 'Python', '.', 'Lo', 'ha', 'hecho', 'la', 'empresa', 'TEST', 'S.A.']\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(nlp.pipe_names): # desactivamos todo\n",
    "    doc = nlp(texto)\n",
    "print([tok.text for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T08:12:57.882306Z",
     "start_time": "2021-09-12T08:12:57.871312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El producto cuesta 3€. Este procedimiento es conocido como tokenización. Vamos a implementar esto en Python. Lo ha hecho la empresa TEST S.A. \n"
     ]
    }
   ],
   "source": [
    "doc = list(nlp.pipe([texto],disable=nlp.pipe_names))[0]\n",
    "print([tok.text for tok in doc])\n",
    "#print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T08:11:26.967774Z",
     "start_time": "2021-09-12T08:11:26.953214Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Defaults',\n",
       " '_AnyContext',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_components',\n",
       " '_config',\n",
       " '_disabled',\n",
       " '_factory_meta',\n",
       " '_get_pipe_index',\n",
       " '_link_components',\n",
       " '_meta',\n",
       " '_multiprocessing_pipe',\n",
       " '_optimizer',\n",
       " '_path',\n",
       " '_pipe_configs',\n",
       " '_pipe_meta',\n",
       " 'add_pipe',\n",
       " 'analyze_pipes',\n",
       " 'batch_size',\n",
       " 'begin_training',\n",
       " 'component',\n",
       " 'component_names',\n",
       " 'components',\n",
       " 'config',\n",
       " 'create_optimizer',\n",
       " 'create_pipe',\n",
       " 'create_pipe_from_source',\n",
       " 'default_config',\n",
       " 'default_error_handler',\n",
       " 'disable_pipe',\n",
       " 'disable_pipes',\n",
       " 'disabled',\n",
       " 'enable_pipe',\n",
       " 'evaluate',\n",
       " 'factories',\n",
       " 'factory',\n",
       " 'factory_names',\n",
       " 'from_bytes',\n",
       " 'from_config',\n",
       " 'from_disk',\n",
       " 'get_factory_meta',\n",
       " 'get_factory_name',\n",
       " 'get_pipe',\n",
       " 'get_pipe_config',\n",
       " 'get_pipe_meta',\n",
       " 'has_factory',\n",
       " 'has_pipe',\n",
       " 'initialize',\n",
       " 'lang',\n",
       " 'make_doc',\n",
       " 'max_length',\n",
       " 'meta',\n",
       " 'path',\n",
       " 'pipe',\n",
       " 'pipe_factories',\n",
       " 'pipe_labels',\n",
       " 'pipe_names',\n",
       " 'pipeline',\n",
       " 'rehearse',\n",
       " 'remove_pipe',\n",
       " 'rename_pipe',\n",
       " 'replace_listeners',\n",
       " 'replace_pipe',\n",
       " 'resume_training',\n",
       " 'select_pipes',\n",
       " 'set_error_handler',\n",
       " 'set_factory_meta',\n",
       " 'to_bytes',\n",
       " 'to_disk',\n",
       " 'tokenizer',\n",
       " 'update',\n",
       " 'use_params',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm') # Crea un objeto de spacy tipo nlp, que tokeniza directamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "tokens = [t.text for t in tokenizer(texto)] # Crea una lista con las palabras del texto\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Son palabras excesivamente comunes que no añaden significado a un texto, debido al gran tamaño de los vocabularios y por ende, de los vectores que codificaban texto (*sparsity*), se eliminaban directamente. La eliminación de las conocidas como **stop words** no es obligatoria, y su utilización ha disminuido recientemente. Queda muy bien explicado en el siguiente fragmento de [Intro to Information Retrieval](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html):\n",
    "\n",
    "\n",
    "    Some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words.\n",
    "\n",
    "    The general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists.\n",
    "\n",
    "Las listas de *stop words* no son únicas, y diferentes librerías tienen listas distintas, como veremos a continuación. Además, lo que es y no es una stop word **depende del problema**, ya que una misma palabra puede tener importancia en un contexto, pero no en otro.\n",
    "\n",
    "Además de las stop words, también es común eliminar la **puntuación** y/o **símbolos** `@,$,€,#` que creemos que no nos van a aportar significado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` también tiene su lista de stop words en inglés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:16:50.626955Z",
     "start_time": "2021-09-10T18:16:50.610371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "sk_stopwords = sklearn.feature_extraction.text.ENGLISH_STOP_WORDS\n",
    "print(sorted(list(sk_stopwords))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos ahora a las librerías especializadas en NLP, como `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:17:00.189070Z",
     "start_time": "2021-09-10T18:17:00.168447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "print(sorted(nltk_stopwords.words('english'))[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk` tiene listas de stop words en varios idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:17:18.231994Z",
     "start_time": "2021-09-10T18:17:18.217404Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual', 'cuando', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el', 'ella']\n"
     ]
    }
   ],
   "source": [
    "stopwords_sp_nltk = sorted(nltk_stopwords.words('spanish'))\n",
    "print(stopwords_sp_nltk[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer lo mismo con `spacy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:17:22.827006Z",
     "start_time": "2021-09-10T18:17:22.808467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actualmente', 'acuerdo', 'adelante', 'ademas', 'además', 'adrede', 'afirmó', 'agregó', 'ahi', 'ahora', 'ahí', 'al', 'algo', 'alguna', 'algunas', 'alguno', 'algunos', 'algún', 'alli', 'allí']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(spacy.lang.es.stop_words.STOP_WORDS)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Ejercicio:</b> \n",
    "Comprueba si palabras como <code>['nada','no','mal','bien','bueno','ninguno']</code> son <i>stop words</i>. ¿Tiene esto sentido?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:18:16.813585Z",
     "start_time": "2021-09-10T18:18:16.801071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_spacy = spacy.lang.es.stop_words.STOP_WORDS\n",
    "\"hola\" in stopwords_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:18:32.112407Z",
     "start_time": "2021-09-10T18:18:32.097227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:18:34.493489Z",
     "start_time": "2021-09-10T18:18:34.472495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:18:57.348954Z",
     "start_time": "2021-09-10T18:18:57.329949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nada' is stop word in nltk: True\n",
      "'nada' is stop word in spacy: True\n",
      "'no' is stop word in nltk: True\n",
      "'no' is stop word in spacy: True\n",
      "'mal' is stop word in nltk: False\n",
      "'mal' is stop word in spacy: True\n",
      "'bien' is stop word in nltk: False\n",
      "'bien' is stop word in spacy: True\n",
      "'bueno' is stop word in nltk: False\n",
      "'bueno' is stop word in spacy: True\n",
      "'ninguno' is stop word in nltk: False\n",
      "'ninguno' is stop word in spacy: True\n"
     ]
    }
   ],
   "source": [
    "stopwords_sp = sorted(spacy.lang.es.stop_words.STOP_WORDS)\n",
    "for word in ['nada','no','mal','bien','bueno','ninguno']:\n",
    "    for lib, sw in zip(['nltk','spacy'],[stopwords_sp_nltk, stopwords_sp]):\n",
    "        print(f\"'{word}' is stop word in {lib}: {word in sw}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Ejercicio:</b> \n",
    "    Calcula los tokens que estén en la lista de <i>stop words</i> de <code>spacy</code>, pero no en la de <code>nltk</code>. También la longitud de cada conjunto. Por ejemplo, para el idioma español.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:21:02.597304Z",
     "start_time": "2021-09-10T18:21:02.573702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'e',\n",
       " 'erais',\n",
       " 'estabais',\n",
       " 'estabas',\n",
       " 'estad',\n",
       " 'estada',\n",
       " 'estadas',\n",
       " 'estando',\n",
       " 'estaremos',\n",
       " 'estarán',\n",
       " 'estarás',\n",
       " 'estaré',\n",
       " 'estaréis',\n",
       " 'estaría',\n",
       " 'estaríais',\n",
       " 'estaríamos',\n",
       " 'estarían',\n",
       " 'estarías',\n",
       " 'estemos',\n",
       " 'estuve',\n",
       " 'estuviera',\n",
       " 'estuvierais',\n",
       " 'estuvieran',\n",
       " 'estuvieras',\n",
       " 'estuvieron',\n",
       " 'estuviese',\n",
       " 'estuvieseis',\n",
       " 'estuviesen',\n",
       " 'estuvieses',\n",
       " 'estuvimos',\n",
       " 'estuviste',\n",
       " 'estuvisteis',\n",
       " 'estuviéramos',\n",
       " 'estuviésemos',\n",
       " 'estábamos',\n",
       " 'estáis',\n",
       " 'estás',\n",
       " 'esté',\n",
       " 'estéis',\n",
       " 'estén',\n",
       " 'estés',\n",
       " 'fuerais',\n",
       " 'fueran',\n",
       " 'fueras',\n",
       " 'fuese',\n",
       " 'fueseis',\n",
       " 'fuesen',\n",
       " 'fueses',\n",
       " 'fuiste',\n",
       " 'fuisteis',\n",
       " 'fuéramos',\n",
       " 'fuésemos',\n",
       " 'habida',\n",
       " 'habidas',\n",
       " 'habido',\n",
       " 'habidos',\n",
       " 'habiendo',\n",
       " 'habremos',\n",
       " 'habrán',\n",
       " 'habrás',\n",
       " 'habré',\n",
       " 'habréis',\n",
       " 'habría',\n",
       " 'habríais',\n",
       " 'habríamos',\n",
       " 'habrían',\n",
       " 'habrías',\n",
       " 'habéis',\n",
       " 'habíais',\n",
       " 'habíamos',\n",
       " 'habías',\n",
       " 'has',\n",
       " 'hayamos',\n",
       " 'hayan',\n",
       " 'hayas',\n",
       " 'hayáis',\n",
       " 'hube',\n",
       " 'hubiera',\n",
       " 'hubierais',\n",
       " 'hubieran',\n",
       " 'hubieras',\n",
       " 'hubieron',\n",
       " 'hubiese',\n",
       " 'hubieseis',\n",
       " 'hubiesen',\n",
       " 'hubieses',\n",
       " 'hubimos',\n",
       " 'hubiste',\n",
       " 'hubisteis',\n",
       " 'hubiéramos',\n",
       " 'hubiésemos',\n",
       " 'o',\n",
       " 'seamos',\n",
       " 'seas',\n",
       " 'sentid',\n",
       " 'sentida',\n",
       " 'sentidas',\n",
       " 'sentido',\n",
       " 'sentidos',\n",
       " 'seremos',\n",
       " 'serás',\n",
       " 'seré',\n",
       " 'seréis',\n",
       " 'seríais',\n",
       " 'seríamos',\n",
       " 'serían',\n",
       " 'serías',\n",
       " 'seáis',\n",
       " 'siente',\n",
       " 'sintiendo',\n",
       " 'suyos',\n",
       " 'tendremos',\n",
       " 'tendrás',\n",
       " 'tendré',\n",
       " 'tendréis',\n",
       " 'tendría',\n",
       " 'tendríais',\n",
       " 'tendríamos',\n",
       " 'tendrían',\n",
       " 'tendrías',\n",
       " 'tened',\n",
       " 'tengamos',\n",
       " 'tengan',\n",
       " 'tengas',\n",
       " 'tengáis',\n",
       " 'tenida',\n",
       " 'tenidas',\n",
       " 'tenidos',\n",
       " 'teniendo',\n",
       " 'tenéis',\n",
       " 'teníais',\n",
       " 'teníamos',\n",
       " 'tenían',\n",
       " 'tenías',\n",
       " 'tienes',\n",
       " 'tuve',\n",
       " 'tuviera',\n",
       " 'tuvierais',\n",
       " 'tuvieran',\n",
       " 'tuvieras',\n",
       " 'tuvieron',\n",
       " 'tuviese',\n",
       " 'tuvieseis',\n",
       " 'tuviesen',\n",
       " 'tuvieses',\n",
       " 'tuvimos',\n",
       " 'tuviste',\n",
       " 'tuvisteis',\n",
       " 'tuviéramos',\n",
       " 'tuviésemos',\n",
       " 'y',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords_sp_nltk) - set(stopwords_sp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Ejercicio:</b>\n",
    "¿En qué tareas de NLP la eliminación de <i>stop words</i> no es viable?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En generación de lenguaje,\n",
    "Traducción.\n",
    "Resumen.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas tareas donde no es aplicable porque se pierde información esencial serían:\n",
    "\n",
    "* Traducción automática\n",
    "* Modelado del lenguage (Language Modeling)\n",
    "* Resumen automático de textos\n",
    "* Sistemas conversacionales o de Pregunta Respuesta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming y Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En general, podemos observar que hay palabras diferentes que representan significados parecidos. En español, por ejemplo, sabemos que *canto, cantas, canta, cantamos, cantáis, cantan* son distintas formas (conjugaciones) de un mismo verbo (*cantar*). Y que *niña, niño, niñita, niños, niñotes, y otras más*, son distintas formas del vocablo *niño*. Así que sería genial poder obviar las diferencias y juntar todas estas variantes en un mismo token.\n",
    "Stemming y Lemmatization son dos transformaciones parecidas, que nos devuelven el lema o raiz de la palabra.\n",
    "\n",
    "* **Lematización**: relaciona una palabra flexionada o derivada con su forma **canónica o lema**. Y un lema no es otra cosa que la **forma que tienen las palabras cuando las buscas en el diccionario**. La lematización tiene dos costes. Primero, es un proceso que consume recursos (sobre todo tiempo). Segundo, suele ser probabilística, así que en algunos casos obtendremos resultados inesperados.\n",
    "\n",
    "\n",
    "* **Radicalización** o **stemming**: procedimiento de convertir palabras en raíces. Estas raíces son la parte invariable de palabras relacionadas sobre todo por su forma. De cierta manera se parece a la lematización, pero los resultados (las raíces) no tienen por qué ser palabras de un idioma. Por ejemplo, el algoritmo de stemming puede decidir que la raíz de *amamos* no es *am-* sino *amam-*. El stemming es mucho más rápido, ya que es un procedimiento heurístico que recorta el final de las palabras.\n",
    "\n",
    "\"Stemming is the poor-man’s lemmatization.\" (Noah Smith, 2011)\n",
    "\n",
    "**Es importante notar que ambos procesos dependen altamente del idioma.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lematizar es importante la etiqueta *Part of Speech* (POS), es decir, si la palabra es un sustantivo, verbo, etc. En el caso de `nltk`, hay que indicar en cada caso esta etiqueta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:23:31.703828Z",
     "start_time": "2021-09-10T18:23:31.560903Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\JPAS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:23:34.699526Z",
     "start_time": "2021-09-10T18:23:34.679993Z"
    }
   },
   "outputs": [],
   "source": [
    "word_list = ['feet', 'foot', 'foots', 'footing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:23:53.175031Z",
     "start_time": "2021-09-10T18:23:49.632704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foot', 'foot', 'foot', 'footing']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "[wnl.lemmatize(word, pos='n') for word in word_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos un lematizador en español, tenemos que ir a `spacy`, que además de manera automática aplica un **PoS-tagger** y hace muy sencillo obtener el *lema*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:24:07.282523Z",
     "start_time": "2021-09-10T18:24:06.599953Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:24:10.380060Z",
     "start_time": "2021-09-10T18:24:10.345452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ella -> él\n",
      "se -> él\n",
      "compró -> comprar\n",
      "el -> el\n",
      "coche -> coche\n",
      "más -> más\n",
      "caro -> caro\n",
      "de -> de\n",
      "la -> el\n",
      "marca -> marca\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Ella se compró el coche más caro de la marca.\")\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "for tok in doc:\n",
    "    print(tok.text, '->', tok.lemma_.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:24:36.715008Z",
     "start_time": "2021-09-10T18:24:36.678385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Él -> él\n",
      "se -> él\n",
      "compró -> comprar\n",
      "el -> el\n",
      "coche -> coche\n",
      "más -> más\n",
      "caro -> caro\n",
      "de -> de\n",
      "la -> el\n",
      "marca -> marca\n",
      ". -> .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Él se compró el coche más caro de la marca.\")\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "for tok in doc:\n",
    "    print(tok.text, '->', tok.lemma_.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:24:40.918562Z",
     "start_time": "2021-09-10T18:24:40.863012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El -> el\n",
      "producto -> producto\n",
      "cuesta -> costar\n",
      "3€. -> 3€.\n",
      "Este -> este\n",
      "procedimiento -> procedimiento\n",
      "es -> ser\n",
      "conocido -> conocer\n",
      "como -> como\n",
      "tokenización -> tokenización\n",
      ". -> .\n",
      "Vamos -> ir\n",
      "a -> a\n",
      "implementar -> implementar\n",
      "esto -> este\n",
      "en -> en\n",
      "Python -> python\n",
      ". -> .\n",
      "Lo -> él\n",
      "ha -> haber\n",
      "hecho -> hacer\n",
      "la -> el\n",
      "empresa -> empresa\n",
      "TEST -> test\n",
      "S.A. -> s.a.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(texto)\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "for tok in doc:\n",
    "    print(tok.text, '->', tok.lemma_.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque te parezca sorprendente, `spacy` no contiene ninguna función para stemming, ya que se basa únicamente en la lematización. Por lo tanto, en esta sección, utilizaremos únicamente `NLTK`.\n",
    "\n",
    "\n",
    "Hay dos tipos de stemmers en NLTK: [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/) and [Snowball stemmers](https://tartarus.org/martin/PorterStemmer/). Cada uno de ellos se ha implementado siguiendo algoritmos distintos. Snowball stemmer es una versión ligeramente mejorada del Porter stemmer y generalmente se prefiere sobre este último, además, **sólo Snowball stemmer funciona para idiomas distintos al inglés**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:25:02.444716Z",
     "start_time": "2021-09-10T18:25:02.418187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El -> el\n",
      "producto -> product\n",
      "cuesta -> cuest\n",
      "3€. -> 3€.\n",
      "Este -> este\n",
      "procedimiento -> proced\n",
      "es -> es\n",
      "conocido -> conoc\n",
      "como -> com\n",
      "tokenización. -> tokenizacion.\n",
      "Vamos -> vam\n",
      "a -> a\n",
      "implementar -> implement\n",
      "esto -> esto\n",
      "en -> en\n",
      "Python. -> python.\n",
      "Lo -> lo\n",
      "ha -> ha\n",
      "hecho -> hech\n",
      "la -> la\n",
      "empresa -> empres\n",
      "TEST -> test\n",
      "S.A. -> s.a.\n"
     ]
    }
   ],
   "source": [
    "sp_snowball = nltk.SnowballStemmer('spanish')\n",
    "tokens = texto.split()  # crear una lista de tokens\n",
    "stems = [sp_snowball.stem(token) for token in tokens]\n",
    "for w, s in zip(tokens, stems):\n",
    "    print(w, '->', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Ejercicio:</b> \n",
    "    Prueba a lematizar y radicalizar las siguientes palabras:\n",
    "\n",
    "* Inglés: fly, flies, flying\n",
    "* Español: universo, universidad, universal\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:28:15.959436Z",
     "start_time": "2021-09-10T18:28:15.935690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fly', 'fly', 'fly']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "[wnl.lemmatize(word, pos='v') for word in [\"fly\", \"flies\", \"flying\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:28:18.811430Z",
     "start_time": "2021-09-10T18:28:18.782879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universo -> universo\n",
      ", -> ,\n",
      "universidad -> universidad\n",
      ", -> ,\n",
      "universal -> universal\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"universo, universidad, universal\")\n",
    "lemmas = [tok.lemma_.lower() for tok in doc]\n",
    "for tok in doc:\n",
    "    print(tok.text, '->', tok.lemma_.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:28:22.950078Z",
     "start_time": "2021-09-10T18:28:22.933908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "universo -> univers\n",
      "universidad -> univers\n",
      "universal -> universal\n"
     ]
    }
   ],
   "source": [
    "sp_snowball = nltk.SnowballStemmer('spanish')\n",
    "tokens = \"universo, universidad, universal\".split(\", \")  # crear una lista de tokens\n",
    "stems = [sp_snowball.stem(token) for token in tokens]\n",
    "for w, s in zip(tokens, stems):\n",
    "    print(w, '->', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy vs NLTK\n",
    "\n",
    "La librería spaCy es una de las más populares en NLP junto con NLTK. La diferencia básica entre las dos es el hecho de que **NLTK contiene una amplia variedad de algoritmos** para resolver un problema, mientras que **spaCy contiene solo uno, el mejor**.\n",
    "\n",
    "NLTK se lanzó en 2001, mientras que spaCy es relativamente nuevo y se desarrolló en 2015. En este curso sobre NLP, trataremos principalmente de spaCy, debido a que en general es **state of the art**. Sin embargo, también usaremos NLTK cuando sea más fácil realizar una tarea que con spaCy.\n",
    "\n",
    "**Si puedes hacerlo en spaCy, mejor usar spacy.** Además, la documentación de spaCy es mucho más intuitiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuándo usar estos métodos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas técnicas fueron consideradas técnicas estándar durante mucho tiempo, pero a menudo pueden **dañar** el rendimiento **si se utiliza deep learning**. El stemming, la lematización y la eliminación *stop words* implican una pérdida de información en muchos casos.\n",
    "\n",
    "Sin embargo, aún pueden ser útiles cuando se trabaja con modelos más simples. A grandes rasgos, estamos simplificando los datos, lo que es una técnica de **regularización**, es decir, evitamos el **overfitting** a features que sólo nos generan ruido. \n",
    "\n",
    "Como siempre en el Machine Learning, no es fácil situar la línea entre el ruido y la señal. Sólo la experiencia, y el sobre todo tiempo para validar entre distintas opciones deben ser la guía que nos indique qué usar en el modelo final.\n",
    "\n",
    "\n",
    "<img src=\"resources/skomoroch.png\" alt=\"\" style=\"width: 65%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizando texto\n",
    "\n",
    "Hasta ahora hemos estudiado formas de reducir la complejidad del texto, todas ellas ayudándonos a reducir el tamaño del vocabulario (conjunto de palabras/tokens únicos) de nuestros modelos. Los algoritmos de **Machine Learning** trabajan con datos numéricos, no pueden usar el texto directamente. Hay muchas formas de convertir el texto en vectores numéricos (**feature engineering**), y vamos a explorar las dos formas más básicas.\n",
    "\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "Para crear la representación de *bag of words*, cada token es una dimensión en un espacio de dimensión el tamaño del vocabulario $V$. Para construir el vector de una frase, será la suma de los vectores de cada token, es decir, convertimos cada frase en un vector que cuenta el número de ocurrencias de cada token. Hacemos la distinción entre palabra y token porque **se pueden usar como tokens n-gramas de palabras**, lo que aumenta la complejidad del modelo, pero permite captar correlaciones en el orden de las palabras.\n",
    "\n",
    "\n",
    "Se siguien los pasos:\n",
    "1. Encuentra los **V** tokens mas comunes del corpus de entrenamiento y se les asigna un índice, este es nuestro **vocabulario**. Creamos un diccionario para convertir de tokens a índices y viceversa.\n",
    "2. Para cada frase en el corpus, creamos un vector de dimensión **V** y lo inicializamos con ceros.\n",
    "3. Iteramos sobre los tokens de cada frase, y si el token está en el diccionario, incrementamos en 1 el índice correspondiente del vector.\n",
    "\n",
    "Veamos el siguiente ejemplo con **V=4**, cuyo vocabulario es:\n",
    "\n",
    "    ['hi', 'you', 'me', 'are']\n",
    "\n",
    "Les asignamos un índice: \n",
    "\n",
    "    {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n",
    "\n",
    "Y si tenemos la siguiente frase:\n",
    "\n",
    "    'hi, how are you? are you ok?'\n",
    "\n",
    "que hemos preprocesado a la siguiente lista de tokens:\n",
    "\n",
    "    ['hi', 'how', 'are', 'you', 'are', 'you', 'ok']\n",
    "\n",
    "Inicializamos el vector:\n",
    "\n",
    "    [0, 0, 0, 0]\n",
    "    \n",
    "Iteramos sobre las palabras, teniendo en cuenta sólo aquellas en nuestro vocabulario:\n",
    "\n",
    "    'hi':  [1, 0, 0, 0]\n",
    "    'how': [1, 0, 0, 0] # word 'how' is not in our dictionary\n",
    "    'are': [1, 0, 0, 1]\n",
    "    'you': [1, 1, 0, 1]\n",
    "    'are': [1, 1, 0, 2]\n",
    "    'you': [1, 2, 0, 2]\n",
    "    'ok':  [1, 2, 0, 2] # word 'ok' is not in our dictionary\n",
    "\n",
    "El vector resultante es:\n",
    "\n",
    "    [1, 2, 0, 2]\n",
    "    \n",
    "En el [s1_challenge](./s1_challenge.ipynb) construiremos este vectorizador a mano, pero viene implementado en sklearn en la función [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Ejercicio:</b> \n",
    "    Usando el vocabulario del ejemplo anterior, cuál sería el vector <i>bag of words</i> de la frase con tokens\n",
    "\n",
    "* <code>['hi', 'i', 'am', 'david,', 'and', 'you?', 'are', 'you', 'ok?']</code>\n",
    "* <code>{'hi': 0, 'you': 1, 'me': 2, 'are': 3}</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Ejercicio:</b> \n",
    "    La variable <code>quijote</code> contiene el texto completo del Quijote:\n",
    "    \n",
    "* ¿De qué tamaño serán los vectores <i>bag of words</i> sin utilizar ninguna de las técnicas de reducción anteriores? Tokeniza con <code>str.split()</code> y con el tokenizador de una de las librerías.\n",
    "* ¿Por qué si tokenizamos con la librería cambia tanto el tamaño del vocabulario?\n",
    "* ¿Y si sólo trabajas con minúsculas?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:37:41.277112Z",
     "start_time": "2021-09-10T18:37:41.253523Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./resources/quijote_largo.txt', 'r', encoding='UTF-8') as f:\n",
    "    quijote = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:37:44.982849Z",
     "start_time": "2021-09-10T18:37:44.894755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22938"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(quijote.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:37:57.601315Z",
     "start_time": "2021-09-10T18:37:52.556930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15615"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "tokens = [t.text for t in tokenizer(quijote)] # Crea una lista con las palabras del texto\n",
    "len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ESCRIBE AQUÍ LA SOLUCIÓN\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:39:44.159996Z",
     "start_time": "2021-09-10T18:39:44.083410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22938"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tamaño del vocabulario\n",
    "\n",
    "len(set(quijote.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:39:47.024019Z",
     "start_time": "2021-09-10T18:39:46.940424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22211"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# esto con minusculas\n",
    "len(set(quijote.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:39:49.463824Z",
     "start_time": "2021-09-10T18:39:49.403825Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DON',\n",
       " 'QUIJOTE',\n",
       " 'DE',\n",
       " 'LA',\n",
       " 'MANCHA',\n",
       " 'Miguel',\n",
       " 'de',\n",
       " 'Cervantes',\n",
       " 'Saavedra',\n",
       " 'PRIMERA',\n",
       " 'PARTE',\n",
       " 'CAPÍTULO',\n",
       " '1:',\n",
       " 'Que',\n",
       " 'trata',\n",
       " 'de',\n",
       " 'la',\n",
       " 'condición',\n",
       " 'y',\n",
       " 'ejercicio',\n",
       " 'del',\n",
       " 'famoso',\n",
       " 'hidalgo',\n",
       " 'D.',\n",
       " 'Quijote',\n",
       " 'de',\n",
       " 'la',\n",
       " 'Mancha',\n",
       " 'En',\n",
       " 'un',\n",
       " 'lugar',\n",
       " 'de',\n",
       " 'la',\n",
       " 'Mancha,',\n",
       " 'de',\n",
       " 'cuyo',\n",
       " 'nombre',\n",
       " 'no',\n",
       " 'quiero',\n",
       " 'acordarme,',\n",
       " 'no',\n",
       " 'ha',\n",
       " 'mucho',\n",
       " 'tiempo',\n",
       " 'que',\n",
       " 'vivía',\n",
       " 'un',\n",
       " 'hidalgo',\n",
       " 'de',\n",
       " 'los',\n",
       " 'de',\n",
       " 'lanza',\n",
       " 'en',\n",
       " 'astillero,',\n",
       " 'adarga',\n",
       " 'antigua,',\n",
       " 'rocín',\n",
       " 'flaco',\n",
       " 'y',\n",
       " 'galgo',\n",
       " 'corredor.',\n",
       " 'Una',\n",
       " 'olla',\n",
       " 'de',\n",
       " 'algo',\n",
       " 'más',\n",
       " 'vaca',\n",
       " 'que',\n",
       " 'carnero,',\n",
       " 'salpicón',\n",
       " 'las',\n",
       " 'más',\n",
       " 'noches,',\n",
       " 'duelos',\n",
       " 'y',\n",
       " 'quebrantos',\n",
       " 'los',\n",
       " 'sábados,',\n",
       " 'lentejas',\n",
       " 'los',\n",
       " 'viernes,',\n",
       " 'algún',\n",
       " 'palomino',\n",
       " 'de',\n",
       " 'añadidura',\n",
       " 'los',\n",
       " 'domingos,',\n",
       " 'consumían',\n",
       " 'las',\n",
       " 'tres',\n",
       " 'partes',\n",
       " 'de',\n",
       " 'su',\n",
       " 'hacienda.',\n",
       " 'El',\n",
       " 'resto',\n",
       " 'della',\n",
       " 'concluían',\n",
       " 'sayo',\n",
       " 'de',\n",
       " 'velarte,',\n",
       " 'calzas',\n",
       " 'de',\n",
       " 'velludo',\n",
       " 'para',\n",
       " 'las',\n",
       " 'fiestas',\n",
       " 'con',\n",
       " 'sus',\n",
       " 'pantuflos',\n",
       " 'de',\n",
       " 'lo',\n",
       " 'mismo,',\n",
       " 'los',\n",
       " 'días',\n",
       " 'de',\n",
       " 'entre',\n",
       " 'semana',\n",
       " 'se',\n",
       " 'honraba',\n",
       " 'con',\n",
       " 'su',\n",
       " 'vellori',\n",
       " 'de',\n",
       " 'lo',\n",
       " 'más',\n",
       " 'fino.',\n",
       " 'Tenía',\n",
       " 'en',\n",
       " 'su',\n",
       " 'casa',\n",
       " 'una',\n",
       " 'ama',\n",
       " 'que',\n",
       " 'pasaba',\n",
       " 'de',\n",
       " 'los',\n",
       " 'cuarenta,',\n",
       " 'y',\n",
       " 'una',\n",
       " 'sobrina',\n",
       " 'que',\n",
       " 'no',\n",
       " 'llegaba',\n",
       " 'a',\n",
       " 'los',\n",
       " 'veinte,',\n",
       " 'y',\n",
       " 'un',\n",
       " 'mozo',\n",
       " 'de',\n",
       " 'campo',\n",
       " 'y',\n",
       " 'plaza,',\n",
       " 'que',\n",
       " 'así',\n",
       " 'ensillaba',\n",
       " 'el',\n",
       " 'rocín',\n",
       " 'como',\n",
       " 'tomaba',\n",
       " 'la',\n",
       " 'podadera.',\n",
       " 'Frisaba',\n",
       " 'la',\n",
       " 'edad',\n",
       " 'de',\n",
       " 'nuestro',\n",
       " 'hidalgo',\n",
       " 'con',\n",
       " 'los',\n",
       " 'cincuenta',\n",
       " 'años,',\n",
       " 'era',\n",
       " 'de',\n",
       " 'complexión',\n",
       " 'recia,',\n",
       " 'seco',\n",
       " 'de',\n",
       " 'carnes,',\n",
       " 'enjuto',\n",
       " 'de',\n",
       " 'rostro;',\n",
       " 'gran',\n",
       " 'madrugador',\n",
       " 'y',\n",
       " 'amigo',\n",
       " 'de',\n",
       " 'la',\n",
       " 'caza.',\n",
       " 'Quieren',\n",
       " 'decir',\n",
       " 'que',\n",
       " 'tenía',\n",
       " 'el',\n",
       " 'sobrenombre',\n",
       " 'de',\n",
       " 'Quijada',\n",
       " 'o',\n",
       " 'Quesada']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quijote.split()[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:40:02.406610Z",
     "start_time": "2021-09-10T18:40:00.347881Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DON',\n",
       " 'QUIJOTE',\n",
       " 'DE',\n",
       " 'LA',\n",
       " 'MANCHA',\n",
       " 'Miguel',\n",
       " 'de',\n",
       " 'Cervantes',\n",
       " 'Saavedra',\n",
       " 'PRIMERA',\n",
       " 'PARTE',\n",
       " 'CAPÍTULO',\n",
       " '1',\n",
       " ':',\n",
       " 'Que',\n",
       " 'trata',\n",
       " 'de',\n",
       " 'la',\n",
       " 'condición',\n",
       " 'y',\n",
       " 'ejercicio',\n",
       " 'del',\n",
       " 'famoso',\n",
       " 'hidalgo',\n",
       " 'D.',\n",
       " 'Quijote',\n",
       " 'de',\n",
       " 'la',\n",
       " 'Mancha',\n",
       " 'En',\n",
       " 'un',\n",
       " 'lugar',\n",
       " 'de',\n",
       " 'la',\n",
       " 'Mancha',\n",
       " ',',\n",
       " 'de',\n",
       " 'cuyo',\n",
       " 'nombre',\n",
       " 'no',\n",
       " 'quiero',\n",
       " 'acordarme',\n",
       " ',',\n",
       " 'no',\n",
       " 'ha',\n",
       " 'mucho',\n",
       " 'tiempo',\n",
       " 'que',\n",
       " 'vivía',\n",
       " 'un',\n",
       " 'hidalgo',\n",
       " 'de',\n",
       " 'los',\n",
       " 'de',\n",
       " 'lanza',\n",
       " 'en',\n",
       " 'astillero',\n",
       " ',',\n",
       " 'adarga',\n",
       " 'antigua',\n",
       " ',',\n",
       " 'rocín',\n",
       " 'flaco',\n",
       " 'y',\n",
       " 'galgo',\n",
       " 'corredor',\n",
       " '.',\n",
       " 'Una',\n",
       " 'olla',\n",
       " 'de',\n",
       " 'algo',\n",
       " 'más',\n",
       " 'vaca',\n",
       " 'que',\n",
       " 'carnero',\n",
       " ',',\n",
       " 'salpicón',\n",
       " 'las',\n",
       " 'más',\n",
       " 'noches',\n",
       " ',',\n",
       " 'duelos',\n",
       " 'y',\n",
       " 'quebrantos',\n",
       " 'los',\n",
       " 'sábados',\n",
       " ',',\n",
       " 'lentejas',\n",
       " 'los',\n",
       " 'viernes',\n",
       " ',',\n",
       " 'algún',\n",
       " 'palomino',\n",
       " 'de',\n",
       " 'añadidura',\n",
       " 'los',\n",
       " 'domingos',\n",
       " ',',\n",
       " 'consumían',\n",
       " 'las',\n",
       " 'tres',\n",
       " 'partes',\n",
       " 'de',\n",
       " 'su',\n",
       " 'hacienda',\n",
       " '.',\n",
       " 'El',\n",
       " 'resto',\n",
       " 'della',\n",
       " 'concluían',\n",
       " 'sayo',\n",
       " 'de',\n",
       " 'velarte',\n",
       " ',',\n",
       " 'calzas',\n",
       " 'de',\n",
       " 'velludo',\n",
       " 'para',\n",
       " 'las',\n",
       " 'fiestas',\n",
       " 'con',\n",
       " 'sus',\n",
       " 'pantuflos',\n",
       " 'de',\n",
       " 'lo',\n",
       " 'mismo',\n",
       " ',',\n",
       " 'los',\n",
       " 'días',\n",
       " 'de',\n",
       " 'entre',\n",
       " 'semana',\n",
       " 'se',\n",
       " 'honraba',\n",
       " 'con',\n",
       " 'su',\n",
       " 'vellori',\n",
       " 'de',\n",
       " 'lo',\n",
       " 'más',\n",
       " 'fino',\n",
       " '.',\n",
       " 'Tenía',\n",
       " 'en',\n",
       " 'su',\n",
       " 'casa',\n",
       " 'una',\n",
       " 'ama',\n",
       " 'que',\n",
       " 'pasaba',\n",
       " 'de',\n",
       " 'los',\n",
       " 'cuarenta',\n",
       " ',',\n",
       " 'y',\n",
       " 'una',\n",
       " 'sobrina',\n",
       " 'que',\n",
       " 'no',\n",
       " 'llegaba',\n",
       " 'a',\n",
       " 'los',\n",
       " 'veinte',\n",
       " ',',\n",
       " 'y',\n",
       " 'un',\n",
       " 'mozo',\n",
       " 'de',\n",
       " 'campo',\n",
       " 'y',\n",
       " 'plaza',\n",
       " ',',\n",
       " 'que',\n",
       " 'así',\n",
       " 'ensillaba',\n",
       " 'el',\n",
       " 'rocín',\n",
       " 'como',\n",
       " 'tomaba',\n",
       " 'la',\n",
       " 'podadera',\n",
       " '.',\n",
       " 'Frisaba',\n",
       " 'la',\n",
       " 'edad',\n",
       " 'de',\n",
       " 'nuestro',\n",
       " 'hidalgo',\n",
       " 'con',\n",
       " 'los',\n",
       " 'cincuenta',\n",
       " 'años',\n",
       " ',',\n",
       " 'era',\n",
       " 'de',\n",
       " 'complexión',\n",
       " 'recia',\n",
       " ',',\n",
       " 'seco',\n",
       " 'de']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(quijote)[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:40:30.624288Z",
     "start_time": "2021-09-10T18:40:28.548988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15804"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize = lambda x: nltk.tokenize.word_tokenize(x, language='spanish')\n",
    "len(set(tokenize(quijote)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:40:36.082775Z",
     "start_time": "2021-09-10T18:40:33.995191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15087"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(tokenize(quijote.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-10T18:40:44.153118Z",
     "start_time": "2021-09-10T18:40:44.068530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python split, V: 22938\n"
     ]
    }
   ],
   "source": [
    "print(f'Python split, V: {len(set(quijote.split()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f'NLTK word_tokenize, V: {len(set(nltk.tokenize.word_tokenize(quijote)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un Tokenizer de la nada, sólo teniendo en cuenta el vocabulario\n",
    "tokenizer_vocab = spacy.tokenizer.Tokenizer(nlp.vocab)\n",
    "print(\n",
    "    f'spacy tokenizer, V: { len({token.text for token in tokenizer_vocab(quijote)})} '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un Tokenizer con las opciones por defecto de un idioma\n",
    "# en este caso incluye el vocabulario, puntuación y excepciones\n",
    "tokenizer_complete = nlp.Defaults.create_tokenizer(nlp)\n",
    "print(\n",
    "    f'spacy tokenizer, V: { len({token.text for token in tokenizer_complete(quijote)})} '\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Python split, V: {len(set(quijote.lower().split()))}')\n",
    "print(\n",
    "    f'NLTK word_tokenize, V: {len(set(nltk.tokenize.word_tokenize(quijote.lower())))}'\n",
    ")\n",
    "print(\n",
    "    f'spacy tokenizer, V: {len({token.text for token in tokenizer_complete(quijote.lower())})} '\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf (term frequency - inverse document frequency)\n",
    "\n",
    "Una extensión sobre *bag of words* es la representación **tf-idf**. [Link](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). En este caso, también se cuenta la ocurrencia de cada token, pero se pondera para penalizar palabras que aparecen en casi todos los textos (como las *stop words*), por lo que no ayudan a diferenciarlos. Es muy popular para tareas de **information retrieval**.\n",
    "\n",
    "Si utilizamos una representación *bag of words* para crear un modelo, esos términos muy frecuentes pueden ensombrecer otros tokens cuyas frecuencias son mucho menores, pero que contienen más información. Una posible solución a este problema es darle un peso distinto a cada una de las dimensiones del vector. El esquema de ponderado más común es:\n",
    "\n",
    "* term frequency: $$\n",
    "\\operatorname{tf}(t, d) = \\frac{|\\{t \\in d\\}| }{|d|} \\;,\n",
    "$$\n",
    "que es el porcentaje de veces que aparece el token $t$ en el documento $d$. Da la importancia que tiene un término en un documento concreto.\n",
    "* inverse document frequency: \n",
    "$$\n",
    "\\operatorname{idf}(t, D)=\\log \\frac{|D|}{1+|\\{d \\in D: t \\in d\\}|} \\;,\n",
    "$$\n",
    "donde $|D|$ es el número de documentos, y $|\\{d \\in D: t \\in d\\}|$ es el número de documentos en los que aparece el término $t$, el factor 1 se añade para que no se divida entre 0. Quiza importancia a términos que están presentes en muchos documentos.\n",
    "\n",
    "Luego, cada término es ponderado por el siguiente factor:\n",
    "$$\n",
    "\\operatorname{tfidf}(t, d, D) = \\operatorname{tf}(t, d)\\times \\operatorname{idf}(t, D) \\; .\n",
    "$$\n",
    "\n",
    "En el caso de `sklearn`, por defecto normaliza los vectores resultante según la norma euclídea o L2 ([Más información](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)). La función que lo implementa es [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Preguntas:</b> \n",
    "    \n",
    "* ¿Qué distancia hay entre dos tokens en una representación *bag of words*? ¿Cambia esta distancia si los tokens tienen un significado más o menos cercano? ¿Cambia esto en la representación *tf-idf*?\n",
    "    MIO- Todo token tiene la misma distancia, aunque esten relacionados. No afecta a tf-idf.\n",
    "    \n",
    "* ¿En una tarea de clasificación, si un término aparece en todos los casos de una clase, cómo afecta al su valor de $\\operatorname{tfidf}(t, d, D)$? ¿Es este término una feature importante para clasificar?\n",
    "* Una vez contruido un vocabulario, ¿qué pasará si en el conjunto de validación, test o en producción aparecen palabras nuevas?\n",
    "    MIO - No las trata.\n",
    "* ¿Cómo actúan las herramientas aprendidas ante las erratas? Analiza el caso de que existan erratas tanto durante el entrenamiento como en producción? Por ejemplo: 'ayuntamiento' vs 'ayuntameinto'.\n",
    "* En el lenguaje, el orden de los factores altera el producto. 'Juan mordió a el perro' vs 'El perro mordió a Juan'. 'Si no viene a comer lo dejamos', 'No viene a comer si lo dejamos'. ¿Cómo actúa *bag of words* y *tf-idf* ante este fenómeno?\n",
    "    MIO\n",
    "    - Si solo va a nivel de 1 grama no tiene efecto ya que no se tiene en cuenta el orden.\n",
    "    - Si van a nivel de más de 1 grama ya si serían distintos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = [1,0,0,0]\n",
    "\n",
    "t2 = [0,1,0,0]\n",
    "\n",
    "t3 = [0,0,1,0]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245.76px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
